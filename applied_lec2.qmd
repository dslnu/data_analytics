---
title: "Intro to Docker"
author: 
  - name: MSDE
    affiliation: Lviv University
code-fold: false
execute:
  enabled: false
  cache: true
diagram:
  cache: true
  cache-dir: ./cache
  engine:
    tikz:
      execpath: lualatex
      additional-packages: |
        \usetikzlibrary{arrows.meta}
        \usetikzlibrary{positioning}
        \usetikzlibrary{decorations.pathreplacing}
filters:
  - diagram
title-slide-attributes:
  data-background-image: img/docker_ship.jpg
  data-background-size: contain
  data-background-opacity: "0.5"
format: 
  revealjs:
    css: ./custom.css
    preview-links: auto
    slide-number: true
    theme: default
    multiplex:
      url: 'https://mplex.vitv.ly'
      secret: 'd0a83ce05978b2d395f1f3aeb2108909'
      id: '3a9cb0ec03e3c47cbf0f2cbcf9d330b0bdf4544a56c6a0dd3625969b8a9b70fa'
---

# Dockerfile

## Dockerfile
:::{.callout-note icon=false}
## What is it?

An **automated** way to construct images, as opposed to **manually** executing commands in a shell.

- A Dockerfile is a build recipe for a Docker image.
- It contains a series of instructions telling Docker how an image is constructed.
- The docker build command builds an image from a Dockerfile.
:::

## Dockerfile
:::{.callout-note icon=false}
## Creating a Dockerfile

Our Dockerfile must be in a new, empty directory.

Create a directory to hold our Dockerfile.
```
$ mkdir myimage
```
Create a Dockerfile inside this directory.
```
$ cd myimage
$ nvim Dockerfile
```
You can use whatever editor you like.
:::

## Dockerfile
:::{.callout-note icon=false}
## Contents
```
FROM ubuntu
RUN apt-get update
RUN apt-get install figlet
```
- [FROM]{style="color:blue;"} indicates the base image for our build.
- Each [RUN]{style="color:blue;"} line will be executed by Docker during the build.
- Our [RUN]{style="color:blue;"} commands must be non-interactive.
(No input can be provided to Docker during the build.)

In many cases, we will add the `-y` flag to `apt-get`.
:::

## Dockerfile
:::{.callout-note icon=false}
## Building

Save our file, then execute:
```
$ docker build -t figlet .
```

- `-t` indicates the tag to apply to the image.
- `.` indicates the location of the build context.
We will talk more about the build context later.
To keep things simple for now: this is the directory where our `Dockerfile` is located.
:::

## Dockerfile
:::{.callout-note icon=false}
## Full log
```
[+] Building 9.3s (8/8) FINISHED                                                                                                                                                                docker:desktop-linux
 => [internal] load build definition from Dockerfile                                                                                                                                                            0.0s
 => => transferring dockerfile: 95B                                                                                                                                                                             0.0s
 => [internal] load metadata for docker.io/library/ubuntu:latest                                                                                                                                                0.1s
 => [internal] load .dockerignore                                                                                                                                                                               0.0s
 => => transferring context: 2B                                                                                                                                                                                 0.0s
 => [1/3] FROM docker.io/library/ubuntu:latest@sha256:353675e2a41babd526e2b837d7ec780c2a05bca0164f7ea5dbbd433d21d166fc                                                                                          3.5s
 => => resolve docker.io/library/ubuntu:latest@sha256:353675e2a41babd526e2b837d7ec780c2a05bca0164f7ea5dbbd433d21d166fc                                                                                          3.4s
 => [auth] library/ubuntu:pull token for registry-1.docker.io                                                                                                                                                   0.0s
 => [2/3] RUN apt-get update                                                                                                                                                                                    2.9s
 => [3/3] RUN apt-get install figlet                                                                                                                                                                            1.3s
 => exporting to image                                                                                                                                                                                          1.6s
 => => exporting layers                                                                                                                                                                                         1.3s
 => => exporting manifest sha256:0d82650ef2fb2b1107ee3332b4e9167a3da20e4368b8a74764827a3091819ec9                                                                                                               0.0s
 => => exporting config sha256:474cb85cf40523c5ac0e1d337e8d97e2d3f9a3e24407a8680fd033d1820e9644                                                                                                                 0.0s
 => => exporting attestation manifest sha256:8c2c8be4e7e3253c54cdc1b68d50dd79dad45c73625a3517c7e79ec4ae2220ea                                                                                                   0.0s
 => => exporting manifest list sha256:6447daabcad8e301a8e7920b201c41f4ffc9889445c10d06050c085fd73ab8ea                                                                                                          0.0s
 => => naming to docker.io/library/figlet:latest                                                                                                                                                                0.0s
 => => unpacking to docker.io/library/figlet:latest
```
:::

## Dockerfile
:::{.callout-note icon=false}
## Steps

- BuildKit transfers the Dockerfile and the build context
(these are the first two [internal] stages)

- Then it executes the steps defined in the Dockerfile
([1/3], [2/3], [3/3])

- Finally, it exports the result of the build
(image definition + collection of layers)
:::

::: aside
In as CI, the output will be different. Revert to old output with `--progress=plain`.
:::

## Dockerfile
:::{.callout-note icon=false}
## Caching system


- After each build step, Docker takes a snapshot of the resulting image.
- Before executing a step, Docker checks if it has already built the same sequence.
- Docker uses the exact strings defined in your Dockerfile, so:
  - `RUN apt-get install figlet cowsay`
  - is different from
  - `RUN apt-get install cowsay figlet`

`RUN apt-get update` is not re-executed when the mirrors are updated

You can force a rebuild with `docker build --no-cache ....`
:::

## Dockerfile
:::{.callout-note icon=false}
## Result
Identical to manual:
```
docker run -it figlet
root@25ac5862b142:/# figlet hey
 _
| |__   ___ _   _
| '_ \ / _ \ | | |
| | | |  __/ |_| |
|_| |_|\___|\__, |
            |___/
root@25ac5862b142:/#
```
:::
## Dockerfile
:::{.callout-note icon=false}
## Image history

- The history command lists all the layers composing an image.
- For each layer, it shows its creation time, size, and creation command.
- When an image was built with a Dockerfile, each layer corresponds to a line of the Dockerfile.
```
docker history figlet
IMAGE          CREATED        CREATED BY                                      SIZE      COMMENT
6447daabcad8   19 hours ago   RUN /bin/sh -c apt-get install figlet # buil…   1.34MB    buildkit.dockerfile.v0
<missing>      19 hours ago   RUN /bin/sh -c apt-get update # buildkit        56.4MB    buildkit.dockerfile.v0
<missing>      12 days ago    /bin/sh -c #(nop)  CMD ["/bin/bash"]            0B
<missing>      12 days ago    /bin/sh -c #(nop) ADD file:4e55519deacaaab35…   110MB
<missing>      12 days ago    /bin/sh -c #(nop)  LABEL org.opencontainers.…   0B
<missing>      12 days ago    /bin/sh -c #(nop)  LABEL org.opencontainers.…   0B
<missing>      12 days ago    /bin/sh -c #(nop)  ARG LAUNCHPAD_BUILD_ARCH     0B
<missing>      12 days ago    /bin/sh -c #(nop)  ARG RELEASE                  0B
```
:::

## Dockerfile
:::{.callout-note}
## Why sh -c?

- On UNIX, to start a new program, we need two system calls:
  - fork(), to create a new child process;
  - execve(), to replace the new child process with the program to run.
- Conceptually, execve() works like this:
  - `execve(program, [list, of, arguments])`
- When we run a command, e.g. `ls -l /tmp`, something needs to parse the command.
(i.e. split the program and its arguments into a list.)
- The shell is usually doing that.
(It also takes care of expanding environment variables and special things like `~`.)
:::

## Dockerfile
:::{.callout-note icon=false}
## Exec syntax
Docker can parse the command by itself.

Instead of plain string, or **shell syntax**:
```
RUN apt-get install figlet
```
we can use JSON list, or **exec syntax**:
```
RUN ["apt-get", "install", "figlet"]
```
:::

## Dockerfile
:::{.callout-note icon=false}
## Check exec syntax

1. Change Dockerfile to
```
FROM ubuntu
RUN apt-get update
RUN ["apt-get", "install", "figlet"]
```

2. Build it:
```
docker build -t figlet .
```

3. Check history:
```
$ docker history figlet
IMAGE          CREATED         CREATED BY                                      SIZE      COMMENT
18c1be63d556   4 seconds ago   RUN apt-get install figlet # buildkit           1.34MB    buildkit.dockerfile.v0
<missing>      19 hours ago    RUN /bin/sh -c apt-get update # buildkit        56.4MB    buildkit.dockerfile.v0
...
```
Exact command!
:::

# CMD and ENTRYPOINT

## CMD and ENTRYPOINT
:::{.callout-note icon=false}
## Default commands

When people run our container, we want to greet them with a nice hello message, and using a custom font.

For that, we will execute:
```
figlet -f script hello
```

`-f script` tells figlet to use a fancy font.

`hello` is the message that we want it to display.
:::
## CMD and ENTRYPOINT
:::{.callout-note icon=false}
## Adding CMD to Dockerfile

Our new Dockerfile will look like this:
```
FROM ubuntu
RUN apt-get update
RUN ["apt-get", "install", "figlet"]
CMD figlet -f script hello
CMD defines a default command to run when none is given.
```

It can appear at any point in the file.

Each CMD will replace and override the previous one.
As a result, while you can have multiple CMD lines, it is useless.
:::

## CMD and ENTRYPOINT
:::{.callout-note icon=false}
## Build and run
```
$ docker build -t figlet .
[+] Building 3.4s (8/8) FINISHED                                                                                                                                                                docker:desktop-linux
...
$ docker run -it figlet
 _          _   _
| |        | | | |
| |     _  | | | |  __
|/ \   |/  |/  |/  /  \_
|   |_/|__/|__/|__/\__/

```
:::

## CMD and ENTRYPOINT
:::{.callout-note icon=false}
## CMD override

If we want to get a shell into our container (instead of running figlet), we just have to specify a different program to run:
```
$ docker run -it figlet bash
root@3e95f6bafdd9:/#
```
We specified bash.

It replaced the value of CMD.
:::
## CMD and ENTRYPOINT
:::{.callout-note icon=false}
## Using ENTRYPOINT

**Objective:** we want to be able to specify a different message on the command line, while retaining figlet and some default parameters.

In other words, we would like to be able to do this:
```
$ docker run figlet salut
           _            
          | |           
 ,   __,  | |       _|_ 
/ \_/  |  |/  |   |  |  
 \/ \_/|_/|__/ \_/|_/|_/
```

We will use the ENTRYPOINT verb in Dockerfile.
:::
## CMD and ENTRYPOINT
:::{.callout-note icon=false}
## Dockerfile with ENTRYPOINT

Our new Dockerfile will look like this:
```
FROM ubuntu
RUN apt-get update
RUN ["apt-get", "install", "figlet"]
ENTRYPOINT ["figlet", "-f", "script"]
```

- ENTRYPOINT defines a base command (and its parameters) for the container.
- The command line arguments are appended to those parameters.
- Like CMD, ENTRYPOINT can appear anywhere, and replaces the previous value.
:::
## CMD and ENTRYPOINT
:::{.callout-note icon=false}
## Build
```
docker build -t figlet .
[+] Building 0.1s (7/7) FINISHED                                                                                                                                                                docker:desktop-linux
...
$ docker run figlet salve
           _
          | |
 ,   __,  | |       _
/ \_/  |  |/  |  |_|/
 \/ \_/|_/|__/ \/  |__/


```
:::
## CMD and ENTRYPOINT
:::{.callout-note icon=false}
## Using CMD and ENTRYPOING together

If we use ENTRYPOINT and CMD together:

- ENTRYPOINT will define the base command for our container.
- CMD will define the default parameter(s) for this command.
- They both have to use JSON syntax.
:::

## CMD and ENTRYPOINT
:::{.callout-note icon=false}
## Dockerfile

```
FROM ubuntu
RUN apt-get update
RUN ["apt-get", "install", "figlet"]
ENTRYPOINT ["figlet", "-f", "script"]
CMD ["hello world"]
```
- `ENTRYPOINT` defines a base command (and its parameters) for the container.
- If we don't specify extra command-line arguments when starting the container, the value of `CMD` is appended.
- Otherwise, our extra command-line arguments are used instead of `CMD.`
:::
## CMD and ENTRYPOINT
:::{.callout-note icon=false}
## Build and run

1. Build:
```
$ docker build -t myfiglet .
[+] Building 0.1s (7/7) FINISHED
...
```

2. Run without parameters:
```
$ docker run myfiglet
 _          _   _                             _
| |        | | | |                           | |    |
| |     _  | | | |  __             __   ,_   | |  __|
|/ \   |/  |/  |/  /  \_  |  |  |_/  \_/  |  |/  /  |
|   |_/|__/|__/|__/\__/    \/ \/  \__/    |_/|__/\_/|_/



```
:::

## CMD and ENTRYPOINT
:::{.callout-note icon=false}
## Build and run

3. Run with parameters:
```
$ docker run myfiglet hey
 _
| |
| |     _
|/ \   |/  |   |
|   |_/|__/ \_/|/
              /|
              \|

```
:::

## CMD and ENTRYPOINT
:::{.callout-note icon=false}
## ENTRYPOINT override

What if we want to run a shell in our container?

We cannot just do `docker run myfiglet bash` because that would just tell figlet to display the word "bash."

We use the `--entrypoint` parameter:
```
$ docker run -it --entrypoint bash myfiglet
root@0e2f53d52f7d:/#

```
:::
## CMD and ENTRYPOINT
:::{.callout-note icon=false}
## CMD and ENTRYPOINT recap

- `docker run myimage` executes `ENTRYPOINT` + `CMD`
- `docker run myimage args` executes `ENTRYPOINT` + `args` (overriding `CMD`)
- `docker run --entrypoint prog myimage` executes `prog` (overriding both)
:::


## CMD and ENTRYPOINT

| Command                         | `ENTRYPOINT`       | `CMD`   | Result
|---------------------------------|--------------------|---------|-------
| `docker run figlet`             | none               | none    | Use values from base image (`bash`)
| `docker run figlet hola`        | none               | none    | Error (executable `hola` not found)
| `docker run figlet`             | `figlet -f script` | none    | `figlet -f script`
| `docker run figlet hola`        | `figlet -f script` | none    | `figlet -f script hola`
| `docker run figlet`             | none    | `figlet -f script` | `figlet -f script`
| `docker run figlet hola`        | none    | `figlet -f script` | Error (executable `hola` not found)
| `docker run figlet`             | `figlet -f script` | `hello` | `figlet -f script hello`
| `docker run figlet hola`        | `figlet -f script` | `hello` | `figlet -f script hola`

# Copying files during the build

## Copying files during the build
:::{.callout-tip icon=false}
## Objectives

- So far, we have installed things in our container images by **downloading packages**.
- We can also copy files from the **build context** to the container that we are building.
- The build context is the directory containing the Dockerfile.
- for that we use  a new Dockerfile keyword: `COPY`.
:::

## Copying files during the build
:::{.callout-tip icon=false}
## Building C code

We want to build a container that compiles a basic "Hello world" program in C.

Here is the program, `hello.c`:
```c
int main () {
  puts("Hello, world!");
  return 0;
}
```
Let's create a new directory, and put this file in there.

Then we will write the Dockerfile.
:::

## Copying files during the build
:::{.callout-tip icon=false}
## Dockerfile for building

On Debian and Ubuntu, the package `build-essential` will get us a compiler.

When installing it, don't forget to specify the `-y` flag, otherwise the build will fail (since the build cannot be interactive).

Then we will use COPY to place the source file into the container.

```dockerfile
FROM ubuntu
RUN ["apt-get", "update"]
RUN ["apt-get", "install", "-y", "build-essential"]
COPY hello.c /
RUN make hello
CMD /hello
```

Create this Dockerfile.
:::

## Copying files during the build
:::{.callout-tip icon=false}
## Testing our C program

- Create hello.c and Dockerfile in the same directory.
- Run `docker build -t hello .` in this directory.
- Run `docker run hello`, you should see `Hello, world!`.
:::

## Copying files during the build
:::{.callout-tip icon=false}
## COPY and the build cache

- Run the build again.
- Now, modify `hello.c` and run the build again.
- Docker can cache steps involving `COPY`.
- Those steps will not be executed again if the files haven't been changed.
:::

## Copying files during the build
:::{.callout-tip icon=false}
## Details

- We can COPY whole directories recursively
It is possible to do e.g. COPY . .
(but it might require some extra precautions to avoid copying too much)

- In older Dockerfiles, you might see the ADD command; consider it deprecated
(it is similar to COPY but can automatically extract archives)

- If we really wanted to compile C code in a container, we would:
  - place it in a different directory, with the WORKDIR instruction
  - even better, use the gcc official image
:::

## Copying files during the build
:::{.callout-tip icon=false}
## .dockerignore

- We can create a file named .dockerignore
(at the top-level of the build context)
- It can contain file names and globs to ignore
- They won't be sent to the builder
(and won't end up in the resulting image)
- See the documentation for the little details
(exceptions can be made with !, multiple directory levels with **...)
:::

# Exercise — writing Dockerfiles

## Exercise — writing Dockerfiles
:::{.callout-tip icon=false}
## Exercise — writing Dockerfiles

- Let's write Dockerfiles for an existing application!
- Check out the code repository
- Read all the instructions
- Write Dockerfiles
- Build and test them individually
:::

## Exercise — writing Dockerfiles
:::{.callout-tip icon=false}
## Code repository

- Clone the repository available at <https://github.com/jpetazzo/wordsmith>
It should look like this:
```
├── LICENSE
├── README
├── db/
│   └── words.sql
├── web/
│   ├── dispatcher.go
│   └── static/
└── words/
    ├── pom.xml
    └── src/
```
:::

## Exercise — writing Dockerfiles
:::{.callout-tip icon=false}
## Instructions

The repository contains instructions in English and French.
<br/>
For now, we only care about the first part (about writing Dockerfiles).
<br/>
Place each Dockerfile in its own directory, like this:
```
├── LICENSE
├── README
├── db/
│   ├── Dockerfile
│   └── words.sql
├── web/
│   ├── Dockerfile
│   ├── dispatcher.go
│   └── static/
└── words/
    ├── Dockerfile
    ├── pom.xml
    └── src/
```
:::

## Exercise — writing Dockerfiles
:::{.callout-tip icon=false}
## Build and test

Build and run each Dockerfile individually.

For `db`, we should be able to see some messages confirming that the data set
was loaded successfully (some `INSERT` lines in the container output).

For `web` and `words`, we should be able to see some message looking like
"server started successfully".

That's all we care about for now!

Bonus question: make sure that each container stops correctly when hitting Ctrl-C.

:::


# Reducing image size

## Reducing image size

:::{.callout-important icon=false}
## Size consideration

In the previous example, our final image contained:

- our hello program
- its source code
- the compiler

Only the first one is strictly necessary.

We are going to see how to obtain an image without the superfluous components.
:::

## Reducing image size
:::{.callout-note icon=false}
## Can't we remove superfluous files with `RUN`?

What happens if we do one of the following commands?

- `RUN rm -rf ...`

- `RUN apt-get remove ...`

- `RUN make clean ...`


This adds a layer which removes a bunch of files.
But the previous layers (which added the files) still exist.
:::

## Reducing image size
:::{.callout-note icon=false}
## Removing files with an extra layer

When downloading an image, all the layers must be downloaded.

| Dockerfile instruction | Layer size | Image size |
| ---------------------- | ---------- | ---------- |
| `FROM ubuntu` | Size of base image | Size of base image |
| `...` | ... | Sum of this layer <br/>+ all previous ones |
| `RUN apt-get install somepackage` | Size of files added <br/>(e.g. a few MB) | Sum of this layer <br/>+ all previous ones |
| `...` | ... | Sum of this layer <br/>+ all previous ones |
| `RUN apt-get remove somepackage` | Almost zero <br/>(just metadata) | Same as previous one |

Therefore, `RUN rm` does not reduce the size of the image or free up disk space.
:::

## Reducing image size
:::{.callout-note icon=false}
## Removing unnecessary files

Various techniques are available to obtain smaller images:

- collapsing layers,

- adding binaries that are built outside of the Dockerfile,

- squashing the final image,

- multi-stage builds.

Let's review them quickly.
:::

## Reducing image size
:::{.callout-note icon=false}
## Collapsing layers

You will frequently see Dockerfiles like this:

```dockerfile
FROM ubuntu
RUN apt-get update && apt-get install xxx && ... && apt-get remove xxx && ...
```

Or the (more readable) variant:

```dockerfile
FROM ubuntu
RUN apt-get update \
 && apt-get install xxx \
 && ... \
 && apt-get remove xxx \
 && ...
```

This `RUN` command gives us a single layer.

The files that are added, then removed in the same layer, do not grow the layer size.
:::

## Reducing image size
:::{.callout-note icon=false}
## Collapsing layers: pros and cons

:::: {.columns}

::: {.column width="50%" background-color="lightgray"}
:::{.callout-tip icon=false}
## Pros:

- works on all versions of Docker

- doesn't require extra tools
:::
:::

::: {.column width="50%" background-color="lightgray"}
:::{.callout-tip icon=false}
## Cons:

- not very readable

- some unnecessary files might still remain if the cleanup is not thorough

- that layer is expensive (slow to build)
:::
:::
::::

:::


## Reducing image size
:::{.callout-note icon=false}
## Building binaries outside of the Dockerfile

This results in a Dockerfile looking like this:

```dockerfile
FROM ubuntu
COPY xxx /usr/local/bin
```

Of course, this implies that the file `xxx` exists in the build context.

That file has to exist before you can run `docker build`.

For instance, it can:

- exist in the code repository,
- be created by another tool (script, Makefile...),
- be created by another container image and extracted from the image.

See for instance the [busybox official image](https://github.com/docker-library/busybox/blob/fe634680e32659aaf0ee0594805f74f332619a90/musl/Dockerfile) or this [older busybox image](https://github.com/jpetazzo/docker-busybox).
:::

## Building binaries outside: pros and cons

:::: {.columns}

::: {.column width="50%" background-color="lightgray"}
:::{.callout-tip icon=false}
## Pros:

- final image can be very small
:::
:::
::: {.column width="50%" background-color="lightgray"}
:::{.callout-tip icon=false}
## Cons:

- requires an extra build tool

- we're back in dependency hell and "works on my machine"

if binary is added to code repository:

- breaks portability across different platforms

- grows repository size a lot if the binary is updated frequently
:::
:::
::::

## Reducing image size
:::{.callout-note icon=false}
## Squashing the final image

The idea is to transform the final image into a single-layer image.

This can be done in (at least) two ways.

- Activate experimental features and squash the final image:
  ```bash
  docker image build --squash ...
  ```

- Export/import the final image.
  ```bash
  docker build -t temp-image .
  docker run --entrypoint true --name temp-container temp-image
  docker export temp-container | docker import - final-image
  docker rm temp-container
  docker rmi temp-image
  ```
:::

## Squashing the image: pros and cons

:::: {.columns}

::: {.column width="50%" background-color="lightgray"}
:::{.callout-tip icon=false}
## Pros:

- single-layer images are smaller and faster to download

- removed files no longer take up storage and network resources
:::
:::

::: {.column width="50%" background-color="lightgray"}
:::{.callout-tip icon=false}
## Cons:

- we still need to actively remove unnecessary files

- squash operation can take a lot of time (on big images)

- squash operation does not benefit from cache
  <br/>
  (even if we change just a tiny file, the whole image needs to be re-squashed)

:::
:::
::::

## Reducing image size
:::{.callout-note icon=false}
## Multi-stage builds

Multi-stage builds allow us to have multiple *stages*.

Each stage is a separate image, and can copy files from previous stages.

We're going to see how they work in more detail.
:::

# Multi-stage builds

## Multi-stage builds
:::{.callout-note icon=false}
## Description

* At any point in our `Dockerfile`, we can add a new `FROM` line.

* This line starts a new stage of our build.

* Each stage can access the files of the previous stages with `COPY --from=...`.

* When a build is tagged (with `docker build -t ...`), the last stage is tagged.

* Previous stages are not discarded: they will be used for caching, and can be referenced.
:::

## Multi-stage builds
:::{.callout-note icon=false}
## Multi-stage builds in practice

* Each stage is numbered, starting at `0`

* We can copy a file from a previous stage by indicating its number, e.g.:

  ```dockerfile
  COPY --from=0 /file/from/first/stage /location/in/current/stage
  ```

* We can also name stages, and reference these names:

  ```dockerfile
  FROM golang AS builder
  RUN ...
  FROM alpine
  COPY --from=builder /go/bin/mylittlebinary /usr/local/bin/
  ```
:::

## Multi-stage builds
:::{.callout-note icon=false}
## Multi-stage builds for our C program

We will change our Dockerfile to:

* give a nickname to the first stage: `compiler`

* add a second stage using the same `ubuntu` base image

* add the `hello` binary to the second stage

* make sure that `CMD` is in the second stage 

The resulting Dockerfile is on the next slide.
:::

## Multi-stage builds
:::{.callout-note icon=false}
## Multi-stage build `Dockerfile`

Here is the final Dockerfile:

```dockerfile
FROM ubuntu AS compiler
RUN ["apt-get", "update"]
RUN ["apt-get", "install", "-y", "build-essential"]
COPY hello.c /
RUN ["make", "hello"]

FROM ubuntu
COPY --from=compiler /hello /hello
CMD /hello
```

Let's build it, and check that it works correctly:

```bash
docker build -t hellomultistage .
docker run hellomultistage
```
:::

## Multi-stage builds
:::{.callout-note icon=false}
## Comparing single/multi-stage build image sizes

List our images with `docker images`, and check the size of:

- the `ubuntu` base image,

- the single-stage `hello` image,

- the multi-stage `hellomultistage` image.

We can achieve even smaller images if we use smaller base images.

However, if we use common base images (e.g. if we standardize on `ubuntu`),
these common images will be pulled only once per node, so they are
virtually "free."
:::

## Multi-stage builds
:::{.callout-note icon=false}
## Results
```bash
$ docker images | grep ubuntu
ubuntu                                         latest         353675e2a41b   2 weeks ago      139MB
$ docker images | grep hello
hellomultistage                                latest         977190f18730   55 seconds ago   139MB
hello                                          latest         09316393a5fe   30 minutes ago   707MB

```
:::

## Multi-stage builds
:::{.callout-note icon=false}
## Build targets

* We can also tag an intermediary stage with the following command:
  ```bash
  docker build --target STAGE --tag NAME
  ```

* This will create an image (named `NAME`) corresponding to stage `STAGE`

* This can be used to easily access an intermediary stage for inspection

  (instead of parsing the output of `docker build` to find out the image ID)

* This can also be used to describe multiple images from a single Dockerfile

  (instead of using multiple Dockerfiles, which could go out of sync)
:::


## Multi-stage builds
:::{.callout-note icon=false}
## Dealing with download caches

* In some cases, our images contain temporary downloaded files or caches

  (examples: packages downloaded by `pip`, Maven, etc.)

* These can sometimes be disabled

  (e.g. `pip install --no-cache-dir ...`)

* The cache can also be cleaned immediately after installing

  (e.g. `pip install ... && rm -rf ~/.cache/pip`)
:::


## Multi-stage builds
:::{.callout-note icon=false}
## Download caches and multi-stage builds

* Download+install packages in a build stage

* Copy the installed packages to a run stage

* Example: in the specific case of Python, use a virtual env

  (install in the virtual env; then copy the virtual env directory)
:::


## Multi-stage builds
:::{.callout-note icon=false}
## Download caches and BuildKit

* BuildKit has a caching feature for run stages

* It can address download caches elegantly

* Example:
  ```bash
  RUN --mount=type=cache,target=/pipcache pip install --cache-dir /pipcache ...
  ```

* The cache won't be in the final image, but it'll persist across builds
:::

# Publishing images to the Docker Hub

## Publishing images to the Docker Hub
:::{.callout-tip icon=false}
## Overview
We have built our first images.

We can now publish it to the Docker Hub!

:::

## Publishing images to the Docker Hub
:::{.callout-tip icon=false}
## Logging into our Docker Hub account

* This can be done from the Docker CLI:
  ```bash
  docker login
  ```

:::

## Publishing images to the Docker Hub
:::{.callout-tip icon=false}
## Image tags and registry addresses

* Docker images tags are like Git tags and branches.

* They are like *bookmarks* pointing at a specific image ID.

* Tagging an image doesn't *rename* an image: it adds another tag.

* When pushing an image to a registry, the registry address is in the tag.

  Example: `registry.example.net:5000/image`

* What about Docker Hub images?

:::

::: aside

* `jpetazzo/clock` is, in fact, `index.docker.io/jpetazzo/clock`

* `ubuntu` is, in fact, `library/ubuntu`, i.e. `index.docker.io/library/ubuntu`

:::

## Publishing images to the Docker Hub
:::{.callout-tip icon=false}
## Tagging an image to push it on the Hub

* Let's tag our `figlet` image (or any other to our liking):
  ```bash
  docker tag figlet jpetazzo/figlet
  ```

* And push it to the Hub:
  ```bash
  docker push jpetazzo/figlet
  ```

* That's it!
:::

::: aside
Anybody can now `docker run jpetazzo/figlet` anywhere.
:::

## Publishing images to the Docker Hub
:::{.callout-tip icon=false}
## The goodness of automated builds

* You can link a Docker Hub repository with a GitHub or BitBucket repository

* Each push to GitHub or BitBucket will trigger a build on Docker Hub

* If the build succeeds, the new image is available on Docker Hub

* You can map tags and branches between source and container images

* If you work with public repositories, this is free
:::


## Publishing images to the Docker Hub {.scrollable}
:::{.callout-tip icon=false}
## Setting up an automated build

* We need a Dockerized repository!
* Let's go to https://github.com/jpetazzo/trainingwheels and fork it.
* Go to the Docker Hub (https://hub.docker.com/) and sign-in. Select "Repositories" in the blue navigation menu.
* Select "Create" in the top-right bar, and select "Create Repository+".
* Connect your Docker Hub account to your GitHub account.
* Click "Create" button.
* Then go to "Builds" folder.
* Click on Github icon and select your user and the repository that we just forked.
* In "Build rules" block near page bottom, put `/www` in "Build Context" column (or whichever directory the Dockerfile is in).
* Click "Save and Build" to build the repository immediately (without waiting for a git push).
* Subsequent builds will happen automatically, thanks to **GitHub hooks**.

:::

## Publishing images to the Docker Hub
:::{.callout-tip icon=false}
## Building on the fly

- Some services can build images on the fly from a repository

- Example: [ctr.run](https://ctr.run/)

There might be a long pause before the first layer is pulled,
because the API behind `docker pull` doesn't allow to stream build logs, and there is no feedback during the build.

It is possible to view the build logs by setting up an account on [ctr.run](https://ctr.run/).
:::


# Tips for efficient Dockerfiles

## Tips for efficient Dockerfiles
:::{.callout-important icon=false}
## Overview
We will see how to:

* Reduce the number of layers.

* Leverage the build cache so that builds can be faster.

* Embed unit testing in the build process.

:::

## Tips for efficient Dockerfiles
:::{.callout-important icon=false}
## Reducing the number of layers

* Each line in a `Dockerfile` creates a new layer.

* Build your `Dockerfile` to take advantage of Docker's caching system.

* Combine commands by using `&&` to continue commands and `\` to wrap lines.

Note: it is frequent to build a Dockerfile line by line:

```dockerfile
RUN apt-get install thisthing
RUN apt-get install andthatthing andthatotherone
RUN apt-get install somemorestuff
```

And then refactor it trivially before shipping:

```dockerfile
RUN apt-get install thisthing andthatthing andthatotherone somemorestuff
```
:::

## Tips for efficient Dockerfiles
:::{.callout-important icon=false}
## Avoid re-installing dependencies at each build

* Classic Dockerfile problem:

>  "each time I change a line of code, all my dependencies are re-installed!"

* Solution: `COPY` dependency lists (`package.json`, `requirements.txt`, etc.)
  by themselves to avoid reinstalling unchanged dependencies every time.

:::

## Tips for efficient Dockerfiles
:::{.callout-important icon=false}
## Example "bad" `Dockerfile`

The dependencies are reinstalled every time, because the build system does not know if `requirements.txt` has been updated.

```bash
FROM python
WORKDIR /src
COPY . .
RUN pip install -qr requirements.txt
EXPOSE 5000
CMD ["python", "app.py"]
```
:::

## Tips for efficient Dockerfiles
:::{.callout-tip icon=false}
## Fixed `Dockerfile`

Adding the dependencies as a separate step means that Docker can cache more efficiently and only install them when `requirements.txt` changes.

```bash
FROM python
WORKDIR /src
COPY requirements.txt .
RUN pip install -qr requirements.txt
COPY . .
EXPOSE 5000
CMD ["python", "app.py"]
```

:::

## Tips for efficient Dockerfiles
:::{.callout-important icon=false}
## Be careful with `chown`, `chmod`, `mv`

* Layers cannot store efficiently changes in permissions or ownership.

* Layers cannot represent efficiently when a file is moved either.

* As a result, operations like `chown`, `chmod`, `mv` can be expensive.

* For instance, in the Dockerfile snippet below, each `RUN` line
  creates a layer with an entire copy of `some-file`.

  ```dockerfile
  COPY some-file .
  RUN chown www-data:www-data some-file
  RUN chmod 644 some-file
  RUN mv some-file /var/www
  ```

* How can we avoid that?
:::

## Tips for efficient Dockerfiles
:::{.callout-important icon=false}
## Put files on the right place

* Instead of using `mv`, directly put files at the right place.

* When extracting archives (tar, zip...), merge operations in a single layer.

  Example:

  ```dockerfile
    ...
    RUN wget http://.../foo.tar.gz \
     && tar -zxf foo.tar.gz \
     && mv foo/fooctl /usr/local/bin \
     && rm -rf foo foo.tar.gz
  ...
  ```

:::

## Tips for efficient Dockerfiles
:::{.callout-important icon=false}
## Use `COPY --chown`

* The Dockerfile instruction `COPY` can take a `--chown` parameter.

  Examples:

  ```dockerfile
  ...
  COPY --chown=1000 some-file .
  COPY --chown=1000:1000 some-file .
  COPY --chown=www-data:www-data some-file .
  ```

* The `--chown` flag can specify a user, or a user:group pair.

* The user and group can be specified as names or numbers.

* When using names, the names must exist in `/etc/passwd` or `/etc/group`.

  *(In the container, not on the host!)*

:::

## Tips for efficient Dockerfiles
:::{.callout-important icon=false}
## Set correct permissions locally

* Instead of using `chmod`, set the right file permissions locally.

* When files are copied with `COPY`, permissions are preserved.

:::

## Tips for efficient Dockerfiles
:::{.callout-important icon=false}
## Embedding unit tests in the build process

```dockerfile
FROM <baseimage>
RUN <install dependencies>
COPY <code>
RUN <build code>
RUN <install test dependencies>
COPY <test data sets and fixtures>
RUN <unit tests>
FROM <baseimage>
RUN <install dependencies>
COPY <code>
RUN <build code>
CMD, EXPOSE ...
```

* The build fails as soon as an instruction fails
* If `RUN <unit tests>` fails, the build doesn't produce an image
* If it succeeds, it produces a clean image (without test libraries and data)

:::

# Dockerfile examples


## Dockerfile examples
:::{.callout-tip icon=false}
## Overview
There are a number of tips, tricks, and techniques that we can use in Dockerfiles.

But sometimes, we have to use different (and even opposed) practices depending on:

- the complexity of our project,

- the programming language or framework that we are using,

- the stage of our project (early MVP vs. super-stable production),

- whether we're building a final image or a base for further images,

- etc.

We are going to show a few examples using very different techniques.

:::

## Dockerfile examples
:::{.callout-tip icon=false}
## When to optimize an image

When authoring official images, it is a good idea to reduce as much as possible:

- the number of layers,

- the size of the final image.

This is often done at the expense of build time and convenience for the image maintainer;
but when an image is downloaded millions of time, saving even a few seconds of pull time
can be worth it.

```dockerfile
RUN apt-get update && apt-get install -y libpng12-dev libjpeg-dev && rm -rf /var/lib/apt/lists/* \
	&& docker-php-ext-configure gd --with-png-dir=/usr --with-jpeg-dir=/usr \
	&& docker-php-ext-install gd
...
RUN curl -o wordpress.tar.gz -SL https://wordpress.org/wordpress-${WORDPRESS_UPSTREAM_VERSION}.tar.gz \
	&& echo "$WORDPRESS_SHA1 *wordpress.tar.gz" | sha1sum -c - \
	&& tar -xzf wordpress.tar.gz -C /usr/src/ \
	&& rm wordpress.tar.gz \
	&& chown -R www-data:www-data /usr/src/wordpress
```
:::

::: aside
(Source: [Wordpress official image](https://github.com/docker-library/wordpress/blob/618490d4bdff6c5774b84b717979bfe3d6ba8ad1/apache/Dockerfile))
:::


## Dockerfile examples
:::{.callout-tip icon=false}
## When to *not* optimize an image

Sometimes, it is better to prioritize *maintainer convenience*.

In particular, if:

- the image changes a lot,

- the image has very few users (e.g. only 1, the maintainer!),

- the image is built and run on the same machine,

- the image is built and run on machines with a very fast link ...

In these cases, just keep things simple!

(Next slide: a Dockerfile that can be used to preview a Jekyll / github pages site.)

:::

## Dockerfile examples
:::{.callout-tip icon=false}
## Dockerfile for Jekyll

```dockerfile
FROM debian:sid

RUN apt-get update -q
RUN apt-get install -yq build-essential make
RUN apt-get install -yq zlib1g-dev
RUN apt-get install -yq ruby ruby-dev
RUN apt-get install -yq python-pygments
RUN apt-get install -yq nodejs
RUN apt-get install -yq cmake
RUN gem install --no-rdoc --no-ri github-pages

COPY . /blog
WORKDIR /blog

VOLUME /blog/_site

EXPOSE 4000
CMD ["jekyll", "serve", "--host", "0.0.0.0", "--incremental"]
```

:::


## Dockerfile examples
:::{.callout-tip icon=false}
## Multi-dimensional versioning systems

Images can have a tag, indicating the version of the image.

But sometimes, there are multiple important components, and we need to indicate the versions
for all of them.

This can be done with environment variables:

```dockerfile
ENV PIP=9.0.3 \
    ZC_BUILDOUT=2.11.2 \
    SETUPTOOLS=38.7.0 \
    PLONE_MAJOR=5.1 \
    PLONE_VERSION=5.1.0 \
    PLONE_MD5=76dc6cfc1c749d763c32fff3a9870d8d
```

:::

::: aside
(Source: [Plone official image](https://github.com/plone/plone.docker/blob/master/5.1/5.1.0/alpine/Dockerfile))

:::

## Dockerfile examples
:::{.callout-tip icon=false}
## Entrypoints and wrappers

It is very common to define a custom entrypoint.

That entrypoint will generally be a script, performing any combination of:

- pre-flights checks (if a required dependency is not available, display
  a nice error message early instead of an obscure one in a deep log file),

- generation or validation of configuration files,

- dropping privileges (with e.g. `su` or `gosu`, sometimes combined with `chown`),

- and more.

:::

## Dockerfile examples
:::{.callout-tip icon=false}
## A typical entrypoint script

```dockerfile
 #!/bin/sh
 set -e
 
 # first arg is '-f' or '--some-option'
 # or first arg is 'something.conf'
 if [ "${1#-}" != "$1" ] || [ "${1%.conf}" != "$1" ]; then
 	set -- redis-server "$@"
 fi
 
 # allow the container to be started with '--user'
 if [ "$1" = 'redis-server' -a "$(id -u)" = '0' ]; then
 	chown -R redis .
 	exec su-exec redis "$0" "$@"
 fi
 
 exec "$@"
```

:::

::: aside
(Source: [Redis official image](https://github.com/docker-library/redis/blob/d24f2be82673ccef6957210cc985e392ebdc65e4/4.0/alpine/docker-entrypoint.sh))

:::

## Dockerfile examples
:::{.callout-tip icon=false}
## Factoring information

To facilitate maintenance (and avoid human errors), avoid to repeat information like:

- version numbers,

- remote asset URLs (e.g. source tarballs) ...

Instead, use environment variables.

```dockerfile
ENV NODE_VERSION 10.2.1
...
RUN ...
    && curl -fsSLO --compressed "https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION.tar.xz" \
    && curl -fsSLO --compressed "https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc" \
    && gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \
    && grep " node-v$NODE_VERSION.tar.xz\$" SHASUMS256.txt | sha256sum -c - \
    && tar -xf "node-v$NODE_VERSION.tar.xz" \
    && cd "node-v$NODE_VERSION" \
...
```
:::

::: aside
(Source: [Nodejs official image](https://github.com/nodejs/docker-node/blob/master/10/alpine/Dockerfile))
:::

## Dockerfile examples
:::{.callout-tip icon=false}
## Overrides

In theory, development and production images should be the same.

In practice, we often need to enable specific behaviors in development (e.g. debug statements).

One way to reconcile both needs is to use Compose to enable these behaviors.

Let's look at the [trainingwheels](https://github.com/jpetazzo/trainingwheels) demo app for an example.

:::

## Dockerfile examples
:::{.callout-tip icon=false}
## Production image

This Dockerfile builds an image leveraging gunicorn:

```dockerfile
FROM python
RUN pip install flask
RUN pip install gunicorn
RUN pip install redis
COPY . /src
WORKDIR /src
CMD gunicorn --bind 0.0.0.0:5000 --workers 10 counter:app
EXPOSE 5000
```

:::

::: aside
(Source: [trainingwheels Dockerfile](https://github.com/jpetazzo/trainingwheels/blob/master/www/Dockerfile))
:::

## Dockerfile examples
:::{.callout-tip icon=false}
## Development Compose file

This Compose file uses the same image, but with a few overrides for development:

- the Flask development server is used (overriding `CMD`),

- the `DEBUG` environment variable is set,

- a volume is used to provide a faster local development workflow.

```yaml
services:
  www:
    build: www
    ports:
      - 8000:5000
    user: nobody
    environment:
      DEBUG: 1
    command: python counter.py
    volumes:
      - ./www:/src
```
:::

::: aside
(Source: [trainingwheels Compose file](https://github.com/jpetazzo/trainingwheels/blob/master/docker-compose.yml))
:::

## Dockerfile examples

:::{.callout-tip icon=false}
## How to know which best practices are better?

- The main goal of containers is to make our lives easier.

- In this chapter, we showed many ways to write Dockerfiles.

- These Dockerfiles use sometimes diametrically opposed techniques.

- Yet, they were the "right" ones *for a specific situation.*

- It's OK (and even encouraged) to start simple and evolve as needed.
:::

## Exercise — multi-stage builds

:::{.callout-note}
## Exercise
Let's update our Dockerfiles to leverage multi-stage builds!

The code is at: <https://github.com/jpetazzo/wordsmith>.

Use a different tag for these images, so that we can compare their sizes.

What's the size difference between single-stage and multi-stage builds?
:::

# Naming and inspecting containers

## Naming and inspecting containers
:::{.callout-tip icon=false}
## Objectives

In this lesson, we will learn about an important
Docker concept: container *naming*.

Naming allows us to:

* Reference easily a container.

* Ensure unicity of a specific container.

We will also see the `inspect` command, which gives a lot of details about a container.

:::

## Naming and inspecting containers
:::{.callout-tip icon=false}
## Naming our containers

So far, we have referenced containers with their ID.

We have copy-pasted the ID, or used a shortened prefix.

But each container can also be referenced by its name.

If a container is named `thumbnail-worker`, I can do:

```bash
$ docker logs thumbnail-worker
$ docker stop thumbnail-worker
etc.
```

:::

## Naming and inspecting containers
:::{.callout-tip icon=false}
## Default names

When we create a container, if we don't give a specific
name, Docker will pick one for us.

It will be the concatenation of:

* A mood (furious, goofy, suspicious, boring...)

* The name of a famous inventor (tesla, darwin, wozniak...)

Examples: `happy_curie`, `clever_hopper`, `jovial_lovelace` ...

:::

## Naming and inspecting containers
:::{.callout-tip icon=false}
## Specifying a name

You can set the name of the container when you create it.

```bash
$ docker run --name ticktock jpetazzo/clock
```

If you specify a name that already exists, Docker will refuse
to create the container.

This lets us enforce unicity of a given resource.

:::

## Naming and inspecting containers
:::{.callout-tip icon=false}
## Renaming containers

* You can rename containers with `docker rename`.

* This allows you to "free up" a name without destroying the associated container.

:::

## Naming and inspecting containers
:::{.callout-tip icon=false}
## Inspecting a container

The `docker inspect` command will output a very detailed JSON map.

```bash
$ docker inspect <containerID>
[{
...
(many pages of JSON here)
...
```

There are multiple ways to consume that information.

:::

## Naming and inspecting containers
:::{.callout-tip icon=false}
## Parsing JSON with the Shell

* You *could* grep and cut or awk the output of `docker inspect`.

* Please, don't.

* It's painful.

* If you really must parse JSON from the Shell, use JQ! (It's great.)

```bash
$ docker inspect <containerID> | jq .
```

* We will see a better solution which doesn't require extra tools.

:::

## Naming and inspecting containers
:::{.callout-tip icon=false}
## Using `--format`

You can specify a format string, which will be parsed by 
Go's text/template package.

```bash
$ docker inspect --format '{{ json .Created }}' <containerID>
"2015-02-24T07:21:11.712240394Z"
```

* The generic syntax is to wrap the expression with double curly braces.

* The expression starts with a dot representing the JSON object.

* Then each field or member can be accessed in dotted notation syntax.

* The optional `json` keyword asks for valid JSON output.
  <br/>(e.g. here it adds the surrounding double-quotes.)

:::

# Labels

## Labels
:::{.callout-tip icon=false}
## Overview
* Labels allow to attach arbitrary metadata to containers.

* Labels are key/value pairs.

* They are specified at container creation.

* You can query them with `docker inspect`.

* They can also be used as filters with some commands (e.g. `docker ps`).

:::

## Labels
:::{.callout-tip icon=false}
## Using labels

Let's create a few containers with a label `owner`.

```bash
docker run -d -l owner=alice nginx
docker run -d -l owner=bob nginx
docker run -d -l owner nginx
```

We didn't specify a value for the `owner` label in the last example.

This is equivalent to setting the value to be an empty string.

:::

## Labels
:::{.callout-tip icon=false}
## Querying labels

We can view the labels with `docker inspect`.

```bash
$ docker inspect $(docker ps -lq) | grep -A3 Labels
            "Labels": {
                "maintainer": "NGINX Docker Maintainers <docker-maint@nginx.com>",
                "owner": ""
            },
```

We can use the `--format` flag to list the value of a label.

```bash
$ docker inspect $(docker ps -q) --format 'OWNER={{.Config.Labels.owner}}'
```

:::

## Labels
:::{.callout-tip icon=false}
## Using labels to select containers

We can list containers having a specific label.

```bash
$ docker ps --filter label=owner
```

Or we can list containers having a specific label with a specific value.

```bash
$ docker ps --filter label=owner=alice
```

:::

## Labels
:::{.callout-tip icon=false}
## Use-cases for labels


* HTTP vhost of a web app or web service.

  (The label is used to generate the configuration for NGINX, HAProxy, etc.)

* Backup schedule for a stateful service.

  (The label is used by a cron job to determine if/when to backup container data.)

* Service ownership.

  (To determine internal cross-billing, or who to page in case of outage.)

* etc.
:::


# Getting inside a container

## Getting inside a container
:::{.callout-important icon=false}
## Objectives

On a traditional server or VM, we sometimes need to:

* log into the machine (with SSH or on the console),

* analyze the disks (by removing them or rebooting with a rescue system).

In this chapter, we will see how to do that with containers.

:::

## Getting inside a container
:::{.callout-important icon=false}
## Getting a shell

Every once in a while, we want to log into a machine.

In an perfect world, this shouldn't be necessary.

* You need to install or update packages (and their configuration)?

  Use configuration management. (e.g. Ansible, Chef, Puppet, Salt...)

* You need to view logs and metrics?

  Collect and access them through a centralized platform.

In the real world, though ... we often need shell access!

:::

## Getting inside a container
:::{.callout-important icon=false}
## Not getting a shell

Even without a perfect deployment system, we can do many operations without getting a shell.

* Installing packages can (and should) be done in the container image.

* Configuration can be done at the image level, or when the container starts.

* Dynamic configuration can be stored in a volume (shared with another container).

* Logs written to stdout are automatically collected by the Docker Engine.

* Other logs can be written to a shared volume.

* Process information and metrics are visible from the host.

_Let's save logging, volumes ... for later, but let's have a look at process information!_

:::

## Getting inside a container
:::{.callout-important icon=false}
## Viewing container processes from the host

If you run Docker on Linux, container processes are visible on the host.

```bash
$ ps faux | less
```

* Scroll around the output of this command.

* You should see the `jpetazzo/clock` container.

* A containerized process is just like any other process on the host.

* We can use tools like `lsof`, `strace`, `gdb` ... To analyze them.

:::

## Getting inside a container
:::{.callout-important icon=false}
## What's the difference between a container process and a host process?

* Each process (containerized or not) belongs to *namespaces* and *cgroups*.

* The namespaces and cgroups determine what a process can "see" and "do".

* Analogy: each process (containerized or not) runs with a specific UID (user ID).

* UID=0 is root, and has elevated privileges. Other UIDs are normal users.

_We will give more details about namespaces and cgroups later._

:::

## Getting inside a container
:::{.callout-important icon=false}
## Getting a shell in a running container

* Sometimes, we need to get a shell anyway.

* We _could_ run some SSH server in the container ...

* But it is easier to use `docker exec`.

```bash
$ docker exec -ti ticktock sh
```

* This creates a new process (running `sh`) _inside_ the container.

* This can also be done "manually" with the tool `nsenter`.

:::

## Getting inside a container
:::{.callout-important icon=false}
## Caveats

* The tool that you want to run needs to exist in the container.

* Some tools (like `ip netns exec`) let you attach to _one_ namespace at a time.

  (This lets you e.g. setup network interfaces, even if you don't have `ifconfig` or `ip` in the container.)

* Most importantly: the container needs to be running.

* What if the container is stopped or crashed?

:::

## Getting inside a container
:::{.callout-important icon=false}
## Getting a shell in a stopped container

* A stopped container is only _storage_ (like a disk drive).

* We cannot SSH into a disk drive or USB stick!

* We need to connect the disk to a running machine.

* How does that translate into the container world?

:::

## Getting inside a container
:::{.callout-important icon=false}
## Analyzing a stopped container

As an exercise, we are going to try to find out what's wrong with `jpetazzo/crashtest`.

```bash
docker run jpetazzo/crashtest
```

The container starts, but then stops immediately, without any output.

What would MacGyver&trade; do?

First, let's check the status of that container.

```bash
docker ps -l
```

:::

## Getting inside a container
:::{.callout-important icon=false}
## Viewing filesystem changes

* We can use `docker diff` to see files that were added / changed / removed.

```bash
docker diff <container_id>
```

* The container ID was shown by `docker ps -l`.

* We can also see it with `docker ps -lq`.

* The output of `docker diff` shows some interesting log files!

:::

## Getting inside a container
:::{.callout-important icon=false}
## Accessing files

* We can extract files with `docker cp`.

```bash
docker cp <container_id>:/var/log/nginx/error.log .
```

* Then we can look at that log file.

```bash
cat error.log
```

(The directory `/run/nginx` doesn't exist.)

:::

## Getting inside a container
:::{.callout-important icon=false}
## Exploring a crashed container

* We can restart a container with `docker start` ...

* ... But it will probably crash again immediately!

* We cannot specify a different program to run with `docker start`

* But we can create a new image from the crashed container

```bash
docker commit <container_id> debugimage
```

* Then we can run a new container from that image, with a custom entrypoint

```bash
docker run -ti --entrypoint sh debugimage
```

:::

## Getting inside a container
:::{.callout-important icon=false}
## Obtaining a complete dump

* We can also dump the entire filesystem of a container.

* This is done with `docker export`.

* It generates a tar archive.

```bash
docker export <container_id> | tar tv
```

This will give a detailed listing of the content of the container.
:::


# Container networking basics

## Container networking basics
:::{.callout-tip icon=false}
## Objectives

We will now run network services (accepting requests) in containers.

At the end of this section, you will be able to:

* Run a network service in a container.

* Connect to that network service.

* Find a container's IP address.

:::

## Container networking basics
:::{.callout-tip icon=false}
## Running a very simple service

- We need something small, simple, easy to configure

  (or, even better, that doesn't require any configuration at all)

- Let's use the official NGINX image (named `nginx`)

- It runs a static web server listening on port 80

- It serves a default "Welcome to nginx!" page

:::

## Container networking basics
:::{.callout-tip icon=false}
## Running an NGINX server

```bash
$ docker run -d -P nginx
66b1ce719198711292c8f34f84a7b68c3876cf9f67015e752b94e189d35a204e
```

- Docker will automatically pull the `nginx` image from the Docker Hub

- `-d` / `--detach` tells Docker to run it in the background

- `P` / `--publish-all` tells Docker to publish all ports

  (publish = make them reachable from other computers)

- ...OK, how do we connect to our web server now?

:::

## Container networking basics
:::{.callout-tip icon=false}
## Finding our web server port

- First, we need to find the *port number* used by Docker

  (the NGINX container listens on port 80, but this port will be *mapped*)

- We can use `docker ps`:
  ```bash
  $ docker ps
  CONTAINER ID  IMAGE  ...  PORTS                  ...
  e40ffb406c9e  nginx  ...  0.0.0.0:`12345`->80/tcp  ...
  ```

- This means:

  *port 12345 on the Docker host is mapped to port 80 in the container*

- Now we need to connect to the Docker host!

:::

## Container networking basics
:::{.callout-tip icon=false}
## Finding the address of the Docker host

- When running Docker on your Linux workstation:

  *use `localhost`, or any IP address of your machine*

- When running Docker on a remote Linux server:

  *use any IP address of the remote machine*

- When running Docker Desktop on Mac or Windows:

  *use `localhost`*

- In other scenarios (`docker-machine`, local VM...):

  *use the IP address of the Docker VM*
  
:::

## Container networking basics
:::{.callout-tip icon=false}
## Connecting to our web server (GUI)

Point your browser to the IP address of your Docker host, on the port
shown by `docker ps` for container port 80.

![Screenshot](img/welcome-to-nginx.png)

:::

## Container networking basics
:::{.callout-tip icon=false}
## Connecting to our web server (CLI)

You can also use `curl` directly from the Docker host.

Make sure to use the right port number if it is different
from the example below:

```bash
$ curl localhost:12345
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
...
```

:::

## Container networking basics
:::{.callout-tip icon=false}
## How does Docker know which port to map?

* There is metadata in the image telling "this image has something on port 80".

* We can see that metadata with `docker inspect`:

```bash
$ docker inspect --format '{{.Config.ExposedPorts}}' nginx
map[80/tcp:{}]
```

* This metadata was set in the Dockerfile, with the `EXPOSE` keyword.

* We can see that with `docker history`:

```bash
$ docker history nginx
IMAGE               CREATED             CREATED BY
7f70b30f2cc6        11 days ago         /bin/sh -c #(nop)  CMD ["nginx" "-g" "…
<missing>           11 days ago         /bin/sh -c #(nop)  STOPSIGNAL [SIGTERM]
<missing>           11 days ago         /bin/sh -c #(nop)  EXPOSE 80/tcp
```

:::

## Container networking basics
:::{.callout-tip icon=false}
## Why can't we just connect to port 80?

- Our Docker host has only one port 80

- Therefore, we can only have one container at a time on port 80

- Therefore, if multiple containers want port 80, only one can get it

- By default, containers *do not* get "their" port number, but a random one

  (not "random" as "crypto random", but as "it depends on various factors")

- We'll see later how to force a port number (including port 80!)

:::

## Container networking basics
:::{.callout-tip icon=false}
## Using multiple IP addresses

*Hey, my network-fu is strong, and I have questions...*

- Can I publish one container on 127.0.0.2:80, and another on 127.0.0.3:80?

- My machine has multiple (public) IP addresses, let's say A.A.A.A and B.B.B.B.
  <br/>
  Can I have one container on A.A.A.A:80 and another on B.B.B.B:80?

- I have a whole IPV4 subnet, can I allocate it to my containers?

- What about IPV6?

You can do all these things when running Docker directly on Linux.

(On other platforms, *generally not*, but there are some exceptions.)

:::

## Container networking basics
:::{.callout-tip icon=false}
## Finding the web server port in a script

Parsing the output of `docker ps` would be painful.

There is a command to help us:

```bash
$ docker port <containerID> 80
0.0.0.0:12345
```

:::

## Container networking basics
:::{.callout-tip icon=false}
## Manual allocation of port numbers

If you want to set port numbers yourself, no problem:

```bash
$ docker run -d -p 80:80 nginx
$ docker run -d -p 8000:80 nginx
$ docker run -d -p 8080:80 -p 8888:80 nginx
```

* We are running three NGINX web servers.
* The first one is exposed on port 80.
* The second one is exposed on port 8000.
* The third one is exposed on ports 8080 and 8888.

Note: the convention is `port-on-host:port-on-container`.

:::

## Container networking basics
:::{.callout-tip icon=false}
## Plumbing containers into your infrastructure

There are many ways to integrate containers in your network.

* Start the container, letting Docker allocate a public port for it.
  <br/>Then retrieve that port number and feed it to your configuration.

* Pick a fixed port number in advance, when you generate your configuration.
  <br/>Then start your container by setting the port numbers manually.

* Use an orchestrator like Kubernetes or Swarm.
  <br/>The orchestrator will provide its own networking facilities.

Orchestrators typically provide mechanisms to enable direct container-to-container
communication across hosts, and publishing/load balancing for inbound traffic.

:::

## Container networking basics
:::{.callout-tip icon=false}
## Finding the container's IP address

We can use the `docker inspect` command to find the IP address of the
container.

```bash
$ docker inspect --format '{{ .NetworkSettings.IPAddress }}' <yourContainerID>
172.17.0.3
```

* `docker inspect` is an advanced command, that can retrieve a ton
  of information about our containers.

* Here, we provide it with a format string to extract exactly the
  private IP address of the container.

:::

## Container networking basics
:::{.callout-tip icon=false}
## Pinging our container

Let's try to ping our container *from another container.*

```bash
docker run alpine ping `<ipaddress>`
PING 172.17.0.X (172.17.0.X): 56 data bytes
64 bytes from 172.17.0.X: seq=0 ttl=64 time=0.106 ms
64 bytes from 172.17.0.X: seq=1 ttl=64 time=0.250 ms
64 bytes from 172.17.0.X: seq=2 ttl=64 time=0.188 ms
```

When running on Linux, we can even ping that IP address directly!

(And connect to a container's ports even if they aren't published.)

:::

## Container networking basics
:::{.callout-tip icon=false}
## How often do we use `-p` and `-P` ?

- When running a stack of containers, we will often use Compose

- Compose will take care of exposing containers

  (through a `ports:` section in the `docker-compose.yml` file)

- It is, however, fairly common to use `docker run -P` for a quick test

- Or `docker run -p ...` when an image doesn't `EXPOSE` a port correctly

:::

## Container networking basics
:::{.callout-tip icon=false}
## Section summary

We've learned how to:

* Expose a network port.

* Connect to an application running in a container.

* Find a container's IP address.

:::

# Container network drivers

## Container network drivers
:::{.callout-tip icon=false}
## Overview
The Docker Engine supports different network drivers.

The built-in drivers include:

* `bridge` (default)

* `null` (for the special network called `none`)

* `host` (for the special network called `host`)

* `container` (that one is a bit magic!)

The network is selected with `docker run --net ...`.

Each network is managed by a driver.

The different drivers are explained with more details on the following slides.

:::

## Container network drivers
:::{.callout-tip icon=false}
## The default bridge

* By default, the container gets a virtual `eth0` interface.
  <br/>(In addition to its own private `lo` loopback interface.)

* That interface is provided by a `veth` pair.

* It is connected to the Docker bridge.
  <br/>(Named `docker0` by default; configurable with `--bridge`.)

* Addresses are allocated on a private, internal subnet.
  <br/>(Docker uses 172.17.0.0/16 by default; configurable with `--bip`.)

* Outbound traffic goes through an iptables MASQUERADE rule.

* Inbound traffic goes through an iptables DNAT rule.

* The container can have its own routes, iptables rules, etc.

:::

## Container network drivers
:::{.callout-tip icon=false}
## The null driver

* Container is started with `docker run --net none ...`

* It only gets the `lo` loopback interface. No `eth0`.

* It can't send or receive network traffic.

* Useful for isolated/untrusted workloads.

:::

## Container network drivers
:::{.callout-tip icon=false}
## The host driver

* Container is started with `docker run --net host ...`

* It sees (and can access) the network interfaces of the host.

* It can bind any address, any port (for ill and for good).

* Network traffic doesn't have to go through NAT, bridge, or veth.

* Performance = native!

Use cases:

* Performance sensitive applications (VOIP, gaming, streaming...)

* Peer discovery (e.g. Erlang port mapper, Raft, Serf...)

:::

## Container network drivers
:::{.callout-tip icon=false}
## The container driver

* Container is started with `docker run --net container:id ...`

* It re-uses the network stack of another container.

* It shares with this other container the same interfaces, IP address(es), routes, iptables rules, etc.

* Those containers can communicate over their `lo` interface.
  <br/>(i.e. one can bind to 127.0.0.1 and the others can connect to it.)

:::


# The Container Network Model

## The Container Network Model
:::{.callout-tip icon=false}
## Objectives

We will learn about the CNM (Container Network Model).

At the end of this lesson, you will be able to:

* Create a private network for a group of containers.

* Use container naming to connect services together.

* Dynamically connect and disconnect containers to networks.

* Set the IP address of a container.

We will also explain the principle of overlay networks and network plugins.

:::

## The Container Network Model
:::{.callout-tip icon=false}
## The Container Network Model

Docker has "networks".

We can manage them with the `docker network` commands; for instance:

```bash
$ docker network ls
NETWORK ID          NAME                DRIVER
6bde79dfcf70        bridge              bridge
8d9c78725538        none                null
eb0eeab782f4        host                host
4c1ff84d6d3f        blog-dev            overlay
228a4355d548        blog-prod           overlay
```

New networks can be created (with `docker network create`).

(Note: networks `none` and `host` are special; let's set them aside for now.)

:::

## The Container Network Model
:::{.callout-tip icon=false}
## What's a network?

- Conceptually, a Docker "network" is a virtual switch

  (we can also think about it like a VLAN, or a WiFi SSID, for instance)

- By default, containers are connected to a single network

  (but they can be connected to zero, or many networks, even dynamically)

- Each network has its own subnet (IP address range)

- A network can be local (to a single Docker Engine) or global (span multiple hosts)

- Containers can have *network aliases* providing DNS-based service discovery

  (and each network has its own "domain", "zone", or "scope")

:::

## The Container Network Model
:::{.callout-tip icon=false}
## Service discovery

- A container can be given a network alias

  (e.g. with `docker run --net some-network --net-alias db ...`)

- The containers running in the same network can resolve that network alias

  (i.e. if they do a DNS lookup on `db`, it will give the container's address)

- We can have a different `db` container in each network

  (this avoids naming conflicts between different stacks)

- When we name a container, it automatically adds the name as a network alias

  (i.e. `docker run --name xyz ...` is like `docker run --net-alias xyz ...`

:::

## The Container Network Model
:::{.callout-tip icon=false}
## Network isolation

- Networks are isolated

- By default, containers in network A cannot reach those in network B

- A container connected to both networks A and B can act as a router or proxy

- Published ports are always reachable through the Docker host address

  (`docker run -P ...` makes a container port available to everyone)

:::

## The Container Network Model
:::{.callout-tip icon=false}
## How to use networks

- We typically create one network per "stack" or app that we deploy

- More complex apps or stacks might require multiple networks

  (e.g. `frontend`, `backend`, ...)

- Networks allow us to deploy multiple copies of the same stack

  (e.g. `prod`, `dev`, `pr-442`, ....)

- If we use Docker Compose, this is managed automatically for us

:::


## The Container Network Model
![Multiple containers on the default bridge network, on a Linux machine](img/docker-networking-default-bridge-linux.png)


## The Container Network Model

![Multiple containers in multiple bridge networks, on a Linux machine](img/docker-networking-networks-linux.png)


## The Container Network Model

![Multiple containers in multiple bridge networks, on a Mac/Windows machine](img/docker-networking-networks-macwin.png)



## The Container Network Model
:::{.callout-tip icon=false}
## CNM vs CNI

- CNM is the model used by Docker

- Kubernetes uses a different model, architectured around CNI

  (CNI is a kind of API between a container engine and *CNI plugins*)

- Docker model:

  - multiple isolated networks
  - per-network service discovery
  - network interconnection requires extra steps

- Kubernetes model:

  - single flat network
  - per-namespace service discovery
  - network isolation requires extra steps (Network Policies)

:::

## The Container Network Model
:::{.callout-tip icon=false}
## Creating a network

Let's create a network called `dev`.

```bash
$ docker network create dev
4c1ff84d6d3f1733d3e233ee039cac276f425a9d5228a4355d54878293a889ba
```

The network is now visible with the `network ls` command:

```bash
$ docker network ls
NETWORK ID          NAME                DRIVER
6bde79dfcf70        bridge              bridge
8d9c78725538        none                null
eb0eeab782f4        host                host
4c1ff84d6d3f        dev                 bridge
```

:::

## The Container Network Model
:::{.callout-tip icon=false}
## Placing containers on a network

We will create a *named* container on this network.

It will be reachable with its name, `es`.

```bash
$ docker run -d --name es --net dev elasticsearch:2
8abb80e229ce8926c7223beb69699f5f34d6f1d438bfc5682db893e798046863
```

:::

## The Container Network Model
:::{.callout-tip icon=false}
## Communication between containers

Now, create another container on this network.

```bash
$ docker run -ti --net dev alpine sh
root@0ecccdfa45ef:/#
```

From this new container, we can resolve and ping the other one, using its assigned name:

```bash
/ # ping es
PING es (172.18.0.2) 56(84) bytes of data.
64 bytes from es.dev (172.18.0.2): icmp_seq=1 ttl=64 time=0.221 ms
64 bytes from es.dev (172.18.0.2): icmp_seq=2 ttl=64 time=0.114 ms
64 bytes from es.dev (172.18.0.2): icmp_seq=3 ttl=64 time=0.114 ms
^C
--- es ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2000ms
rtt min/avg/max/mdev = 0.114/0.149/0.221/0.052 ms
root@0ecccdfa45ef:/#
```

:::

## The Container Network Model
:::{.callout-tip icon=false}
## Resolving container addresses

Since Docker Engine 1.10, name resolution is implemented by a dynamic resolver.

Archeological note: when CNM was intoduced (in Docker Engine 1.9, November 2015)
name resolution was implemented with `/etc/hosts`, and it was updated each time
CONTAINERs were added/removed. This could cause interesting race conditions
since `/etc/hosts` was a bind-mount (and couldn't be updated atomically).

```bash
[root@0ecccdfa45ef /]# cat /etc/hosts
172.18.0.3  0ecccdfa45ef
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.18.0.2      es
172.18.0.2      es.dev
```

:::

# Service discovery with containers

## Service discovery with containers
:::{.callout-tip icon=false}
## Overview
* Let's try to run an application that requires two containers.

* The first container is a web server.

* The other one is a redis data store.

* We will place them both on the `dev` network created before.

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Running the web server

* The application is provided by the container image `jpetazzo/trainingwheels`.

* We don't know much about it so we will try to run it and see what happens!

Start the container, exposing all its ports:

```bash
$ docker run --net dev -d -P jpetazzo/trainingwheels
```

Check the port that has been allocated to it:

```bash
$ docker ps -l
```

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Test the web server

* If we connect to the application now, we will see an error page:

![Trainingwheels error](img/trainingwheels-error.png)

* This is because the Redis service is not running.
* This container tries to resolve the name `redis`.

Note: we're not using a FQDN or an IP address here; just `redis`.

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Start the data store

* We need to start a Redis container.

* That container must be on the same network as the web server.

* It must have the right network alias (`redis`) so the application can find it.

Start the container:

```bash
$ docker run --net dev --net-alias redis -d redis
```

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Test the web server again

* If we connect to the application now, we should see that the app is working correctly:

![Trainingwheels OK](images/trainingwheels-ok.png)

* When the app tries to resolve `redis`, instead of getting a DNS error, it gets the IP address of our Redis container.

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## A few words on *scope*

- Container names are unique (there can be only one `--name redis`)

- Network aliases are not unique

- We can have the same network alias in different networks:
  ```bash
  docker run --net dev --net-alias redis ...
  docker run --net prod --net-alias redis ...
  ```

- We can even have multiple containers with the same alias in the same network

  (in that case, we get multiple DNS entries, aka "DNS round robin")

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Names are *local* to each network

Let's try to ping our `es` container from another container, when that other container is *not* on the `dev` network.

```bash
$ docker run --rm alpine ping es
ping: bad address 'es'
```

Names can be resolved only when containers are on the same network.

Containers can contact each other only when they are on the same network (you can try to ping using the IP address to verify).

:::


## Service discovery with containers
:::{.callout-tip icon=false}
## Network aliases

We would like to have another network, `prod`, with its own `es` container. But there can be only one container named `es`!

We will use *network aliases*.

A container can have multiple network aliases.

Network aliases are *local* to a given network (only exist in this network).

Multiple containers can have the same network alias (even on the same network).

Since Docker Engine 1.11, resolving a network alias yields the IP addresses of all containers holding this alias.

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Creating containers on another network

Create the `prod` network.

```bash
$ docker network create prod
5a41562fecf2d8f115bedc16865f7336232a04268bdf2bd816aecca01b68d50c
```

We can now create multiple containers with the `es` alias on the new `prod` network.

```bash
$ docker run -d --name prod-es-1 --net-alias es --net prod elasticsearch:2
38079d21caf0c5533a391700d9e9e920724e89200083df73211081c8a356d771
$ docker run -d --name prod-es-2 --net-alias es --net prod elasticsearch:2
1820087a9c600f43159688050dcc164c298183e1d2e62d5694fd46b10ac3bc3d
```

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Resolving network aliases

Let's try DNS resolution first, using the `nslookup` tool that ships with the `alpine` image.

```bash
$ docker run --net prod --rm alpine nslookup es
Name:      es
Address 1: 172.23.0.3 prod-es-2.prod
Address 2: 172.23.0.2 prod-es-1.prod
```

(You can ignore the `can't resolve '(null)'` errors.)

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Connecting to aliased containers

Each ElasticSearch instance has a name (generated when it is started). This name can be seen when we issue a simple HTTP request on the ElasticSearch API endpoint.

Try the following command a few times:

.small[
```bash
$ docker run --rm --net dev centos curl -s es:9200
{
  "name" : "Tarot",
...
}
```
]

Then try it a few times by replacing `--net dev` with `--net prod`:

.small[
```bash
$ docker run --rm --net prod centos curl -s es:9200
{
  "name" : "The Symbiote",
...
}
```
]

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Good to know ...

* Docker will not create network names and aliases on the default `bridge` network.

* Therefore, if you want to use those features, you have to create a custom network first.

* Network aliases are *not* unique on a given network.

* i.e., multiple containers can have the same alias on the same network.

* In that scenario, the Docker DNS server will return multiple records.
  <br/>
  (i.e. you will get DNS round robin out of the box.)

* Enabling *Swarm Mode* gives access to clustering and load balancing with IPVS.

* Creation of networks and network aliases is generally automated with tools like Compose.

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## A few words about round robin DNS

Don't rely exclusively on round robin DNS to achieve load balancing.

Many factors can affect DNS resolution, and you might see:

- all traffic going to a single instance;
- traffic being split (unevenly) between some instances;
- different behavior depending on your application language;
- different behavior depending on your base distro;
- different behavior depending on other factors (sic).

It's OK to use DNS to discover available endpoints, but remember that you have to re-resolve every now and then to discover new endpoints.

:::


## Service discovery with containers
:::{.callout-tip icon=false}
## Custom networks

When creating a network, extra options can be provided.

* `--internal` disables outbound traffic (the network won't have a default gateway).

* `--gateway` indicates which address to use for the gateway (when outbound traffic is allowed).

* `--subnet` (in CIDR notation) indicates the subnet to use.

* `--ip-range` (in CIDR notation) indicates the subnet to allocate from.

* `--aux-address` allows specifying a list of reserved addresses (which won't be allocated to containers).

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Setting containers' IP address

* It is possible to set a container's address with `--ip`.
* The IP address has to be within the subnet used for the container.

A full example would look like this.

```bash
$ docker network create --subnet 10.66.0.0/16 pubnet
42fb16ec412383db6289a3e39c3c0224f395d7f85bcb1859b279e7a564d4e135
$ docker run --net pubnet --ip 10.66.66.66 -d nginx
b2887adeb5578a01fd9c55c435cad56bbbe802350711d2743691f95743680b09
```

*Note: don't hard code container IP addresses in your code!*

*I repeat: don't hard code container IP addresses in your code!*

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Network drivers

* A network is managed by a *driver*.

* The built-in drivers include:

  * `bridge` (default)
  * `none`
  * `host`
  * `macvlan`
  * `overlay` (for Swarm clusters)

* More drivers can be provided by plugins (OVS, VLAN...)

* A network can have a custom IPAM (IP allocator).

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Overlay networks

* The features we've seen so far only work when all containers are on a single host.

* If containers span multiple hosts, we need an *overlay* network to connect them together.

* Docker ships with a default network plugin, `overlay`, implementing an overlay network leveraging
  VXLAN, *enabled with Swarm Mode*.

* Other plugins (Weave, Calico...) can provide overlay networks as well.

* Once you have an overlay network, *all the features that we've used in this chapter work identically
  across multiple hosts.*

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Multi-host networking (overlay)

Out of the scope for this intro-level workshop!

Very short instructions:

- enable Swarm Mode (`docker swarm init` then `docker swarm join` on other nodes)
- `docker network create mynet --driver overlay`
- `docker service create --network mynet myimage`

If you want to learn more about Swarm mode, you can check
[this video](https://www.youtube.com/watch?v=EuzoEaE6Cqs)
or [these slides](https://container.training/swarm-selfpaced.yml.html).

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Multi-host networking (plugins)

Out of the scope for this intro-level workshop!

General idea:

- install the plugin (they often ship within containers)

- run the plugin (if it's in a container, it will often require extra parameters; don't just `docker run` it blindly!)

- some plugins require configuration or activation (creating a special file that tells Docker "use the plugin whose control socket is at the following location")

- you can then `docker network create --driver pluginname`

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Connecting and disconnecting dynamically

* So far, we have specified which network to use when starting the container.

* The Docker Engine also allows connecting and disconnecting while the container is running.

* This feature is exposed through the Docker API, and through two Docker CLI commands:

  * `docker network connect <network> <container>`

  * `docker network disconnect <network> <container>`

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Dynamically connecting to a network

* We have a container named `es` connected to a network named `dev`.

* Let's start a simple alpine container on the default network:

  ```bash
  $ docker run -ti alpine sh
  / #
  ```

* In this container, try to ping the `es` container:

  ```bash
  / # ping es
  ping: bad address 'es'
  ```

  This doesn't work, but we will change that by connecting the container.

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Finding the container ID and connecting it

* Figure out the ID of our alpine container; here are two methods:

  * looking at `/etc/hostname` in the container,

  * running `docker ps -lq` on the host.

* Run the following command on the host:

  ```bash
  $ docker network connect dev `<container_id>`
  ```

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Checking what we did

* Try again to `ping es` from the container.

* It should now work correctly:

  ```bash
  / # ping es
  PING es (172.20.0.3): 56 data bytes
  64 bytes from 172.20.0.3: seq=0 ttl=64 time=0.376 ms
  64 bytes from 172.20.0.3: seq=1 ttl=64 time=0.130 ms
  ^C
  ```

* Interrupt it with Ctrl-C.

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Looking at the network setup in the container

We can look at the list of network interfaces with `ifconfig`, `ip a`, or `ip l`:

```bash
/ # ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
18: eth0@if19: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever
20: eth1@if21: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP
    link/ether 02:42:ac:14:00:04 brd ff:ff:ff:ff:ff:ff
    inet 172.20.0.4/16 brd 172.20.255.255 scope global eth1
       valid_lft forever preferred_lft forever
/ #
```

Each network connection is materialized with a virtual network interface.

As we can see, we can be connected to multiple networks at the same time.

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Disconnecting from a network

* Let's try the symmetrical command to disconnect the container:
  ```bash
  $ docker network disconnect dev <container_id>
  ```

* From now on, if we try to ping `es`, it will not resolve:
  ```bash
  / # ping es
  ping: bad address 'es'
  ```

* Trying to ping the IP address directly won't work either:
  ```bash
  / # ping 172.20.0.3
  ... (nothing happens until we interrupt it with Ctrl-C)
  ```

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Network aliases are scoped per network

* Each network has its own set of network aliases.

* We saw this earlier: `es` resolves to different addresses in `dev` and `prod`.

* If we are connected to multiple networks, the resolver looks up names in each of them
  (as of Docker Engine 18.03, it is the connection order) and stops as soon as the name
  is found.

* Therefore, if we are connected to both `dev` and `prod`, resolving `es` will **not**
  give us the addresses of all the `es` services; but only the ones in `dev` or `prod`.

* However, we can lookup `es.dev` or `es.prod` if we need to.

:::

## Service discovery with containers
:::{.callout-tip icon=false}
## Finding out about our networks and names

* We can do reverse DNS lookups on containers' IP addresses.

* If the IP address belongs to a network (other than the default bridge), the result will be:

  ```
  name-or-first-alias-or-container-id.network-name
  ```

* Example:

```bash
$ docker run -ti --net prod --net-alias hello alpine
/ # apk add --no-cache drill
...
OK: 5 MiB in 13 packages
/ # ifconfig
eth0      Link encap:Ethernet  HWaddr 02:42:AC:15:00:03
          inet addr:`172.21.0.3`  Bcast:172.21.255.255  Mask:255.255.0.0
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
...
/ # drill -t ptr `3.0.21.172`.in-addr.arpa
...
;; ANSWER SECTION:
3.0.21.172.in-addr.arpa.	600	IN	PTR	`hello.prod`.
...
```

:::


## Service discovery with containers
:::{.callout-tip icon=false}
## Building with a custom network

* We can build a Dockerfile with a custom network with `docker build --network NAME`.

* This can be used to check that a build doesn't access the network.

  (But keep in mind that most Dockerfiles will fail,
  <br/>because they need to install remote packages and dependencies!)

* This may be used to access an internal package repository.

  (But try to use a multi-stage build instead, if possible!)

:::
