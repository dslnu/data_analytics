---
title: "Kubernetes part 1"
author: 
  - name: MSDE
    affiliation: Lviv University
code-fold: false
execute:
  enabled: false
  cache: true
diagram:
  cache: true
  cache-dir: ./cache
  engine:
    tikz:
      execpath: lualatex
      additional-packages: |
        \usetikzlibrary{arrows.meta}
        \usetikzlibrary{positioning}
        \usetikzlibrary{decorations.pathreplacing}
filters:
  - diagram
title-slide-attributes:
  data-background-image: img/k8s_logo.svg
  data-background-size: contain
  data-background-opacity: "0.3"
format: 
  revealjs:
    css: ./custom.css
    preview-links: auto
    slide-number: true
    theme: default
    multiplex:
      url: 'https://mplex.vitv.ly'
      secret: '5d4ee4b5f89732ecd284ca89063930b8'
      id: 'eb61bbf995b1c8abaf276cdae9bc92cd05e8c2fef7880c56f1f6d8869ea66286'
---

## Distributed intro {.font8 .scrollable}

:::{.callout-tip icon=false}
## Definitions

|Keyword|Description|
|----|------|
|VM|A virtual machine (VM) is a software simulation of a physical computer that runs on a host computer. It provides a separate operating system and resources, allowing multiple operating systems to run on a single physical machine.|
|Cluster|A cluster is a group of connected servers that work together as a single system to provide high availability, scalability, and increased performance for applications. The nodes in a cluster are connected through a network and share resources to provide a unified, highly available solution.|
|Node|A cluster node is a single server within a cluster computing system. It provides computing resources and works together with other nodes to perform tasks as a unified system, providing high availability and scalability for applications.|
:::

## Distributed intro {.font8 .scrollable}

:::{.callout-tip icon=false}
## Definitions

|Keyword|Description|
|----|------|
|Network|A network is a group of interconnected devices that can exchange data and information. Networks can be used to connect computers, servers, mobile devices, and other types of devices and allow them to communicate with each other and share resources, such as printers and storage. More specifically in our case, these are physical and software-defined communication paths between individual nodes of a cluster and programs running on those nodes.|
|Port|A port is a communication endpoint in a network-attached device, such as a computer or server. It allows the device to receive and send data to other devices on the network through a specific network protocol, such as TCP or UDP. Each port has a unique number that is used to identify it, and different services and applications use specific ports to communicate.|
|Service|A piece of software that implements a limited set of functionalities that are then used by other parts of the application.|
:::

## Distributed intro
:::{.callout-note icon=false}
## Monolithic vs distributed
**Monolithic**: single (or a couple of) tightly coupled programs running on a single server. 

**Distributed**: programs collected into **services** running on multiple servers, communicating with each other.
:::

![](img/monolith_vs_microservices.png)


## Distributed intro
:::{.callout-note icon=false}
## Problems

- much more complex
- need rethinking
- components have to be separated
- and loosely coupled
- meaning: components have well-defined interfaces
:::

## Distributed intro
:::{.callout-note icon=false}
## Stateful vs stateless

**Stateful**: create or modify **persistent** data.

- Much harder to reason about.
- Should be pushed to the boundaries of the system.

**Stateless**: don't create or modify **persistent** data.

- Much easier to reason about.
- Should form core of the system.

:::


## Distributed intro
:::{.callout-note icon=false}
## Discovery
Individual components should be able to find each other in a cluster.
:::

:::{.callout-important icon=false}
## Option 1: some configuration file.

![](img/discovery_config.png){height=400}
:::


## Distributed intro
:::{.callout-note icon=false}
## Option 2 - DNS
An external authority that knows where to find services - Domain Name Service.

![](img/discovery_dns.png){height=400}
:::

## Distributed intro
:::{.callout-note icon=false}
## Load balancing
**Service A** needs to talk to **Service B**, but the latter runs on many instances.

Load balancer:

- distributes tasks using some algorithm
- checks health

![](img/load_balancing.png){height=300}

:::

## Distributed intro
:::{.callout-warning}
## Reliability matters

- retries
- logs
- proper error handling
- redundancy
- health checks
- monitoring
- rate limiting
:::

## Distributed intro
:::{.callout-note icon=false}
## Application updates

- code changes
- data updates
:::

![](img/blue_green_deployment_fowler.png)


## Intro to orchestration
:::{.callout-note icon=false}
## What is it?

A standard way to manage distributed systems built on top of containers (e.g. Docker)
:::

![](img/orchestration.png)



## Intro to orchestration
:::{.callout-note icon=false}
## Declarative vs imperative

- Our container orchestrator puts a very strong emphasis on being *declarative*

- Declarative:

  *I would like a cup of tea.*

- Imperative:

  *Boil some water. Pour it in a teapot. Add tea leaves. Steep for a while. Serve in a cup.*

- Declarative seems simpler at first ... 

- ... As long as you know how to brew tea

:::

## Intro to orchestration
:::{.callout-note icon=false}
## Declarative vs imperative

- What declarative would really be:

  *I want a cup of tea, obtained by pouring an infusion¬π of tea leaves in a cup.*

  *¬πAn infusion is obtained by letting the object steep a few minutes in hot¬≤ water.*

  *¬≤Hot liquid is obtained by pouring it in an appropriate container¬≥ and setting it on a stove.*

  *¬≥Ah, finally, containers! Something we know about. Let's get to work, shall we?*

:::


## Intro to orchestration
:::{.callout-note icon=false}
## Declarative vs imperative
- Imperative systems:

  - simpler

  - if a task is interrupted, we have to restart from scratch

- Declarative systems:

  - if a task is interrupted (or if we show up to the party half-way through),
    we can figure out what's missing and do only what's necessary

  - we need to be able to *observe* the system

  - ... and compute a "diff" between *what we have* and *what we want*

:::


## Intro to orchestration
:::{.callout-note icon=false}
## Objectives

- keeping desired state (or **reconciling**, described in a **declarative** fashion)
- making sure **global** services run on all **workers**
- and **replicated** services run a on a specified number of **workers**
- service discovery
- routing
- load-balancing
- scaling
- health checks/fixes
- data persistence
- node affinity
- security/secrets management
- introspection
:::

## Intro to orchestration
:::{.callout-important icon=false}
## Declarative vs imperative in Kubernetes

- With Kubernetes, we cannot say: "run this container"

- All we can do is write a *spec* and push it to the API server

  (by creating a resource like e.g. a Pod or a Deployment)

- The API server will validate that spec (and reject it if it's invalid)

- Then it will store it in etcd

- A *controller* will "notice" that spec and act upon it
:::

## Intro to orchestration
:::{.callout-note icon=false}
## Reconciling state

- Watch for the `spec` fields in the YAML files later!

- The *spec* describes *how we want the thing to be*

- Kubernetes will *reconcile* the current state with the spec
  <br/>(technically, this is done by a number of *controllers*)

- When we want to change some resource, we update the *spec*

- Kubernetes will then *converge* that resource
:::



## Kubernetes concepts
:::{.callout-tip icon=false}
## Kubernetes concepts

- Kubernetes is a container management system

- It runs and manages containerized applications on a cluster

- What does that really mean?

:::

## Kubernetes concepts
:::{.callout-note icon=false}
## History

- Originally designed by Google (2014)
- Ancient Greek Œ∫œÖŒ≤ŒµœÅŒΩŒÆœÑŒ∑œÇ for "pilot"
- Evolution of Google's proprietary Borg system
- Based on Promise theory
:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## What can we do with Kubernetes?

- Let's imagine that we have a 3-tier e-commerce app:

  - web frontend

  - API backend

  - database

- We have built images for our frontend and backend components

  (e.g. with Dockerfiles and `docker build`)

- We are running them successfully with a local environment

  (e.g. with Docker Compose)

- Let's see how we would deploy our app on Kubernetes!

:::


## Kubernetes concepts
:::{.callout-tip icon=false}
## Kubernetes, level 1

- Leave our database outside of Kubernetes (because database be scaryü•∫)

- Deploy a managed Kubernetes cluster (cloud or [professional services][enix-k8s-expert])

- Start 5 containers using image `atseashop/api:v1.3`

- Place an internal load balancer in front of these containers

- Start 10 containers using image `atseashop/webfront:v1.3`

- Place a public load balancer in front of these containers

- It's Black Friday (or Christmas), traffic spikes, grow our cluster and add containers

- New release! Replace my containers with the new image `atseashop/webfront:v1.4`

- Keep processing requests during the upgrade; update my containers one at a time

[enix-k8s-expert]: https://enix.io/en/kubernetes-expert/

:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Kubernetes, level 2

- Deploy a pre-production environment

  (still using our external database, for now)

- Resource management and scheduling

  (reserve CPU/RAM for containers; placement constraints; priorities)

- Autoscaling

  (straightforward on CPU; more complex on other metrics)

- Advanced rollout patterns

  (blue/green deployment, canary deployment)

<!-- .footnote[ -->
<!-- On the next page: canary cage with an oxygen bottle, designed to keep the canary alive. -->
<!-- <br/> -->
<!-- (See https://post.lurk.org/@zilog/109632335293371919 for details.) -->
<!-- ] -->

:::

## Kubernetes concepts
![Blue-green deployment](img/blue_green_deployment.png)

## Kubernetes concepts
![Canary cage](img/canary-cage.jpg)


## Kubernetes concepts
:::{.callout-tip icon=false}
## Kubernetes, level 3

- Run staging databases on the cluster

  (no replication, no backups, no scaling)

- Automatic or semi-automatic deployment of feature branches

  (each with its own database)

- Fine-grained access control

  (defining *what* can be done by *whom* on *which* resources)

- Batch jobs

  (one-off; parallel; also cron-style periodic execution)

- Package applications with e.g. Helm charts

:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Kubernetes, level 4

- Stateful services with persistence, replication, backups

  (databases, message queues, etc.)

- Automate complex tasks with *operators*

  (e.g. database replication, failover, etc.)

- Combine the two previous points with database operators like [CloudNativePG][cnpg]

  (learn more about database operators: [FR][pirates-video-fr], [EN][pirates-video-en])

- Leverage advanced storage with e.g. local ZFS volumes

  (learn more about ZFS and databases on k8s: [FR][zfs-video-fr], [EN][zfs-video-en])

- Deploy and manage clusters in-house

[cnpg]: https://cloudnative-pg.io/
[pirates-video-fr]: https://www.youtube.com/watch?v=d_ka7PlWo1I
[pirates-video-en]: https://www.youtube.com/watch?v=ojUdBjbiKWk&t=5s
[zfs-video-fr]: https://www.youtube.com/watch?v=XN9YL93f8tI
[zfs-video-en]: https://www.youtube.com/watch?v=3sJIYiDnod4

:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Kubernetes, level 5

- Deploying and managing clusters at scale

  (hundreds of clusters, thousands of nodes...)

- Writing custom operators

- Hybrid deployments

:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Disclaimer

The levels mentioned in the previous slides are not necessarily linear.

They aren't exhaustive either (we didn't mention e.g. observability and alerting).

:::

## Kubernetes concepts {.scrollable}
:::{.callout-tip icon=false}
## Kubernetes architecture


![haha only kidding](img/k8s-arch1.png)

:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Kubernetes architecture

- Ha ha ha ha

- OK, I was trying to scare you, it's much simpler than that ‚ù§Ô∏è
:::

## Kubernetes concepts

![that one is more like the real thing](img/k8s-arch2.png)

## Kubernetes concepts
:::{.callout-tip icon=false}
## Credits

- The first schema is a Kubernetes cluster with storage backed by multi-path iSCSI

  (Courtesy of [Yongbok Kim](https://www.yongbok.net/blog/))

- The second one is a simplified representation of a Kubernetes cluster

  (Courtesy of [Imesh Gunaratne](https://medium.com/containermind/a-reference-architecture-for-deploying-wso2-middleware-on-kubernetes-d4dee7601e8e))

:::

## Kubernetes concepts

![Another version](img/k8s_arch.png)

## Kubernetes concepts
:::{.callout-note icon=false}
## Architecture

- a Kubernetes cluster consists of a set of servers (VMs/bare metal)
- each server is either a **master** or a *worker.**
- small and odd number of masters
- no limit on worker nodes
- worker nodes run **pods**
- pods in turn run containers (one or more)
- ReplicaSets are collection of identical pods
:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Pods

![](img/k8s_pods.png)
:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Kubernetes architecture: the nodes

- The nodes executing our containers run a collection of services:

  - a container Engine (typically Docker)

  - kubelet (the "node agent")

  - kube-proxy (a necessary but not sufficient network component)

- Nodes were formerly called "minions"

  (You might see that word in older articles or documentation)

:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Kubernetes architecture: the control plane

- The Kubernetes logic (its "brains") is a collection of services:

  - the API server (our point of entry to everything!)

  - core services like the scheduler and controller manager

  - `etcd` (a highly available key/value store; the "database" of Kubernetes)

- Together, these services form the control plane of our cluster

- The control plane is also called the "master"
:::


## Kubernetes concepts
![One of the best Kubernetes architecture diagrams available](img/k8s-arch4-thanks-luxas.png)


## Kubernetes concepts
:::{.callout-tip icon=false}
## Running the control plane on special nodes

- It is common to reserve a dedicated node for the control plane

  (Except for single-node development clusters, like when using minikube)

- This node is then called a "master"

  (Yes, this is ambiguous: is the "master" a node, or the whole control plane?)

- Normal applications are restricted from running on this node

  (By using a mechanism called ["taints"](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/))

- When high availability is required, each service of the control plane must be resilient

- The control plane is then replicated on multiple nodes

  (This is sometimes called a "multi-master" setup)

:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Running the control plane outside containers

- The services of the control plane can run in or out of containers

- For instance: since `etcd` is a critical service, some people
  deploy it directly on a dedicated cluster (without containers)

  (This is illustrated on the first "super complicated" schema)

- In some hosted Kubernetes offerings (e.g. AKS, GKE, EKS), the control plane is invisible

  (We only "see" a Kubernetes API endpoint)

- In that case, there is no "master node"

*For this reason, it is more accurate to say "control plane" rather than "master."*

:::

## Kubernetes concepts
![](img/control-planes/single-node-dev.svg)


## Kubernetes concepts
![](img/control-planes/managed-kubernetes.svg)


## Kubernetes concepts
![](img/control-planes/single-control-and-workers.svg)

## Kubernetes concepts
![](img/control-planes/stacked-control-plane.svg)

## Kubernetes concepts
![](img/control-planes/non-dedicated-stacked-nodes.svg)

## Kubernetes concepts
![](img/control-planes/advanced-control-plane.svg)

## Kubernetes concepts
![](img/control-planes/advanced-control-plane-split-events.svg)

## Kubernetes concepts
:::{.callout-tip icon=false}
## How many nodes should a cluster have?

- There is no particular constraint

  (no need to have an odd number of nodes for quorum)

- A cluster can have zero nodes

  (but then it won't be able to start any pods)

- For testing and development, having a single node is fine

- For production, make sure that you have extra capacity

  (so that your workload still fits if you lose a node or a group of nodes)

- Kubernetes is tested with [up to 5000 nodes](https://kubernetes.io/docs/setup/best-practices/cluster-large/)

  (however, running a cluster of that size requires a lot of tuning)

:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Do we need to run Docker at all?

No!

- The Docker Engine used to be the default option to run containers with Kubernetes

- Support for Docker (specifically: dockershim) was removed in Kubernetes 1.24

- We can leverage other pluggable runtimes through the *Container Runtime Interface*

- <del>We could also use `rkt` ("Rocket") from CoreOS</del> (deprecated)

:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Some runtimes available through CRI

- [containerd](https://github.com/containerd/containerd/blob/master/README.md)

  - maintained by Docker, IBM, and community
  - used by Docker Engine, microk8s, k3s, GKE; also standalone
  - comes with its own CLI, `ctr`

- [CRI-O](https://github.com/cri-o/cri-o/blob/master/README.md):

  - maintained by Red Hat, SUSE, and community
  - used by OpenShift and Kubic
  - designed specifically as a minimal runtime for Kubernetes

- [And more](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)

:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Do we need to run Docker at all?

Yes!

<!-- - In this workshop, we run our app on a single node first -->

- We will need to build images and ship them around

- We can do these things without Docker
  <br/>
  (but with some languages/frameworks, it might be much harder)

- Docker is still the most stable container engine today
  <br/>
  (but other options are maturing very quickly)

:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Do we need to run Docker at all?

- On our Kubernetes clusters:

  *Not anymore*

- On our development environments, CI pipelines ... :

  *Yes, almost certainly*

:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Interacting with Kubernetes

- We will interact with our Kubernetes cluster through the Kubernetes API

- The Kubernetes API is (mostly) RESTful

- It allows us to create, read, update, delete *resources*

- A few common resource types are:

  - node (a machine ‚Äî physical or virtual ‚Äî in our cluster)

  - pod (group of containers running together on a node)

  - service (stable network endpoint to connect to one or multiple containers)

:::

## Kubernetes concepts
![Node, pod, container](img/k8s-arch3-thanks-weave.png)

## Kubernetes concepts
:::{.callout-tip icon=false}
## Scaling

- How would we scale the pod shown on the previous slide?

- **Do** create additional pods

  - each pod can be on a different node

  - each pod will have its own IP address

- **Do not** add more NGINX containers in the pod

  - all the NGINX containers would be on the same node

  - they would all have the same IP address
    <br/>(resulting in `Address already in use` errors)

:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Together or separate

- Should we put e.g. a web application server and a cache together?
  <br/>
  ("cache" being something like e.g. Memcached or Redis)

- Putting them **in the same pod** means:

  - they have to be scaled together

  - they can communicate very efficiently over `localhost`

- Putting them **in different pods** means:

  - they can be scaled separately

  - they must communicate over remote IP addresses
    <br/>(incurring more latency, lower performance)

- Both scenarios can make sense, depending on our goals

:::

## Kubernetes concepts
:::{.callout-tip icon=false}
## Credits

- The first diagram is courtesy of Lucas K√§ldstr√∂m, in [this presentation](https://speakerdeck.com/luxas/kubeadm-cluster-creation-internals-from-self-hosting-to-upgradability-and-ha)

  - it's one of the best Kubernetes architecture diagrams available!

- The second diagram is courtesy of Weave Works

  - a *pod* can have multiple containers working together

  - IP addresses are associated with *pods*, not with individual containers

Both diagrams used with permission.

:::


## Intro to `kubectl`
:::{.callout-note}
## Installation

- login to your EC2 instance
- install [Docker](https://docs.docker.com/engine/install/ubuntu/)
- install [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-using-native-package-management)
- install [minikube](https://minikube.sigs.k8s.io/docs/start/?arch=%2Flinux%2Fx86-64%2Fstable%2Fbinary+download)
- install [Kompose](https://kompose.io/installation/)
- we'll use [Sample compose app](https://github.com/docker/awesome-compose/tree/master/flask-redis) as an example
:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Intro to `kubectl`

- `kubectl` is (almost) the only tool we'll need to talk to Kubernetes

- It is a rich CLI tool around the Kubernetes API

  (Everything you can do with `kubectl`, you can do directly with the API)

- On our machines, there is a `~/.kube/config` file with:

  - the Kubernetes API address

  - the path to our TLS certificates used to authenticate

- You can also use the `--kubeconfig` flag to pass a config file

- Or directly `--server`, `--user`, etc.

- `kubectl` can be pronounced "Cube C T L", "Cube cuttle", "Cube cuddle"...

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## `kubectl` is the new SSH

- We often start managing servers with SSH

  (installing packages, troubleshooting ...)

- At scale, it becomes tedious, repetitive, error-prone

- Instead, we use config management, central logging, etc.

- In many cases, we still need SSH:

  - as the underlying access method (e.g. Ansible)

  - to debug tricky scenarios

  - to inspect and poke at things

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## The parallel with `kubectl`

- We often start managing Kubernetes clusters with `kubectl`

  (deploying applications, troubleshooting ...)

- At scale (with many applications or clusters), it becomes tedious, repetitive, error-prone

- Instead, we use automated pipelines, observability tooling, etc.

- In many cases, we still need `kubectl`:

  - to debug tricky scenarios

  - to inspect and poke at things

- The Kubernetes API is always the underlying access method

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## `kubectl get`

- Let's look at our `Node` resources with `kubectl get`!

- Look at the composition of our cluster:
  ```bash
  kubectl get node
  ```

- These commands are equivalent:
  ```bash
  kubectl get no
  kubectl get node
  kubectl get nodes
  ```
:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Obtaining machine-readable output

- `kubectl get` can output JSON, YAML, or be directly formatted

- Give us more info about the nodes:
  ```bash
  kubectl get nodes -o wide
  ```

- Let's have some YAML:
  ```bash
  kubectl get no -o yaml
  ```
  See that `kind: List` at the end? It's the type of our result!

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## (Ab)using `kubectl` and `jq`

- It's super easy to build custom reports

- Show the capacity of all our nodes as a stream of JSON objects:
  ```bash
    kubectl get nodes -o json |
            jq ".items[] | {name:.metadata.name} + .status.capacity"
  ```
:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Exploring types and definitions

- We can list all available resource types by running `kubectl api-resources`
  <br/>
  (In Kubernetes 1.10 and prior, this command used to be `kubectl get`)

- We can view the definition for a resource type with:
  ```bash
  kubectl explain type
  ```

- We can view the definition of a field in a resource, for instance:
  ```bash
  kubectl explain node.spec
  ```

- Or get the full definition of all fields and sub-fields:
  ```bash
  kubectl explain node --recursive
  ```

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Introspection vs. documentation

- We can access the same information by reading the [API documentation](https://kubernetes.io/docs/reference/#api-reference)

- The API documentation is usually easier to read, but:

  - it won't show custom types (like Custom Resource Definitions)

  - we need to make sure that we look at the correct version

- `kubectl api-resources` and `kubectl explain` perform *introspection*

  (they communicate with the API server and obtain the exact type definitions)

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Type names

- The most common resource names have three forms:

  - singular (e.g. `node`, `service`, `deployment`)

  - plural (e.g. `nodes`, `services`, `deployments`)

  - short (e.g. `no`, `svc`, `deploy`)

- Some resources do not have a short name

- `Endpoints` only have a plural form

  (because even a single `Endpoints` resource is actually a list of endpoints)

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Viewing details

- We can use `kubectl get -o yaml` to see all available details

- However, YAML output is often simultaneously too much and not enough

- For instance, `kubectl get node node1 -o yaml` is:

  - too much information (e.g.: list of images available on this node)

  - not enough information (e.g.: doesn't show pods running on this node)

  - difficult to read for a human operator

- For a comprehensive overview, we can use `kubectl describe` instead

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## `kubectl describe`

- `kubectl describe` needs a resource type and (optionally) a resource name

- It is possible to provide a resource name *prefix*

  (all matching objects will be displayed)

- `kubectl describe` will retrieve some extra information about the resource

- Look at the information available for `node1` with one of the following commands:
  ```bash
  kubectl describe node/node1
  kubectl describe node node1
  ```

(We should notice a bunch of control plane pods.)

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Listing running containers

- Containers are manipulated through *pods*

- A pod is a group of containers:

 - running together (on the same node)

 - sharing resources (RAM, CPU; but also network, volumes)

- List pods on our cluster:
  ```bash
  kubectl get pods
  ```
*Where are the pods that we saw just a moment earlier?!?*

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Namespaces

- Namespaces allow us to segregate resources

- List the namespaces on our cluster with one of these commands:
  ```bash
  kubectl get namespaces
  kubectl get namespace
  kubectl get ns
  ```
*You know what ... This `kube-system` thing looks suspicious.*

*In fact, I'm pretty sure it showed up earlier, when we did:*

`kubectl describe node node1`

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Accessing namespaces

- By default, `kubectl` uses the `default` namespace

- We can see resources in all namespaces with `--all-namespaces`

- List the pods in all namespaces:
  ```bash
  kubectl get pods --all-namespaces
  ```

- Since Kubernetes 1.14, we can also use `-A` as a shorter version:
  ```bash
  kubectl get pods -A
  ```

*Here are our system pods!*

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## What are all these control plane pods?

- `etcd` is our etcd server

- `kube-apiserver` is the API server

- `kube-controller-manager` and `kube-scheduler` are other control plane components

- `coredns` provides DNS-based service discovery ([replacing kube-dns as of 1.11](https://kubernetes.io/blog/2018/07/10/coredns-ga-for-kubernetes-cluster-dns/))

- `kube-proxy` is the (per-node) component managing port mappings and such

- `weave` is the (per-node) component managing the network overlay

- the `READY` column indicates the number of containers in each pod

  (1 for most pods, but `weave` has 2, for instance)

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Scoping another namespace

- We can also look at a different namespace (other than `default`)

- List only the pods in the `kube-system` namespace:
  ```bash
  kubectl get pods --namespace=kube-system
  kubectl get pods -n kube-system
  ```

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Namespaces and other `kubectl` commands

- We can use `-n`/`--namespace` with almost every `kubectl` command

- Example:

  - `kubectl create --namespace=X` to create something in namespace X

- We can use `-A`/`--all-namespaces` with most commands that manipulate multiple objects

- Examples:

  - `kubectl delete` can delete resources across multiple namespaces

  - `kubectl label` can add/remove/update labels across multiple namespaces

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## What about `kube-public`?

- List the pods in the `kube-public` namespace:
  ```bash
  kubectl -n kube-public get pods
  ```

Nothing!

`kube-public` is created by kubeadm & [used for security bootstrapping](https://kubernetes.io/blog/2017/01/stronger-foundation-for-creating-and-managing-kubernetes-clusters).

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Exploring `kube-public`

- The only interesting object in `kube-public` is a ConfigMap named `cluster-info`

- List ConfigMap objects:
  ```bash
  kubectl -n kube-public get configmaps
  ```

- Inspect `cluster-info`:
  ```bash
  kubectl -n kube-public get configmap cluster-info -o yaml
  ```

Note the `selfLink` URI: `/api/v1/namespaces/kube-public/configmaps/cluster-info`

We can use that!

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Accessing `cluster-info`

- Earlier, when trying to access the API server, we got a `Forbidden` message

- But `cluster-info` is readable by everyone (even without authentication)

- Retrieve `cluster-info`:
  ```bash
  curl -k https://10.96.0.1/api/v1/namespaces/kube-public/configmaps/cluster-info
  ```

- We were able to access `cluster-info` (without auth)

- It contains a `kubeconfig` file

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Retrieving `kubeconfig`

- We can easily extract the `kubeconfig` file from this ConfigMap

- Display the content of `kubeconfig`:
  ```bash
    curl -sk https://10.96.0.1/api/v1/namespaces/kube-public/configmaps/cluster-info \
         | jq -r .data.kubeconfig
  ```

- This file holds the canonical address of the API server, and the public key of the CA

- This file *does not* hold client keys or tokens

- This is not sensitive information, but allows us to establish trust

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## What about `kube-node-lease`?

- Starting with Kubernetes 1.14, there is a `kube-node-lease` namespace

  (or in Kubernetes 1.13 if the NodeLease feature gate is enabled)

- That namespace contains one Lease object per node

- *Node leases* are a new way to implement node heartbeats

  (i.e. node regularly pinging the control plane to say "I'm alive!")

- For more details, see [Efficient Node Heartbeats KEP] or the [node controller documentation]

[Efficient Node Heartbeats KEP]: https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/589-efficient-node-heartbeats/README.md
[node controller documentation]: https://kubernetes.io/docs/concepts/architecture/nodes/#node-controller

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Services

- A *service* is a stable endpoint to connect to "something"

  (In the initial proposal, they were called "portals")

- List the services on our cluster with one of these commands:
  ```bash
  kubectl get services
  kubectl get svc
  ```
There is already one service on our cluster: the Kubernetes API itself.

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## ClusterIP services

- A `ClusterIP` service is internal, available from the cluster only

- This is useful for introspection from within containers

- Try to connect to the API:
  ```bash
  curl -k https://`10.96.0.1`
  ```

  - `-k` is used to skip certificate verification

  - Make sure to replace 10.96.0.1 with the CLUSTER-IP shown by `kubectl get svc`

The command above should either time out, or show an authentication error. Why?

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Time out

- Connections to ClusterIP services only work *from within the cluster*

- If we are outside the cluster, the `curl` command will probably time out

  (Because the IP address, e.g. 10.96.0.1, isn't routed properly outside the cluster)

- This is the case with most "real" Kubernetes clusters

- To try the connection from within the cluster, we can use [shpod](https://github.com/jpetazzo/shpod)

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Authentication error

This is what we should see when connecting from within the cluster:
```json
$ curl -k https://10.96.0.1
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {

  },
  "status": "Failure",
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",
  "reason": "Forbidden",
  "details": {

  },
  "code": 403
}
```

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## Explanations

- We can see `kind`, `apiVersion`, `metadata`

- These are typical of a Kubernetes API reply

- Because we *are* talking to the Kubernetes API

- The Kubernetes API tells us "Forbidden"

  (because it requires authentication)

- The Kubernetes API is reachable from within the cluster

  (many apps integrating with Kubernetes will use this)

:::

## Intro to `kubectl`
:::{.callout-tip icon=false}
## DNS integration

- Each service also gets a DNS record

- The Kubernetes DNS resolver is available *from within pods*

  (and sometimes, from within nodes, depending on configuration)

- Code running in pods can connect to services using their name

  (e.g. https://kubernetes/...)

:::


## Running containers on K8s
:::{.callout-tip icon=false}
## Running containers on K8s

- First things first: we cannot run a container

- We are going to run a pod, and in that pod there will be a single container

- In that container in the pod, we are going to run a simple `ping` command

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## Starting a simple pod with `kubectl run`

- `kubectl run` is convenient to start a single pod

- We need to specify at least a *name* and the image we want to use

- Optionally, we can specify the command to run in the pod

- Let's ping the address of `localhost`, the loopback interface:
  ```bash
  kubectl run pingpong --image alpine ping 127.0.0.1
  ```

<!-- ```hide kubectl wait pod --selector=run=pingpong --for condition=ready``` -->

The output tells us that a Pod was created:
```
pod/pingpong created
```

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## Viewing container output

- Let's use the `kubectl logs` command

- It takes a Pod name as argument

- Unless specified otherwise, it will only show logs of the first container in the pod

  (Good thing there's only one in ours!)

- View the result of our `ping` command:
  ```bash
  kubectl logs pingpong
  ```

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## Streaming logs in real time

- Just like `docker logs`, `kubectl logs` supports convenient options:

  - `-f`/`--follow` to stream logs in real time (√† la `tail -f`)

  - `--tail` to indicate how many lines you want to see (from the end)

  - `--since` to get logs only after a given timestamp

- View the latest logs of our `ping` command:
  ```bash
  kubectl logs pingpong --tail 1 --follow
  ```

- Stop it with Ctrl-C

<!--
```wait seq=3```
```keys ^C```
-->

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## Scaling our application

- `kubectl` gives us a simple command to scale a workload:

  `kubectl scale TYPE NAME --replicas=HOWMANY`

- Let's try it on our Pod, so that we have more Pods!

- Try to scale the Pod:
  ```bash
  kubectl scale pod pingpong --replicas=3
  ```

ü§î We get the following error, what does that mean?

```
Error from server (NotFound): the server could not find the requested resource
```

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## Scaling a Pod

- We cannot "scale a Pod"

  (that's not completely true; we could give it more CPU/RAM)

- If we want more Pods, we need to create more Pods

  (i.e. execute `kubectl run` multiple times)

- There must be a better way!

  (spoiler alert: yes, there is a better way!)

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## `NotFound`

- What's the meaning of that error?
  ```
  Error from server (NotFound): the server could not find the requested resource
  ```

- When we execute `kubectl scale THAT-RESOURCE --replicas=THAT-MANY`,
  <br/>
  it is like telling Kubernetes:

  *go to THAT-RESOURCE and set the scaling button to position THAT-MANY*

- Pods do not have a "scaling button"

- Try to execute the `kubectl scale pod` command with `-v6`

- We see a `PATCH` request to `/scale`: that's the "scaling button"

  (technically it's called a *subresource* of the Pod)

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## Creating more pods

- We are going to create a ReplicaSet

  (= set of replicas = set of identical pods)

- In fact, we will create a Deployment, which itself will create a ReplicaSet

- Why so many layers? We'll explain that shortly, don't worry!

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## Creating a Deployment running `ping`

- Let's create a Deployment instead of a single Pod

- Create the Deployment; pay attention to the `--`:
  ```bash
  kubectl create deployment pingpong --image=alpine -- ping 127.0.0.1
  ```

- The `--` is used to separate:

  - options/flags of `kubectl create`

  - command to run in the container

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## What has been created?

<!-- ```hide kubectl wait pod --selector=app=pingpong --for condition=ready ``` -->

- Check the resources that were created:
  ```bash
  kubectl get all
  ```
Note: `kubectl get all` is a lie. It doesn't show everything.

(But it shows a lot of "usual suspects", i.e. commonly used resources.)

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## There's a lot going on here!

```
NAME                            READY   STATUS        RESTARTS   AGE
pod/pingpong                    1/1     Running       0          4m17s
pod/pingpong-6ccbc77f68-kmgfn   1/1     Running       0          11s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3h45

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pingpong   1/1     1            1           11s

NAME                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/pingpong-6ccbc77f68   1         1         1       11s
```

Our new Pod is not named `pingpong`, but `pingpong-xxxxxxxxxxx-yyyyy`.

We have a Deployment named `pingpong`, and an extra ReplicaSet, too. What's going on?

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## From Deployment to Pod

We have the following resources:

- `deployment.apps/pingpong`

  This is the Deployment that we just created.

- `replicaset.apps/pingpong-xxxxxxxxxx`

  This is a Replica Set created by this Deployment.

- `pod/pingpong-xxxxxxxxxx-yyyyy`

  This is a *pod* created by the Replica Set.

Let's explain what these things are.

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## Pod

- Can have one or multiple containers

- Runs on a single node

  (Pod cannot "straddle" multiple nodes)

- Pods cannot be moved

  (e.g. in case of node outage)

- Pods cannot be scaled horizontally

  (except by manually creating more Pods)

:::

## Running containers on K8s
:::{.callout-note}
## Pod details

- A Pod is not a process; it's an environment for containers

  - it cannot be "restarted"

  - it cannot "crash"

- The containers in a Pod can crash

- They may or may not get restarted

  (depending on Pod's restart policy)

- If all containers exit successfully, the Pod ends in "Succeeded" phase

- If some containers fail and don't get restarted, the Pod ends in "Failed" phase

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## Replica Set

- Set of identical (replicated) Pods

- Defined by a pod template + number of desired replicas

- If there are not enough Pods, the Replica Set creates more

  (e.g. in case of node outage; or simply when scaling up)

- If there are too many Pods, the Replica Set deletes some

  (e.g. if a node was disconnected and comes back; or when scaling down)

- We can scale up/down a Replica Set

  - we update the manifest of the Replica Set

  - as a consequence, the Replica Set controller creates/deletes Pods

:::

## Running containers on K8s
:::{.callout-note}
## Deployment

- Replica Sets control *identical* Pods

- Deployments are used to roll out different Pods

  (different image, command, environment variables, ...)

- When we update a Deployment with a new Pod definition:

  - a new Replica Set is created with the new Pod definition

  - that new Replica Set is progressively scaled up

  - meanwhile, the old Replica Set(s) is(are) scaled down

- This is a *rolling update*, minimizing application downtime

- When we scale up/down a Deployment, it scales up/down its Replica Set

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## Can we scale now?

- Let's try `kubectl scale` again, but on the Deployment!

- Scale our `pingpong` deployment:
  ```bash
  kubectl scale deployment pingpong --replicas 3
  ```

- Note that we could also write it like this:
  ```bash
  kubectl scale deployment/pingpong --replicas 3
  ```

- Check that we now have multiple pods:
  ```bash
  kubectl get pods
  ```

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## Scaling a Replica Set

- What if we scale the Replica Set instead of the Deployment?

- The Deployment would notice it right away and scale back to the initial level

- The Replica Set makes sure that we have the right numbers of Pods

- The Deployment makes sure that the Replica Set has the right size

  (conceptually, it delegates the management of the Pods to the Replica Set)

- This might seem weird (why this extra layer?) but will soon make sense

  (when we will look at how rolling updates work!)

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## Checking Deployment logs

- `kubectl logs` needs a Pod name

- But it can also work with a *type/name*

  (e.g. `deployment/pingpong`)

- View the result of our `ping` command:
  ```bash
  kubectl logs deploy/pingpong --tail 2
  ```

- It shows us the logs of the first Pod of the Deployment

- We'll see later how to get the logs of *all* the Pods!

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## Resilience

- The *deployment* `pingpong` watches its *replica set*

- The *replica set* ensures that the right number of *pods* are running

- What happens if pods disappear?

- In a separate window, watch the list of pods:
  ```bash
  watch kubectl get pods
  ```

<!--
```wait Every 2.0s```
```tmux split-pane -v```
-->

- Destroy the pod currently shown by `kubectl logs`:
  ```
  kubectl delete pod pingpong-xxxxxxxxxx-yyyyy
  ```

<!--
```tmux select-pane -t 0```
```copy pingpong-[^-]*-.....```
```tmux last-pane```
```keys kubectl delete pod ```
```paste```
```key ^J```
```check```
-->

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## What happened?

- `kubectl delete pod` terminates the pod gracefully

  (sending it the TERM signal and waiting for it to shutdown)

- As soon as the pod is in "Terminating" state, the Replica Set replaces it

- But we can still see the output of the "Terminating" pod in `kubectl logs`

- Until 30 seconds later, when the grace period expires

- The pod is then killed, and `kubectl logs` exits

:::

## Running containers on K8s
:::{.callout-tip icon=false}
## Deleting a standalone Pod

- What happens if we delete a standalone Pod?
 
  (like the first `pingpong` Pod that we created)

- Delete the Pod:
  ```bash
  kubectl delete pod pingpong
  ```

<!--
```key ^D```
```key ^C```
-->

- No replacement Pod gets created because there is no *controller* watching it

- That's why we will rarely use standalone Pods in practice

  (except for e.g. punctual debugging or executing a short supervised task)

:::
