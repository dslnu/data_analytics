---
title: "Big Data Analytics: Lab 9"
execute:
  enabled: true
  echo: true
  cache: true
format:
  html:
    code-fold: false
jupyter: python3
diagram:
  cache: true
  cache-dir: ./cache
  engine:
    tikz:
      execpath: lualatex
filters:
  - diagram
---

# Postgres for analytics

## Installation
 1. Install Dockerized Postgres: <https://hub.docker.com/_/postgres/>
 
 2. For python:
`pip install "psycopg[binary,pool]"`


Use the docs from <https://www.psycopg.org/psycopg3/docs/basic/usage.html>.

## Dataset
We'll use a shrunken version of this dataset: <https://ual.sg/project/global-streetscapes/>.

A subset of the dataset is available in the `data` folder.

  - `info.csv`: file/column descriptions
  - `gadm.csv`: administrative areas for each image
  - `ghsl.csv`: degree of urbanisation associated with the location of the image, calculated using the Global Human Settlement Layer (GHSL)
  - `instances.csv`: counts of object instances detected on each image
  - `metadata.csv`: common metadata attributes, e.g. when and where the image was taken
  - `perception.csv`: contains the 0-10 scores for each of six perceptual dimensions (Safe, Lively, Beautiful, Wealthy, Boring, Depressing)
  - `places365.csv`: contains the place/scene classification for each image.


## Exercises

### 1. Load the data into Postgres.

There are at least 3 ways how to do this:

#### 1.1. Use SQLAlchemy package:
    
```bash
uv add sqlalchemy
```

```python
from sqlalchemy import create_engine

sqlAlchemyUri = 'postgresql+psycopg://<username>:<password>@localhost:5432/lab10'
engine = create_engine(sqlAlchemyUri)

df = pd.read_csv("somedata.csv")
df.to_sql('somedata', engine)
```

#### 1.2. Dask DataFrame built-in

Use Dask's DataFrame `to_sql` method (<https://docs.dask.org/en/latest/generated/dask.dataframe.to_sql.html>). It also relies on SQLAlchemy.

#### 1.3. Use PostgreSQL `COPY FROM` and `csvkit` Python package.

First, install `csvkit`
```bash
uv add csvkit
```

Use `csvsql` to generate a `CREATE TABLE` statement:
```bash
csvsql -i postgresql somedata.csv
```

Execute the statement generated by the above line. And then execute the `COPY FROM` from Python:

```python
with open('data/places365.csv', 'r') as f:
    # Notice that we don't need the csv module.
    with cur.copy("COPY places365 FROM STDIN WITH (FORMAT CSV, header)") as copy:
        for line in f:
            copy.write(line)
```
**Also**: please compare performance of the above 3 methods on a larger (~2GB) file from the original dataset.
   
### 2. Table setup
1. Create primary keys on all tables.
2. Make `metadata` the master table, link other tables to it via foreign keys.


### 3. Some queries
1. Calculate average perception scores for:
   - `urban_term` column from `ghsl` table
   - `place` column from `places365` table
   - `country` column from `gadm` table
   
2. Create a `MATERIALIZED VIEW` (https://www.postgresql.org/docs/15/sql-creatematerializedview.html) that will contain total object instance counts (`instance.csv`) for each gadm level2 area (`gadm.csv`).
