---
title: "Big Data: Speeding up computation"
author: 
  - name: MSDE
    affiliation: Lviv University
code-fold: false
execute:
  enabled: false
  cache: true
diagram:
  cache: true
  cache-dir: ./cache
  engine:
    tikz:
      execpath: lualatex
      additional-packages: |
        \usetikzlibrary{arrows.meta}
        \usetikzlibrary{positioning}
        \usetikzlibrary{decorations.pathreplacing}
filters:
  - diagram
format: 
  revealjs:
    preview-links: auto
    slide-number: true
    theme: default
    multiplex:
      url: 'https://mplex.vitv.ly'
      secret: 'fad1947f4c744674d25d0769c690f8d7'
      id: 'ceb96efc411836bb262565b446653921d89b7ccf140a4d923b70660482df2ced'
---

# Computational improvements

## Moore's Law
:::{.callout-important icon=false}
## Definition (1965)
Number of transistors in an integrated circuit (IC) doubles about every two years. 
:::


## Moore's Law
![](img/MooresLaw)

## Moore's Law
![](img/cpu_speed_graph.jpeg)

## Moore's Law
:::{.callout-tip icon=false}
## Trends

- Gordon Moore: Moore's law will end by around 2025. 
- Nvidia CEO Jensen Huang: declared Moore's law dead in 2022.
:::

:::{.callout-note icon=false}
## Pat Gelsinger, Intel CEO, end of 2023
> We're no longer in the golden era of Moore's Law, it's much, much harder now, so we're probably doubling effectively closer to every three years now, so we've definitely seen a slowing.
:::

## Edholm's law
:::{.callout-tip icon=false}
## Definition (2004)

- three categories of telecommunication, namely 
  - wireless (mobile),
  - nomadic (wireless without mobility)
  - and wired networks (fixed),

  are in lockstep and gradually converging
- the bandwidth and data rates double every 18 months, which has proven to be true since the 1970s
:::


## Edholm's law
![](img/moore_edholm_comparison)

## Edholm's law
![](img/internet_traffic)

:::{.callout-warning icon=false}
## Data deluge

- 90\% of data humankind has produced happened in the last two years.
- 80\% of data could be unstructured
- 99\% of data produced is never analyzed
:::


# Multithreading vs multiprocessing

## Core counts
![](img/core_count)

## Concurrency vs Parallelism


:::: {.columns}

::: {.column width="50%" background-color="lightgray"}
:::{.callout-tip icon=false}
## Concurrency Parallelism
Individual steps of both tasks are executed in an *interleaved* fashion
:::

:::
::: {.column width="50%" background-color="lightgray"}
:::{.callout-note icon=false}
## Parallelism
Task statements are executed *at the same time*.
:::

:::
::::


## Process
:::{.callout-note icon=false}
## Definition
A process can be defined as an instance of a running program with its own memory. 

Alternatively: a context maintained for an executing program.

Processes have:

- lifetimes
- parents
- children.
- memory/resources allocated.
:::

## Process
![](img/process)


## Threads
:::{.callout-tip icon=false}
## Definition
A lightweight unit of execution within a process that can operate independently.

Thread is a basic unit to which the operating system allocates processor time.
  
Managed with help of **thread context**, which consists of processor context and information required for thread management.
:::

## Threads
![](img/thread_diagram)

## Green threads/Coroutines
:::{.callout-tip icon=false}
## Definition
These are threads managed by the process runtime, *multiplexed* onto OS threads.
:::

<!-- ![](img/goroutines) -->

## Comparison table
![](img/threads_vs_processes.jpg)


# Programming paradigms

## Imperative
:::{.callout-important icon=false}
## Wikipedia definition
A programming paradigm of software that uses statements that change a program's state.

Imperative program is a step-by-step description of program's algorithm.
:::
  
:::{.callout-note icon=false}
## Examples

- Fortran
- COBOL
- C
- Python
- Go
:::

## Imperative
:::{.callout-important icon=false}
## Cons

- Difficult to **parallelize**
- Lots of state management
:::

:::{.callout-tip icon=false}
## Pros

- Intuitive concept, maps well to how people think, program as a recipe.
- Easy to optimize by the compiler
:::

## Functional
:::{.callout-important icon=false}
## Wikipedia definition
A programming paradigm where programs are constructed by applying and composing functions.
:::

:::{.callout-note icon=false}
## Examples

- ML (Ocaml)
- Lisp
- Haskell
- Julia
:::

## Functional
:::{.callout-important icon=false}
## Cons

- Often non-intuitive to reason about
- Depending on a specific algorithm, might be slower
:::

:::{.callout-tip icon=false}
## Pros

- Easier to parallelize
- Lend themselves beautifully to certain types of problems
:::

## Object-oriented
:::{.callout-important icon=false}
## Wikipedia definition
A programming paradigm based on the concept of objects, which can contain data and code: data in the form of fields (often known as attributes or properties), and code in the form of procedures (often known as methods).
:::

:::{.callout-note icon=false}
## Examples

- Smalltalk
- Java
- C++
- Python
- C\#
:::

## Object-oriented
![](img/uml)

## Object-oriented
:::{.callout-important icon=false}
## Cons

- Does not map well to many problems
- Lots of state management
:::

:::{.callout-tip icon=false}
## Pros

- Intuitively easy to grasp, as human thinking is largely **noun**-oriented
- Useful for UIs
:::

## Symbolic
:::{.callout-tip icon=false}
## Wikipedia definition
A programming paradigm in which the program can manipulate its own formulas and program components as if they were plain data.
:::

:::{.callout-note icon=false}
## Examples

- Lisp
- Prolog
- Julia
:::

## Lisp
![](img/lisp_macro)

## Prolog
![](img/prolog)
:::

# Typing

## Typing
:::{.callout-tip icon=false}
## Static vs dynamic

- **static:** types are known and checked before running the program
- **dynamic:** types become known when the program is running
:::

:::{.callout-note icon=false}
## Strong vs weak

- **strong:** variable types are not changed easily
- **weak:** types can be changed by the compiler if necessary
:::

## Typing
![](img/lang_graph)
