---
title: "Big Data: Dask intro part2"
author: 
  - name: MSDE
    affiliation: Lviv University
code-fold: false
execute:
  enabled: false
  cache: true
diagram:
  cache: true
  cache-dir: ./cache
  engine:
    tikz:
      execpath: lualatex
      additional-packages: |
        \usetikzlibrary{arrows.meta}
        \usetikzlibrary{positioning}
        \usetikzlibrary{decorations.pathreplacing}
filters:
  - diagram
format: 
  revealjs:
    preview-links: auto
    slide-number: true
    theme: default
    multiplex:
      url: 'https://mplex.vitv.ly'
      secret: '4c0c95a649f66881411f51df6609036b'
      id: '0b111fe44d0bbf30998913098b95e03345ee051ede45c9fb47d29cef5ff2bb18'
---

# Dask Internals

## Dask
:::{.callout-tip icon=false}
## Parallel
We refer to algorithms that use multiple cores simultaneously as **parallel**. 
:::

:::{.callout-note icon=false}
## Out-of-core
We refer to systems that efficiently use disk as extensions
of memory as **out-of-core**.
:::

## Dask
:::{.callout-note icon=false}
## How to execute parallel code?

- represent the structure of our program explicitly as data
within the program itself
- encode task schedules programmatically within a framework
:::

## Dask
:::{.callout-important icon=false}
## Dask Graph definition

- **A Python dictionary** mapping keys to tasks or values. 
- **A key** is any Python hashable 
- **a value** is any Python object that is not a **task**
- **a task** is a Python tuple with a callable first element.
:::

## Dask
```python
def inc(i):
  return i + 1

def add(a, b):
  return a + b

x = 1
y = inc(x)
z = add(y, 10)
```

## Dask
![](img/dask_simple_dag)

## Dask
:::{.callout-tip icon=false}
## Dictionary representation
```python
d = {'x': 1,
     'y': (inc, 'x'),
     'z': (add, 'y', 10)}
```
:::

## Dask
:::{.callout-important icon=false}
## Dask computation
Dask represents a computation as a directed acyclic graph of tasks
with data dependencies. 

It can be said that Dask is a specification to encode such a
graph using ordinary Python data structures, namely dicts, tuples,
functions, and arbitrary Python values.
:::

## Dask
```python
{'x': 1,
 'y': 2,
 'z': (add, 'x', 'y'),
 'w': (sum, ['x', 'y', 'z'])}
```

:::{.callout-tip icon=false}
## Examples

- **key**: `'x'`, `('x', 2, 3)`
- **task**: `(add, 'x', 'y')`
- **task argument**: `'x'`, `1`, `(inc, 'x')`, `[1, 'x', (inc, 'x')]`
:::

## Dask
:::{.callout-tip icon=false}
## Valid tasks in a Dask graph
```python
(add, 1, 2)
(add, 'x' , 2)
(add, (inc, 'x'), 2)
(sum, [1, 2])
(sum, ['x', (inc, 'x')])
(np.dot, np.array([...]), np.array([...]))
```
:::

# Arrays

## Dask
:::{.callout-important icon=false}
## Dask Array
The `dask.array` submodule uses dask graphs to create a
NumPy-like library that uses all of your cores and operates on
datasets that do not fit in memory. 

It does this by building up a
dask graph of **blocked array algorithms**.

Dask array functions produce Array objects that hold on to Dask
graphs. These Dask graphs use several NumPy functions to achieve
the full result.
:::

## Dask
:::{.callout-note icon=false}
## Blocked Array Algorithms
Blocked algorithms compute a large result like 

- "take the sum of these trillion numbers"

with many small computations like

- "break up the trillion numbers into one million chunks of size one million",
- "sum each chunk", 
- "then sum all of the intermediate sums."

Through tricks like this we can evaluate one large problem by solving very many small problems.

Blocked algorithm organizes a computation so that it works on contiguous chunks of data.
:::

## Dask
:::{.callout-tip icon=false}
## Blocked Array Algorithms
![](img/blocked_matrix_mult){height=600}
:::

## Dask
:::{.callout-tip icon=false}
## Unblocked
```python
for i in range(N):
 for k in range(N):
   r = X[i,k]
   for j in range(N):
     Z[i,j] += r*Y[k,j]
```
:::

:::{.callout-important icon=false}
## Blocked
```python
for kk in range(N/B):
 for jj in range(N/B): 
   for i in range(N):
     for k in range(kk, min(kk+B-1, N)):
       r = X[i,k]
       for j in range(jj, min(jj+B-1,N)):
         Z[i,j] += r*Y[k,j]
```
:::

## Dask

:::{.callout-note}
It is well known that the memory hierarchy can be better utilized if scientific algorithms are blocked.

Blocking is also known as **tiling**. 

In matrix multiplication example: instead of operating on individual matrix entries, the calculation is performed on submatrices.

`B` is a **blocking factor**.
:::

## Dask
:::{.callout-important icon=false}
## Blocking features

- Blocking is a general optimization technique for increasing the effectiveness of a memory hierarchy. 
- By reusing data in the faster level of the hierarchy, it cuts down the average **access latency**.
- It also reduces the **number of references** made to slower levels of the hierarchy. 
- Blocking is superior to optimization such as **prefetching**, which hides the latency but does not reduce the memory bandwidth requirement. 
- This reduction is especially im portant for multiprocessors since memory bandwidth is often the bottleneck of the system. 
:::

## Dask
```python
import dask.array as da
x = da.arange(15, chunks=(5,))
```

![](img/dask_array_chunks)

## Dask
:::{.callout-important icon=false}
## Metadata
```python
x # Array object metadata
dask.array<x-1, shape=(15,), chunks=((5, 5, 5)), dtype=int64>
```
:::

:::{.callout-tip icon=false}
## Dask Graph
```python
x.dask # Every dask array holds a dask graph
{('x' , 0): (np.arange, 0, 5),
 ('x', 1): (np.arange, 5, 10),
 ('x' , 2): (np.arange, 10, 15)}
```
:::


## Dask
:::{.callout-tip icon=false}
## More complex graph
```python
z = (x + 100).sum()
z.dask
{('x', 0): (np.arange, 0, 5),
('x', 1): (np.arange, 5, 10),
('x', 2): (np.arange, 10, 15),
('y', 0): (add, ('x', 0), 100),
('y', 1): (add, ('x', 1), 100),
('y', 2): (add, ('x', 2), 100),
('z', 0): (np.sum, ('y', 0)),
('z', 1): (np.sum, ('y', 1)),
('z', 2): (np.sum, ('y', 2)),
('z',): (sum, [('z', 0), ('z', 1), ('z', 2)])}
```
:::

:::{.callout-note icon=false}
## Execute the Graph
```python
z.compute()
1605
```
:::


## Dask
:::{.callout-important icon=false}
## dask.array.Array objects
`x` and `z` are both `dask.array.Array` objects containing:

- Dask graph `.dask`
- array shape and chunk shape `.chunks`
- a name identifying which keys in the graph correspond
to the result, `.name`
- a dtype
:::

## Dask
:::{.callout-note icon=false}
## Chunks
    
A normal NumPy array knows its shape, a dask array must
know its shape and the shape of all of the internal NumPy blocks
that make up the larger array. 

These shapes can be concisely
described by a tuple of tuples of integers, where each internal
tuple corresponds to the lengths along a single dimension.

In the example above we have a 20 by 24 array cut into
uniform blocks of size 5 by 8. The chunks attribute describing
this array is the following:

```python
chunks = ((5, 5, 5, 5), (8, 8, 8))
```
:::


## Dask
:::{.callout-important icon=false}
## Chunks need not be uniform!
```python
x[::2].chunks
((3, 2, 3, 2), (8, 8, 8))
x[::2].T.chunks
((8, 8, 8), (3, 2, 3, 2))
```
:::


## Dask
:::{.callout-tip icon=false}
## Dask Array operations

- arithmetic and scalar math: `+`, `*`, `exp`, `log`
- reductions along axes: `sum()`, `mean()`, `std()`, `sum(axis=0)`
- tensor contractions / dot products / matrix multiplication: `tensordot`
- axis reordering / transposition: `transpose`
- slicing: `x[:100, 500:100:-2]`
- utility functions: `bincount`, `where`
:::


## Dask
:::{.callout-important icon=false}
## Ahead-of-time shape limitations
```python
x[x > 0]
```
:::

# Dask Task Scheduling

## Dask
Graph creation and graph scheduling are separate problems!

Current Dask scheduler is **dynamic**.

:::{.callout-important icon=false}
## Current Dask scheduler logic

- A worker reports that it has completed a task and that it
is ready for another.
- We update runtime state to record the finished
task, 
- mark which new tasks can be run, which data can be released,
etc. 
- We then choose a task to give to this worker from among the
set of ready-to-run tasks. This small choice governs the macroscale performance of the scheduler.
:::

## Dask
:::{.callout-tip icon=false}
## Out-of-core computation - which task to choose?

- last in, first out 
- select tasks whose data dependencies were most recently made available. 
- this causes a behavior where long chains of related tasks trigger each other
- it forces the scheduler to finish related tasks before starting new ones. 
- **implementation:** a simple stack, which can operate in constant time.
:::

## Dask
![](img/dask_custom_scheduler)

## Dask
![](img/dask_blas_comparison)


## Taskvine
:::{.callout-note icon=false}
## Description
TaskVine is a framework for building large scale data intensive dynamic workflows for:

- high performance computing (HPC) clusters
- GPU clusters
- cloud service providers
- and other distributed computing systems.

TaskVine is our third-generation workflow system, built on our twenty years of experience creating scalable applications in fields such as

- high energy physics
- bioinformatics
- molecular dynamics
- and machine learning.

:::

## Taskvine
:::{.callout-note icon=false}
## Workflow
A **workflow** is a collection of programs and files that are organized in a graph structure, allowing parts of the workflow to run in a parallel, reproducible way.

![](img/taskvine_workflow.svg)

:::

## Taskvine
:::{.callout-note icon=false}
## Steps

- A TaskVine workflow requires:
  - a **manager** 
  - and a large number of **worker** processes.
- The application generates a large number of small tasks, which are distributed to workers.
- As tasks access external data sources and produce their own outputs, more and more data is pulled into local storage on cluster nodes.
- This data is used to accelerate future tasks and avoid re-computing existing results. The application gradually grows "like a vine" through the cluster.
:::

## Taskvine
:::{.callout-note icon=false}
## Architecture
![](img/taskvine_architecture.svg)
:::


## Taskvine
:::{.callout-note icon=false}
## Features and error handling

- While an application is running, workers may be added or removed as computing resources become available. (**elasticity**)
- Newly added workers will gradually accumulate data within the cluster.
- Removed (or failed) workers are handled gracefully, and tasks will be **retried** elsewhere as needed. 
- If a worker failure results in the loss of files, tasks will be re-executed as necessary to re-create them.
:::

## Taskvine
:::{.callout-note icon=false}
## Coding it

- Individual tasks can be simple Python functions, complex Unix applications, or serverless function invocations.
- The **key idea** is that you declare file objects, and then declare tasks that consume them and produce new file objects.
  <!-- For example, this snippet draws an input file from the Project Gutenberg repository and runs a Task to search for the string "needle", producing the file output.txt: -->
```python
f = m.declare_url("https://www.gutenberg.org/cache/epub/2600/pg2600.txt")
g = m.declare_file("myoutput.txt")

t = Task("grep needle warandpeace.txt > output.txt")
t.add_input(f, "warandpeace.txt")
t.add_output(g, "output.txt")
```
:::

## Taskvine
:::{.callout-tip icon=false}
## Task options

- Each task can be labelled with the resources (CPU cores, GPU devices, memory, disk space) that it needs to execute. 
- If you don't know the resources needed, you can enable a resource monitor to automatically track, report, and allocate what each task uses.
- This allows each worker to pack the appropriate number of tasks. For example, a worker running on a 64-core machine could run 32 dual-core tasks, 16 four-core tasks, or any other combination that adds up to 64 cores. 
:::

## Taskvine
:::{.callout-note icon=false}
## As Dask Scheduler
TaskVine manager can be used as Dask scheduler:
```python
try:
    import dask
    import awkward as ak
    import dask_awkward as dak
    import numpy as np
except ImportError:
    print("You need dask, awkward, and numpy installed")
    print("(e.g. conda install -c conda-forge dask dask-awkward numpy) to run this example.")

behavior: dict = {}

@ak.mixin_class(behavior)
class Point:
    def distance(self, other):
    return np.sqrt((self.x - other.x) ** 2 + (self.y - other.y) ** 2)


if __name__ == "__main__":
    # data arrays
    points1 = ak.Array([
    [{"x": 1.0, "y": 1.1}, {"x": 2.0, "y": 2.2}, {"x": 3, "y": 3.3}],
    [],
    [{"x": 4.0, "y": 4.4}, {"x": 5.0, "y": 5.5}],
    [{"x": 6.0, "y": 6.6}],
    [{"x": 7.0, "y": 7.7}, {"x": 8.0, "y": 8.8}, {"x": 9, "y": 9.9}],
    ])

    points2 = ak.Array([
    [{"x": 0.9, "y": 1.0}, {"x": 2.0, "y": 2.2}, {"x": 2.9, "y": 3.0}],
    [],
    [{"x": 3.9, "y": 4.0}, {"x": 5.0, "y": 5.5}],
    [{"x": 5.9, "y": 6.0}],
    [{"x": 6.9, "y": 7.0}, {"x": 8.0, "y": 8.8}, {"x": 8.9, "y": 9.0}],
    ])
    array1 = dak.from_awkward(points1, npartitions=3)
    array2 = dak.from_awkward(points2, npartitions=3)

    array1 = dak.with_name(array1, name="Point", behavior=behavior)
    array2 = dak.with_name(array2, name="Point", behavior=behavior)

    distance = array1.distance(array2)

    m = vine.DaskVine(port=9123, ssl=True)
    m.set_name("test_manager")
    print(f"Listening for workers at port: {m.port}")

    f = vine.Factory(manager=m)
    f.cores = 4
    f.max_workers = 1
    f.min_workers = 1
    with f:
        with dask.config.set(scheduler=m.get):
            result = distance.compute(resources={"cores": 1}, resources_mode="max", lazy_transfers=True)
            print(f"distance = {result}")
        print("Terminating workers...", end="")
    print("done!")
```
:::

# Other Dask collections

## Dask
:::{.callout-important icon=false}
## Collections

- **dask.array** = numpy+ threading
- **dask.bag** = toolz+ multiprocessing
- **dask.dataframe** = pandas+ threading
:::

## Dask
:::{.callout-tip icon=false}
## Dask Bag - Definition
A **bag** is an unordered collection with repeats. 

It is like a Python list but does not guarantee the order of elements. 

The `dask.bag` API contains functions like **map** and **filter** and generally follows the PyToolz API. 

<!-- %We find that it is particularly useful on the front lines of data analysis, particularly in parsing and cleaning up initial data dumps like JSON or log files because it combines the streaming properties and solid performance of projects like cytoolz. -->
:::

## Dask
```python
>>> import dask.bag as db
>>> import json
>>> b = db.from_filenames('2014-*.json.gz').map(json.loads)
>>> alices = b.filter(lambda d: d['name'] == 'Alice')
>>> alices.take(3)
({'name': 'Alice', 'city': 'LA', 'balance': 100},
{'name': 'Alice', 'city': 'LA', 'balance': 200},
{'name': 'Alice', 'city': 'NYC', 'balance': 300},)

>>> dict(alices.pluck('city').frequencies())
{'LA': 10000, 'NYC': 20000, ...}
```

## Dask
:::{.callout-important icon=false}
## S3 example
```python
>>> import dask.bag as db
>>> b = db.from_s3('githubarchive-data', '2015-01-01-*.json.gz')
          .map(json.loads)
          .map(lambda d: d['type'] == 'PushEvent')
          .count()
```
:::

## Dask
  <!-- %https://blog.dask.org/2015/06/26/Complex-Graphs -->
![](img/dask_bag_graph)

## Dask
:::{.callout-note icon=false}
## Dask DataFrame - Definition
The **dask.dataframe** module implements a large dataframe
out of many Pandas DataFrames.

It uses a threaded scheduler.
:::

## Dask
:::{.callout-important icon=false}
## Partitioned datasets
The dask dataframe can compute efficiently on **partitioned datasets** where the different blocks are well separated along an index. 

For example in time series data we may know that all of
January is in one block while all of February is in another.

`Join`, `groupby`, and `range` queries along this index are significantly faster
when working on partitioned datasets.
:::

## Dask
:::{.callout-tip icon=false}
## Dask DataFrame `join`
![](img/dask_ddf_join){height=550}
:::

## Dask
:::{.callout-important icon=false}
## Out-of-core parallel SVD example
```python
>>> import dask.array as da
>>> x = da.ones((5000, 1000), chunks=(1000, 1000))
>>> u, s, v = da.svd(x)
```
Out-of-core parallel non-negative matrix factorizations on top of `dask.array`.
:::

## Dask
:::{.callout-tip icon=false}
## Out-of-core parallel SVD
![](img/dask_parallel_svd){height=600}
:::

# Usage

## Usage
![scida.io - astrophysical simulations](img/dask_scida)

## Usage
![Pangeo - open, reproducible, scalable geoscience. A global slice of Sea Water Temperature](img/pangeo_sea_water_temp)

  <!-- %https://medium.com/pangeo/using-kerchunk-with-uncompressed-netcdf-64-bit-offset-files-cloud-optimized-access-to-hycom-ocean-9008ba6d0d67 -->
  <!-- %https://www.earthdata.nasa.gov/learn/articles/pangeo-project -->

## Usage
  <!-- %\textsc{Pangeo - open, reproducible, scalable geoscience} -->
  <!-- %https://stories.dask.org/en/latest/network-modeling.html -->
![Line-Of-Sight (LOS) coverage from a lamp post in San Jose.](img/dask_wireless_modeling){height=600}

# Dask vs Spark

## Comparison with Spark
:::{.callout-important icon=false}
## Setup

- BigBrain20, a 3-D image of the human brain, **total data size** of 606 GiB. 
- dataset provided by the Consortium for Reliability and Reproducibility, entire dataset is 379.83 GiB, used all 3,491 anatomical images, representing 26.67 GiB overall.
<!-- %containing anatomical, diffusion, and functional images of 1,654 subjects acquired in 35 sites, -->
:::

## Comparison with Spark

![](img/dask_spark_graphs.png){height=600}

## Comparison with Spark
![](img/dask_spark_increment.png){height=600}

## Comparison with Spark
![](img/dask_spark_histogram.png){height=600}

