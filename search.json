[
  {
    "objectID": "applied_lab1.html",
    "href": "applied_lab1.html",
    "title": "Applied Analytics: Lab 1",
    "section": "",
    "text": "This lab is about environment setup. We will use uv, a package and project manager for Python."
  },
  {
    "objectID": "applied_lab1.html#recommended-reading",
    "href": "applied_lab1.html#recommended-reading",
    "title": "Applied Analytics: Lab 1",
    "section": "Recommended reading:",
    "text": "Recommended reading:\n\nGit Book\nGit Magic"
  },
  {
    "objectID": "applied_lab2.html",
    "href": "applied_lab2.html",
    "title": "Applied Analytics: Lab 2",
    "section": "",
    "text": "Previous lab listed some methods of optimizing Pandas work:\n\ndata storage optimizations\nCython conversion\nNumba decorators\n\nIn this lab we’ll start looking into parallelisation as a way of optimizing data analysis workflows.\nWhy - because today’s systems are multicore (sometimes very much so).\nCheck yours:\n\nimport os\nos.cpu_count()\n\n12\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are two kinds of parallelism, distributed and shared-memory. Here we’ll talk about shared-memory version.\nDistributed parallelism is when we use multiple machines/VMs for computations.\n\n\n\n\nIn functional programming paradigm, functions are first-class citizens of the languages. They can be composed, applied, passed as arguments to another functions, or returned as values.\nFunctional programming is inherently better suited for parallel programs.\nBenefits of functional programming:\n\nminimization of state\nno side-effects\npure functions are easier to reason about\nand parallelize (!)\n\nIt is based on lambda calculus. We’ll learn some of its concepts first:\n\nlambda function definition\napplication and partial application\ncurrying\nclosures\n\nFirst off, functions can be values in Python:\n\nadd = lambda x, y: x + y  \nprint (f\"add = {add}\")\n\nadd = &lt;function &lt;lambda&gt; at 0x10efab100&gt;\n\n\nAll variables support some operations on them: addition for integers, or e.g. read_csv on Pandas DataFrame.\nFunction variables support a single operation: application.\n\ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \n  \nk = i + j  \nprint (f\"k = {k}\")  \n  \nz = add (i, j)  \nprint (f\"z = {z}\")\n\nk = 1\nz = 1\n\n\nPartial function application is also supported. We’ll use functools package (https://docs.python.org/3/library/functools.html).\n\nfrom functools import partial  \ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \ninc = partial (add, 1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\")  \n\nii = 1, jj = 2\n\n\nNote: the code above both passes add as an argument to partial, and stores the return value of partial function into a new variable inc.\nThe lambda in the example above accepts two arguments essentially. This can be simplified by currying:\n\ni = 0  \nj = 1    \nadd = lambda x: lambda y: x + y  \ninc = add(1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\")\n\nii = 1, jj = 2\n\n\n\n\n\n\n\n\nTipDefinition for currying\n\n\n\nCurrying is the technique of translating a function that takes multiple arguments into a sequence of families of functions, each taking a single argument.\nAlternatively, we can say that currying is a method to transform a function of arity n to n functions of arity 1.\n\n\nWith currying, we can express partial application without and extra partial function.\n\n\n\n\n\n\nNote\n\n\n\nNote: currying relies on closures - functions that can refer to variables defined in surrounding context (like x in add definition above).\n\n\nA monstrous example:\n\ndef f_5(a, b, c, d, e):\n    return a + b + c + d + e\n\ndef c_5(a):\n    def c_4(b):\n        def c_3(c):\n            def c_2(d):\n                def c_1(e):\n                    return f_5(a, b, c, d, e)\n                return c_1\n            return c_2\n        return c_3\n    return c_4\n\nHow to use?\n\nf_5(1,2,3,4,5)\n\n15\n\n\n\nc_5(1)(2)(3)(4)(5)\n\n15\n\n\n\n\n\nIn functional programming, we do not iterate - we map. Why? Because map can be parallelised.\nIteration (like a for-loop) is sequential. Mapping is not.\nmap takes its name from a mathematical term for functions - mappings.\n\n\n\n\n\n\nWarningMap\n\n\n\nArguments:\n\na sequence to iterate on\na function to apply to each element of the sequence.\n\nReturn value:\n\na processed sequence of the same size as input\n\n\n\nNow, if we want to convert our sequence to a sequence of a different length, or different object altogether, we use reduce.\n\n\n\n\n\n\nTipReduce\n\n\n\nArguments:\n\na sequence to iterate on\naccumulation seed to start reducing on\na function of two arguments, accumulation result and next element\n\nReturn value::\n\nresult of accumulation\n\n\n\nUsing above examples, we can easily implement map using list comprehensions as as:\n\nmap = lambda f, l: [f(x) for x in l]\n\nLet’s use it:\n\nint_list = [1,2,3,4,5,6]\ninc_list = map(inc, int_list)\nprint(inc_list)\n\n[2, 3, 4, 5, 6, 7]\n\n\nAnd reduce can be implemented as:\n\ndef reduce_v1 (f, id, s):  \n    n = len(s)  \n    if n == 0:  \n        return id  \n    elif n == 1:  \n        return s[0]  \n    else:  \n        return f(reduce_v1(f, id, s[1:]), s[0])\n\nUsage example:\n\nadd = lambda x, y: x + y  \nres = reduce_v1(add, 0, int_list)\nprint(res)\n\n21\n\n\n\n\nSome tasks can be trivially parallelised - these are called embarrassingly parallel. inc is a simple example of this type.\nOther tasks have a degree of interdependence. Consider below example, adapted from the reduce above:\n\n# Sum all elements of int_list\ndef sum(lst):\n    sum = 0\n    \n    for i in range(len(lst)):\n      sum += lst[i]\n\n    return sum\n\nprint(sum(int_list))\n\n21\n\n\nHow can it be parallelised? By using a technique called chunking:\n\nchunk1 = int_list[:3]\nchunk2 = int_list[3:]\n\nsum_1 = sum(chunk1)\nsum_2 = sum(chunk2)\n\nresult = sum_1 + sum_2\n\nprint(result)\n\n21\n\n\nThen there are algorithms that are inherently serial. An example is computation of a Fibonacci sequence.\n\n\n\nSuppose we want to square each number in a list. A naive approach would be:\n\nnumber_list = [1,2,3,4]\n\ndef squarev1(lst):\n    squared = [] # sometimes pre-allocation can also work, like [None]*1000\n    for el in lst:\n        squared.append(el*el)\n    return squared\n\nsquarev1(number_list)\n\n[1, 4, 9, 16]\n\n\nOr we can use a built-in (https://docs.python.org/3/library/functions.html#map):\n\n# Note that we return a map object that has to be converted to a list first\ndef squarev2(lst):\n    return list(map(lambda x: x*x, lst))\n\nsquarev2(number_list)\n\n[1, 4, 9, 16]\n\n\n\n\n\nMap function can be made to execute in parallel on multiple CPU cores. How? We need to use multiprocessing library (https://docs.python.org/3/library/multiprocessing.html)\n\nfrom multiprocessing import Pool\n\nThere are limitations though: in order to arrange communication between proceses, Python uses pickling: a method of serializing/deserializing objects so that they can be sent between processes.\nPython cannot pickle closures, nested functions or classes! More info on pickling: https://docs.python.org/3/library/pickle.html.\nMoreover, we have to move sqr definition into a separate file (https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror)\n\nimport _files.defs as defs\n\ndef squarev3(lst):\n    pool = Pool()\n    return pool.map(defs.sqr, number_list)\n\nsquarev3(number_list)\n\n[1, 4, 9, 16]\n\n\n\n\n\n\n\n\nNotePathos map with Dill\n\n\n\nHowever, Dill (https://pypi.org/project/dill/) can overcome this limitation!\nHow to use Dill?\nuv add pathos\n\nuv add toolz\nAnd then\nfrom pathos.multiprocessing import ProcessPool\n\ndef squarev4(lst):\n    pool = ProcessPool()\n    return pool.map(lambda x: x*x, lst)\n\nsquarev4([1,3,5])\n\n\n\n\n\n\nNumpy automatically vectorizes array operations. However, we can explicitly invoke vectorize():\nIn this context vectorization refers to low-level CPU vectorization (single CPU instruction processing items in a vector)\n\nimport numpy as np\n\nnp_array = np.array(number_list)\n\nvectorized_fn = np.vectorize(lambda x: x*x)\n\nresult = vectorized_fn(np_array)\nprint(result)\n\n[ 1  4  9 16]\n\n\n\n\n\n\nWrite a version of curried function c_5 using lambda syntax.\nWrite a program calculating a factorial using parallelisation and map-reduce.\nWrite a parallelised version of reduce (hint: use chunking).\nCompare performance of Numpy vectorization vs parallel map on a really large array of integers.\nUsing computations from previous labs, apply chunking approach to df.apply(...) using parallel map."
  },
  {
    "objectID": "applied_lab2.html#short-intro-to-functional-programming",
    "href": "applied_lab2.html#short-intro-to-functional-programming",
    "title": "Applied Analytics: Lab 2",
    "section": "",
    "text": "In functional programming paradigm, functions are first-class citizens of the languages. They can be composed, applied, passed as arguments to another functions, or returned as values.\nFunctional programming is inherently better suited for parallel programs.\nBenefits of functional programming:\n\nminimization of state\nno side-effects\npure functions are easier to reason about\nand parallelize (!)\n\nIt is based on lambda calculus. We’ll learn some of its concepts first:\n\nlambda function definition\napplication and partial application\ncurrying\nclosures\n\nFirst off, functions can be values in Python:\n\nadd = lambda x, y: x + y  \nprint (f\"add = {add}\")\n\nadd = &lt;function &lt;lambda&gt; at 0x10efab100&gt;\n\n\nAll variables support some operations on them: addition for integers, or e.g. read_csv on Pandas DataFrame.\nFunction variables support a single operation: application.\n\ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \n  \nk = i + j  \nprint (f\"k = {k}\")  \n  \nz = add (i, j)  \nprint (f\"z = {z}\")\n\nk = 1\nz = 1\n\n\nPartial function application is also supported. We’ll use functools package (https://docs.python.org/3/library/functools.html).\n\nfrom functools import partial  \ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \ninc = partial (add, 1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\")  \n\nii = 1, jj = 2\n\n\nNote: the code above both passes add as an argument to partial, and stores the return value of partial function into a new variable inc.\nThe lambda in the example above accepts two arguments essentially. This can be simplified by currying:\n\ni = 0  \nj = 1    \nadd = lambda x: lambda y: x + y  \ninc = add(1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\")\n\nii = 1, jj = 2\n\n\n\n\n\n\n\n\nTipDefinition for currying\n\n\n\nCurrying is the technique of translating a function that takes multiple arguments into a sequence of families of functions, each taking a single argument.\nAlternatively, we can say that currying is a method to transform a function of arity n to n functions of arity 1.\n\n\nWith currying, we can express partial application without and extra partial function.\n\n\n\n\n\n\nNote\n\n\n\nNote: currying relies on closures - functions that can refer to variables defined in surrounding context (like x in add definition above).\n\n\nA monstrous example:\n\ndef f_5(a, b, c, d, e):\n    return a + b + c + d + e\n\ndef c_5(a):\n    def c_4(b):\n        def c_3(c):\n            def c_2(d):\n                def c_1(e):\n                    return f_5(a, b, c, d, e)\n                return c_1\n            return c_2\n        return c_3\n    return c_4\n\nHow to use?\n\nf_5(1,2,3,4,5)\n\n15\n\n\n\nc_5(1)(2)(3)(4)(5)\n\n15"
  },
  {
    "objectID": "applied_lab2.html#map-reduce",
    "href": "applied_lab2.html#map-reduce",
    "title": "Applied Analytics: Lab 2",
    "section": "",
    "text": "In functional programming, we do not iterate - we map. Why? Because map can be parallelised.\nIteration (like a for-loop) is sequential. Mapping is not.\nmap takes its name from a mathematical term for functions - mappings.\n\n\n\n\n\n\nWarningMap\n\n\n\nArguments:\n\na sequence to iterate on\na function to apply to each element of the sequence.\n\nReturn value:\n\na processed sequence of the same size as input\n\n\n\nNow, if we want to convert our sequence to a sequence of a different length, or different object altogether, we use reduce.\n\n\n\n\n\n\nTipReduce\n\n\n\nArguments:\n\na sequence to iterate on\naccumulation seed to start reducing on\na function of two arguments, accumulation result and next element\n\nReturn value::\n\nresult of accumulation\n\n\n\nUsing above examples, we can easily implement map using list comprehensions as as:\n\nmap = lambda f, l: [f(x) for x in l]\n\nLet’s use it:\n\nint_list = [1,2,3,4,5,6]\ninc_list = map(inc, int_list)\nprint(inc_list)\n\n[2, 3, 4, 5, 6, 7]\n\n\nAnd reduce can be implemented as:\n\ndef reduce_v1 (f, id, s):  \n    n = len(s)  \n    if n == 0:  \n        return id  \n    elif n == 1:  \n        return s[0]  \n    else:  \n        return f(reduce_v1(f, id, s[1:]), s[0])\n\nUsage example:\n\nadd = lambda x, y: x + y  \nres = reduce_v1(add, 0, int_list)\nprint(res)\n\n21\n\n\n\n\nSome tasks can be trivially parallelised - these are called embarrassingly parallel. inc is a simple example of this type.\nOther tasks have a degree of interdependence. Consider below example, adapted from the reduce above:\n\n# Sum all elements of int_list\ndef sum(lst):\n    sum = 0\n    \n    for i in range(len(lst)):\n      sum += lst[i]\n\n    return sum\n\nprint(sum(int_list))\n\n21\n\n\nHow can it be parallelised? By using a technique called chunking:\n\nchunk1 = int_list[:3]\nchunk2 = int_list[3:]\n\nsum_1 = sum(chunk1)\nsum_2 = sum(chunk2)\n\nresult = sum_1 + sum_2\n\nprint(result)\n\n21\n\n\nThen there are algorithms that are inherently serial. An example is computation of a Fibonacci sequence.\n\n\n\nSuppose we want to square each number in a list. A naive approach would be:\n\nnumber_list = [1,2,3,4]\n\ndef squarev1(lst):\n    squared = [] # sometimes pre-allocation can also work, like [None]*1000\n    for el in lst:\n        squared.append(el*el)\n    return squared\n\nsquarev1(number_list)\n\n[1, 4, 9, 16]\n\n\nOr we can use a built-in (https://docs.python.org/3/library/functions.html#map):\n\n# Note that we return a map object that has to be converted to a list first\ndef squarev2(lst):\n    return list(map(lambda x: x*x, lst))\n\nsquarev2(number_list)\n\n[1, 4, 9, 16]\n\n\n\n\n\nMap function can be made to execute in parallel on multiple CPU cores. How? We need to use multiprocessing library (https://docs.python.org/3/library/multiprocessing.html)\n\nfrom multiprocessing import Pool\n\nThere are limitations though: in order to arrange communication between proceses, Python uses pickling: a method of serializing/deserializing objects so that they can be sent between processes.\nPython cannot pickle closures, nested functions or classes! More info on pickling: https://docs.python.org/3/library/pickle.html.\nMoreover, we have to move sqr definition into a separate file (https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror)\n\nimport _files.defs as defs\n\ndef squarev3(lst):\n    pool = Pool()\n    return pool.map(defs.sqr, number_list)\n\nsquarev3(number_list)\n\n[1, 4, 9, 16]\n\n\n\n\n\n\n\n\nNotePathos map with Dill\n\n\n\nHowever, Dill (https://pypi.org/project/dill/) can overcome this limitation!\nHow to use Dill?\nuv add pathos\n\nuv add toolz\nAnd then\nfrom pathos.multiprocessing import ProcessPool\n\ndef squarev4(lst):\n    pool = ProcessPool()\n    return pool.map(lambda x: x*x, lst)\n\nsquarev4([1,3,5])"
  },
  {
    "objectID": "applied_lab2.html#numpy-vectorization",
    "href": "applied_lab2.html#numpy-vectorization",
    "title": "Applied Analytics: Lab 2",
    "section": "",
    "text": "Numpy automatically vectorizes array operations. However, we can explicitly invoke vectorize():\nIn this context vectorization refers to low-level CPU vectorization (single CPU instruction processing items in a vector)\n\nimport numpy as np\n\nnp_array = np.array(number_list)\n\nvectorized_fn = np.vectorize(lambda x: x*x)\n\nresult = vectorized_fn(np_array)\nprint(result)\n\n[ 1  4  9 16]"
  },
  {
    "objectID": "applied_lab2.html#exercises",
    "href": "applied_lab2.html#exercises",
    "title": "Applied Analytics: Lab 2",
    "section": "",
    "text": "Write a version of curried function c_5 using lambda syntax.\nWrite a program calculating a factorial using parallelisation and map-reduce.\nWrite a parallelised version of reduce (hint: use chunking).\nCompare performance of Numpy vectorization vs parallel map on a really large array of integers.\nUsing computations from previous labs, apply chunking approach to df.apply(...) using parallel map."
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-1",
    "href": "applied_lec3.html#container-networking-basics-1",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nObjectives\n\n\nWe will now run network services (accepting requests) in containers.\nAt the end of this section, you will be able to:\n\nRun a network service in a container.\nConnect to that network service.\nFind a container’s IP address."
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-2",
    "href": "applied_lec3.html#container-networking-basics-2",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nDefinitions\n\n\n\n\n\n\n\n\n\nKeyword\nDescription\n\n\n\n\nVM\nA virtual machine (VM) is a software simulation of a physical computer that runs on a host computer. It provides a separate operating system and resources, allowing multiple operating systems to run on a single physical machine.\n\n\nCluster\nA cluster is a group of connected servers that work together as a single system to provide high availability, scalability, and increased performance for applications. The nodes in a cluster are connected through a network and share resources to provide a unified, highly available solution.\n\n\nNode\nA cluster node is a single server within a cluster computing system. It provides computing resources and works together with other nodes to perform tasks as a unified system, providing high availability and scalability for applications."
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-3",
    "href": "applied_lec3.html#container-networking-basics-3",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nDefinitions\n\n\n\n\n\n\n\n\n\nKeyword\nDescription\n\n\n\n\nNetwork\nA network is a group of interconnected devices that can exchange data and information. Networks can be used to connect computers, servers, mobile devices, and other types of devices and allow them to communicate with each other and share resources, such as printers and storage. More specifically in our case, these are physical and software-defined communication paths between individual nodes of a cluster and programs running on those nodes.\n\n\nPort\nA port is a communication endpoint in a network-attached device, such as a computer or server. It allows the device to receive and send data to other devices on the network through a specific network protocol, such as TCP or UDP. Each port has a unique number that is used to identify it, and different services and applications use specific ports to communicate.\n\n\nService\nA piece of software that implements a limited set of functionalities that are then used by other parts of the application."
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-4",
    "href": "applied_lec3.html#container-networking-basics-4",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nRunning a very simple service\n\n\n\nWe need something small, simple, easy to configure\n(or, even better, that doesn’t require any configuration at all)\nLet’s use the official NGINX image (named nginx)\nIt runs a static web server listening on port 80\nIt serves a default “Welcome to nginx!” page"
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-5",
    "href": "applied_lec3.html#container-networking-basics-5",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nRunning an NGINX server\n\n\n$ docker run -d -P nginx\n66b1ce719198711292c8f34f84a7b68c3876cf9f67015e752b94e189d35a204e\n\nDocker will automatically pull the nginx image from the Docker Hub\n-d / --detach tells Docker to run it in the background\nP / --publish-all tells Docker to publish all ports\n(publish = make them reachable from other computers)\n…OK, how do we connect to our web server now?"
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-6",
    "href": "applied_lec3.html#container-networking-basics-6",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nFinding our web server port\n\n\n\nFirst, we need to find the port number used by Docker\n(the NGINX container listens on port 80, but this port will be mapped)\nWe can use docker ps:\n$ docker ps\nCONTAINER ID  IMAGE  ...  PORTS                  ...\ne40ffb406c9e  nginx  ...  0.0.0.0:`12345`-&gt;80/tcp  ...\nThis means:\nport 12345 on the Docker host is mapped to port 80 in the container\nNow we need to connect to the Docker host!"
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-7",
    "href": "applied_lec3.html#container-networking-basics-7",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nFinding the address of the Docker host\n\n\n\nWhen running Docker on your Linux workstation:\nuse localhost, or any IP address of your machine\nWhen running Docker on a remote Linux server:\nuse any IP address of the remote machine\nWhen running Docker Desktop on Mac or Windows:\nuse localhost\nIn other scenarios (docker-machine, local VM…):\nuse the IP address of the Docker VM"
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-8",
    "href": "applied_lec3.html#container-networking-basics-8",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nConnecting to our web server (GUI)\n\n\nPoint your browser to the IP address of your Docker host, on the port shown by docker ps for container port 80.\n\n\n\nScreenshot"
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-9",
    "href": "applied_lec3.html#container-networking-basics-9",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nConnecting to our web server (CLI)\n\n\nYou can also use curl directly from the Docker host.\nMake sure to use the right port number if it is different from the example below:\n$ curl localhost:12345\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n..."
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-10",
    "href": "applied_lec3.html#container-networking-basics-10",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nHow does Docker know which port to map?\n\n\n\nThere is metadata in the image telling “this image has something on port 80”.\nWe can see that metadata with docker inspect:\n\n$ docker inspect --format '{{.Config.ExposedPorts}}' nginx\nmap[80/tcp:{}]\n\nThis metadata was set in the Dockerfile, with the EXPOSE keyword.\nWe can see that with docker history:\n\n$ docker history nginx\nIMAGE               CREATED             CREATED BY\n7f70b30f2cc6        11 days ago         /bin/sh -c #(nop)  CMD [\"nginx\" \"-g\" \"…\n&lt;missing&gt;           11 days ago         /bin/sh -c #(nop)  STOPSIGNAL [SIGTERM]\n&lt;missing&gt;           11 days ago         /bin/sh -c #(nop)  EXPOSE 80/tcp"
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-11",
    "href": "applied_lec3.html#container-networking-basics-11",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nWhy can’t we just connect to port 80?\n\n\n\nOur Docker host has only one port 80\nTherefore, we can only have one container at a time on port 80\nTherefore, if multiple containers want port 80, only one can get it\nBy default, containers do not get “their” port number, but a random one\n(not “random” as “crypto random”, but as “it depends on various factors”)\nWe’ll see later how to force a port number (including port 80!)"
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-12",
    "href": "applied_lec3.html#container-networking-basics-12",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nUsing multiple IP addresses\n\n\nHey, my network-fu is strong, and I have questions…\n\nCan I publish one container on 127.0.0.2:80, and another on 127.0.0.3:80?\nMy machine has multiple (public) IP addresses, let’s say A.A.A.A and B.B.B.B.  Can I have one container on A.A.A.A:80 and another on B.B.B.B:80?\nI have a whole IPV4 subnet, can I allocate it to my containers?\nWhat about IPV6?\n\nYou can do all these things when running Docker directly on Linux.\n(On other platforms, generally not, but there are some exceptions.)"
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-13",
    "href": "applied_lec3.html#container-networking-basics-13",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nFinding the web server port in a script\n\n\nParsing the output of docker ps would be painful.\nThere is a command to help us:\n$ docker port &lt;containerID&gt; 80\n0.0.0.0:12345"
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-14",
    "href": "applied_lec3.html#container-networking-basics-14",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nManual allocation of port numbers\n\n\nIf you want to set port numbers yourself, no problem:\n$ docker run -d -p 80:80 nginx\n$ docker run -d -p 8000:80 nginx\n$ docker run -d -p 8080:80 -p 8888:80 nginx\n\nWe are running three NGINX web servers.\nThe first one is exposed on port 80.\nThe second one is exposed on port 8000.\nThe third one is exposed on ports 8080 and 8888.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe convention is port-on-host:port-on-container."
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-15",
    "href": "applied_lec3.html#container-networking-basics-15",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nPlumbing containers into your infrastructure\n\n\nThere are many ways to integrate containers in your network.\n\nStart the container, letting Docker allocate a public port for it. Then retrieve that port number and feed it to your configuration.\nPick a fixed port number in advance, when you generate your configuration. Then start your container by setting the port numbers manually.\nUse an orchestrator like Kubernetes or Swarm. The orchestrator will provide its own networking facilities.\n\nOrchestrators typically provide mechanisms to enable direct container-to-container communication across hosts, and publishing/load balancing for inbound traffic."
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-16",
    "href": "applied_lec3.html#container-networking-basics-16",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nFinding the container’s IP address\n\n\nWe can use the docker inspect command to find the IP address of the container.\n$ docker inspect --format '{{ .NetworkSettings.IPAddress }}' &lt;yourContainerID&gt;\n172.17.0.3\n\ndocker inspect is an advanced command, that can retrieve a ton of information about our containers.\nHere, we provide it with a format string to extract exactly the private IP address of the container."
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-17",
    "href": "applied_lec3.html#container-networking-basics-17",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nPinging our container\n\n\nLet’s try to ping our container from another container.\ndocker run alpine ping `&lt;ipaddress&gt;`\nPING 172.17.0.X (172.17.0.X): 56 data bytes\n64 bytes from 172.17.0.X: seq=0 ttl=64 time=0.106 ms\n64 bytes from 172.17.0.X: seq=1 ttl=64 time=0.250 ms\n64 bytes from 172.17.0.X: seq=2 ttl=64 time=0.188 ms\nWhen running on Linux, we can even ping that IP address directly!\n(And connect to a container’s ports even if they aren’t published.)"
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-18",
    "href": "applied_lec3.html#container-networking-basics-18",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nHow often do we use -p and -P ?\n\n\n\nWhen running a stack of containers, we will often use Compose\nCompose will take care of exposing containers\n(through a ports: section in the docker-compose.yml file)\nIt is, however, fairly common to use docker run -P for a quick test\nOr docker run -p ... when an image doesn’t EXPOSE a port correctly"
  },
  {
    "objectID": "applied_lec3.html#container-networking-basics-19",
    "href": "applied_lec3.html#container-networking-basics-19",
    "title": "Docker Networking",
    "section": "Container networking basics",
    "text": "Container networking basics\n\n\n\nSection summary\n\n\nWe’ve learned how to:\n\nExpose a network port.\nConnect to an application running in a container.\nFind a container’s IP address."
  },
  {
    "objectID": "applied_lec3.html#container-network-drivers-1",
    "href": "applied_lec3.html#container-network-drivers-1",
    "title": "Docker Networking",
    "section": "Container network drivers",
    "text": "Container network drivers\n\n\n\nOverview\n\n\nThe Docker Engine supports different network drivers.\nThe built-in drivers include:\n\nbridge (default)\nnull (for the special network called none)\nhost (for the special network called host)\ncontainer (that one is a bit magic!)\n\nThe network is selected with docker run --net ....\nEach network is managed by a driver.\nThe different drivers are explained with more details on the following slides."
  },
  {
    "objectID": "applied_lec3.html#container-network-drivers-2",
    "href": "applied_lec3.html#container-network-drivers-2",
    "title": "Docker Networking",
    "section": "Container network drivers",
    "text": "Container network drivers\n\n\n\nThe default bridge\n\n\n\nBy default, the container gets a virtual eth0 interface. (In addition to its own private lo loopback interface.)\nThat interface is provided by a veth pair.\nIt is connected to the Docker bridge. (Named docker0 by default; configurable with --bridge.)\nAddresses are allocated on a private, internal subnet. (Docker uses 172.17.0.0/16 by default; configurable with --bip.)\nOutbound traffic goes through an iptables MASQUERADE rule.\nInbound traffic goes through an iptables DNAT rule.\nThe container can have its own routes, iptables rules, etc."
  },
  {
    "objectID": "applied_lec3.html#container-network-drivers-3",
    "href": "applied_lec3.html#container-network-drivers-3",
    "title": "Docker Networking",
    "section": "Container network drivers",
    "text": "Container network drivers\n\n\n\nThe null driver\n\n\n\nContainer is started with docker run --net none ...\nIt only gets the lo loopback interface. No eth0.\nIt can’t send or receive network traffic.\nUseful for isolated/untrusted workloads."
  },
  {
    "objectID": "applied_lec3.html#container-network-drivers-4",
    "href": "applied_lec3.html#container-network-drivers-4",
    "title": "Docker Networking",
    "section": "Container network drivers",
    "text": "Container network drivers\n\n\n\nThe host driver\n\n\n\nContainer is started with docker run --net host ...\nIt sees (and can access) the network interfaces of the host.\nIt can bind any address, any port (for ill and for good).\nNetwork traffic doesn’t have to go through NAT, bridge, or veth.\nPerformance = native!\n\nUse cases:\n\nPerformance sensitive applications (VOIP, gaming, streaming…)\nPeer discovery (e.g. Erlang port mapper, Raft, Serf…)"
  },
  {
    "objectID": "applied_lec3.html#container-network-drivers-5",
    "href": "applied_lec3.html#container-network-drivers-5",
    "title": "Docker Networking",
    "section": "Container network drivers",
    "text": "Container network drivers\n\n\n\nThe container driver\n\n\n\nContainer is started with docker run --net container:id ...\nIt re-uses the network stack of another container.\nIt shares with this other container the same interfaces, IP address(es), routes, iptables rules, etc.\nThose containers can communicate over their lo interface. (i.e. one can bind to 127.0.0.1 and the others can connect to it.)"
  },
  {
    "objectID": "applied_lec3.html#the-container-network-model-1",
    "href": "applied_lec3.html#the-container-network-model-1",
    "title": "Docker Networking",
    "section": "The Container Network Model",
    "text": "The Container Network Model\n\n\n\nObjectives\n\n\nWe will learn about the CNM (Container Network Model).\nAt the end of this lesson, you will be able to:\n\nCreate a private network for a group of containers.\nUse container naming to connect services together.\nDynamically connect and disconnect containers to networks.\nSet the IP address of a container.\n\nWe will also explain the principle of overlay networks and network plugins."
  },
  {
    "objectID": "applied_lec3.html#the-container-network-model-2",
    "href": "applied_lec3.html#the-container-network-model-2",
    "title": "Docker Networking",
    "section": "The Container Network Model",
    "text": "The Container Network Model\n\n\n\nThe Container Network Model\n\n\nDocker has “networks”.\nWe can manage them with the docker network commands; for instance:\n$ docker network ls\nNETWORK ID          NAME                DRIVER\n6bde79dfcf70        bridge              bridge\n8d9c78725538        none                null\neb0eeab782f4        host                host\n4c1ff84d6d3f        blog-dev            overlay\n228a4355d548        blog-prod           overlay\nNew networks can be created (with docker network create).\n(Note: networks none and host are special; let’s set them aside for now.)"
  },
  {
    "objectID": "applied_lec3.html#the-container-network-model-4",
    "href": "applied_lec3.html#the-container-network-model-4",
    "title": "Docker Networking",
    "section": "The Container Network Model",
    "text": "The Container Network Model\n\n\n\nWhat’s a network?\n\n\n\nConceptually, a Docker “network” is a virtual switch\n(we can also think about it like a VLAN, or a WiFi SSID, for instance)\nBy default, containers are connected to a single network\n(but they can be connected to zero, or many networks, even dynamically)\nEach network has its own subnet (IP address range)\nA network can be local (to a single Docker Engine) or global (span multiple hosts)\nContainers can have network aliases providing DNS-based service discovery\n(and each network has its own “domain”, “zone”, or “scope”)"
  },
  {
    "objectID": "applied_lec3.html#the-container-network-model-5",
    "href": "applied_lec3.html#the-container-network-model-5",
    "title": "Docker Networking",
    "section": "The Container Network Model",
    "text": "The Container Network Model\n\n\n\nService discovery\n\n\n\nA container can be given a network alias\n(e.g. with docker run --net some-network --net-alias db ...)\nThe containers running in the same network can resolve that network alias\n(i.e. if they do a DNS lookup on db, it will give the container’s address)\nWe can have a different db container in each network\n(this avoids naming conflicts between different stacks)\nWhen we name a container, it automatically adds the name as a network alias\n(i.e. docker run --name xyz ... is like docker run --net-alias xyz ..."
  },
  {
    "objectID": "applied_lec3.html#the-container-network-model-6",
    "href": "applied_lec3.html#the-container-network-model-6",
    "title": "Docker Networking",
    "section": "The Container Network Model",
    "text": "The Container Network Model\n\n\n\nNetwork isolation\n\n\n\nNetworks are isolated\nBy default, containers in network A cannot reach those in network B\nA container connected to both networks A and B can act as a router or proxy\nPublished ports are always reachable through the Docker host address\n(docker run -P ... makes a container port available to everyone)"
  },
  {
    "objectID": "applied_lec3.html#the-container-network-model-7",
    "href": "applied_lec3.html#the-container-network-model-7",
    "title": "Docker Networking",
    "section": "The Container Network Model",
    "text": "The Container Network Model\n\n\n\nHow to use networks\n\n\n\nWe typically create one network per “stack” or app that we deploy\nMore complex apps or stacks might require multiple networks\n(e.g. frontend, backend, …)\nNetworks allow us to deploy multiple copies of the same stack\n(e.g. prod, dev, pr-442, ….)\nIf we use Docker Compose, this is managed automatically for us"
  },
  {
    "objectID": "applied_lec3.html#the-container-network-model-8",
    "href": "applied_lec3.html#the-container-network-model-8",
    "title": "Docker Networking",
    "section": "The Container Network Model",
    "text": "The Container Network Model\n\nMultiple containers on the default bridge network, on a Linux machine"
  },
  {
    "objectID": "applied_lec3.html#the-container-network-model-9",
    "href": "applied_lec3.html#the-container-network-model-9",
    "title": "Docker Networking",
    "section": "The Container Network Model",
    "text": "The Container Network Model\n\nMultiple containers in multiple bridge networks, on a Linux machine"
  },
  {
    "objectID": "applied_lec3.html#the-container-network-model-10",
    "href": "applied_lec3.html#the-container-network-model-10",
    "title": "Docker Networking",
    "section": "The Container Network Model",
    "text": "The Container Network Model\n\nMultiple containers in multiple bridge networks, on a Mac/Windows machine"
  },
  {
    "objectID": "applied_lec3.html#the-container-network-model-11",
    "href": "applied_lec3.html#the-container-network-model-11",
    "title": "Docker Networking",
    "section": "The Container Network Model",
    "text": "The Container Network Model\n\n\n\nCNM vs CNI\n\n\n\nCNM is the model used by Docker\nKubernetes uses a different model, architectured around CNI\n(CNI is a kind of API between a container engine and CNI plugins)\nDocker model:\n\nmultiple isolated networks\nper-network service discovery\nnetwork interconnection requires extra steps\n\nKubernetes model:\n\nsingle flat network\nper-namespace service discovery\nnetwork isolation requires extra steps (Network Policies)"
  },
  {
    "objectID": "applied_lec3.html#the-container-network-model-12",
    "href": "applied_lec3.html#the-container-network-model-12",
    "title": "Docker Networking",
    "section": "The Container Network Model",
    "text": "The Container Network Model\n\n\n\nCreating a network\n\n\nLet’s create a network called dev.\n$ docker network create dev\n4c1ff84d6d3f1733d3e233ee039cac276f425a9d5228a4355d54878293a889ba\nThe network is now visible with the network ls command:\n$ docker network ls\nNETWORK ID          NAME                DRIVER\n6bde79dfcf70        bridge              bridge\n8d9c78725538        none                null\neb0eeab782f4        host                host\n4c1ff84d6d3f        dev                 bridge"
  },
  {
    "objectID": "applied_lec3.html#the-container-network-model-13",
    "href": "applied_lec3.html#the-container-network-model-13",
    "title": "Docker Networking",
    "section": "The Container Network Model",
    "text": "The Container Network Model\n\n\n\nPlacing containers on a network\n\n\nWe will create a named container on this network.\nIt will be reachable with its name, es.\n$ docker run -d --name es --net dev elasticsearch:2\n8abb80e229ce8926c7223beb69699f5f34d6f1d438bfc5682db893e798046863"
  },
  {
    "objectID": "applied_lec3.html#the-container-network-model-14",
    "href": "applied_lec3.html#the-container-network-model-14",
    "title": "Docker Networking",
    "section": "The Container Network Model",
    "text": "The Container Network Model\n\n\n\nCommunication between containers\n\n\nNow, create another container on this network.\n$ docker run -ti --net dev alpine sh\nroot@0ecccdfa45ef:/#\nFrom this new container, we can resolve and ping the other one, using its assigned name:\n/ # ping es\nPING es (172.18.0.2) 56(84) bytes of data.\n64 bytes from es.dev (172.18.0.2): icmp_seq=1 ttl=64 time=0.221 ms\n64 bytes from es.dev (172.18.0.2): icmp_seq=2 ttl=64 time=0.114 ms\n64 bytes from es.dev (172.18.0.2): icmp_seq=3 ttl=64 time=0.114 ms\n^C\n--- es ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2000ms\nrtt min/avg/max/mdev = 0.114/0.149/0.221/0.052 ms\nroot@0ecccdfa45ef:/#"
  },
  {
    "objectID": "applied_lec3.html#the-container-network-model-15",
    "href": "applied_lec3.html#the-container-network-model-15",
    "title": "Docker Networking",
    "section": "The Container Network Model",
    "text": "The Container Network Model\n\n\n\nResolving container addresses\n\n\nSince Docker Engine 1.10, name resolution is implemented by a dynamic resolver.\nArcheological note: when CNM was introduced (in Docker Engine 1.9, November 2015) name resolution was implemented with /etc/hosts, and it was updated each time CONTAINERs were added/removed. This could cause interesting race conditions since /etc/hosts was a bind-mount (and couldn’t be updated atomically).\n[root@0ecccdfa45ef /]# cat /etc/hosts\n172.18.0.3  0ecccdfa45ef\n127.0.0.1       localhost\n::1     localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n172.18.0.2      es\n172.18.0.2      es.dev"
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-1",
    "href": "applied_lec3.html#service-discovery-with-containers-1",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nOverview\n\n\n\nLet’s try to run an application that requires two containers.\nThe first container is a web server.\nThe other one is a redis data store.\nWe will place them both on the dev network created before."
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-2",
    "href": "applied_lec3.html#service-discovery-with-containers-2",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nRunning the web server\n\n\n\nThe application is provided by the container image jpetazzo/trainingwheels.\nWe don’t know much about it so we will try to run it and see what happens!\n\nStart the container, exposing all its ports:\n$ docker run --net dev -d -P jpetazzo/trainingwheels\nCheck the port that has been allocated to it:\n$ docker ps -l"
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-3",
    "href": "applied_lec3.html#service-discovery-with-containers-3",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nTest the web server\n\n\n\nIf we connect to the application now, we will see an error page:\n\n\n\n\nTrainingwheels error\n\n\n\nThis is because the Redis service is not running.\nThis container tries to resolve the name redis.\n\nNote: we’re not using a FQDN or an IP address here; just redis."
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-4",
    "href": "applied_lec3.html#service-discovery-with-containers-4",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nStart the data store\n\n\n\nWe need to start a Redis container.\nThat container must be on the same network as the web server.\nIt must have the right network alias (redis) so the application can find it.\n\nStart the container:\n$ docker run --net dev --net-alias redis -d redis"
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-5",
    "href": "applied_lec3.html#service-discovery-with-containers-5",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nTest the web server again\n\n\n\nIf we connect to the application now, we should see that the app is working correctly:\n\n\n\n\nTrainingwheels OK\n\n\n\nWhen the app tries to resolve redis, instead of getting a DNS error, it gets the IP address of our Redis container."
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-6",
    "href": "applied_lec3.html#service-discovery-with-containers-6",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nA few words on scope\n\n\n\nContainer names are unique (there can be only one --name redis)\nNetwork aliases are not unique\nWe can have the same network alias in different networks:\ndocker run --net dev --net-alias redis ...\ndocker run --net prod --net-alias redis ...\nWe can even have multiple containers with the same alias in the same network\n(in that case, we get multiple DNS entries, aka “DNS round robin”)"
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-7",
    "href": "applied_lec3.html#service-discovery-with-containers-7",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nNames are local to each network\n\n\nLet’s try to ping our es container from another container, when that other container is not on the dev network.\n$ docker run --rm alpine ping es\nping: bad address 'es'\nNames can be resolved only when containers are on the same network.\nContainers can contact each other only when they are on the same network (you can try to ping using the IP address to verify)."
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-8",
    "href": "applied_lec3.html#service-discovery-with-containers-8",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nNetwork aliases\n\n\n\nWe would like to have another network, prod, with its own es container. But there can be only one container named es!\nWe will use network aliases.\nA container can have multiple network aliases.\nNetwork aliases are local to a given network (only exist in this network).\nMultiple containers can have the same network alias (even on the same network).\nSince Docker Engine 1.11, resolving a network alias yields the IP addresses of all containers holding this alias."
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-9",
    "href": "applied_lec3.html#service-discovery-with-containers-9",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nCreating containers on another network\n\n\nCreate the prod network.\n$ docker network create prod\n5a41562fecf2d8f115bedc16865f7336232a04268bdf2bd816aecca01b68d50c\nWe can now create multiple containers with the es alias on the new prod network.\n$ docker run -d --name prod-es-1 --net-alias es --net prod elasticsearch:2\n38079d21caf0c5533a391700d9e9e920724e89200083df73211081c8a356d771\n$ docker run -d --name prod-es-2 --net-alias es --net prod elasticsearch:2\n1820087a9c600f43159688050dcc164c298183e1d2e62d5694fd46b10ac3bc3d"
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-10",
    "href": "applied_lec3.html#service-discovery-with-containers-10",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nResolving network aliases\n\n\nLet’s try DNS resolution first, using the nslookup tool that ships with the alpine image.\n$ docker run --net prod --rm alpine nslookup es\nName:      es\nAddress 1: 172.23.0.3 prod-es-2.prod\nAddress 2: 172.23.0.2 prod-es-1.prod\n(You can ignore the can't resolve '(null)' errors.)"
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-11",
    "href": "applied_lec3.html#service-discovery-with-containers-11",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nConnecting to aliased containers\n\n\nEach ElasticSearch instance has a name (generated when it is started). This name can be seen when we issue a simple HTTP request on the ElasticSearch API endpoint.\nTry the following command a few times:\n$ docker run --rm --net dev centos curl -s es:9200\n{\n  \"name\" : \"Tarot\",\n...\n}\nThen try it a few times by replacing --net dev with --net prod:\n$ docker run --rm --net prod centos curl -s es:9200\n{\n  \"name\" : \"The Symbiote\",\n...\n}"
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-12",
    "href": "applied_lec3.html#service-discovery-with-containers-12",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nGood to know …\n\n\n\nDocker will not create network names and aliases on the default bridge network.\nTherefore, if you want to use those features, you have to create a custom network first.\nNetwork aliases are not unique on a given network.\ni.e., multiple containers can have the same alias on the same network.\nIn that scenario, the Docker DNS server will return multiple records.  (i.e. you will get DNS round robin out of the box.)\nEnabling Swarm Mode gives access to clustering and load balancing with IPVS.\nCreation of networks and network aliases is generally automated with tools like Compose."
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-13",
    "href": "applied_lec3.html#service-discovery-with-containers-13",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nCustom networks\n\n\nWhen creating a network, extra options can be provided.\n\n--internal disables outbound traffic (the network won’t have a default gateway).\n--gateway indicates which address to use for the gateway (when outbound traffic is allowed).\n--subnet (in CIDR notation) indicates the subnet to use.\n--ip-range (in CIDR notation) indicates the subnet to allocate from.\n--aux-address allows specifying a list of reserved addresses (which won’t be allocated to containers)."
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-14",
    "href": "applied_lec3.html#service-discovery-with-containers-14",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nSetting containers’ IP address\n\n\n\nIt is possible to set a container’s address with --ip.\nThe IP address has to be within the subnet used for the container.\n\nA full example would look like this.\n$ docker network create --subnet 10.66.0.0/16 pubnet\n42fb16ec412383db6289a3e39c3c0224f395d7f85bcb1859b279e7a564d4e135\n$ docker run --net pubnet --ip 10.66.66.66 -d nginx\nb2887adeb5578a01fd9c55c435cad56bbbe802350711d2743691f95743680b09\n\n\n\n\n\n\n\n\n\nImportant\n\n\nDon’t hard code container IP addresses in your code!"
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-15",
    "href": "applied_lec3.html#service-discovery-with-containers-15",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nConnecting and disconnecting dynamically\n\n\n\nSo far, we have specified which network to use when starting the container.\nThe Docker Engine also allows connecting and disconnecting while the container is running.\nThis feature is exposed through the Docker API, and through two Docker CLI commands:\n\ndocker network connect &lt;network&gt; &lt;container&gt;\ndocker network disconnect &lt;network&gt; &lt;container&gt;"
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-16",
    "href": "applied_lec3.html#service-discovery-with-containers-16",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nDynamically connecting to a network\n\n\n\nWe have a container named es connected to a network named dev.\nLet’s start a simple alpine container on the default network:\n$ docker run -ti alpine sh\n/ #\nIn this container, try to ping the es container:\n/ # ping es\nping: bad address 'es'\nThis doesn’t work, but we will change that by connecting the container."
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-17",
    "href": "applied_lec3.html#service-discovery-with-containers-17",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nFinding the container ID and connecting it\n\n\n\nFigure out the ID of our alpine container; here are two methods:\n\nlooking at /etc/hostname in the container,\nrunning docker ps -lq on the host.\n\nRun the following command on the host:\n$ docker network connect dev &lt;container_id&gt;"
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-18",
    "href": "applied_lec3.html#service-discovery-with-containers-18",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nChecking what we did\n\n\n\nTry again to ping es from the container.\nIt should now work correctly:\n/ # ping es\nPING es (172.20.0.3): 56 data bytes\n64 bytes from 172.20.0.3: seq=0 ttl=64 time=0.376 ms\n64 bytes from 172.20.0.3: seq=1 ttl=64 time=0.130 ms\n^C\nInterrupt it with Ctrl-C."
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-19",
    "href": "applied_lec3.html#service-discovery-with-containers-19",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nLooking at the network setup in the container\n\n\nWe can look at the list of network interfaces with ifconfig, ip a, or ip l:\n/ # ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n18: eth0@if19: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP\n    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0\n       valid_lft forever preferred_lft forever\n20: eth1@if21: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP\n    link/ether 02:42:ac:14:00:04 brd ff:ff:ff:ff:ff:ff\n    inet 172.20.0.4/16 brd 172.20.255.255 scope global eth1\n       valid_lft forever preferred_lft forever\n/ #\nEach network connection is materialized with a virtual network interface.\nAs we can see, we can be connected to multiple networks at the same time."
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-20",
    "href": "applied_lec3.html#service-discovery-with-containers-20",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nDisconnecting from a network\n\n\n\nLet’s try the symmetrical command to disconnect the container:\n$ docker network disconnect dev &lt;container_id&gt;\nFrom now on, if we try to ping es, it will not resolve:\n/ # ping es\nping: bad address 'es'\nTrying to ping the IP address directly won’t work either:\n/ # ping 172.20.0.3\n... (nothing happens until we interrupt it with Ctrl-C)"
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-21",
    "href": "applied_lec3.html#service-discovery-with-containers-21",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nNetwork aliases are scoped per network\n\n\n\nEach network has its own set of network aliases.\nWe saw this earlier: es resolves to different addresses in dev and prod.\nIf we are connected to multiple networks, the resolver looks up names in each of them (as of Docker Engine 18.03, it is the connection order) and stops as soon as the name is found.\nTherefore, if we are connected to both dev and prod, resolving es will not give us the addresses of all the es services; but only the ones in dev or prod.\nHowever, we can lookup es.dev or es.prod if we need to."
  },
  {
    "objectID": "applied_lec3.html#service-discovery-with-containers-22",
    "href": "applied_lec3.html#service-discovery-with-containers-22",
    "title": "Docker Networking",
    "section": "Service discovery with containers",
    "text": "Service discovery with containers\n\n\n\nBuilding with a custom network\n\n\n\nWe can build a Dockerfile with a custom network with docker build --network NAME.\nThis can be used to check that a build doesn’t access the network.\n(But keep in mind that most Dockerfiles will fail, because they need to install remote packages and dependencies!)\nThis may be used to access an internal package repository.\n(But try to use a multi-stage build instead, if possible!)"
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-1",
    "href": "applied_lec3.html#working-with-volumes-1",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nObjectives\n\n\nAt the end of this section, you will be able to:\n\nCreate containers holding volumes.\nShare volumes across containers.\nShare a host directory with one or many containers."
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-2",
    "href": "applied_lec3.html#working-with-volumes-2",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nWorking with volumes\n\n\nDocker volumes can be used to achieve many things, including:\n\nBypassing the copy-on-write system to obtain native disk I/O performance.\nBypassing copy-on-write to leave some files out of docker commit.\nSharing a directory between multiple containers.\nSharing a directory between the host and a container.\nSharing a single file between the host and a container.\nUsing remote storage and custom storage with volume drivers."
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-4",
    "href": "applied_lec3.html#working-with-volumes-4",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nVolumes are special directories in a container\n\n\nVolumes can be declared in two different ways:\n\nWithin a Dockerfile, with a VOLUME instruction.\n\nVOLUME /uploads\n\nOn the command-line, with the -v flag for docker run.\n\n$ docker run -d -v /uploads myapp\nIn both cases, /uploads (inside the container) will be a volume."
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-5",
    "href": "applied_lec3.html#working-with-volumes-5",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nVolumes bypass the copy-on-write system\n\n\nVolumes act as passthroughs to the host filesystem.\n\nThe I/O performance on a volume is exactly the same as I/O performance on the Docker host.\nWhen you docker commit, the content of volumes is not brought into the resulting image.\nIf a RUN instruction in a Dockerfile changes the content of a volume, those changes are not recorded neither.\nIf a container is started with the --read-only flag, the volume will still be writable (unless the volume is a read-only volume)."
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-6",
    "href": "applied_lec3.html#working-with-volumes-6",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nVolumes can be shared across containers\n\n\n\nYou can start a container with exactly the same volumes as another one.\nThe new container will have the same volumes, in the same directories.\nThey will contain exactly the same thing, and remain in sync.\nUnder the hood, they are actually the same directories on the host anyway.\nThis is done using the --volumes-from flag for docker run."
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-7",
    "href": "applied_lec3.html#working-with-volumes-7",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nSharing app server logs with another container\n\n\nLet’s start a Tomcat container:\n$ docker run --name webapp -d -p 8080:8080 -v /usr/local/tomcat/logs tomcat\nNow, start an alpine container accessing the same volume:\n$ docker run --volumes-from webapp alpine sh -c \"tail -f /usr/local/tomcat/logs/*\"\nThen, from another window, send requests to our Tomcat container:\n$ curl localhost:8080"
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-8",
    "href": "applied_lec3.html#working-with-volumes-8",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nVolumes exist independently of containers\n\n\nIf a container is stopped or removed, its volumes still exist and are available.\nVolumes can be listed and manipulated with docker volume subcommands:\n$ docker volume ls\nDRIVER              VOLUME NAME\nlocal               5b0b65e4316da67c2d471086640e6005ca2264f3...\nlocal               pgdata-prod\nlocal               pgdata-dev\nlocal               13b59c9936d78d109d094693446e174e5480d973...\nSome of those volume names were explicit (pgdata-prod, pgdata-dev).\nThe others (the hex IDs) were generated automatically by Docker."
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-9",
    "href": "applied_lec3.html#working-with-volumes-9",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nNaming volumes\n\n\n\nVolumes can be created without a container, then used in multiple containers.\n\nLet’s create a couple of volumes directly.\n$ docker volume create webapps\nwebapps\n$ docker volume create logs\nlogs\nVolumes are not anchored to a specific path."
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-10",
    "href": "applied_lec3.html#working-with-volumes-10",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nPopulating volumes\n\n\n\nWhen an empty volume is mounted on a non-empty directory, the directory is copied to the volume.\nThis makes it easy to “promote” a normal directory to a volume.\nNon-empty volumes are always mounted as-is.\n\nLet’s populate the webapps volume with the webapps.dist directory from the Tomcat image.\n$ docker run -v webapps:/usr/local/tomcat/webapps.dist tomcat true\n\n\n\n\n\n\n\n\n\nNote\n\n\nRunning true will cause the container to exit successfully once the webapps.dist directory has been copied to the webapps volume, instead of starting tomcat."
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-11",
    "href": "applied_lec3.html#working-with-volumes-11",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nUsing our named volumes\n\n\n\nVolumes are used with the -v option.\nWhen a host path does not contain a /, it is considered a volume name.\n\nLet’s start a web server using the two previous volumes.\n$ docker run -d -p 1234:8080 \\\n         -v logs:/usr/local/tomcat/logs \\\n         -v webapps:/usr/local/tomcat/webapps \\\n         tomcat\nCheck that it’s running correctly:\n$ curl localhost:1234\n... (Tomcat tells us how happy it is to be up and running) ..."
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-12",
    "href": "applied_lec3.html#working-with-volumes-12",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nUsing a volume in another container\n\n\n\nWe will make changes to the volume from another container.\nIn this example, we will run a text editor in the other container.\n(But this could be an FTP server, a WebDAV server, a Git receiver…)\n\nLet’s start another container using the webapps volume.\n$ docker run -v webapps:/webapps -w /webapps -ti alpine vi ROOT/index.jsp\nVandalize the page, save, exit.\nThen run curl localhost:1234 again to see your changes."
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-13",
    "href": "applied_lec3.html#working-with-volumes-13",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nMigrating data with --volumes-from\n\n\nThe --volumes-from option tells Docker to re-use all the volumes of an existing container.\n\nScenario: migrating from Redis 2.8 to Redis 3.0.\nWe have a container (myredis) running Redis 2.8.\nStop the myredis container.\nStart a new container, using the Redis 3.0 image, and the --volumes-from option.\nThe new container will inherit the data of the old one.\nNewer containers can use --volumes-from too.\nDoesn’t work across servers, so not usable in clusters (Swarm, Kubernetes)."
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-14",
    "href": "applied_lec3.html#working-with-volumes-14",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nData migration in practice\n\n\nLet’s create a Redis container.\n$ docker run -d --name redis28 redis:2.8\nConnect to the Redis container and set some data.\n$ docker run -ti --link redis28:redis busybox telnet redis 6379\nIssue the following commands:\nSET counter 42\nINFO server\nSAVE\nQUIT"
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-15",
    "href": "applied_lec3.html#working-with-volumes-15",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nUpgrading Redis\n\n\nStop the Redis container.\n$ docker stop redis28\nStart the new Redis container.\n$ docker run -d --name redis30 --volumes-from redis28 redis:3.0"
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-16",
    "href": "applied_lec3.html#working-with-volumes-16",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nTesting the new Redis\n\n\nConnect to the Redis container and see our data.\ndocker run -ti --link redis30:redis busybox telnet redis 6379\nIssue a few commands.\nGET counter\nINFO server\nQUIT"
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-17",
    "href": "applied_lec3.html#working-with-volumes-17",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nVolumes lifecycle\n\n\n\nWhen you remove a container, its volumes are kept around.\nYou can list them with docker volume ls.\nYou can access them by creating a container with docker run -v.\nYou can remove them with docker volume rm or docker system prune.\n\nUltimately, you are the one responsible for logging, monitoring, and backup of your volumes."
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-18",
    "href": "applied_lec3.html#working-with-volumes-18",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nChecking volumes defined by an image\n\n\nWondering if an image has volumes? Just use docker inspect:\n$ # docker inspect training/datavol\n[{\n  \"config\": {\n    . . .\n    \"Volumes\": {\n        \"/var/webapp\": {}\n    },\n    . . .\n}]"
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-19",
    "href": "applied_lec3.html#working-with-volumes-19",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nChecking volumes used by a container\n\n\nTo look which paths are actually volumes, and to what they are bound, use docker inspect (again):\n$ docker inspect &lt;yourContainerID&gt;\n[{\n  \"ID\": \"&lt;yourContainerID&gt;\",\n. . .\n  \"Volumes\": {\n     \"/var/webapp\": \"/var/lib/docker/vfs/dir/f4280c5b6207ed531efd4cc673ff620cef2a7980f747dbbcca001db61de04468\"\n  },\n  \"VolumesRW\": {\n     \"/var/webapp\": true\n  },\n}]\n\nWe can see that our volume is present on the file system of the Docker host."
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-20",
    "href": "applied_lec3.html#working-with-volumes-20",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nSharing a single file\n\n\nThe same -v flag can be used to share a single file (instead of a directory).\nOne of the most interesting examples is to share the Docker control socket.\n$ docker run -it -v /var/run/docker.sock:/var/run/docker.sock docker sh\nFrom that container, you can now run docker commands communicating with the Docker Engine running on the host. Try docker ps!\n\n\n\n\n\n\n\n\n\nWarning\n\n\nSince that container has access to the Docker socket, it has root-like access to the host."
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-21",
    "href": "applied_lec3.html#working-with-volumes-21",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nVolumes vs. Mounts\n\n\n\nSince Docker 17.06, a new options is available: --mount.\nIt offers a new, richer syntax to manipulate data in containers.\nIt makes an explicit difference between:\n\nvolumes (identified with a unique name, managed by a storage plugin),\nbind mounts (identified with a host path, not managed).\n\nThe former -v / --volume option is still usable."
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-22",
    "href": "applied_lec3.html#working-with-volumes-22",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\n--mount syntax\n\n\nBinding a host path to a container path:\n$ docker run \\\n  --mount type=bind,source=/path/on/host,target=/path/in/container alpine\nMounting a volume to a container path:\n$ docker run \\\n  --mount source=myvolume,target=/path/in/container alpine\nMounting a tmpfs (in-memory, for temporary files):\n$ docker run \\\n  --mount type=tmpfs,destination=/path/in/container,tmpfs-size=1000000 alpine"
  },
  {
    "objectID": "applied_lec3.html#working-with-volumes-23",
    "href": "applied_lec3.html#working-with-volumes-23",
    "title": "Docker Networking",
    "section": "Working with volumes",
    "text": "Working with volumes\n\n\n\nSection summary\n\n\nWe’ve learned how to:\n\nCreate and manage volumes.\nShare volumes across containers.\nShare a host directory with one or many containers."
  },
  {
    "objectID": "applied_lec7.html#distributed-intro",
    "href": "applied_lec7.html#distributed-intro",
    "title": "Kubernetes part 1",
    "section": "Distributed intro",
    "text": "Distributed intro\n\n\n\nDefinitions\n\n\n\n\n\n\n\n\n\nKeyword\nDescription\n\n\n\n\nVM\nA virtual machine (VM) is a software simulation of a physical computer that runs on a host computer. It provides a separate operating system and resources, allowing multiple operating systems to run on a single physical machine.\n\n\nCluster\nA cluster is a group of connected servers that work together as a single system to provide high availability, scalability, and increased performance for applications. The nodes in a cluster are connected through a network and share resources to provide a unified, highly available solution.\n\n\nNode\nA cluster node is a single server within a cluster computing system. It provides computing resources and works together with other nodes to perform tasks as a unified system, providing high availability and scalability for applications."
  },
  {
    "objectID": "applied_lec7.html#distributed-intro-1",
    "href": "applied_lec7.html#distributed-intro-1",
    "title": "Kubernetes part 1",
    "section": "Distributed intro",
    "text": "Distributed intro\n\n\n\nDefinitions\n\n\n\n\n\n\n\n\n\nKeyword\nDescription\n\n\n\n\nNetwork\nA network is a group of interconnected devices that can exchange data and information. Networks can be used to connect computers, servers, mobile devices, and other types of devices and allow them to communicate with each other and share resources, such as printers and storage. More specifically in our case, these are physical and software-defined communication paths between individual nodes of a cluster and programs running on those nodes.\n\n\nPort\nA port is a communication endpoint in a network-attached device, such as a computer or server. It allows the device to receive and send data to other devices on the network through a specific network protocol, such as TCP or UDP. Each port has a unique number that is used to identify it, and different services and applications use specific ports to communicate.\n\n\nService\nA piece of software that implements a limited set of functionalities that are then used by other parts of the application."
  },
  {
    "objectID": "applied_lec7.html#distributed-intro-2",
    "href": "applied_lec7.html#distributed-intro-2",
    "title": "Kubernetes part 1",
    "section": "Distributed intro",
    "text": "Distributed intro\n\n\n\nMonolithic vs distributed\n\n\nMonolithic: single (or a couple of) tightly coupled programs running on a single server.\nDistributed: programs collected into services running on multiple servers, communicating with each other."
  },
  {
    "objectID": "applied_lec7.html#distributed-intro-3",
    "href": "applied_lec7.html#distributed-intro-3",
    "title": "Kubernetes part 1",
    "section": "Distributed intro",
    "text": "Distributed intro\n\n\n\nProblems\n\n\n\nmuch more complex\nneed rethinking\ncomponents have to be separated\nand loosely coupled\nmeaning: components have well-defined interfaces"
  },
  {
    "objectID": "applied_lec7.html#distributed-intro-4",
    "href": "applied_lec7.html#distributed-intro-4",
    "title": "Kubernetes part 1",
    "section": "Distributed intro",
    "text": "Distributed intro\n\n\n\nStateful vs stateless\n\n\nStateful: create or modify persistent data.\n\nMuch harder to reason about.\nShould be pushed to the boundaries of the system.\n\nStateless: don’t create or modify persistent data.\n\nMuch easier to reason about.\nShould form core of the system."
  },
  {
    "objectID": "applied_lec7.html#distributed-intro-5",
    "href": "applied_lec7.html#distributed-intro-5",
    "title": "Kubernetes part 1",
    "section": "Distributed intro",
    "text": "Distributed intro\n\n\n\nDiscovery\n\n\nIndividual components should be able to find each other in a cluster.\n\n\n\n\n\n\nOption 1: some configuration file."
  },
  {
    "objectID": "applied_lec7.html#distributed-intro-6",
    "href": "applied_lec7.html#distributed-intro-6",
    "title": "Kubernetes part 1",
    "section": "Distributed intro",
    "text": "Distributed intro\n\n\n\nOption 2 - DNS\n\n\nAn external authority that knows where to find services - Domain Name Service."
  },
  {
    "objectID": "applied_lec7.html#distributed-intro-7",
    "href": "applied_lec7.html#distributed-intro-7",
    "title": "Kubernetes part 1",
    "section": "Distributed intro",
    "text": "Distributed intro\n\n\n\nLoad balancing\n\n\nService A needs to talk to Service B, but the latter runs on many instances.\nLoad balancer:\n\ndistributes tasks using some algorithm\nchecks health"
  },
  {
    "objectID": "applied_lec7.html#distributed-intro-8",
    "href": "applied_lec7.html#distributed-intro-8",
    "title": "Kubernetes part 1",
    "section": "Distributed intro",
    "text": "Distributed intro\n\n\n\n\n\n\nReliability matters\n\n\n\nretries\nlogs\nproper error handling\nredundancy\nhealth checks\nmonitoring\nrate limiting"
  },
  {
    "objectID": "applied_lec7.html#distributed-intro-9",
    "href": "applied_lec7.html#distributed-intro-9",
    "title": "Kubernetes part 1",
    "section": "Distributed intro",
    "text": "Distributed intro\n\n\n\nApplication updates\n\n\n\ncode changes\ndata updates"
  },
  {
    "objectID": "applied_lec7.html#intro-to-orchestration",
    "href": "applied_lec7.html#intro-to-orchestration",
    "title": "Kubernetes part 1",
    "section": "Intro to orchestration",
    "text": "Intro to orchestration\n\n\n\nWhat is it?\n\n\nA standard way to manage distributed systems built on top of containers (e.g. Docker)"
  },
  {
    "objectID": "applied_lec7.html#intro-to-orchestration-1",
    "href": "applied_lec7.html#intro-to-orchestration-1",
    "title": "Kubernetes part 1",
    "section": "Intro to orchestration",
    "text": "Intro to orchestration\n\n\n\nDeclarative vs imperative\n\n\n\nOur container orchestrator puts a very strong emphasis on being declarative\nDeclarative:\nI would like a cup of tea.\nImperative:\nBoil some water. Pour it in a teapot. Add tea leaves. Steep for a while. Serve in a cup.\nDeclarative seems simpler at first …\n… As long as you know how to brew tea"
  },
  {
    "objectID": "applied_lec7.html#intro-to-orchestration-2",
    "href": "applied_lec7.html#intro-to-orchestration-2",
    "title": "Kubernetes part 1",
    "section": "Intro to orchestration",
    "text": "Intro to orchestration\n\n\n\nDeclarative vs imperative\n\n\n\nWhat declarative would really be:\nI want a cup of tea, obtained by pouring an infusion¹ of tea leaves in a cup.\n¹An infusion is obtained by letting the object steep a few minutes in hot² water.\n²Hot liquid is obtained by pouring it in an appropriate container³ and setting it on a stove.\n³Ah, finally, containers! Something we know about. Let’s get to work, shall we?"
  },
  {
    "objectID": "applied_lec7.html#intro-to-orchestration-3",
    "href": "applied_lec7.html#intro-to-orchestration-3",
    "title": "Kubernetes part 1",
    "section": "Intro to orchestration",
    "text": "Intro to orchestration\n\n\n\nDeclarative vs imperative\n\n\n\nImperative systems:\n\nsimpler\nif a task is interrupted, we have to restart from scratch\n\nDeclarative systems:\n\nif a task is interrupted (or if we show up to the party half-way through), we can figure out what’s missing and do only what’s necessary\nwe need to be able to observe the system\n… and compute a “diff” between what we have and what we want"
  },
  {
    "objectID": "applied_lec7.html#intro-to-orchestration-4",
    "href": "applied_lec7.html#intro-to-orchestration-4",
    "title": "Kubernetes part 1",
    "section": "Intro to orchestration",
    "text": "Intro to orchestration\n\n\n\nObjectives\n\n\n\nkeeping desired state (or reconciling, described in a declarative fashion)\nmaking sure global services run on all workers\nand replicated services run a on a specified number of workers\nservice discovery\nrouting\nload-balancing\nscaling\nhealth checks/fixes\ndata persistence\nnode affinity\nsecurity/secrets management\nintrospection"
  },
  {
    "objectID": "applied_lec7.html#intro-to-orchestration-5",
    "href": "applied_lec7.html#intro-to-orchestration-5",
    "title": "Kubernetes part 1",
    "section": "Intro to orchestration",
    "text": "Intro to orchestration\n\n\n\nDeclarative vs imperative in Kubernetes\n\n\n\nWith Kubernetes, we cannot say: “run this container”\nAll we can do is write a spec and push it to the API server\n(by creating a resource like e.g. a Pod or a Deployment)\nThe API server will validate that spec (and reject it if it’s invalid)\nThen it will store it in etcd\nA controller will “notice” that spec and act upon it"
  },
  {
    "objectID": "applied_lec7.html#intro-to-orchestration-6",
    "href": "applied_lec7.html#intro-to-orchestration-6",
    "title": "Kubernetes part 1",
    "section": "Intro to orchestration",
    "text": "Intro to orchestration\n\n\n\nReconciling state\n\n\n\nWatch for the spec fields in the YAML files later!\nThe spec describes how we want the thing to be\nKubernetes will reconcile the current state with the spec (technically, this is done by a number of controllers)\nWhen we want to change some resource, we update the spec\nKubernetes will then converge that resource"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts",
    "href": "applied_lec7.html#kubernetes-concepts",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nKubernetes concepts\n\n\n\nKubernetes is a container management system\nIt runs and manages containerized applications on a cluster\nWhat does that really mean?"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-2",
    "href": "applied_lec7.html#kubernetes-concepts-2",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nHistory\n\n\n\nOriginally designed by Google (2014)\nAncient Greek κυβερνήτης for “pilot”\nEvolution of Google’s proprietary Borg system\nBased on Promise theory"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-3",
    "href": "applied_lec7.html#kubernetes-concepts-3",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nWhat can we do with Kubernetes?\n\n\n\nLet’s imagine that we have a 3-tier e-commerce app:\n\nweb frontend\nAPI backend\ndatabase\n\nWe have built images for our frontend and backend components\n(e.g. with Dockerfiles and docker build)\nWe are running them successfully with a local environment\n(e.g. with Docker Compose)\nLet’s see how we would deploy our app on Kubernetes!"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-4",
    "href": "applied_lec7.html#kubernetes-concepts-4",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nKubernetes, level 1\n\n\n\nLeave our database outside of Kubernetes (because database be scary🥺)\nDeploy a managed Kubernetes cluster (cloud or professional services)\nStart 5 containers using image atseashop/api:v1.3\nPlace an internal load balancer in front of these containers\nStart 10 containers using image atseashop/webfront:v1.3\nPlace a public load balancer in front of these containers\nIt’s Black Friday (or Christmas), traffic spikes, grow our cluster and add containers\nNew release! Replace my containers with the new image atseashop/webfront:v1.4\nKeep processing requests during the upgrade; update my containers one at a time"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-5",
    "href": "applied_lec7.html#kubernetes-concepts-5",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nKubernetes, level 2\n\n\n\nDeploy a pre-production environment\n(still using our external database, for now)\nResource management and scheduling\n(reserve CPU/RAM for containers; placement constraints; priorities)\nAutoscaling\n(straightforward on CPU; more complex on other metrics)\nAdvanced rollout patterns\n(blue/green deployment, canary deployment)"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-6",
    "href": "applied_lec7.html#kubernetes-concepts-6",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\nBlue-green deployment"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-7",
    "href": "applied_lec7.html#kubernetes-concepts-7",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\nCanary cage"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-8",
    "href": "applied_lec7.html#kubernetes-concepts-8",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nKubernetes, level 3\n\n\n\nRun staging databases on the cluster\n(no replication, no backups, no scaling)\nAutomatic or semi-automatic deployment of feature branches\n(each with its own database)\nFine-grained access control\n(defining what can be done by whom on which resources)\nBatch jobs\n(one-off; parallel; also cron-style periodic execution)\nPackage applications with e.g. Helm charts"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-9",
    "href": "applied_lec7.html#kubernetes-concepts-9",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nKubernetes, level 4\n\n\n\nStateful services with persistence, replication, backups\n(databases, message queues, etc.)\nAutomate complex tasks with operators\n(e.g. database replication, failover, etc.)\nCombine the two previous points with database operators like CloudNativePG\n(learn more about database operators: FR, EN)\nLeverage advanced storage with e.g. local ZFS volumes\n(learn more about ZFS and databases on k8s: FR, EN)\nDeploy and manage clusters in-house"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-10",
    "href": "applied_lec7.html#kubernetes-concepts-10",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nKubernetes, level 5\n\n\n\nDeploying and managing clusters at scale\n(hundreds of clusters, thousands of nodes…)\nWriting custom operators\nHybrid deployments"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-11",
    "href": "applied_lec7.html#kubernetes-concepts-11",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nDisclaimer\n\n\nThe levels mentioned in the previous slides are not necessarily linear.\nThey aren’t exhaustive either (we didn’t mention e.g. observability and alerting)."
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-12",
    "href": "applied_lec7.html#kubernetes-concepts-12",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nKubernetes architecture\n\n\n\n\n\nhaha only kidding"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-13",
    "href": "applied_lec7.html#kubernetes-concepts-13",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nKubernetes architecture\n\n\n\nHa ha ha ha\nOK, I was trying to scare you, it’s much simpler than that ❤️"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-14",
    "href": "applied_lec7.html#kubernetes-concepts-14",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\nthat one is more like the real thing"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-15",
    "href": "applied_lec7.html#kubernetes-concepts-15",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nCredits\n\n\n\nThe first schema is a Kubernetes cluster with storage backed by multi-path iSCSI\n(Courtesy of Yongbok Kim)\nThe second one is a simplified representation of a Kubernetes cluster\n(Courtesy of Imesh Gunaratne)"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-16",
    "href": "applied_lec7.html#kubernetes-concepts-16",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\nAnother version"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-17",
    "href": "applied_lec7.html#kubernetes-concepts-17",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nArchitecture\n\n\n\na Kubernetes cluster consists of a set of servers (VMs/bare metal)\neach server is either a master or a *worker.**\nsmall and odd number of masters\nno limit on worker nodes\nworker nodes run pods\npods in turn run containers (one or more)\nReplicaSets are collection of identical pods"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-18",
    "href": "applied_lec7.html#kubernetes-concepts-18",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nPods"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-19",
    "href": "applied_lec7.html#kubernetes-concepts-19",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nKubernetes architecture: the nodes\n\n\n\nThe nodes executing our containers run a collection of services:\n\na container Engine (typically Docker)\nkubelet (the “node agent”)\nkube-proxy (a necessary but not sufficient network component)\n\nNodes were formerly called “minions”\n(You might see that word in older articles or documentation)"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-20",
    "href": "applied_lec7.html#kubernetes-concepts-20",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nKubernetes architecture: the control plane\n\n\n\nThe Kubernetes logic (its “brains”) is a collection of services:\n\nthe API server (our point of entry to everything!)\ncore services like the scheduler and controller manager\netcd (a highly available key/value store; the “database” of Kubernetes)\n\nTogether, these services form the control plane of our cluster\nThe control plane is also called the “master”"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-21",
    "href": "applied_lec7.html#kubernetes-concepts-21",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\nOne of the best Kubernetes architecture diagrams available"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-22",
    "href": "applied_lec7.html#kubernetes-concepts-22",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nRunning the control plane on special nodes\n\n\n\nIt is common to reserve a dedicated node for the control plane\n(Except for single-node development clusters, like when using minikube)\nThis node is then called a “master”\n(Yes, this is ambiguous: is the “master” a node, or the whole control plane?)\nNormal applications are restricted from running on this node\n(By using a mechanism called “taints”)\nWhen high availability is required, each service of the control plane must be resilient\nThe control plane is then replicated on multiple nodes\n(This is sometimes called a “multi-master” setup)"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-23",
    "href": "applied_lec7.html#kubernetes-concepts-23",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nRunning the control plane outside containers\n\n\n\nThe services of the control plane can run in or out of containers\nFor instance: since etcd is a critical service, some people deploy it directly on a dedicated cluster (without containers)\n(This is illustrated on the first “super complicated” schema)\nIn some hosted Kubernetes offerings (e.g. AKS, GKE, EKS), the control plane is invisible\n(We only “see” a Kubernetes API endpoint)\nIn that case, there is no “master node”\n\nFor this reason, it is more accurate to say “control plane” rather than “master.”"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-24",
    "href": "applied_lec7.html#kubernetes-concepts-24",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-25",
    "href": "applied_lec7.html#kubernetes-concepts-25",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-26",
    "href": "applied_lec7.html#kubernetes-concepts-26",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-27",
    "href": "applied_lec7.html#kubernetes-concepts-27",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-28",
    "href": "applied_lec7.html#kubernetes-concepts-28",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-29",
    "href": "applied_lec7.html#kubernetes-concepts-29",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-30",
    "href": "applied_lec7.html#kubernetes-concepts-30",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-31",
    "href": "applied_lec7.html#kubernetes-concepts-31",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nHow many nodes should a cluster have?\n\n\n\nThere is no particular constraint\n(no need to have an odd number of nodes for quorum)\nA cluster can have zero nodes\n(but then it won’t be able to start any pods)\nFor testing and development, having a single node is fine\nFor production, make sure that you have extra capacity\n(so that your workload still fits if you lose a node or a group of nodes)\nKubernetes is tested with up to 5000 nodes\n(however, running a cluster of that size requires a lot of tuning)"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-32",
    "href": "applied_lec7.html#kubernetes-concepts-32",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nDo we need to run Docker at all?\n\n\nNo!\n\nThe Docker Engine used to be the default option to run containers with Kubernetes\nSupport for Docker (specifically: dockershim) was removed in Kubernetes 1.24\nWe can leverage other pluggable runtimes through the Container Runtime Interface\n\nWe could also use rkt (“Rocket”) from CoreOS\n\n(deprecated)"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-33",
    "href": "applied_lec7.html#kubernetes-concepts-33",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nSome runtimes available through CRI\n\n\n\ncontainerd\n\nmaintained by Docker, IBM, and community\nused by Docker Engine, microk8s, k3s, GKE; also standalone\ncomes with its own CLI, ctr\n\nCRI-O:\n\nmaintained by Red Hat, SUSE, and community\nused by OpenShift and Kubic\ndesigned specifically as a minimal runtime for Kubernetes\n\nAnd more"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-34",
    "href": "applied_lec7.html#kubernetes-concepts-34",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nDo we need to run Docker at all?\n\n\nYes!\n\n\nWe will need to build images and ship them around\nWe can do these things without Docker  (but with some languages/frameworks, it might be much harder)\nDocker is still the most stable container engine today  (but other options are maturing very quickly)"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-35",
    "href": "applied_lec7.html#kubernetes-concepts-35",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nDo we need to run Docker at all?\n\n\n\nOn our Kubernetes clusters:\nNot anymore\nOn our development environments, CI pipelines … :\nYes, almost certainly"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-36",
    "href": "applied_lec7.html#kubernetes-concepts-36",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nInteracting with Kubernetes\n\n\n\nWe will interact with our Kubernetes cluster through the Kubernetes API\nThe Kubernetes API is (mostly) RESTful\nIt allows us to create, read, update, delete resources\nA few common resource types are:\n\nnode (a machine — physical or virtual — in our cluster)\npod (group of containers running together on a node)\nservice (stable network endpoint to connect to one or multiple containers)"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-37",
    "href": "applied_lec7.html#kubernetes-concepts-37",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\nNode, pod, container"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-38",
    "href": "applied_lec7.html#kubernetes-concepts-38",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nScaling\n\n\n\nHow would we scale the pod shown on the previous slide?\nDo create additional pods\n\neach pod can be on a different node\neach pod will have its own IP address\n\nDo not add more NGINX containers in the pod\n\nall the NGINX containers would be on the same node\nthey would all have the same IP address (resulting in Address already in use errors)"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-39",
    "href": "applied_lec7.html#kubernetes-concepts-39",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nTogether or separate\n\n\n\nShould we put e.g. a web application server and a cache together?  (“cache” being something like e.g. Memcached or Redis)\nPutting them in the same pod means:\n\nthey have to be scaled together\nthey can communicate very efficiently over localhost\n\nPutting them in different pods means:\n\nthey can be scaled separately\nthey must communicate over remote IP addresses (incurring more latency, lower performance)\n\nBoth scenarios can make sense, depending on our goals"
  },
  {
    "objectID": "applied_lec7.html#kubernetes-concepts-40",
    "href": "applied_lec7.html#kubernetes-concepts-40",
    "title": "Kubernetes part 1",
    "section": "Kubernetes concepts",
    "text": "Kubernetes concepts\n\n\n\nCredits\n\n\n\nThe first diagram is courtesy of Lucas Käldström, in this presentation\n\nit’s one of the best Kubernetes architecture diagrams available!\n\nThe second diagram is courtesy of Weave Works\n\na pod can have multiple containers working together\nIP addresses are associated with pods, not with individual containers\n\n\nBoth diagrams used with permission."
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl",
    "href": "applied_lec7.html#intro-to-kubectl",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\n\n\n\nInstallation\n\n\n\nlogin to your EC2 instance\ninstall Docker\ninstall kubectl\ninstall minikube\ninstall Kompose\nwe’ll use Sample compose app as an example"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-1",
    "href": "applied_lec7.html#intro-to-kubectl-1",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nIntro to kubectl\n\n\n\nkubectl is (almost) the only tool we’ll need to talk to Kubernetes\nIt is a rich CLI tool around the Kubernetes API\n(Everything you can do with kubectl, you can do directly with the API)\nOn our machines, there is a ~/.kube/config file with:\n\nthe Kubernetes API address\nthe path to our TLS certificates used to authenticate\n\nYou can also use the --kubeconfig flag to pass a config file\nOr directly --server, --user, etc.\nkubectl can be pronounced “Cube C T L”, “Cube cuttle”, “Cube cuddle”…"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-3",
    "href": "applied_lec7.html#intro-to-kubectl-3",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nkubectl is the new SSH\n\n\n\nWe often start managing servers with SSH\n(installing packages, troubleshooting …)\nAt scale, it becomes tedious, repetitive, error-prone\nInstead, we use config management, central logging, etc.\nIn many cases, we still need SSH:\n\nas the underlying access method (e.g. Ansible)\nto debug tricky scenarios\nto inspect and poke at things"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-4",
    "href": "applied_lec7.html#intro-to-kubectl-4",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nThe parallel with kubectl\n\n\n\nWe often start managing Kubernetes clusters with kubectl\n(deploying applications, troubleshooting …)\nAt scale (with many applications or clusters), it becomes tedious, repetitive, error-prone\nInstead, we use automated pipelines, observability tooling, etc.\nIn many cases, we still need kubectl:\n\nto debug tricky scenarios\nto inspect and poke at things\n\nThe Kubernetes API is always the underlying access method"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-5",
    "href": "applied_lec7.html#intro-to-kubectl-5",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nkubectl get\n\n\n\nLet’s look at our Node resources with kubectl get!\nLook at the composition of our cluster:\nkubectl get node\nThese commands are equivalent:\nkubectl get no\nkubectl get node\nkubectl get nodes"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-6",
    "href": "applied_lec7.html#intro-to-kubectl-6",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nObtaining machine-readable output\n\n\n\nkubectl get can output JSON, YAML, or be directly formatted\nGive us more info about the nodes:\nkubectl get nodes -o wide\nLet’s have some YAML:\nkubectl get no -o yaml\nSee that kind: List at the end? It’s the type of our result!"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-7",
    "href": "applied_lec7.html#intro-to-kubectl-7",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\n(Ab)using kubectl and jq\n\n\n\nIt’s super easy to build custom reports\nShow the capacity of all our nodes as a stream of JSON objects:\n  kubectl get nodes -o json |\n          jq \".items[] | {name:.metadata.name} + .status.capacity\""
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-8",
    "href": "applied_lec7.html#intro-to-kubectl-8",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nExploring types and definitions\n\n\n\nWe can list all available resource types by running kubectl api-resources  (In Kubernetes 1.10 and prior, this command used to be kubectl get)\nWe can view the definition for a resource type with:\nkubectl explain type\nWe can view the definition of a field in a resource, for instance:\nkubectl explain node.spec\nOr get the full definition of all fields and sub-fields:\nkubectl explain node --recursive"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-9",
    "href": "applied_lec7.html#intro-to-kubectl-9",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nIntrospection vs. documentation\n\n\n\nWe can access the same information by reading the API documentation\nThe API documentation is usually easier to read, but:\n\nit won’t show custom types (like Custom Resource Definitions)\nwe need to make sure that we look at the correct version\n\nkubectl api-resources and kubectl explain perform introspection\n(they communicate with the API server and obtain the exact type definitions)"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-10",
    "href": "applied_lec7.html#intro-to-kubectl-10",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nType names\n\n\n\nThe most common resource names have three forms:\n\nsingular (e.g. node, service, deployment)\nplural (e.g. nodes, services, deployments)\nshort (e.g. no, svc, deploy)\n\nSome resources do not have a short name\nEndpoints only have a plural form\n(because even a single Endpoints resource is actually a list of endpoints)"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-11",
    "href": "applied_lec7.html#intro-to-kubectl-11",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nViewing details\n\n\n\nWe can use kubectl get -o yaml to see all available details\nHowever, YAML output is often simultaneously too much and not enough\nFor instance, kubectl get node node1 -o yaml is:\n\ntoo much information (e.g.: list of images available on this node)\nnot enough information (e.g.: doesn’t show pods running on this node)\ndifficult to read for a human operator\n\nFor a comprehensive overview, we can use kubectl describe instead"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-12",
    "href": "applied_lec7.html#intro-to-kubectl-12",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nkubectl describe\n\n\n\nkubectl describe needs a resource type and (optionally) a resource name\nIt is possible to provide a resource name prefix\n(all matching objects will be displayed)\nkubectl describe will retrieve some extra information about the resource\nLook at the information available for node1 with one of the following commands:\nkubectl describe node/node1\nkubectl describe node node1\n\n(We should notice a bunch of control plane pods.)"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-13",
    "href": "applied_lec7.html#intro-to-kubectl-13",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nListing running containers\n\n\n\nContainers are manipulated through pods\nA pod is a group of containers:\nrunning together (on the same node)\nsharing resources (RAM, CPU; but also network, volumes)\nList pods on our cluster:\nkubectl get pods\nWhere are the pods that we saw just a moment earlier?!?"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-14",
    "href": "applied_lec7.html#intro-to-kubectl-14",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nNamespaces\n\n\n\nNamespaces allow us to segregate resources\nList the namespaces on our cluster with one of these commands:\nkubectl get namespaces\nkubectl get namespace\nkubectl get ns\nYou know what … This kube-system thing looks suspicious.\n\nIn fact, I’m pretty sure it showed up earlier, when we did:\nkubectl describe node node1"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-15",
    "href": "applied_lec7.html#intro-to-kubectl-15",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nAccessing namespaces\n\n\n\nBy default, kubectl uses the default namespace\nWe can see resources in all namespaces with --all-namespaces\nList the pods in all namespaces:\nkubectl get pods --all-namespaces\nSince Kubernetes 1.14, we can also use -A as a shorter version:\nkubectl get pods -A\n\nHere are our system pods!"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-16",
    "href": "applied_lec7.html#intro-to-kubectl-16",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nWhat are all these control plane pods?\n\n\n\netcd is our etcd server\nkube-apiserver is the API server\nkube-controller-manager and kube-scheduler are other control plane components\ncoredns provides DNS-based service discovery (replacing kube-dns as of 1.11)\nkube-proxy is the (per-node) component managing port mappings and such\nweave is the (per-node) component managing the network overlay\nthe READY column indicates the number of containers in each pod\n(1 for most pods, but weave has 2, for instance)"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-17",
    "href": "applied_lec7.html#intro-to-kubectl-17",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nScoping another namespace\n\n\n\nWe can also look at a different namespace (other than default)\nList only the pods in the kube-system namespace:\nkubectl get pods --namespace=kube-system\nkubectl get pods -n kube-system"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-18",
    "href": "applied_lec7.html#intro-to-kubectl-18",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nNamespaces and other kubectl commands\n\n\n\nWe can use -n/--namespace with almost every kubectl command\nExample:\n\nkubectl create --namespace=X to create something in namespace X\n\nWe can use -A/--all-namespaces with most commands that manipulate multiple objects\nExamples:\n\nkubectl delete can delete resources across multiple namespaces\nkubectl label can add/remove/update labels across multiple namespaces"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-19",
    "href": "applied_lec7.html#intro-to-kubectl-19",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nWhat about kube-public?\n\n\n\nList the pods in the kube-public namespace:\nkubectl -n kube-public get pods\n\nNothing!\nkube-public is created by kubeadm & used for security bootstrapping."
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-20",
    "href": "applied_lec7.html#intro-to-kubectl-20",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nExploring kube-public\n\n\n\nThe only interesting object in kube-public is a ConfigMap named cluster-info\nList ConfigMap objects:\nkubectl -n kube-public get configmaps\nInspect cluster-info:\nkubectl -n kube-public get configmap cluster-info -o yaml\n\nNote the selfLink URI: /api/v1/namespaces/kube-public/configmaps/cluster-info\nWe can use that!"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-21",
    "href": "applied_lec7.html#intro-to-kubectl-21",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nAccessing cluster-info\n\n\n\nEarlier, when trying to access the API server, we got a Forbidden message\nBut cluster-info is readable by everyone (even without authentication)\nRetrieve cluster-info:\ncurl -k https://10.96.0.1/api/v1/namespaces/kube-public/configmaps/cluster-info\nWe were able to access cluster-info (without auth)\nIt contains a kubeconfig file"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-22",
    "href": "applied_lec7.html#intro-to-kubectl-22",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nRetrieving kubeconfig\n\n\n\nWe can easily extract the kubeconfig file from this ConfigMap\nDisplay the content of kubeconfig:\n  curl -sk https://10.96.0.1/api/v1/namespaces/kube-public/configmaps/cluster-info \\\n       | jq -r .data.kubeconfig\nThis file holds the canonical address of the API server, and the public key of the CA\nThis file does not hold client keys or tokens\nThis is not sensitive information, but allows us to establish trust"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-23",
    "href": "applied_lec7.html#intro-to-kubectl-23",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nWhat about kube-node-lease?\n\n\n\nStarting with Kubernetes 1.14, there is a kube-node-lease namespace\n(or in Kubernetes 1.13 if the NodeLease feature gate is enabled)\nThat namespace contains one Lease object per node\nNode leases are a new way to implement node heartbeats\n(i.e. node regularly pinging the control plane to say “I’m alive!”)\nFor more details, see Efficient Node Heartbeats KEP or the node controller documentation"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-24",
    "href": "applied_lec7.html#intro-to-kubectl-24",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nServices\n\n\n\nA service is a stable endpoint to connect to “something”\n(In the initial proposal, they were called “portals”)\nList the services on our cluster with one of these commands:\nkubectl get services\nkubectl get svc\nThere is already one service on our cluster: the Kubernetes API itself."
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-25",
    "href": "applied_lec7.html#intro-to-kubectl-25",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nClusterIP services\n\n\n\nA ClusterIP service is internal, available from the cluster only\nThis is useful for introspection from within containers\nTry to connect to the API:\ncurl -k https://`10.96.0.1`\n\n-k is used to skip certificate verification\nMake sure to replace 10.96.0.1 with the CLUSTER-IP shown by kubectl get svc\n\n\nThe command above should either time out, or show an authentication error. Why?"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-26",
    "href": "applied_lec7.html#intro-to-kubectl-26",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nTime out\n\n\n\nConnections to ClusterIP services only work from within the cluster\nIf we are outside the cluster, the curl command will probably time out\n(Because the IP address, e.g. 10.96.0.1, isn’t routed properly outside the cluster)\nThis is the case with most “real” Kubernetes clusters\nTo try the connection from within the cluster, we can use shpod"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-27",
    "href": "applied_lec7.html#intro-to-kubectl-27",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nAuthentication error\n\n\nThis is what we should see when connecting from within the cluster:\n$ curl -k https://10.96.0.1\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n\n  },\n  \"status\": \"Failure\",\n  \"message\": \"forbidden: User \\\"system:anonymous\\\" cannot get path \\\"/\\\"\",\n  \"reason\": \"Forbidden\",\n  \"details\": {\n\n  },\n  \"code\": 403\n}"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-28",
    "href": "applied_lec7.html#intro-to-kubectl-28",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nExplanations\n\n\n\nWe can see kind, apiVersion, metadata\nThese are typical of a Kubernetes API reply\nBecause we are talking to the Kubernetes API\nThe Kubernetes API tells us “Forbidden”\n(because it requires authentication)\nThe Kubernetes API is reachable from within the cluster\n(many apps integrating with Kubernetes will use this)"
  },
  {
    "objectID": "applied_lec7.html#intro-to-kubectl-29",
    "href": "applied_lec7.html#intro-to-kubectl-29",
    "title": "Kubernetes part 1",
    "section": "Intro to kubectl",
    "text": "Intro to kubectl\n\n\n\nDNS integration\n\n\n\nEach service also gets a DNS record\nThe Kubernetes DNS resolver is available from within pods\n(and sometimes, from within nodes, depending on configuration)\nCode running in pods can connect to services using their name\n(e.g. https://kubernetes/…)"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s",
    "href": "applied_lec7.html#running-containers-on-k8s",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nRunning containers on K8s\n\n\n\nFirst things first: we cannot run a container\nWe are going to run a pod, and in that pod there will be a single container\nIn that container in the pod, we are going to run a simple ping command"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-2",
    "href": "applied_lec7.html#running-containers-on-k8s-2",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nStarting a simple pod with kubectl run\n\n\n\nkubectl run is convenient to start a single pod\nWe need to specify at least a name and the image we want to use\nOptionally, we can specify the command to run in the pod\nLet’s ping the address of localhost, the loopback interface:\nkubectl run pingpong --image alpine ping 127.0.0.1\n\n\nThe output tells us that a Pod was created:\npod/pingpong created"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-3",
    "href": "applied_lec7.html#running-containers-on-k8s-3",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nViewing container output\n\n\n\nLet’s use the kubectl logs command\nIt takes a Pod name as argument\nUnless specified otherwise, it will only show logs of the first container in the pod\n(Good thing there’s only one in ours!)\nView the result of our ping command:\nkubectl logs pingpong"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-4",
    "href": "applied_lec7.html#running-containers-on-k8s-4",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nStreaming logs in real time\n\n\n\nJust like docker logs, kubectl logs supports convenient options:\n\n-f/--follow to stream logs in real time (à la tail -f)\n--tail to indicate how many lines you want to see (from the end)\n--since to get logs only after a given timestamp\n\nView the latest logs of our ping command:\nkubectl logs pingpong --tail 1 --follow\nStop it with Ctrl-C"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-5",
    "href": "applied_lec7.html#running-containers-on-k8s-5",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nScaling our application\n\n\n\nkubectl gives us a simple command to scale a workload:\nkubectl scale TYPE NAME --replicas=HOWMANY\nLet’s try it on our Pod, so that we have more Pods!\nTry to scale the Pod:\nkubectl scale pod pingpong --replicas=3\n\n🤔 We get the following error, what does that mean?\nError from server (NotFound): the server could not find the requested resource"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-6",
    "href": "applied_lec7.html#running-containers-on-k8s-6",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nScaling a Pod\n\n\n\nWe cannot “scale a Pod”\n(that’s not completely true; we could give it more CPU/RAM)\nIf we want more Pods, we need to create more Pods\n(i.e. execute kubectl run multiple times)\nThere must be a better way!\n(spoiler alert: yes, there is a better way!)"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-7",
    "href": "applied_lec7.html#running-containers-on-k8s-7",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nNotFound\n\n\n\nWhat’s the meaning of that error?\nError from server (NotFound): the server could not find the requested resource\nWhen we execute kubectl scale THAT-RESOURCE --replicas=THAT-MANY,  it is like telling Kubernetes:\ngo to THAT-RESOURCE and set the scaling button to position THAT-MANY\nPods do not have a “scaling button”\nTry to execute the kubectl scale pod command with -v6\nWe see a PATCH request to /scale: that’s the “scaling button”\n(technically it’s called a subresource of the Pod)"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-8",
    "href": "applied_lec7.html#running-containers-on-k8s-8",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nCreating more pods\n\n\n\nWe are going to create a ReplicaSet\n(= set of replicas = set of identical pods)\nIn fact, we will create a Deployment, which itself will create a ReplicaSet\nWhy so many layers? We’ll explain that shortly, don’t worry!"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-9",
    "href": "applied_lec7.html#running-containers-on-k8s-9",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nCreating a Deployment running ping\n\n\n\nLet’s create a Deployment instead of a single Pod\nCreate the Deployment; pay attention to the --:\nkubectl create deployment pingpong --image=alpine -- ping 127.0.0.1\nThe -- is used to separate:\n\noptions/flags of kubectl create\ncommand to run in the container"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-10",
    "href": "applied_lec7.html#running-containers-on-k8s-10",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nWhat has been created?\n\n\n\n\nCheck the resources that were created:\nkubectl get all\nNote: kubectl get all is a lie. It doesn’t show everything.\n\n(But it shows a lot of “usual suspects”, i.e. commonly used resources.)"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-11",
    "href": "applied_lec7.html#running-containers-on-k8s-11",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nThere’s a lot going on here!\n\n\nNAME                            READY   STATUS        RESTARTS   AGE\npod/pingpong                    1/1     Running       0          4m17s\npod/pingpong-6ccbc77f68-kmgfn   1/1     Running       0          11s\n\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   3h45\n\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/pingpong   1/1     1            1           11s\n\nNAME                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/pingpong-6ccbc77f68   1         1         1       11s\nOur new Pod is not named pingpong, but pingpong-xxxxxxxxxxx-yyyyy.\nWe have a Deployment named pingpong, and an extra ReplicaSet, too. What’s going on?"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-12",
    "href": "applied_lec7.html#running-containers-on-k8s-12",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nFrom Deployment to Pod\n\n\nWe have the following resources:\n\ndeployment.apps/pingpong\nThis is the Deployment that we just created.\nreplicaset.apps/pingpong-xxxxxxxxxx\nThis is a Replica Set created by this Deployment.\npod/pingpong-xxxxxxxxxx-yyyyy\nThis is a pod created by the Replica Set.\n\nLet’s explain what these things are."
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-13",
    "href": "applied_lec7.html#running-containers-on-k8s-13",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nPod\n\n\n\nCan have one or multiple containers\nRuns on a single node\n(Pod cannot “straddle” multiple nodes)\nPods cannot be moved\n(e.g. in case of node outage)\nPods cannot be scaled horizontally\n(except by manually creating more Pods)"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-14",
    "href": "applied_lec7.html#running-containers-on-k8s-14",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\n\n\n\nPod details\n\n\n\nA Pod is not a process; it’s an environment for containers\n\nit cannot be “restarted”\nit cannot “crash”\n\nThe containers in a Pod can crash\nThey may or may not get restarted\n(depending on Pod’s restart policy)\nIf all containers exit successfully, the Pod ends in “Succeeded” phase\nIf some containers fail and don’t get restarted, the Pod ends in “Failed” phase"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-15",
    "href": "applied_lec7.html#running-containers-on-k8s-15",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nReplica Set\n\n\n\nSet of identical (replicated) Pods\nDefined by a pod template + number of desired replicas\nIf there are not enough Pods, the Replica Set creates more\n(e.g. in case of node outage; or simply when scaling up)\nIf there are too many Pods, the Replica Set deletes some\n(e.g. if a node was disconnected and comes back; or when scaling down)\nWe can scale up/down a Replica Set\n\nwe update the manifest of the Replica Set\nas a consequence, the Replica Set controller creates/deletes Pods"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-16",
    "href": "applied_lec7.html#running-containers-on-k8s-16",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\n\n\n\nDeployment\n\n\n\nReplica Sets control identical Pods\nDeployments are used to roll out different Pods\n(different image, command, environment variables, …)\nWhen we update a Deployment with a new Pod definition:\n\na new Replica Set is created with the new Pod definition\nthat new Replica Set is progressively scaled up\nmeanwhile, the old Replica Set(s) is(are) scaled down\n\nThis is a rolling update, minimizing application downtime\nWhen we scale up/down a Deployment, it scales up/down its Replica Set"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-17",
    "href": "applied_lec7.html#running-containers-on-k8s-17",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nCan we scale now?\n\n\n\nLet’s try kubectl scale again, but on the Deployment!\nScale our pingpong deployment:\nkubectl scale deployment pingpong --replicas 3\nNote that we could also write it like this:\nkubectl scale deployment/pingpong --replicas 3\nCheck that we now have multiple pods:\nkubectl get pods"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-18",
    "href": "applied_lec7.html#running-containers-on-k8s-18",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nScaling a Replica Set\n\n\n\nWhat if we scale the Replica Set instead of the Deployment?\nThe Deployment would notice it right away and scale back to the initial level\nThe Replica Set makes sure that we have the right numbers of Pods\nThe Deployment makes sure that the Replica Set has the right size\n(conceptually, it delegates the management of the Pods to the Replica Set)\nThis might seem weird (why this extra layer?) but will soon make sense\n(when we will look at how rolling updates work!)"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-19",
    "href": "applied_lec7.html#running-containers-on-k8s-19",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nChecking Deployment logs\n\n\n\nkubectl logs needs a Pod name\nBut it can also work with a type/name\n(e.g. deployment/pingpong)\nView the result of our ping command:\nkubectl logs deploy/pingpong --tail 2\nIt shows us the logs of the first Pod of the Deployment\nWe’ll see later how to get the logs of all the Pods!"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-20",
    "href": "applied_lec7.html#running-containers-on-k8s-20",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nResilience\n\n\n\nThe deployment pingpong watches its replica set\nThe replica set ensures that the right number of pods are running\nWhat happens if pods disappear?\nIn a separate window, watch the list of pods:\nwatch kubectl get pods\n\n\n\nDestroy the pod currently shown by kubectl logs:\nkubectl delete pod pingpong-xxxxxxxxxx-yyyyy"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-21",
    "href": "applied_lec7.html#running-containers-on-k8s-21",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nWhat happened?\n\n\n\nkubectl delete pod terminates the pod gracefully\n(sending it the TERM signal and waiting for it to shutdown)\nAs soon as the pod is in “Terminating” state, the Replica Set replaces it\nBut we can still see the output of the “Terminating” pod in kubectl logs\nUntil 30 seconds later, when the grace period expires\nThe pod is then killed, and kubectl logs exits"
  },
  {
    "objectID": "applied_lec7.html#running-containers-on-k8s-22",
    "href": "applied_lec7.html#running-containers-on-k8s-22",
    "title": "Kubernetes part 1",
    "section": "Running containers on K8s",
    "text": "Running containers on K8s\n\n\n\nDeleting a standalone Pod\n\n\n\nWhat happens if we delete a standalone Pod?\n(like the first pingpong Pod that we created)\nDelete the Pod:\nkubectl delete pod pingpong\n\n\n\nNo replacement Pod gets created because there is no controller watching it\nThat’s why we will rarely use standalone Pods in practice\n(except for e.g. punctual debugging or executing a short supervised task)"
  },
  {
    "objectID": "applied_lab6.html",
    "href": "applied_lab6.html",
    "title": "Applied Analytics: Lab 6",
    "section": "",
    "text": "Docker on AWS\n\nComplete exercise from Docker Compose lecture"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-1",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-1",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\nGentle introduction to YAML\n\n\n\nYAML Ain’t Markup Language (according to yaml.org)\nAlmost required when working with containers:\n\nDocker Compose files\nKubernetes manifests\nMany CI pipelines (GitHub, GitLab…)\n\nMight be a bit difficult to understand though"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-3",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-3",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\nWhat is it?\n\n\n\nData representation language\n\n- country: France\n  capital: Paris\n  code: fr\n  population: 68042591\n- country: Germany\n  capital: Berlin\n  code: de\n  population: 84270625\n- country: Norway\n  capital: Oslo\n  code: no # It's a trap!\n  population: 5425270\n\nEven without knowing YAML, we probably can add a country to that file :)"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-4",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-4",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\nTrying YAML\n\n\n\nMethod 1: in the browser\nhttps://onlineyamltools.com/convert-yaml-to-json\nhttps://onlineyamltools.com/highlight-yaml\nMethod 2: in a shell\nyq . foo.yaml\nMethod 3: in Python\n\nimport yaml;\n\nyaml.safe_load(\"\"\"\n- country: France\n  capital: Paris\n\"\"\")"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-5",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-5",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\nBasic stuff\n\n\n\nStrings, numbers, boolean values, null\nSequences (=arrays, lists)\nMappings (=objects)\nSuperset of JSON\n(if you know JSON, you can just write JSON)\nComments start with #\nA single file can have multiple documents\n(separated by --- on a single line)"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-6",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-6",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\nSequences\n\n\n\nExample: sequence of strings\n[ \"france\", \"germany\", \"norway\" ]\nExample: the same sequence, without the double-quotes\n[ france, germany, norway ]\nExample: the same sequence, in “block collection style” (=multi-line)\n- france\n- germany\n- norway"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-7",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-7",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\nMappings\n\n\n\nExample: mapping strings to numbers\n{ \"france\": 68042591, \"germany\": 84270625, \"norway\": 5425270 }\nExample: the same mapping, without the double-quotes\n{ france: 68042591, germany: 84270625, norway: 5425270 }\nExample: the same mapping, in “block collection style”\nfrance: 68042591\ngermany: 84270625\nnorway: 5425270"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-8",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-8",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\nCombining types\n\n\n\nIn a sequence (or mapping) we can have different types\n(including other sequences or mappings)\nExample:\nquestions: [ name, quest, favorite color ]\nanswers: [ \"Arthur, King of the Britons\", Holy Grail, purple, 42 ]\nNote that we need to quote “Arthur” because of the comma\nNote that we don’t have the same number of elements in questions and answers"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-9",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-9",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\nMore combinations\n\n\n\nExample:\n  - service: nginx\n    ports: [ 80, 443 ]\n  - service: bind\n    ports: [ 53/tcp, 53/udp ]\n  - service: ssh\n    ports: 22\nNote that ports doesn’t always have the same type\n(the code handling that data will probably have to be smart!)"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-10",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-10",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\n⚠️ Automatic booleans\n\n\ncodes:\n  france: fr\n  germany: de\n  norway: no\n{\n  \"codes\": {\n    \"france\": \"fr\",\n    \"germany\": \"de\",\n    \"norway\": false\n  }\n}"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-11",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-11",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\n⚠️ Automatic booleans\n\n\n\nno can become false\n(it depends on the YAML parser used)\nIt should be quoted instead:\n  codes:\n    france: fr\n    germany: de\n    norway: \"no\""
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-12",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-12",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\n⚠️ Automatic floats\n\n\nversion:\n  libfoo: 1.10\n  fooctl: 1.0\n{\n  \"version\": {\n    \"libfoo\": 1.1,\n    \"fooctl\": 1\n  }\n}"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-13",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-13",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\n⚠️ Automatic floats\n\n\n\nTrailing zeros disappear\nThese should also be quoted:\n  version:\n    libfoo: \"1.10\"\n    fooctl: \"1.0\""
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-14",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-14",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\n⚠️ Automatic times\n\n\nportmap:\n- 80:80\n- 22:22\n{\n  \"portmap\": [\n    \"80:80\",\n    1342\n  ]\n}"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-15",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-15",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\n⚠️ Automatic times\n\n\n\n22:22 becomes 1342\nThats 22 minutes and 22 seconds = 1342 seconds\nAgain, it should be quoted"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-16",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-16",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\nDocument separator\n\n\n\nA single YAML file can have multiple documents separated by ---:\n  This is a document consisting of a single string.\n  --- 💡\n  name: The second document\n  type: This one is a mapping (key→value)\n  --- 💡\n  - Third document\n  - This one is a sequence\nSome folks like to add an extra --- at the beginning and/or at the end\n(it’s not mandatory but can help e.g. to cat multiple files together)"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-17",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-17",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\nMulti-line strings\n\n\nTry the following block in a YAML parser:\nadd line breaks: \"in double quoted strings\\n(like this)\"\npreserve line break: |\n  by using\n  a pipe (|)\n  (this is great for embedding shell scripts, configuration files...)\ndo not preserve line breaks: &gt;\n  by using\n  a greater-than (&gt;)\n  (this is great for embedding very long lines)\nSee https://yaml-multiline.info/ for advanced multi-line tips!\n(E.g. to strip or keep extra \\n characters at the end of the block.)"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-18",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-18",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\nAdvanced features\n\n\nAnchors let you “memorize” and re-use content:\ndebian: &debian\n  packages: deb\n  latest-stable: bullseye\n\nalso-debian: *debian\n\nubuntu:\n  &lt;&lt;: *debian\n  latest-stable: jammy"
  },
  {
    "objectID": "applied_lec4.html#gentle-introduction-to-yaml-19",
    "href": "applied_lec4.html#gentle-introduction-to-yaml-19",
    "title": "Docker Compose",
    "section": "Gentle introduction to YAML",
    "text": "Gentle introduction to YAML\n\n\n\nYAML, good or evil?\n\n\n\nNatural progression from XML to JSON to YAML\nThere are other data languages out there\n(e.g. TOML, HCL, domain-specific things crafted with Ruby, CUE…)\nCompromises are made, for instance:\n\nmore user-friendly → more “magic” with side effects\nmore powerful → steeper learning curve\n\nLove it or loathe it but it’s a good idea to understand it!\nInteresting tool if you appreciate YAML: https://carvel.dev/ytt/"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-1",
    "href": "applied_lec4.html#compose-for-development-stacks-1",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nCompose for development stacks\n\n\nDockerfile = great to build one container image.\nWhat if we have multiple containers?\nWhat if some of them require particular docker run parameters?\nHow do we connect them all together?\n… Compose solves these use-cases (and a few more)."
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-3",
    "href": "applied_lec4.html#compose-for-development-stacks-3",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nLife before Compose\n\n\nBefore we had Compose, we would typically write custom scripts to:\n\nbuild container images,\nrun containers using these images,\nconnect the containers together,\nrebuild, restart, update these images and containers."
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-4",
    "href": "applied_lec4.html#compose-for-development-stacks-4",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nLife with Compose\n\n\nCompose enables a simple, powerful onboarding workflow:\n\nCheckout our code.\nRun docker compose up.\nOur app is up and running!"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-5",
    "href": "applied_lec4.html#compose-for-development-stacks-5",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-6",
    "href": "applied_lec4.html#compose-for-development-stacks-6",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nLife after Compose\n\n\n(Or: when do we need something else?)\n\nCompose is not an orchestrator\nIt isn’t designed to need to run containers on multiple nodes\n(it can, however, work with Docker Swarm Mode)\nCompose isn’t ideal if we want to run containers on Kubernetes\n\nit uses different concepts (Compose services ≠ Kubernetes services)\nit needs a Docker Engine (although containerd support might be coming)"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-7",
    "href": "applied_lec4.html#compose-for-development-stacks-7",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nCompose walkthrough\n\n\n\nWrite Dockerfiles\nDescribe our stack of containers in a YAML file (the “Compose file”)\ndocker compose up (or docker compose up -d to run in the background)\nCompose pulls and builds the required images, and starts the containers\nCompose shows the combined logs of all the containers\n(if running in the background, use docker compose logs)\nHit Ctrl-C to stop the whole stack\n(if running in the background, use docker compose stop)"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-8",
    "href": "applied_lec4.html#compose-for-development-stacks-8",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nIterating\n\n\nAfter making changes to our source code, we can:\n\ndocker compose build to rebuild container images\ndocker compose up to restart the stack with the new images\n\nWe can also combine both with docker compose up --build\nCompose will be smart, and only recreate the containers that have changed.\nWhen working with interpreted languages:\n\ndon’t rebuild each time\nleverage a volumes section instead"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-9",
    "href": "applied_lec4.html#compose-for-development-stacks-9",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nLaunching Our First Stack with Compose\n\n\nFirst step: clone the source code for the app we will be working on.\ngit clone https://github.com/jpetazzo/trainingwheels\ncd trainingwheels\nSecond step: start the app.\ndocker compose up\nWatch Compose build and run the app.\nThat Compose stack exposes a web server on port 8000; try connecting to it."
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-10",
    "href": "applied_lec4.html#compose-for-development-stacks-10",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nLaunching Our First Stack with Compose\n\n\nWe should see a web page like this:\n\nEach time we reload, the counter should increase."
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-11",
    "href": "applied_lec4.html#compose-for-development-stacks-11",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nStopping the app\n\n\nWhen we hit Ctrl-C, Compose tries to gracefully terminate all of the containers.\nAfter ten seconds (or if we press ^C again) it will forcibly kill them."
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-12",
    "href": "applied_lec4.html#compose-for-development-stacks-12",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nThe Compose file\n\n\n\nHistorically: docker-compose.yml or .yaml\nRecently (kind of): can also be named compose.yml or .yaml\n\n(Since version 1.28.6, March 2021)"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-13",
    "href": "applied_lec4.html#compose-for-development-stacks-13",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nExample\n\n\nHere is the file used in the demo:\nversion: \"3\"\n\nservices:\n  www:\n    build: www\n    ports:\n      - ${PORT-8000}:5000\n    user: nobody\n    environment:\n      DEBUG: 1\n    command: python counter.py\n    volumes:\n      - ./www:/src\n\n  redis:\n    image: redis"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-14",
    "href": "applied_lec4.html#compose-for-development-stacks-14",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nCompose file structure\n\n\nA Compose file has multiple sections:\n\nservices is mandatory. Each service corresponds to a container.\nversion is optional (it used to be mandatory). It can be ignored.\nnetworks is optional and indicates to which networks containers should be connected. (By default, containers will be connected on a private, per-compose-file network.)\nvolumes is optional and can define volumes to be used and/or shared by the containers."
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-15",
    "href": "applied_lec4.html#compose-for-development-stacks-15",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nCompose file versions\n\n\n\nVersion 1 is legacy and shouldn’t be used.\n(If you see a Compose file without a services block, it’s a legacy v1 file.)\nVersion 2 added support for networks and volumes.\nVersion 3 added support for deployment options (scaling, rolling updates, etc).\n\nThe Docker documentation has excellent information about the Compose file format if you need to know more about versions."
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-16",
    "href": "applied_lec4.html#compose-for-development-stacks-16",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nContainers in Compose file\n\n\nEach service in the YAML file must contain either build, or image.\n\nbuild indicates a path containing a Dockerfile.\nimage indicates an image name (local, or on a registry).\nIf both are specified, an image will be built from the build directory and named image.\n\nThe other parameters are optional.\nThey encode the parameters that you would typically add to docker run.\nSometimes they have several minor improvements."
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-17",
    "href": "applied_lec4.html#compose-for-development-stacks-17",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nContainer parameters\n\n\n\ncommand indicates what to run (like CMD in a Dockerfile).\nports translates to one (or multiple) -p options to map ports. You can specify local ports (i.e. x:y to expose public port x).\nvolumes translates to one (or multiple) -v options. You can use relative paths here.\n\nFor the full list, check: https://docs.docker.com/compose/compose-file/"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-18",
    "href": "applied_lec4.html#compose-for-development-stacks-18",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nEnvironment variables\n\n\n\nWe can use environment variables in Compose files\n(like $THIS or ${THAT})\nWe can provide default values, e.g. ${PORT-8000}\nCompose will also automatically load the environment file .env\n(it should contain VAR=value, one per line)\nThis is a great way to customize build and run parameters\n(base image versions to use, build and run secrets, port numbers…)"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-19",
    "href": "applied_lec4.html#compose-for-development-stacks-19",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nConfiguring a Compose stack\n\n\n\nFollow 12-factor app configuration principles\n(configure the app through environment variables)\nProvide (in the repo) a default environment file suitable for development\n(no secret or sensitive value)\nCopy the default environment file to .env and tweak it\n(or: provide a script to generate .env from a template)"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-20",
    "href": "applied_lec4.html#compose-for-development-stacks-20",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nRunning multiple copies of a stack\n\n\n\nCopy the stack in two different directories, e.g. front and frontcopy\nCompose prefixes images and containers with the directory name:\nfront_www, front_www_1, front_db_1\nfrontcopy_www, frontcopy_www_1, frontcopy_db_1\nAlternatively, use docker compose -p frontcopy\n(to set the --project-name of a stack, which default to the dir name)\nEach copy is isolated from the others (runs on a different network)"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-21",
    "href": "applied_lec4.html#compose-for-development-stacks-21",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nChecking stack status\n\n\nWe have ps, docker ps, and similarly, docker compose ps:\n$ docker compose ps\nName                      Command             State           Ports          \n----------------------------------------------------------------------------\ntrainingwheels_redis_1   /entrypoint.sh red   Up      6379/tcp               \ntrainingwheels_www_1     python counter.py    Up      0.0.0.0:8000-&gt;5000/tcp \nShows the status of all the containers of our stack.\nDoesn’t show the other containers."
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-22",
    "href": "applied_lec4.html#compose-for-development-stacks-22",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nCleaning up (1)\n\n\nIf you have started your application in the background with Compose and want to stop it easily, you can use the kill command:\n$ docker compose kill\nLikewise, docker compose rm will let you remove containers (after confirmation):\n$ docker compose rm\nGoing to remove trainingwheels_redis_1, trainingwheels_www_1\nAre you sure? [yN] y\nRemoving trainingwheels_redis_1...\nRemoving trainingwheels_www_1..."
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-23",
    "href": "applied_lec4.html#compose-for-development-stacks-23",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nCleaning up (2)\n\n\nAlternatively, docker compose down will stop and remove containers.\nIt will also remove other resources, like networks that were created for the application.\n$ docker compose down\nStopping trainingwheels_www_1 ... done\nStopping trainingwheels_redis_1 ... done\nRemoving trainingwheels_www_1 ... done\nRemoving trainingwheels_redis_1 ... done\nUse docker compose down -v to remove everything including volumes."
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-24",
    "href": "applied_lec4.html#compose-for-development-stacks-24",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nSpecial handling of volumes\n\n\n\nWhen an image gets updated, Compose automatically creates a new container\nThe data in the old container is lost…\n…Except if the container is using a volume\nCompose will then re-attach that volume to the new container\n(and data is then retained across database upgrades)\nAll good database images use volumes\n(e.g. all official images)"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-25",
    "href": "applied_lec4.html#compose-for-development-stacks-25",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nGotchas with volumes\n\n\n\nUnfortunately, Docker volumes don’t have labels or metadata\nCompose tracks volumes thanks to their associated container\nIf the container is deleted, the volume gets orphaned\nExample: docker compose down && docker compose up\n\nthe old volume still exists, detached from its container\na new volume gets created\n\ndocker compose down -v/--volumes deletes volumes\n(but not docker compose down && docker compose down -v!)"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-26",
    "href": "applied_lec4.html#compose-for-development-stacks-26",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nManaging volumes explicitly\n\n\nOption 1: named volumes\nservices:\n  app:\n    volumes:\n    - data:/some/path\nvolumes:\n  data:\n\nVolume will be named &lt;project&gt;_data\nIt won’t be orphaned with docker compose down\nIt will correctly be removed with docker compose down -v"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-27",
    "href": "applied_lec4.html#compose-for-development-stacks-27",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nManaging volumes explicitly\n\n\nOption 2: relative paths\nservices:\n  app:\n    volumes:\n    - ./data:/some/path\n\nMakes it easy to colocate the app and its data\n(for migration, backups, disk usage accounting…)\nWon’t be removed by docker compose down -v"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-28",
    "href": "applied_lec4.html#compose-for-development-stacks-28",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nManaging complex stacks\n\n\n\nCompose provides multiple features to manage complex stacks\n(with many containers)\n-f/--file/$COMPOSE_FILE can be a list of Compose files\n(separated by : and merged together)\nServices can be assigned to one or more profiles\n--profile/$COMPOSE_PROFILE can be a list of comma-separated profiles\n(see Using service profiles in the Compose documentation)\nThese variables can be set in .env"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-29",
    "href": "applied_lec4.html#compose-for-development-stacks-29",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nDependencies\n\n\n\nA service can have a depends_on section\n(listing one or more other services)\nThis is used when bringing up individual services\n(e.g. docker compose up blah or docker compose run foo)\n\n⚠️ It doesn’t make a service “wait” for another one to be up!"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-30",
    "href": "applied_lec4.html#compose-for-development-stacks-30",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nA bit of history and trivia\n\n\n\nCompose was initially named “Fig”\nCompose is one of the only components of Docker written in Python\n(almost everything else is in Go)\nIn 2020, Docker introduced “Compose CLI”:\n\ndocker compose command to deploy Compose stacks to some clouds\nin Go instead of Python\nprogressively getting feature parity with docker compose\nalso provides numerous improvements (e.g. leverages BuildKit by default)"
  },
  {
    "objectID": "applied_lec4.html#compose-for-development-stacks-31",
    "href": "applied_lec4.html#compose-for-development-stacks-31",
    "title": "Docker Compose",
    "section": "Compose for development stacks",
    "text": "Compose for development stacks\n\n\n\nExercise — writing a Compose file\n\n\nLet’s write a Compose file for the wordsmith app!\nThe code is at: https://github.com/jpetazzo/wordsmith"
  },
  {
    "objectID": "applied_lab5.html",
    "href": "applied_lab5.html",
    "title": "Applied Analytics: Lab 5",
    "section": "",
    "text": "Overview\nReproduce a Python uv environment in a Dockerfile:\n\nInstall uv in Docker.\nInstall Jupyter Notebook via uv.\nEXPOSE Jupyter notebook port via Docker\nSpecify a volume for notebooks dir that can be shared between container and host."
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations",
    "href": "bigdata_lec8.html#common-geometric-operations",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nDescription\n\n\nGeometric operations refer to a set of methods that can be used to process and analyze geometric features, like points, lines and polygons.\nThese answer the question about how two or more geographic objects relate to each other:\n\nintersect\ntouch\noverlap\nadjacent to one another?\ndistance between them"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-1",
    "href": "bigdata_lec8.html#common-geometric-operations-1",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nRead census data\n\n\n\nimport geopandas as gpd\nfrom pathlib import Path\n\n# Define path do the data\ndata_folder = Path(\"_data/Austin\")\nfp = data_folder / \"austin_pop_density_2019.gpkg\"\n\n# Read in the data and check the contents\ndata = gpd.read_file(fp)\ndata.head()\n\n\n\n\n\n\n\n\npop2019\ntract\narea_km2\npop_density_km2\ngeometry\n\n\n\n\n0\n6070.0\n002422\n4.029772\n1506.288778\nMULTIPOLYGON (((615643.488 3338728.496, 615645...\n\n\n1\n2203.0\n001751\n1.532030\n1437.961394\nMULTIPOLYGON (((618576.586 3359381.053, 618614...\n\n\n2\n7419.0\n002411\n3.960344\n1873.322161\nMULTIPOLYGON (((619200.163 3341784.654, 619270...\n\n\n3\n4229.0\n000401\n2.181762\n1938.341859\nMULTIPOLYGON (((621623.757 3350508.165, 621656...\n\n\n4\n4589.0\n002313\n2.431208\n1887.538658\nMULTIPOLYGON (((621630.247 3345130.744, 621717..."
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-2",
    "href": "bigdata_lec8.html#common-geometric-operations-2",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nCheck geometries\n\n\n\ndata[\"geometry\"].head()\n\n0    MULTIPOLYGON (((615643.488 3338728.496, 615645...\n1    MULTIPOLYGON (((618576.586 3359381.053, 618614...\n2    MULTIPOLYGON (((619200.163 3341784.654, 619270...\n3    MULTIPOLYGON (((621623.757 3350508.165, 621656...\n4    MULTIPOLYGON (((621630.247 3345130.744, 621717...\nName: geometry, dtype: geometry\n\n\n\n# Check data type of the geometry column\ntype(data[\"geometry\"])\n\ngeopandas.geoseries.GeoSeries\n\n\n\n# Check data type of a value in the geometry column\ntype(data[\"geometry\"].values[0])\n\nshapely.geometry.multipolygon.MultiPolygon"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-3",
    "href": "bigdata_lec8.html#common-geometric-operations-3",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nCheck geometries\n\n\nLet’s first plot the original geometries.\n\nimport matplotlib.pyplot as plt\n\ndata.plot(facecolor=\"none\", linewidth=0.2)\n\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-4",
    "href": "bigdata_lec8.html#common-geometric-operations-4",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nCentroid\n\n\n\nThe centroid of a geometry is the geometric center of a given geometry (line, polygon or a geometry collection).\nGeometric centroids can, for example, be used for locating text labels in visualizations.\nThe data should be in a projected coordinate reference system when calculating the centroids.\n\n\ndata.crs.name\n\n'WGS 84 / UTM zone 14N'\n\n\n\ndata[\"geometry\"].centroid.head()\n\n0     POINT (616990.19 3339736.002)\n1    POINT (619378.303 3359650.002)\n2    POINT (620418.753 3342194.171)\n3    POINT (622613.506 3351414.386)\n4    POINT (622605.359 3343869.554)\ndtype: geometry"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-5",
    "href": "bigdata_lec8.html#common-geometric-operations-5",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nCentroid\n\n\nWe can also apply the method directly to the GeoDataFrame, and plot it:\n\ndata.centroid.plot(markersize=1)\n\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-6",
    "href": "bigdata_lec8.html#common-geometric-operations-6",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nUnary union\n\n\nA unary union operation combines multiple geometric objects into a single, unified geometric shape.\nThere are 2 methods: .union_all() and .dissolve().\nBoth the .union_all() and the .dissolve() support two different kind of algorithms to form the merged geometry:\n\n\"unary\" (the default)\n\"coverage\" which is optimized for non-overlapping polygons and can be significantly faster to compute."
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-7",
    "href": "bigdata_lec8.html#common-geometric-operations-7",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nUnary union\n\n\n\nThe .union_all() returns a single geometry object:\n\n\ndata.union_all()\n\n\n\n\n\n\n\n\nThe .union_all() method returns a shapely polygon object out of the results."
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-8",
    "href": "bigdata_lec8.html#common-geometric-operations-8",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nUnary union\n\n\n\nThe .dissolve() method returns a GeoDataFrame as an output (with aggregated attribute information).\n\nAs our data here, does not have any overlapping polygons, we can test how merging the geometries work with the \"coverage\" algorithm:\n\n# Merge geometries using coverage algorithm\ndissolved = data.dissolve(method=\"coverage\")\ndissolved.head()\n\n\n\n\n\n\n\n\ngeometry\npop2019\ntract\narea_km2\npop_density_km2\n\n\n\n\n0\nMULTIPOLYGON (((618353.957 3340531.11, 618350....\n6070.0\n002422\n4.029772\n1506.288778"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-9",
    "href": "bigdata_lec8.html#common-geometric-operations-9",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nUnary union\n\n\n\n# Get the shapely geometry\ndissolved.geometry[0]\n\n\n\n\n\n\n\n\nNotice that by default the .dissolve() simply stores the first row of data as an attribute information. It is, however, possible to control how the attribute information should be aggregated/summarized."
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-10",
    "href": "bigdata_lec8.html#common-geometric-operations-10",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nBounding polygon\n\n\nA bounding polygon, often referred to as a bounding box or envelope, is the smallest rectangular polygon that encloses a given geometry or a set of geometries.\n\nused for preliminary filtering, because it provides a computationally simple way to test for possible intersections or proximities between geometries\nmultiple methods:\n\naxis-aligned envelope,\nminimum rotated rectangle,\nand minimum bounding circle"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-11",
    "href": "bigdata_lec8.html#common-geometric-operations-11",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nBounding polygon\n\n\nIn a GeoDataFrame, we can easily return the minimum axis-aligned bounding rectangle of geometries by using the .envelope attribute which returns the bounding rectangle for each geometry:\n\ndata.envelope.head()\n\n0    POLYGON ((615643.488 3337909.895, 618358.033 3...\n1    POLYGON ((618529.497 3358797, 620192.632 33587...\n2    POLYGON ((619198.456 3340875.421, 621733.88 33...\n3    POLYGON ((621599.087 3350329.32, 623714.365 33...\n4    POLYGON ((621630.247 3343015.679, 624133.189 3...\ndtype: geometry"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-12",
    "href": "bigdata_lec8.html#common-geometric-operations-12",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nBounding polygon\n\n\nIn order to get the bounding rectangle for the whole layer, we first create an union of all geometries using the .union_all() method, and then extract the bounding rectangle for that polygon using the .envelope:\n\ndata.union_all().envelope"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-13",
    "href": "bigdata_lec8.html#common-geometric-operations-13",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nBounding polygon\n\n\nCorner coordinates of the bounding box for a GeoDataFrame can be fetched via the total_bounds attribute:\n\ndata.total_bounds\n\narray([ 608125.39429998, 3337909.89499998,  629828.38850021,\n       3370513.68260002])\n\n\nThe bounds attribute returns the bounding coordinates of each feature:\n\ndata.bounds.head()\n\n\n\n\n\n\n\n\nminx\nminy\nmaxx\nmaxy\n\n\n\n\n0\n615643.4875\n3.337910e+06\n618358.0327\n3.341257e+06\n\n\n1\n618529.4971\n3.358797e+06\n620192.6319\n3.360614e+06\n\n\n2\n619198.4560\n3.340875e+06\n621733.8796\n3.343443e+06\n\n\n3\n621599.0866\n3.350329e+06\n623714.3655\n3.352436e+06\n\n\n4\n621630.2470\n3.343016e+06\n624133.1887\n3.345131e+06"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-14",
    "href": "bigdata_lec8.html#common-geometric-operations-14",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nBounding polygon\n\n\nIn order to get the minimum rotated rectangle, use:\n\ndata.dissolve().minimum_rotated_rectangle().geometry[0]"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-15",
    "href": "bigdata_lec8.html#common-geometric-operations-15",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nBounding polygon\n\n\nAnd for minimum bounding circle:\n\ndata.dissolve().minimum_bounding_circle().geometry[0]"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-16",
    "href": "bigdata_lec8.html#common-geometric-operations-16",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nConvex hull\n\n\nA smallest possible polygon that contains all points in an object.\nIn geocomputation, the convex hull is used for various tasks as it can be used to easily provide a simplified representation of a set of points or a more complex geometry.\n\ndata.convex_hull.head()\n\n0    POLYGON ((616870.883 3337909.895, 616852.964 3...\n1    POLYGON ((619496.705 3358797, 618962.703 33590...\n2    POLYGON ((619848.5 3340875.421, 619811.394 334...\n3    POLYGON ((622145.426 3350329.32, 622132.429 33...\n4    POLYGON ((623931.77 3343015.679, 622426.307 33...\ndtype: geometry"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-17",
    "href": "bigdata_lec8.html#common-geometric-operations-17",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nConvex hull\n\n\nIn order to create a convex hull for the whole extent, we need to first create an union of all polygons.\n\ndata.union_all().convex_hull"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-18",
    "href": "bigdata_lec8.html#common-geometric-operations-18",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nConcave hull\n\n\nA concave hull is a polygon that encloses a set of points but, unlike the convex hull, is allowed to have concavities.\nIn geopandas, the hull is constructed by removing border triangles of a process called Delaunay Triangulation of the points based on specific criteria.\n\ndata.concave_hull().head()\n\n0    POLYGON ((616686.447 3341251.083, 616689.559 3...\n1    POLYGON ((619414.904 3360602.695, 619418.528 3...\n2    POLYGON ((620165.029 3343417.634, 620177.845 3...\n3    POLYGON ((622361.001 3352408.88, 622364.565 33...\n4    POLYGON ((621654.405 3344983.596, 621630.247 3...\ndtype: geometry"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-19",
    "href": "bigdata_lec8.html#common-geometric-operations-19",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nConcave hull\n\n\nThis functionality does not come directly from shapely but is implemented only on geopandas:\n\nconcave_hull = data.dissolve().concave_hull()\nconcave_hull.plot();\n\n\n\n\n\n\n\n\nLooks weird!"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-20",
    "href": "bigdata_lec8.html#common-geometric-operations-20",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nConcave hull\n\n\nThere are parameters we can tinker with:\n\nratio: value between 0.0 - 1.0. The higher the number, the fewer the number of vertices will be kept in the output.\nallow_holes\n\n\n# Create GeoDataFrame of the union\ngdf_union = data.dissolve()\n\n# Ratio 0.05\nconcave_hull_a = gdf_union.concave_hull(ratio=0.05)\n\n# Ratio 0.2\nconcave_hull_b = gdf_union.concave_hull(ratio=0.2)\n\n# Ratio 0.4\nconcave_hull_c = gdf_union.concave_hull(ratio=0.4)"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-21",
    "href": "bigdata_lec8.html#common-geometric-operations-21",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nConcave hull\n\n\nNow, let’s plot these geometries side by side, so it is easy to compare them:\n\nimport matplotlib.pyplot as plt\n\n# Create subplots\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3)\n\n# Plot side by side\nconcave_hull_a.plot(ax=ax1)\nconcave_hull_b.plot(ax=ax2)\nconcave_hull_c.plot(ax=ax3)\n\n# Remove axis texts\nax1.axis(\"off\")\nax2.axis(\"off\")\nax3.axis(\"off\")\n\n# Add titles\nax1.set_title(\"Ratio 0.05\")\nax2.set_title(\"Ratio 0.2\")\nax3.set_title(\"Ratio 0.4\");"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-22",
    "href": "bigdata_lec8.html#common-geometric-operations-22",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nSimplifying geometries\n\n\nAn function in geopandas called .simplify() can also be used to simplify geometries.\n\nIn comparison to the concave hull, this function also works with LineString geometries in addition to polygons.\nThe tolerance parameter can be used to control the level of simplification. The units for this parameter can be meters/degrees depending on coordinates.\n\nUses a Douglas-Peucker algorithm:\n\nrecursively split the original line into smaller parts\nconnect these parts’ endpoints by a straight line\nremove all points whose distance to the straight line is smaller than the tolerance.\ndoes not move any points and it always preserves endpoints of the original line or polygon."
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-23",
    "href": "bigdata_lec8.html#common-geometric-operations-23",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nSimplifying geometries\n\n\n\ndata.simplify(tolerance=1000).head()\n\n0    POLYGON ((615643.488 3338728.496, 616689.559 3...\n1    POLYGON ((619496.705 3358797, 619418.528 33606...\n2    POLYGON ((619200.163 3341784.654, 620177.845 3...\n3    POLYGON ((621623.757 3350508.165, 622387.916 3...\n4    POLYGON ((621630.247 3345130.744, 624133.189 3...\ndtype: geometry\n\n\nIn a similar manner as before, we can easily apply .simplify() to the extent of all geometries by first getting the unary union of the input geometries:\n\ndata.union_all().simplify(tolerance=1000)"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-24",
    "href": "bigdata_lec8.html#common-geometric-operations-24",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nBuffer\n\n\nA buffer refers to a geometric operation that creates a zone around a given geometry (or geometries), usually representing a certain distance from the shape.\n\nDistance can be both positive and negative.\nExample: in transport network analyses, it is good to fetch the transport network also from outside the study area in order to capture routes that go beyond the study area border.\nBuffer can be used with different geometry types, also with Point and LineString objects."
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-25",
    "href": "bigdata_lec8.html#common-geometric-operations-25",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nBuffer\n\n\n\n# 1000 m buffer for each polygon\ndata.buffer(1000).plot(edgecolor=\"white\")\n\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-26",
    "href": "bigdata_lec8.html#common-geometric-operations-26",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nBuffer\n\n\nIf we want only one buffer for the whole area, we first need to combine the geometries into one object before the buffer analysis:\n\n# 1000 m buffer for each polygon\ndata.union_all().buffer(1000)"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-27",
    "href": "bigdata_lec8.html#common-geometric-operations-27",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nDissolving and merging geometries\n\n\nMore details on GeoPandas’s .dissolve():\n\nSpatial data aggregation refers to combining geometries into coarser spatial units based on some attributes\nThe process may also include the calculation of summary statistics.\nSimilar to pandas’s .groupby() method.\n\n\n# Create a new column and add a constant value\ndata[\"dense\"] = 0\n\n# Filter rows with above average pop density and update the column dense\ndata.loc[data[\"pop_density_km2\"] &gt; data[\"pop_density_km2\"].mean(), \"dense\"] = 1\n\nWe can easily check how many densily populated census tracts we got by using the .value_counts() method from pandas:\n\ndata.dense.value_counts()\n\ndense\n0    86\n1    44\nName: count, dtype: int64"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-28",
    "href": "bigdata_lec8.html#common-geometric-operations-28",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nDissolving and merging geometries\n\n\n\nwe can use this dense column to .dissolve() the data into two groups\nand sum up the values in pop2019 and the area_km2 columns by using the aggfunc parameter.\n\n\n# Conduct the aggregation\ndissolved = data[[\"pop2019\", \"area_km2\", \"dense\", \"geometry\"]].dissolve(\n    by=\"dense\", aggfunc=\"sum\"\n)\ndissolved\n\n\n\n\n\n\n\n\ngeometry\npop2019\narea_km2\n\n\ndense\n\n\n\n\n\n\n\n0\nMULTIPOLYGON (((614108.23 3339640.551, 614288....\n368992.0\n231.131494\n\n\n1\nMULTIPOLYGON (((612263.531 3338931.8, 612265.2...\n242943.0\n71.234570"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-29",
    "href": "bigdata_lec8.html#common-geometric-operations-29",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nDissolving and merging geometries\n\n\nVisualize:\n\ndissolved = dissolved.reset_index()\n\n\ndissolved.plot(column=\"dense\")\n\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-30",
    "href": "bigdata_lec8.html#common-geometric-operations-30",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nExercise\n\n\nCreate a 500m buffer zone around the dense areas in Austin and plot a simple map of this zone."
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-31",
    "href": "bigdata_lec8.html#common-geometric-operations-31",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nChanging the active geometry in a GeoDataFrame\n\n\n\nhow do we store the results of the geometric operations into GeoDataFrame?\nhow can we change/update the active geometry of the GeoDataFrame?\n\nApproaches:\n\nOverwrite the existing geometries in the geometry column by storing the new (manipulated) geometries into it.\nCreate a new column (e.g. centroid) and store the new geometries into this one. Then activate/set this column as the “active geometry” for your GeoDataFrame.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nWhen saving the geographic data into disk, you can in most cases only include one column with geometries. An exception to this is GeoParquet file format which supports saving multiple geometry columns."
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-32",
    "href": "bigdata_lec8.html#common-geometric-operations-32",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nChanging the active geometry in a GeoDataFrame\n\n\n\nOverwriting\n\n\n# Make a copy\noption_1 = data.copy()\noption_1[\"geometry\"].head(2)\n\n0    MULTIPOLYGON (((615643.488 3338728.496, 615645...\n1    MULTIPOLYGON (((618576.586 3359381.053, 618614...\nName: geometry, dtype: geometry\n\n\n\n# Update the geometry column with centroids\noption_1[\"geometry\"] = option_1.centroid\nprint(option_1.head(2))\n\n   pop2019   tract  area_km2  pop_density_km2                        geometry  \\\n0   6070.0  002422  4.029772      1506.288778   POINT (616990.19 3339736.002)   \n1   2203.0  001751  1.532030      1437.961394  POINT (619378.303 3359650.002)   \n\n   dense  \n0      0  \n1      0"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-33",
    "href": "bigdata_lec8.html#common-geometric-operations-33",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nChanging the active geometry in a GeoDataFrame\n\n\n\nCreate a new column for storing the centroids.\n\n\n# Make a copy and Create a column with centroids\noption_2 = data.copy()\noption_2[\"centroid\"] = data.centroid\noption_2.head(2)\n\n\n\n\n\n\n\n\npop2019\ntract\narea_km2\npop_density_km2\ngeometry\ndense\ncentroid\n\n\n\n\n0\n6070.0\n002422\n4.029772\n1506.288778\nMULTIPOLYGON (((615643.488 3338728.496, 615645...\n0\nPOINT (616990.19 3339736.002)\n\n\n1\n2203.0\n001751\n1.532030\n1437.961394\nMULTIPOLYGON (((618576.586 3359381.053, 618614...\n0\nPOINT (619378.303 3359650.002)\n\n\n\n\n\n\n\nBy default, geopandas always uses the geometry column. However, we can easily change the active geometry with .set_geometry():\n\n# Use centroids as the GeoDataFrame geometries\noption2 = option_2.set_geometry(\"centroid\")\noption2.head(2)\n\n\n\n\n\n\n\n\npop2019\ntract\narea_km2\npop_density_km2\ngeometry\ndense\ncentroid\n\n\n\n\n0\n6070.0\n002422\n4.029772\n1506.288778\nMULTIPOLYGON (((615643.488 3338728.496, 615645...\n0\nPOINT (616990.19 3339736.002)\n\n\n1\n2203.0\n001751\n1.532030\n1437.961394\nMULTIPOLYGON (((618576.586 3359381.053, 618614...\n0\nPOINT (619378.303 3359650.002)\n\n\n\n\n\n\n\n\noption2.geometry.name\n\n'centroid'"
  },
  {
    "objectID": "bigdata_lec8.html#common-geometric-operations-34",
    "href": "bigdata_lec8.html#common-geometric-operations-34",
    "title": "Big Data: GeoScience part 2",
    "section": "Common geometric operations",
    "text": "Common geometric operations\n\n\n\nChanging the active geometry in a GeoDataFrame\n\n\nVisually check that everything is ok:\n\noption2.plot();"
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-1",
    "href": "bigdata_lec8.html#working-with-map-projections-1",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nCRS\n\n\nWe will use PROJ library which can be used through the pyproj Python library.\nWe will reproject the dataset from the original WGS84 coordinate system into a Lambert Azimuthal Equal Area projection which is the projection that European Union recommends for Europe.\nLet’s start by reading the data from the eu_countries_2022.gpkg file. The CRS information is stored into the .crs attribute:\n\nimport geopandas as gpd\n\n# Read the file\nfp = \"_data/EU_countries/eu_countries_2022.gpkg\"\ndata = gpd.read_file(fp)\n\n# What is the type?\nprint(type(data.crs))\n\n# Check the coordinate reference system information\ndata.crs\n\n&lt;class 'pyproj.crs.crs.CRS'&gt;\n\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich"
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-2",
    "href": "bigdata_lec8.html#working-with-map-projections-2",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nCRS\n\n\nWhat geopandas returns here is in fact a CRS object from the pyproj library. The EPSG code of our data is 4326 which refers to the WGS84 coordinate system. We can look at the coordinates values in the geometry column which are longitude and latitudes decimal degrees:\n\ndata[\"geometry\"].head()\n\n0    MULTIPOLYGON (((13.684 46.4375, 13.511 46.3484...\n1    MULTIPOLYGON (((6.3156 50.497, 6.405 50.3233, ...\n2    MULTIPOLYGON (((28.498 43.4341, 28.0602 43.316...\n3    MULTIPOLYGON (((16.9498 48.5358, 16.8511 48.43...\n4    MULTIPOLYGON (((32.9417 34.6418, 32.559 34.687...\nName: geometry, dtype: geometry"
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-3",
    "href": "bigdata_lec8.html#working-with-map-projections-3",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nReprojecting a GeoDataFrame\n\n\nIn geopandas, the .to_crs() -method of a GeoDataFrame will perform coordinate transformations with given parameters. The method has two alternative parameters:\n\ncrs which accepts CRS information from various formats, such as proj-strings or OGS WKT text;\nepgs that accepts the EPSG-code of a given coordinate system as a number.\n\n\n# Let's make a backup copy of our data\ndata_wgs84 = data.copy()\n\n# Reproject the data\ndata = data.to_crs(epsg=3035)\n\n# Check the new geometry values\ndata[\"geometry\"].head()\n\n0    MULTIPOLYGON (((4604288.477 2598607.47, 459144...\n1    MULTIPOLYGON (((4059689.242 3049361.18, 406508...\n2    MULTIPOLYGON (((5805367.757 2442801.252, 57739...\n3    MULTIPOLYGON (((4833567.363 2848881.974, 48272...\n4    MULTIPOLYGON (((6413299.362 1602181.345, 63782...\nName: geometry, dtype: geometry\n\n\n\n# Check the new EPSG code\ndata.crs.to_epsg()\n\n3035"
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-4",
    "href": "bigdata_lec8.html#working-with-map-projections-4",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nReprojecting a GeoDataFrame\n\n\nLet’s plot the original and reprojected data side-to-side.\n\nimport matplotlib.pyplot as plt\n\n# Make subplots that are next to each other\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 12))\n\n# Plot the data in WGS84 CRS\ndata_wgs84.plot(ax=ax1, facecolor=\"gray\")\n\n# Plot the one with ETRS-LAEA projection\ndata.plot(ax=ax2, facecolor=\"blue\", edgecolor=\"white\", lw=0.5)\n\n# Add titles\nax1.set_title(\"WGS84\")\nax2.set_title(\"ETRS Lambert Azimuthal Equal Area projection\")\n\n# Set aspect ratio as 1\nax1.set_aspect(aspect=1)\nax2.set_aspect(aspect=1)\n\n# Remove empty white space around the plot\nplt.tight_layout()"
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-5",
    "href": "bigdata_lec8.html#working-with-map-projections-5",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nReprojecting a GeoDataFrame\n\n\nLet’s save our projected layer into a Shapefile so that we can use it later.\n\n# Ouput filepath\noutfp = \"_data/EU_countries/Europe_borders_epsg3035.shp\"\n\n# Save to disk\ndata.to_file(outfp)"
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-6",
    "href": "bigdata_lec8.html#working-with-map-projections-6",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nExercise\n\n\nPick some CRS, e.g. ETRS89 / TM35FIN (EPSG:3067). And pick an EU country of your choice (e.g. Finland)\n\nFirst, select Finland from our dataset of EU countries;\nthen, plot two maps of Finland where you compare the WGS84 (EPSG:4326) representation and ETRS89 / TM35FIN (EPSG:3067);\nyou can achieve this by modifying the previous example for the whole EU."
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-7",
    "href": "bigdata_lec8.html#working-with-map-projections-7",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nParsing Coordinate Reference System characteristics\n\n\n\nfrom pyproj import CRS\n\ncrs_object = CRS.from_epsg(3035)\n\nThe resulting CRS object (here stored in variable crs_object) contains:\n\nName of the CRS (ETRS89/LAEA Europe)\nAxis Info of the coordinate system (ellipsoidal or cartesian)\nArea of Use where the given CRS is in use (Europe with bounds (-35.58, 24.6, 44.83, 84.73))\nDatum (European Terrestrial Reference System 1989).\n\n\nprint(\"Name:\", crs_object.name)\nprint(\"Coordinate system:\", crs_object.coordinate_system)\nprint(\"Bounds:\", crs_object.area_of_use.bounds)\nprint(\"Datum:\", crs_object.datum)\n\nName: ETRS89-extended / LAEA Europe\nCoordinate system: cartesian\nBounds: (-35.58, 24.6, 44.83, 84.73)\nDatum: European Terrestrial Reference System 1989 ensemble"
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-8",
    "href": "bigdata_lec8.html#working-with-map-projections-8",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nExport\n\n\n\ncrs_wkt = crs_object.to_wkt()\n\nThe WKT format contains a lot of information. If we were to run print(crs_wkt) we would see output like that below (truncated to save space):\nPROJCRS[\"ETRS89-extended / LAEA Europe\",BASEGEOGCRS[\"ETRS89\",ENSEMBLE[\"European\nTerrestrial Reference System 1989 ensemble\",MEMBER[\"European Terrestrial Referen\nce Frame 1989\"],MEMBER[\"European Terrestrial Reference Frame 1990\"],MEMBER[\"Euro\npean Terrestrial Reference Frame 1991\"],MEMBER[\"European Terrestrial Reference F\nrame 1992\"],MEMBER[\"European Terrestrial Reference Frame 1993\"],MEMBER[\"European\n Terrestrial Reference Frame 1994\"],MEMBER[\"European Terrestrial Reference Frame\n 1996\"],MEMBER[\"European Terrestrial Reference Frame 1997\"],MEMBER[\"European Ter\nrestrial Reference Frame 2000\"],MEMBER[\"European Terrestrial Reference Frame 200\n5\"],MEMBER[\"European Terrestrial Reference Frame 2014\"],ELLIPSOID[\"GRS 1980\",637\n..."
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-9",
    "href": "bigdata_lec8.html#working-with-map-projections-9",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nConvert to EPSG\n\n\nWe first re-initialize the CRS object from the WKT text presentation, and then parse the EPSG code from the CRS:\n\n# Retrieve EPSG code from WKT text\nepsg = CRS(crs_wkt).to_epsg()\nprint(epsg)\n\n3035\n\n\nWe can use a parameter min_confidence=25:\n\nCRS(crs_wkt).to_epsg(min_confidence=25)\n\n3035"
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-10",
    "href": "bigdata_lec8.html#working-with-map-projections-10",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nDefining CRS for a GeoDataFrame\n\n\nFor a GeoDataFrame created from scratch:\n\nfrom shapely.geometry import Point\n\n# Create GeoDataFrame with one point\ngdf = gpd.GeoDataFrame({\"geometry\": Point(24.950899, 60.169158)}, index=[0])\nprint(gdf.crs)\n\nNone\n\n\nThe coordinates for our point are represented in decimal degrees, hence the CRS of our GeoDataFrame should be WGS84.\n\nfrom pyproj import CRS\n\ngdf = gdf.set_crs(CRS.from_epsg(4326))\ngdf.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nAlternatively:\n\ngdf = gdf.set_crs(epsg=4326)\nprint(gdf.crs)\n\nEPSG:4326"
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-11",
    "href": "bigdata_lec8.html#working-with-map-projections-11",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nDefining CRS for a GeoDataFrame\n\n\nYou can provide the CRS information with the crs parameter when creating the dataset as follows:\n\n# Create GeoDataFrame with one point and define the CRS\ngdf = gpd.GeoDataFrame(geometry=[Point(24.950899, 60.169158)], crs=\"EPSG:4326\")\ngdf.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich"
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-12",
    "href": "bigdata_lec8.html#working-with-map-projections-12",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nDefining different map projections\n\n\nLet’s use a global country border dataset obtained from Natural Earth:\n\nimport geopandas\n\nfp = \"_data/Natural_Earth/ne_110m_admin_0_countries.zip\"\nadmin = gpd.read_file(fp)\nadmin.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich"
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-13",
    "href": "bigdata_lec8.html#working-with-map-projections-13",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nDefining different map projections\n\n\n\n# Plot in original crs\nadmin.plot(figsize=(12, 6))\nplt.title(\"WGS84\");"
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-14",
    "href": "bigdata_lec8.html#working-with-map-projections-14",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nDefining different map projections\n\n\nReproject into Web Mercator:\n# Required module\nuv add branca\nuv add folium\nuv add mapclassify\n\nadmin.to_crs(epsg=3857).explore()\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-15",
    "href": "bigdata_lec8.html#working-with-map-projections-15",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nDefining different map projections\n\n\nReproject into Eckert IV:\n\nadmin.to_crs(crs=\"ESRI:54012\").plot(figsize=(12, 12))\nplt.title(\"Eckert IV\")\nplt.axis(\"off\");"
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-16",
    "href": "bigdata_lec8.html#working-with-map-projections-16",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nDefining different map projections\n\n\nWe can also define an Orthographic projection for our map which can be centered to specific point in the world.\nFor doing this, we can specify the CRS using a proj-string and specify the center point with a few of CRS parameters: +lat, +lon (see PROJ documentation for details).\n\nproj_string = \"+proj=ortho +lat_0=60.00 +lon_0=24.0000\"\nadmin.to_crs(crs=proj_string).plot()\nplt.axis(\"off\")\nplt.title(\"Orthographic\");"
  },
  {
    "objectID": "bigdata_lec8.html#working-with-map-projections-17",
    "href": "bigdata_lec8.html#working-with-map-projections-17",
    "title": "Big Data: GeoScience part 2",
    "section": "Working with map projections",
    "text": "Working with map projections\n\n\n\nExercise\n\n\nReproject the map around the place you were born in."
  },
  {
    "objectID": "bigdata_lec8.html#geocoding-1",
    "href": "bigdata_lec8.html#geocoding-1",
    "title": "Big Data: GeoScience part 2",
    "section": "Geocoding",
    "text": "Geocoding\n\n\n\nGeocoding addresses\n\n\nGeocoding is the process of transforming place names or addresses into coordinates (and vice versa).\nWe will use Geopy with Nominatim geocoder.\n\nThe Nominatim API is not meant for super heavy use\nNominatim doesn’t require the use of an API key, but the usage of the Nominatim service is rate-limited to 1 request per second (3600 / hour)\nUsers also need to provide an identifier for their application, and give appropriate attribution to using OpenStreetMap data\nWhen using Nominatim via geopandas and geopy, we can specify a custom user_agent parameter to idenfy our application, and we can add a timeout."
  },
  {
    "objectID": "bigdata_lec8.html#geocoding-2",
    "href": "bigdata_lec8.html#geocoding-2",
    "title": "Big Data: GeoScience part 2",
    "section": "Geocoding",
    "text": "Geocoding\n\n\n\nPackage installation\n\n\nuv add geopy"
  },
  {
    "objectID": "bigdata_lec8.html#geocoding-3",
    "href": "bigdata_lec8.html#geocoding-3",
    "title": "Big Data: GeoScience part 2",
    "section": "Geocoding",
    "text": "Geocoding\n\n\n\nGeocoding addresses\n\n\nWe will geocode addresses stored in a text file called addresses.txt. These addresses are located in the Helsinki Region in Southern Finland. The first rows of the data look like this:\nid;addr\n1000;Itämerenkatu 14, 00101 Helsinki, Finland\n1001;Kampinkuja 1, 00100 Helsinki, Finland\n1002;Kaivokatu 8, 00101 Helsinki, Finland\n1003;Hermannin rantatie 1, 00580 Helsinki, Finland\nLet’s read the data into a DataFrame:\n\n# Import necessary modules\nimport pandas as pd\nimport geopandas as gpd\n\n# Filepath\nfp = \"_data/Helsinki/addresses.txt\"\n\n# Read the data\ndata = pd.read_csv(fp, sep=\";\")\ndata.head()\n\n\n\n\n\n\n\n\nid\naddr\n\n\n\n\n0\n1000\nItämerenkatu 14, 00101 Helsinki, Finland\n\n\n1\n1001\nKampinkuja 1, 00100 Helsinki, Finland\n\n\n2\n1002\nKaivokatu 8, 00101 Helsinki, Finland\n\n\n3\n1003\nHermannin rantatie 1, 00580 Helsinki, Finland\n\n\n4\n1005\nTyynenmerenkatu 9, 00220 Helsinki, Finland"
  },
  {
    "objectID": "bigdata_lec8.html#geocoding-4",
    "href": "bigdata_lec8.html#geocoding-4",
    "title": "Big Data: GeoScience part 2",
    "section": "Geocoding",
    "text": "Geocoding\n\n\n\nGeocoding addresses\n\n\nProceed with geocoding, this might take a while. Don’t forget to set your user_agent.\n\nfrom geopandas.tools import geocode\n\ngeo = geocode(\n    data[\"addr\"], provider=\"nominatim\", user_agent=\"quarto_lnu\", timeout=10\n)\ngeo.head()\n\n\n\n\n\n\n\n\ngeometry\naddress\n\n\n\n\n0\nPOINT (24.91556 60.1632)\nRuoholahti, 14, Itämerenkatu, Ruoholahti, Läns...\n\n\n1\nPOINT (24.93166 60.16905)\nKamppi, 1, Kampinkuja, Kamppi, Eteläinen suurp...\n\n\n2\nPOINT (24.94146 60.17011)\nSocial Burgerjoint, 8, Kaivokatu, Keskusta, Kl...\n\n\n3\nPOINT (24.97687 60.18748)\nKalasatama, 1, Hermannin rantatie, Suvilahti, ...\n\n\n4\nPOINT (24.92151 60.15662)\n9, Tyynenmerenkatu, Jätkäsaari, Länsisatama, E..."
  },
  {
    "objectID": "bigdata_lec8.html#geocoding-5",
    "href": "bigdata_lec8.html#geocoding-5",
    "title": "Big Data: GeoScience part 2",
    "section": "Geocoding",
    "text": "Geocoding\n\n\n\nGeocoding addresses\n\n\nWe can join the data from the original text file to the geocoded result: \n\njoin = geo.join(data)\njoin.head()\n\n\n\n\n\n\n\n\ngeometry\naddress\nid\naddr\n\n\n\n\n0\nPOINT (24.91556 60.1632)\nRuoholahti, 14, Itämerenkatu, Ruoholahti, Läns...\n1000\nItämerenkatu 14, 00101 Helsinki, Finland\n\n\n1\nPOINT (24.93166 60.16905)\nKamppi, 1, Kampinkuja, Kamppi, Eteläinen suurp...\n1001\nKampinkuja 1, 00100 Helsinki, Finland\n\n\n2\nPOINT (24.94146 60.17011)\nSocial Burgerjoint, 8, Kaivokatu, Keskusta, Kl...\n1002\nKaivokatu 8, 00101 Helsinki, Finland\n\n\n3\nPOINT (24.97687 60.18748)\nKalasatama, 1, Hermannin rantatie, Suvilahti, ...\n1003\nHermannin rantatie 1, 00580 Helsinki, Finland\n\n\n4\nPOINT (24.92151 60.15662)\n9, Tyynenmerenkatu, Jätkäsaari, Länsisatama, E...\n1005\nTyynenmerenkatu 9, 00220 Helsinki, Finland\n\n\n\n\n\n\n\nSave to file:\n\n# Output file path\noutfp = \"_data/Helsinki/addresses.gpkg\"\n\n# Save to Shapefile\njoin.to_file(outfp)"
  },
  {
    "objectID": "bigdata_lec8.html#geocoding-6",
    "href": "bigdata_lec8.html#geocoding-6",
    "title": "Big Data: GeoScience part 2",
    "section": "Geocoding",
    "text": "Geocoding\n\n\n\nGeocoding addresses\n\n\nFinally, we can have a look how the points look on a map:\n\njoin.explore(color=\"red\", marker_kwds={\"radius\": 5})\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "bigdata_lec8.html#geocoding-7",
    "href": "bigdata_lec8.html#geocoding-7",
    "title": "Big Data: GeoScience part 2",
    "section": "Geocoding",
    "text": "Geocoding\n\n\n\nExercise\n\n\nDo another round of geocoding with your own list of addresses from anywhere in the world. Are you happy with the result?\nIn the above example we passed the address column to the geocoding function. The geopandas documentation of geopandas.tools.geocode confirms that we should also be able to pass a list of strings to the geocoding tool. So, you should be able to answer this question by defining a list of addresses and using this list as input strings. \n# Use this cell to enter your solution.\naddress_list = [...]"
  },
  {
    "objectID": "bigdata_lec8.html#geocoding-8",
    "href": "bigdata_lec8.html#geocoding-8",
    "title": "Big Data: GeoScience part 2",
    "section": "Geocoding",
    "text": "Geocoding\n\n\n\nReverse geocoding\n\n\nTo do the reverse geocoding, i.e. finding addresses to these points, we can use the reverse_geocode() function of geopandas.\n\npoints = geo[[\"geometry\"]].copy()\npoints.head()\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOINT (24.91556 60.1632)\n\n\n1\nPOINT (24.93166 60.16905)\n\n\n2\nPOINT (24.94146 60.17011)\n\n\n3\nPOINT (24.97687 60.18748)\n\n\n4\nPOINT (24.92151 60.15662)\n\n\n\n\n\n\n\nThe reverse_geocode() works similarly as the geocode() function that we saw earlier, but in this case the function accepts as an input either a GeoSeries, or a list of shapely Point objects.\n\nfrom geopandas.tools import reverse_geocode\n\nreverse_geocoded = reverse_geocode(\n    points.geometry, provider=\"nominatim\", user_agent=\"quarto_lnu\", timeout=10\n)\nreverse_geocoded.head()\n\n\n\n\n\n\n\n\ngeometry\naddress\n\n\n\n\n0\nPOINT (24.91556 60.1632)\nRuoholahti, 14, Itämerenkatu, Ruoholahti, Läns...\n\n\n1\nPOINT (24.93166 60.16905)\nKamppi, 1, Kampinkuja, Kamppi, Eteläinen suurp...\n\n\n2\nPOINT (24.94146 60.17011)\nSocial Burgerjoint, 8, Kaivokatu, Keskusta, Kl...\n\n\n3\nPOINT (24.97687 60.18748)\nKalasatama, 1, Hermannin rantatie, Suvilahti, ...\n\n\n4\nPOINT (24.92151 60.15662)\n9, Tyynenmerenkatu, Jätkäsaari, Länsisatama, E..."
  },
  {
    "objectID": "bigdata_lab9.html",
    "href": "bigdata_lab9.html",
    "title": "Big Data Analytics: Lab 9",
    "section": "",
    "text": "Install Dockerized Postgres: https://hub.docker.com/_/postgres/\nFor python: pip install \"psycopg[binary,pool]\"\n\nUse the docs from https://www.psycopg.org/psycopg3/docs/basic/usage.html.\n\n\n\nWe’ll use a shrunken version of this dataset: https://ual.sg/project/global-streetscapes/.\nA subset of the dataset is available in the data folder.\n\ninfo.csv: file/column descriptions\ngadm.csv: administrative areas for each image\nghsl.csv: degree of urbanisation associated with the location of the image, calculated using the Global Human Settlement Layer (GHSL)\ninstances.csv: counts of object instances detected on each image\nmetadata.csv: common metadata attributes, e.g. when and where the image was taken\nperception.csv: contains the 0-10 scores for each of six perceptual dimensions (Safe, Lively, Beautiful, Wealthy, Boring, Depressing)\nplaces365.csv: contains the place/scene classification for each image.\n\n\n\n\n\n\nThere are at least 3 ways how to do this:\n\n\nuv add sqlalchemy\nfrom sqlalchemy import create_engine\n\nsqlAlchemyUri = 'postgresql+psycopg://&lt;username&gt;:&lt;password&gt;@localhost:5432/lab10'\nengine = create_engine(sqlAlchemyUri)\n\ndf = pd.read_csv(\"somedata.csv\")\ndf.to_sql('somedata', engine)\n\n\n\nUse Dask’s DataFrame to_sql method (https://docs.dask.org/en/latest/generated/dask.dataframe.to_sql.html). It also relies on SQLAlchemy.\n\n\n\nFirst, install csvkit\nuv add csvkit\nUse csvsql to generate a CREATE TABLE statement:\ncsvsql -i postgresql somedata.csv\nExecute the statement generated by the above line. And then execute the COPY FROM from Python:\nwith open('data/places365.csv', 'r') as f:\n    # Notice that we don't need the csv module.\n    with cur.copy(\"COPY places365 FROM STDIN WITH (FORMAT CSV, header)\") as copy:\n        for line in f:\n            copy.write(line)\nAlso: please compare performance of the above 3 methods on a larger (~2GB) file from the original dataset.\n\n\n\n\n\nCreate primary keys on all tables.\nMake metadata the master table, link other tables to it via foreign keys.\n\n\n\n\n\nCalculate average perception scores for:\n\nurban_term column from ghsl table\nplace column from places365 table\ncountry column from gadm table\n\nCreate a MATERIALIZED VIEW (https://www.postgresql.org/docs/15/sql-creatematerializedview.html) that will contain total object instance counts (instance.csv) for each gadm level2 area (gadm.csv)."
  },
  {
    "objectID": "bigdata_lab9.html#installation",
    "href": "bigdata_lab9.html#installation",
    "title": "Big Data Analytics: Lab 9",
    "section": "",
    "text": "Install Dockerized Postgres: https://hub.docker.com/_/postgres/\nFor python: pip install \"psycopg[binary,pool]\"\n\nUse the docs from https://www.psycopg.org/psycopg3/docs/basic/usage.html."
  },
  {
    "objectID": "bigdata_lab9.html#dataset",
    "href": "bigdata_lab9.html#dataset",
    "title": "Big Data Analytics: Lab 9",
    "section": "",
    "text": "We’ll use a shrunken version of this dataset: https://ual.sg/project/global-streetscapes/.\nA subset of the dataset is available in the data folder.\n\ninfo.csv: file/column descriptions\ngadm.csv: administrative areas for each image\nghsl.csv: degree of urbanisation associated with the location of the image, calculated using the Global Human Settlement Layer (GHSL)\ninstances.csv: counts of object instances detected on each image\nmetadata.csv: common metadata attributes, e.g. when and where the image was taken\nperception.csv: contains the 0-10 scores for each of six perceptual dimensions (Safe, Lively, Beautiful, Wealthy, Boring, Depressing)\nplaces365.csv: contains the place/scene classification for each image."
  },
  {
    "objectID": "bigdata_lab9.html#exercises",
    "href": "bigdata_lab9.html#exercises",
    "title": "Big Data Analytics: Lab 9",
    "section": "",
    "text": "There are at least 3 ways how to do this:\n\n\nuv add sqlalchemy\nfrom sqlalchemy import create_engine\n\nsqlAlchemyUri = 'postgresql+psycopg://&lt;username&gt;:&lt;password&gt;@localhost:5432/lab10'\nengine = create_engine(sqlAlchemyUri)\n\ndf = pd.read_csv(\"somedata.csv\")\ndf.to_sql('somedata', engine)\n\n\n\nUse Dask’s DataFrame to_sql method (https://docs.dask.org/en/latest/generated/dask.dataframe.to_sql.html). It also relies on SQLAlchemy.\n\n\n\nFirst, install csvkit\nuv add csvkit\nUse csvsql to generate a CREATE TABLE statement:\ncsvsql -i postgresql somedata.csv\nExecute the statement generated by the above line. And then execute the COPY FROM from Python:\nwith open('data/places365.csv', 'r') as f:\n    # Notice that we don't need the csv module.\n    with cur.copy(\"COPY places365 FROM STDIN WITH (FORMAT CSV, header)\") as copy:\n        for line in f:\n            copy.write(line)\nAlso: please compare performance of the above 3 methods on a larger (~2GB) file from the original dataset.\n\n\n\n\n\nCreate primary keys on all tables.\nMake metadata the master table, link other tables to it via foreign keys.\n\n\n\n\n\nCalculate average perception scores for:\n\nurban_term column from ghsl table\nplace column from places365 table\ncountry column from gadm table\n\nCreate a MATERIALIZED VIEW (https://www.postgresql.org/docs/15/sql-creatematerializedview.html) that will contain total object instance counts (instance.csv) for each gadm level2 area (gadm.csv)."
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nObjectives\n\n\n\nwe will learn how to find k number of closest neighbors based on two GeoDataFrames.\nwe will first aim to find the three nearest public transport stops for each building in the Helsinki Region\nthen we will see how to make a radius query to find all neighbors within specific distance apart from a given location."
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-1",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-1",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nDetails\n\n\n\nK-Nearest Neighbor search techniques are also typically built on top of spatial indices to make the queries more efficient.\nWe need to use another tree structure called KD-Tree (K-dimensional tree) that can provide us information about K-nearest neighbors (i.e. not only the closest).\nKD-tree is similar to R-tree, but the data is ordered and sorted in a bit different manner (see Appendices for further details)."
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-2",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-2",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nKNN search in Python\n\n\n\nwe will use scipy library\nbefore we can do the actual query, we need to build the KD-Tree spatial index.\nin scipy, we can use the KDTree to build the spatial index which is available from the scipy.spatial submodule."
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-3",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-3",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nKNN search in Python\n\n\nIn the following, we use the building_points and stops GeoDataFrames that we already used earlier to find three closest public transport stops for each building.\nLet’s start by reading the data and reproject the data into a metric coordinate reference system (EPSG:3067) so that the distances will be presented as meters:\n\nimport geopandas as gpd\n\n# Read the files and reproject to EPSG:3067\nstops = gpd.read_file(\"_data/Helsinki/pt_stops_helsinki.gpkg\").to_crs(epsg=3067)\nbuilding_points = gpd.read_file(\"_data/Helsinki/building_points_helsinki.zip\").to_crs(\n    epsg=3067\n)\n\nbuilding_points.head(2)\n\n\n\n\n\n\n\n\nname\ngeometry\n\n\n\n\n0\nNone\nPOINT (381166.6 6676424.438)\n\n\n1\nUimastadion\nPOINT (385236.565 6674238.472)"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-4",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-4",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nKNN search in Python\n\n\n\nstops.head(2)\n\n\n\n\n\n\n\n\nstop_name\nstop_lat\nstop_lon\nstop_id\ngeometry\n\n\n\n\n0\nRitarihuone\n60.16946\n24.95667\n1010102\nPOINT (386623.301 6672037.884)\n\n\n1\nKirkkokatu\n60.17127\n24.95657\n1010103\nPOINT (386623.991 6672239.572)\n\n\n\n\n\n\n\n\nstops.shape\n\n(8377, 5)"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-5",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-5",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nKNN search in Python\n\n\n\nAs a first step, we need to build a KDTRee index structure based on the Point coordinates.\nThe KDTree class expects the Point coordinates to be in array format, i.e. not as shapely Point objects which we have stored in the geometry column.\nLuckily, it is very easy to convert the shapely geometries into numpy.array format by chaining a method .get_coordinates() with the .to_numpy() method as follows:\n\n\nbuilding_coords = building_points.get_coordinates().to_numpy()\nstop_coords = stops.geometry.get_coordinates().to_numpy()\n\nstop_coords\n\narray([[ 386623.30055697, 6672037.88387716],\n       [ 386623.99053928, 6672239.57164472],\n       [ 386629.00011373, 6672130.5382358 ],\n       ...,\n       [ 393706.51571504, 6707260.21305267],\n       [ 391372.74617002, 6697807.78260742],\n       [ 388733.41604041, 6714694.15542812]], shape=(8377, 2))"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-6",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-6",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nKNN search in Python\n\n\nWe can now create a KD-Tree index structure as follows:\n\nfrom scipy.spatial import KDTree\n\nstop_kdt = KDTree(stop_coords)\nstop_kdt\n\n&lt;scipy.spatial._kdtree.KDTree at 0x1220d8850&gt;\n\n\nNow we have initialized a KDTree index structure by populating it with stop coordinates."
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-7",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-7",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nKNN search in Python\n\n\nWe can now use the .query() method which goes through all the input coordinates (i.e. buildings) and very quickly calculates which of them is the closest, 2nd closest etc.\nThe method returns the distances to the K-number of nearest neighbors as well as the index of the closest stop to the given building.\nBy passing an argument k=3, we can specify that we want to find three closest neighbors for each building:\n\n# Find the three nearest neighbors from stop KD-Tree for each building\nk_nearest_dist, k_nearest_ix = stop_kdt.query(building_coords, k=3)\n\nlen(k_nearest_dist)\n\n158731"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-8",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-8",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nKNN search in Python\n\n\n\n# Distances to 3 nearest stops\nk_nearest_dist\n\narray([[ 92.67989301, 461.43820422, 466.16915044],\n       [400.24336963, 409.49707253, 410.06137016],\n       [109.81963349, 130.59749777, 133.6424814 ],\n       ...,\n       [135.34174505, 136.28586705, 274.93549394],\n       [ 99.40810774, 118.1492825 , 209.42172325],\n       [ 67.79042163,  71.91370036, 103.08138812]], shape=(158731, 3))\n\n\n\n# Index values of the 3 nearest stops\nk_nearest_ix\n\narray([[1131, 1135, 1125],\n       [ 467,  465,  475],\n       [  61,   64,   13],\n       ...,\n       [4655, 4678, 4614],\n       [4624, 4625, 4680],\n       [4665, 4617, 4619]], shape=(158731, 3))"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-9",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-9",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nKNN search in Python\n\n\n\nNext, we will attach this information back to our GeoDataFrame so that it is easier to do further analyses with the data and create some nice maps out of the data.\nThe data which is returned by the stop_kdt.query() command comes out as an array of lists, where each item (list) contains three values that show the distances between three nearest stops and the given building.\nTo be able to easily merge this data with the original GeoDataFrame containing the building data, we need to transpose our arrays.\nAfter the transpose, the data will be restructured in a way that there will be three arrays and each of these arrays contains the distances/stop-ids for all the buildings in a single list."
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-10",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-10",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nKNN search in Python\n\n\n\n# Make a copy\nk_nearest = building_points.copy()\n\n# Add indices of nearest stops\nk_nearest[\"1st_nearest_idx\"] = k_nearest_ix.T[0]\nk_nearest[\"2nd_nearest_idx\"] = k_nearest_ix.T[1]\nk_nearest[\"3rd_nearest_idx\"] = k_nearest_ix.T[2]\n\n# Add distances\nk_nearest[\"1st_nearest_dist\"] = k_nearest_dist.T[0]\nk_nearest[\"2nd_nearest_dist\"] = k_nearest_dist.T[1]\nk_nearest[\"3rd_nearest_dist\"] = k_nearest_dist.T[2]\n\n\nk_nearest.head()\n\n\n\n\n\n\n\n\nname\ngeometry\n1st_nearest_idx\n2nd_nearest_idx\n3rd_nearest_idx\n1st_nearest_dist\n2nd_nearest_dist\n3rd_nearest_dist\n\n\n\n\n0\nNone\nPOINT (381166.6 6676424.438)\n1131\n1135\n1125\n92.679893\n461.438204\n466.169150\n\n\n1\nUimastadion\nPOINT (385236.565 6674238.472)\n467\n465\n475\n400.243370\n409.497073\n410.061370\n\n\n2\nNone\nPOINT (386317.478 6672100.648)\n61\n64\n13\n109.819633\n130.597498\n133.642481\n\n\n3\nHartwall Arena\nPOINT (385225.109 6676120.56)\n532\n533\n506\n104.632434\n137.706391\n377.331985\n\n\n4\nTalli\nPOINT (385079.733 6676989.745)\n496\n497\n498\n472.248282\n519.685534\n551.358778"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-11",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-11",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nKNN search in Python\n\n\n\nIn the following, we create three separate GeoDataFrames that correspond to the nearest, second nearest and third nearest stops from the buildings.\nWe start by storing the stop_index as a column which allows us to easily merge the data between stops and k_nearest (buildings) GeoDataFrames.\nFor making the table join, we can use the pandas .merge() function in which we use the 1st_nearest_idx, 2nd_nearest_idx and 3rd_nearest_idx as the key on the left GeoDataFrame, while the stop_index is the key on the right GeoDataFrame.\nWe also pass the suffixes=('', '_knearest) argument to the .merge() method\n\n\n# Store the stop index for making the table join\nstops[\"stop_index\"] = stops.index\n\n\n# Merge the geometries of the nearest stops to the GeoDataFrame\nk_nearest_1 = k_nearest.merge(\n    stops[[\"stop_index\", \"geometry\"]],\n    left_on=\"1st_nearest_idx\",\n    right_on=\"stop_index\",\n    suffixes=(\"\", \"_knearest\"),\n)\nk_nearest_1.head(2)\n\n\n\n\n\n\n\n\nname\ngeometry\n1st_nearest_idx\n2nd_nearest_idx\n3rd_nearest_idx\n1st_nearest_dist\n2nd_nearest_dist\n3rd_nearest_dist\nstop_index\ngeometry_knearest\n\n\n\n\n0\nNone\nPOINT (381166.6 6676424.438)\n1131\n1135\n1125\n92.679893\n461.438204\n466.16915\n1131\nPOINT (381256.66 6676446.317)\n\n\n1\nUimastadion\nPOINT (385236.565 6674238.472)\n467\n465\n475\n400.243370\n409.497073\n410.06137\n467\nPOINT (384973.331 6674539.973)"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-12",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-12",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nKNN search in Python\n\n\n\n# Merge the geometries of the 2nd nearest stops to the GeoDataFrame\nk_nearest_2 = k_nearest.merge(\n    stops[[\"stop_index\", \"geometry\"]],\n    left_on=\"2nd_nearest_idx\",\n    right_on=\"stop_index\",\n    suffixes=(\"\", \"_knearest\"),\n)\nk_nearest_2.head(2)\n\n\n\n\n\n\n\n\nname\ngeometry\n1st_nearest_idx\n2nd_nearest_idx\n3rd_nearest_idx\n1st_nearest_dist\n2nd_nearest_dist\n3rd_nearest_dist\nstop_index\ngeometry_knearest\n\n\n\n\n0\nNone\nPOINT (381166.6 6676424.438)\n1131\n1135\n1125\n92.679893\n461.438204\n466.16915\n1135\nPOINT (381625.316 6676474.488)\n\n\n1\nUimastadion\nPOINT (385236.565 6674238.472)\n467\n465\n475\n400.243370\n409.497073\n410.06137\n465\nPOINT (385270.775 6674646.538)"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-13",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-13",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nKNN search in Python\n\n\n\n# Merge the geometries of the 3rd nearest stops to the GeoDataFrame\nk_nearest_3 = k_nearest.merge(\n    stops[[\"stop_index\", \"geometry\"]],\n    left_on=\"3rd_nearest_idx\",\n    right_on=\"stop_index\",\n    suffixes=(\"\", \"_knearest\"),\n)\nk_nearest_3.head(2)\n\n\n\n\n\n\n\n\nname\ngeometry\n1st_nearest_idx\n2nd_nearest_idx\n3rd_nearest_idx\n1st_nearest_dist\n2nd_nearest_dist\n3rd_nearest_dist\nstop_index\ngeometry_knearest\n\n\n\n\n0\nNone\nPOINT (381166.6 6676424.438)\n1131\n1135\n1125\n92.679893\n461.438204\n466.16915\n1125\nPOINT (381632.74 6676429.668)\n\n\n1\nUimastadion\nPOINT (385236.565 6674238.472)\n467\n465\n475\n400.243370\n409.497073\n410.06137\n475\nPOINT (385122.17 6674632.254)"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-14",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-14",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nKNN search in Python\n\n\nNow we can create LineString geometries connecting these Point objects to each other:\n\nfrom shapely import LineString\n\n# Generate LineStrings connecting the building point and K-nearest neighbor\nk_nearest_1[\"geometry\"] = k_nearest_1.apply(\n    lambda row: LineString([row[\"geometry\"], row[\"geometry_knearest\"]]), axis=1\n)\nk_nearest_2[\"geometry\"] = k_nearest_2.apply(\n    lambda row: LineString([row[\"geometry\"], row[\"geometry_knearest\"]]), axis=1\n)\nk_nearest_3[\"geometry\"] = k_nearest_3.apply(\n    lambda row: LineString([row[\"geometry\"], row[\"geometry_knearest\"]]), axis=1\n)\n\nk_nearest_1.head(2)\n\n\n\n\n\n\n\n\nname\ngeometry\n1st_nearest_idx\n2nd_nearest_idx\n3rd_nearest_idx\n1st_nearest_dist\n2nd_nearest_dist\n3rd_nearest_dist\nstop_index\ngeometry_knearest\n\n\n\n\n0\nNone\nLINESTRING (381166.6 6676424.438, 381256.66 66...\n1131\n1135\n1125\n92.679893\n461.438204\n466.16915\n1131\nPOINT (381256.66 6676446.317)\n\n\n1\nUimastadion\nLINESTRING (385236.565 6674238.472, 384973.331...\n467\n465\n475\n400.243370\n409.497073\n410.06137\n467\nPOINT (384973.331 6674539.973)"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-15",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-15",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nKNN search in Python\n\n\nIn the following, we select a specific building and the closest stops from that building. The name column contains information about the names of the buildings which we can use to choose a building of our interest for visualization:\n\n# Find unique building names\nk_nearest.name.unique()\n\narray([None, 'Uimastadion', 'Hartwall Arena', ..., 'Peltolan koulu',\n       'Hilton Helsinki Airport', 'Posti Oy Logistiikkakeskus'],\n      shape=(3010,), dtype=object)"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-16",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-16",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nKNN search in Python\n\n\nThus, let’s filter the data for Hartwall Arena and create an interactive map out of the results, showing the three closest stops indicated with different colors:\n\n# Visualize 3 nearest stops to\nselected_name = \"Hartwall Arena\"\n\nm = k_nearest_1.loc[k_nearest_1[\"name\"] == selected_name].explore(\n    color=\"red\", tiles=\"CartoDB Positron\", max_zoom=16\n)\nm = k_nearest_2.loc[k_nearest_2[\"name\"] == selected_name].explore(m=m, color=\"orange\")\nm = k_nearest_3.loc[k_nearest_3[\"name\"] == selected_name].explore(m=m, color=\"blue\")\nm = stops.explore(m=m, color=\"green\")\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-17",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-17",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nNearest neighbors within radius\n\n\n\nWe aim to find and calculate the number of buildings that are within 200 meters from a given public transport stop.\nFinding all neighbors within a specific search radius can also be done using the KD-Tree spatial index.\nHowever, in this case we actually build the KDTree index for both datasets (buildings and stops) and then use a .query_ball_tree() method to find all neighbors within the radius r:\n\n\nfrom scipy.spatial import KDTree\n\n# Build KDTree indices\nstop_kdt = KDTree(stop_coords)\nbuilding_kdt = KDTree(building_coords)\n\n# Find the three nearest neighbors from stop KD-Tree for each building\nk_nearest_ix = stop_kdt.query_ball_tree(building_kdt, r=200)\n\n\n\n8377"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-18",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-18",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nNearest neighbors within radius\n\n\nNow we have found all the building points within 200 meters from the stops (n=8377).\nThe following shows all the indices for the first stop at index 0:\nk_nearest_ix[0]\n[1129,\n 1130,\n 1155,\n 2054,\n 2055,\n 2056,\n... (output truncated)"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-19",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-19",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nNearest neighbors within radius\n\n\nNow we can easily store the building indices as a new column to the stops GeoDataFrame:\n\nstops[\"building_ids_within_range\"] = k_nearest_ix\nstops.head()\n\n\n\n\n\n\n\n\nstop_name\nstop_lat\nstop_lon\nstop_id\ngeometry\nstop_index\nbuilding_ids_within_range\n\n\n\n\n0\nRitarihuone\n60.169460\n24.956670\n1010102\nPOINT (386623.301 6672037.884)\n0\n[1129, 1130, 1155, 2054, 2055, 2056, 2057, 205...\n\n\n1\nKirkkokatu\n60.171270\n24.956570\n1010103\nPOINT (386623.991 6672239.572)\n1\n[1130, 2054, 2055, 2057, 2058, 2059, 2066, 206...\n\n\n2\nKirkkokatu\n60.170293\n24.956721\n1010104\nPOINT (386629 6672130.538)\n2\n[1129, 1130, 2054, 2055, 2056, 2057, 2058, 205...\n\n\n3\nVironkatu\n60.172580\n24.956554\n1010105\nPOINT (386627.617 6672385.448)\n3\n[2060, 2062, 2063, 2064, 2065, 2066, 2067, 206...\n\n\n4\nVironkatu\n60.172990\n24.956380\n1010106\nPOINT (386619.379 6672431.394)\n4\n[1136, 2060, 2061, 2062, 2063, 2064, 2065, 206..."
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-20",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-20",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nNearest neighbors within radius\n\n\nWith this information, we can for example calculate the number of buildings within 200 meters from each stop. To do this, we can create a simple lambda function that checks the length of the id-list and store the result into column building_cnt:\n\nstops[\"building_cnt\"] = stops[\"building_ids_within_range\"].apply(\n    lambda id_list: len(id_list)\n)\nstops.head()\n\n\n\n\n\n\n\n\nstop_name\nstop_lat\nstop_lon\nstop_id\ngeometry\nstop_index\nbuilding_ids_within_range\nbuilding_cnt\n\n\n\n\n0\nRitarihuone\n60.169460\n24.956670\n1010102\nPOINT (386623.301 6672037.884)\n0\n[1129, 1130, 1155, 2054, 2055, 2056, 2057, 205...\n50\n\n\n1\nKirkkokatu\n60.171270\n24.956570\n1010103\nPOINT (386623.991 6672239.572)\n1\n[1130, 2054, 2055, 2057, 2058, 2059, 2066, 206...\n69\n\n\n2\nKirkkokatu\n60.170293\n24.956721\n1010104\nPOINT (386629 6672130.538)\n2\n[1129, 1130, 2054, 2055, 2056, 2057, 2058, 205...\n56\n\n\n3\nVironkatu\n60.172580\n24.956554\n1010105\nPOINT (386627.617 6672385.448)\n3\n[2060, 2062, 2063, 2064, 2065, 2066, 2067, 206...\n74\n\n\n4\nVironkatu\n60.172990\n24.956380\n1010106\nPOINT (386619.379 6672431.394)\n4\n[1136, 2060, 2061, 2062, 2063, 2064, 2065, 206...\n78"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-21",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-21",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nNearest neighbors within radius\n\n\n\nprint(\"Max number of buildings:\", stops[\"building_cnt\"].max())\nprint(\"Average number of buildings:\", stops[\"building_cnt\"].mean().round(1))\n\nMax number of buildings: 181\nAverage number of buildings: 32.2"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-22",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-22",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nNearest neighbors within radius\n\n\nBy calculating simple statistics from the building_cnt column, we can see that on average there are 32.2 buildings within 200 meters from the public transport stops and the maximum number of buildings within this distance is whopping 181 buildings.\n\nThis indicates very dense neighborhood having numerous buildings in a small area.\nTo better understand, where this kind of neighborhood is located and what does it look like, we can make a map by selecting the rows with highest number of buildings and then plotting the stop and building points within radius:\n\n\nfiltered = stops[\"building_cnt\"] == stops[\"building_cnt\"].max()\nbuilding_ids = stops.loc[filtered].building_ids_within_range.values[0]\n\nm = stops.loc[filtered].explore(\n    tiles=\"CartoDB Positron\", color=\"red\", marker_kwds={\"radius\": 5}, max_zoom=16\n)\nbuilding_points.loc[building_ids].explore(m=m)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "bigdata_lec10.html#k-nearest-neighbor-search-23",
    "href": "bigdata_lec10.html#k-nearest-neighbor-search-23",
    "title": "Big Data: GeoScience part 4",
    "section": "K-Nearest Neighbor search",
    "text": "K-Nearest Neighbor search\n\n\n\nExercise\n\n\nThere is also an alternative approach for making a radius query by calculating a buffer around the stop points and then making a spatial join between these Polygon geometries and the buildings. This approach also allows to make queries between other type of geometries than Points.\n\nTest and try to find all buildings within 200 meters from the transit stops by creating a 200 meter buffer around the stops and then making a spatial join between the buffers and building points.\nCalculate the number of buildings per stop_id.\nDid it take longer to find the nearest buildings using this approach?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "bigdata_lab1.html",
    "href": "bigdata_lab1.html",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "Primary docs:\n\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html\n\nPerformance deps for Pandas:\n\nhttps://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#install-recommended-dependencies\n\n\n\n\n\n\n\nchunksize parameter in Pandas functions.\n\n\n\nUse to_numeric (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html) for downcasting\n\n\n\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n\n\n\n\nuse DataFrame.memory_usage(deep=True) func.\nuse DataFrame.info() func\n\n\n\n\n\nWe’ll get the sample dataset from https://www.kaggle.com/datasets/anthonytherrien/depression-dataset.\n\nimport pandas as pd\n\ndd = pd.read_csv('_files/depression_data.csv')\ndd.head()\n\n\n\n\n\n\n\n\nName\nAge\nMarital Status\nEducation Level\nNumber of Children\nSmoking Status\nPhysical Activity Level\nEmployment Status\nIncome\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\n0\nChristine Barker\n31\nMarried\nBachelor's Degree\n2\nNon-smoker\nActive\nUnemployed\n26265.67\nModerate\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n1\nJacqueline Lewis\n55\nMarried\nHigh School\n1\nNon-smoker\nSedentary\nEmployed\n42710.36\nHigh\nUnhealthy\nFair\nYes\nNo\nNo\nYes\n\n\n2\nShannon Church\n78\nWidowed\nMaster's Degree\n1\nNon-smoker\nSedentary\nEmployed\n125332.79\nLow\nUnhealthy\nGood\nNo\nNo\nYes\nNo\n\n\n3\nCharles Jordan\n58\nDivorced\nMaster's Degree\n3\nNon-smoker\nModerate\nUnemployed\n9992.78\nModerate\nModerate\nPoor\nNo\nNo\nNo\nNo\n\n\n4\nMichael Rich\n18\nSingle\nHigh School\n0\nNon-smoker\nSedentary\nUnemployed\n8595.08\nLow\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n\n\n\n\n\nLet’s see some memory usage stats:\n\ndd.info(memory_usage='deep')\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 413768 entries, 0 to 413767\nData columns (total 16 columns):\n #   Column                        Non-Null Count   Dtype  \n---  ------                        --------------   -----  \n 0   Name                          413768 non-null  object \n 1   Age                           413768 non-null  int64  \n 2   Marital Status                413768 non-null  object \n 3   Education Level               413768 non-null  object \n 4   Number of Children            413768 non-null  int64  \n 5   Smoking Status                413768 non-null  object \n 6   Physical Activity Level       413768 non-null  object \n 7   Employment Status             413768 non-null  object \n 8   Income                        413768 non-null  float64\n 9   Alcohol Consumption           413768 non-null  object \n 10  Dietary Habits                413768 non-null  object \n 11  Sleep Patterns                413768 non-null  object \n 12  History of Mental Illness     413768 non-null  object \n 13  History of Substance Abuse    413768 non-null  object \n 14  Family History of Depression  413768 non-null  object \n 15  Chronic Medical Conditions    413768 non-null  object \ndtypes: float64(1), int64(2), object(13)\nmemory usage: 294.9 MB\n\n\nNote: Pandas stores memory in blocks, managed by BlockManager class. There is a separate block class for each type, like ObjectBlock or FloatBlock.\nNice write-up here: https://uwekorn.com/2020/05/24/the-one-pandas-internal.html.\nLet’s examine memory usage for each type:\n\nfor dtype in ['float','int','object']:\n    selected_dtype = dd.select_dtypes(include=[dtype])\n    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n    mean_usage_mb = mean_usage_b / 1024 ** 2\n    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))\n\nAverage memory usage for float columns: 1.58 MB\nAverage memory usage for int columns: 2.10 MB\nAverage memory usage for object columns: 20.39 MB\n\n\n\n\nLet’s first use iinfo to check ranges for different subtypes:\n\nimport numpy as np\nint_types = [\"uint8\", \"int8\", \"int16\", \"int32\", \"int64\"]\nfor it in int_types:\n    print(np.iinfo(it))\n\nMachine parameters for uint8\n---------------------------------------------------------------\nmin = 0\nmax = 255\n---------------------------------------------------------------\n\nMachine parameters for int8\n---------------------------------------------------------------\nmin = -128\nmax = 127\n---------------------------------------------------------------\n\nMachine parameters for int16\n---------------------------------------------------------------\nmin = -32768\nmax = 32767\n---------------------------------------------------------------\n\nMachine parameters for int32\n---------------------------------------------------------------\nmin = -2147483648\nmax = 2147483647\n---------------------------------------------------------------\n\nMachine parameters for int64\n---------------------------------------------------------------\nmin = -9223372036854775808\nmax = 9223372036854775807\n---------------------------------------------------------------\n\n\n\nWe can use pd.to_numeric() to downcast numeric types.\nFirst, let’s write a helper function for memory usage display:\n\ndef mem_usage(pandas_obj):\n    if isinstance(pandas_obj,pd.DataFrame):\n        usage_b = pandas_obj.memory_usage(deep=True).sum()\n    else: # we assume if not a df it's a series\n        usage_b = pandas_obj.memory_usage(deep=True)\n    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n    return \"{:03.2f} MB\".format(usage_mb)\n\nNote that “Age” and “Number of Children” columns can be presented as unsigned ints. Let’s convert:\n\ndd_int = dd.select_dtypes(include=['int'])\nconverted_int = dd_int.apply(pd.to_numeric,downcast='unsigned')\nprint(\"Before: \", mem_usage(dd_int))\nprint(\"After: \", mem_usage(converted_int))\ncompare_ints = pd.concat([dd_int.dtypes,converted_int.dtypes],axis=1)\ncompare_ints.columns = ['before','after']\ncompare_ints.apply(pd.Series.value_counts)\n\nBefore:  6.31 MB\nAfter:  0.79 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nuint8\nNaN\n2.0\n\n\nint64\n2.0\nNaN\n\n\n\n\n\n\n\nNice. Now let’s process float columns (Income):\n\ndd_float = dd.select_dtypes(include=['float'])\nconverted_float = dd_float.apply(pd.to_numeric,downcast='float')\nprint(\"Before: \", mem_usage(dd_float))\nprint(\"After: \", mem_usage(converted_float))\ncompare_floats = pd.concat([dd_float.dtypes,converted_float.dtypes],axis=1)\ncompare_floats.columns = ['before','after']\ncompare_floats.apply(pd.Series.value_counts)\n\nBefore:  3.16 MB\nAfter:  3.16 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nfloat64\n1\n1\n\n\n\n\n\n\n\nNothing spectacular.\nNow let’s create a new optimized DataFrame:\n\noptimized_dd = dd.copy()\noptimized_dd[converted_int.columns] = converted_int\noptimized_dd[converted_float.columns] = converted_float\nprint(mem_usage(dd))\nprint(mem_usage(optimized_dd))\n\n294.92 MB\n289.40 MB\n\n\nJust a bit. Let’s proceed with object types.\nFirst, read https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/.\nNow, back to the dataset.\nWe can use categoricals (https://pandas.pydata.org/pandas-docs/stable/categorical.html) to optimize object columns.\n\nLet’s look at the number of unique values for each object type:\n\ndd_obj = dd.select_dtypes(include=['object']).copy()\ndd_obj.describe()\n\n\n\n\n\n\n\n\nName\nMarital Status\nEducation Level\nSmoking Status\nPhysical Activity Level\nEmployment Status\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\ncount\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n\n\nunique\n196851\n4\n5\n3\n3\n2\n3\n3\n3\n2\n2\n2\n2\n\n\ntop\nMichael Smith\nMarried\nBachelor's Degree\nNon-smoker\nSedentary\nEmployed\nModerate\nUnhealthy\nFair\nNo\nNo\nNo\nNo\n\n\nfreq\n198\n240444\n124329\n247416\n176850\n265659\n173440\n170817\n196789\n287943\n284880\n302515\n277561\n\n\n\n\n\n\n\nLet’s start with one column first: “Education Level”\n\nedu_level = dd_obj[\"Education Level\"]\nprint(edu_level.head())\nedu_level_cat = edu_level.astype('category')\nprint(edu_level_cat.head())\n\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: object\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: category\nCategories (5, object): ['Associate Degree', 'Bachelor's Degree', 'High School', 'Master's Degree', 'PhD']\n\n\nWe can look at the category codes:\n\nedu_level_cat.head().cat.codes\n\n0    1\n1    2\n2    3\n3    3\n4    2\ndtype: int8\n\n\nCompare memory usage:\n\nprint(mem_usage(edu_level))\nprint(mem_usage(edu_level_cat))\n\n24.92 MB\n0.40 MB\n\n\nWe should only convert objects to categoricals if most values are repeated. We have to pick a threshold, say, 25% of unique values.\nNote: when reading a csv, we can also provide a dtype dictionary param with preferred types.\nConverting all object columns and creating a new optimized DataFrame is left as part of Exercise 1.\n\n\n\n\n\nPick your own dataset from\n\n\nKaggle\nHuggingFace\nor https://archive.ics.uci.edu.\n\n\nUse Dora library installed in Applied lab1 for data cleaning.\n\n\n\n\n\n\n\nPerform numerical and object types conversions aimed at minimizing storage space, as outlined above.\nMeasure performance impact and plot it via e.g. matplotlib."
  },
  {
    "objectID": "bigdata_lab1.html#notes",
    "href": "bigdata_lab1.html#notes",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "chunksize parameter in Pandas functions.\n\n\n\nUse to_numeric (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html) for downcasting\n\n\n\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n\n\n\n\nuse DataFrame.memory_usage(deep=True) func.\nuse DataFrame.info() func"
  },
  {
    "objectID": "bigdata_lab1.html#storage-optimization",
    "href": "bigdata_lab1.html#storage-optimization",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "We’ll get the sample dataset from https://www.kaggle.com/datasets/anthonytherrien/depression-dataset.\n\nimport pandas as pd\n\ndd = pd.read_csv('_files/depression_data.csv')\ndd.head()\n\n\n\n\n\n\n\n\nName\nAge\nMarital Status\nEducation Level\nNumber of Children\nSmoking Status\nPhysical Activity Level\nEmployment Status\nIncome\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\n0\nChristine Barker\n31\nMarried\nBachelor's Degree\n2\nNon-smoker\nActive\nUnemployed\n26265.67\nModerate\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n1\nJacqueline Lewis\n55\nMarried\nHigh School\n1\nNon-smoker\nSedentary\nEmployed\n42710.36\nHigh\nUnhealthy\nFair\nYes\nNo\nNo\nYes\n\n\n2\nShannon Church\n78\nWidowed\nMaster's Degree\n1\nNon-smoker\nSedentary\nEmployed\n125332.79\nLow\nUnhealthy\nGood\nNo\nNo\nYes\nNo\n\n\n3\nCharles Jordan\n58\nDivorced\nMaster's Degree\n3\nNon-smoker\nModerate\nUnemployed\n9992.78\nModerate\nModerate\nPoor\nNo\nNo\nNo\nNo\n\n\n4\nMichael Rich\n18\nSingle\nHigh School\n0\nNon-smoker\nSedentary\nUnemployed\n8595.08\nLow\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n\n\n\n\n\nLet’s see some memory usage stats:\n\ndd.info(memory_usage='deep')\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 413768 entries, 0 to 413767\nData columns (total 16 columns):\n #   Column                        Non-Null Count   Dtype  \n---  ------                        --------------   -----  \n 0   Name                          413768 non-null  object \n 1   Age                           413768 non-null  int64  \n 2   Marital Status                413768 non-null  object \n 3   Education Level               413768 non-null  object \n 4   Number of Children            413768 non-null  int64  \n 5   Smoking Status                413768 non-null  object \n 6   Physical Activity Level       413768 non-null  object \n 7   Employment Status             413768 non-null  object \n 8   Income                        413768 non-null  float64\n 9   Alcohol Consumption           413768 non-null  object \n 10  Dietary Habits                413768 non-null  object \n 11  Sleep Patterns                413768 non-null  object \n 12  History of Mental Illness     413768 non-null  object \n 13  History of Substance Abuse    413768 non-null  object \n 14  Family History of Depression  413768 non-null  object \n 15  Chronic Medical Conditions    413768 non-null  object \ndtypes: float64(1), int64(2), object(13)\nmemory usage: 294.9 MB\n\n\nNote: Pandas stores memory in blocks, managed by BlockManager class. There is a separate block class for each type, like ObjectBlock or FloatBlock.\nNice write-up here: https://uwekorn.com/2020/05/24/the-one-pandas-internal.html.\nLet’s examine memory usage for each type:\n\nfor dtype in ['float','int','object']:\n    selected_dtype = dd.select_dtypes(include=[dtype])\n    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n    mean_usage_mb = mean_usage_b / 1024 ** 2\n    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))\n\nAverage memory usage for float columns: 1.58 MB\nAverage memory usage for int columns: 2.10 MB\nAverage memory usage for object columns: 20.39 MB\n\n\n\n\nLet’s first use iinfo to check ranges for different subtypes:\n\nimport numpy as np\nint_types = [\"uint8\", \"int8\", \"int16\", \"int32\", \"int64\"]\nfor it in int_types:\n    print(np.iinfo(it))\n\nMachine parameters for uint8\n---------------------------------------------------------------\nmin = 0\nmax = 255\n---------------------------------------------------------------\n\nMachine parameters for int8\n---------------------------------------------------------------\nmin = -128\nmax = 127\n---------------------------------------------------------------\n\nMachine parameters for int16\n---------------------------------------------------------------\nmin = -32768\nmax = 32767\n---------------------------------------------------------------\n\nMachine parameters for int32\n---------------------------------------------------------------\nmin = -2147483648\nmax = 2147483647\n---------------------------------------------------------------\n\nMachine parameters for int64\n---------------------------------------------------------------\nmin = -9223372036854775808\nmax = 9223372036854775807\n---------------------------------------------------------------\n\n\n\nWe can use pd.to_numeric() to downcast numeric types.\nFirst, let’s write a helper function for memory usage display:\n\ndef mem_usage(pandas_obj):\n    if isinstance(pandas_obj,pd.DataFrame):\n        usage_b = pandas_obj.memory_usage(deep=True).sum()\n    else: # we assume if not a df it's a series\n        usage_b = pandas_obj.memory_usage(deep=True)\n    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n    return \"{:03.2f} MB\".format(usage_mb)\n\nNote that “Age” and “Number of Children” columns can be presented as unsigned ints. Let’s convert:\n\ndd_int = dd.select_dtypes(include=['int'])\nconverted_int = dd_int.apply(pd.to_numeric,downcast='unsigned')\nprint(\"Before: \", mem_usage(dd_int))\nprint(\"After: \", mem_usage(converted_int))\ncompare_ints = pd.concat([dd_int.dtypes,converted_int.dtypes],axis=1)\ncompare_ints.columns = ['before','after']\ncompare_ints.apply(pd.Series.value_counts)\n\nBefore:  6.31 MB\nAfter:  0.79 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nuint8\nNaN\n2.0\n\n\nint64\n2.0\nNaN\n\n\n\n\n\n\n\nNice. Now let’s process float columns (Income):\n\ndd_float = dd.select_dtypes(include=['float'])\nconverted_float = dd_float.apply(pd.to_numeric,downcast='float')\nprint(\"Before: \", mem_usage(dd_float))\nprint(\"After: \", mem_usage(converted_float))\ncompare_floats = pd.concat([dd_float.dtypes,converted_float.dtypes],axis=1)\ncompare_floats.columns = ['before','after']\ncompare_floats.apply(pd.Series.value_counts)\n\nBefore:  3.16 MB\nAfter:  3.16 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nfloat64\n1\n1\n\n\n\n\n\n\n\nNothing spectacular.\nNow let’s create a new optimized DataFrame:\n\noptimized_dd = dd.copy()\noptimized_dd[converted_int.columns] = converted_int\noptimized_dd[converted_float.columns] = converted_float\nprint(mem_usage(dd))\nprint(mem_usage(optimized_dd))\n\n294.92 MB\n289.40 MB\n\n\nJust a bit. Let’s proceed with object types.\nFirst, read https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/.\nNow, back to the dataset.\nWe can use categoricals (https://pandas.pydata.org/pandas-docs/stable/categorical.html) to optimize object columns.\n\nLet’s look at the number of unique values for each object type:\n\ndd_obj = dd.select_dtypes(include=['object']).copy()\ndd_obj.describe()\n\n\n\n\n\n\n\n\nName\nMarital Status\nEducation Level\nSmoking Status\nPhysical Activity Level\nEmployment Status\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\ncount\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n\n\nunique\n196851\n4\n5\n3\n3\n2\n3\n3\n3\n2\n2\n2\n2\n\n\ntop\nMichael Smith\nMarried\nBachelor's Degree\nNon-smoker\nSedentary\nEmployed\nModerate\nUnhealthy\nFair\nNo\nNo\nNo\nNo\n\n\nfreq\n198\n240444\n124329\n247416\n176850\n265659\n173440\n170817\n196789\n287943\n284880\n302515\n277561\n\n\n\n\n\n\n\nLet’s start with one column first: “Education Level”\n\nedu_level = dd_obj[\"Education Level\"]\nprint(edu_level.head())\nedu_level_cat = edu_level.astype('category')\nprint(edu_level_cat.head())\n\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: object\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: category\nCategories (5, object): ['Associate Degree', 'Bachelor's Degree', 'High School', 'Master's Degree', 'PhD']\n\n\nWe can look at the category codes:\n\nedu_level_cat.head().cat.codes\n\n0    1\n1    2\n2    3\n3    3\n4    2\ndtype: int8\n\n\nCompare memory usage:\n\nprint(mem_usage(edu_level))\nprint(mem_usage(edu_level_cat))\n\n24.92 MB\n0.40 MB\n\n\nWe should only convert objects to categoricals if most values are repeated. We have to pick a threshold, say, 25% of unique values.\nNote: when reading a csv, we can also provide a dtype dictionary param with preferred types.\nConverting all object columns and creating a new optimized DataFrame is left as part of Exercise 1."
  },
  {
    "objectID": "bigdata_lab1.html#exercises",
    "href": "bigdata_lab1.html#exercises",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "Pick your own dataset from\n\n\nKaggle\nHuggingFace\nor https://archive.ics.uci.edu.\n\n\nUse Dora library installed in Applied lab1 for data cleaning.\n\n\n\n\n\n\n\nPerform numerical and object types conversions aimed at minimizing storage space, as outlined above.\nMeasure performance impact and plot it via e.g. matplotlib."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analytics",
    "section": "",
    "text": "Big data analytics / Applied data analytics courses."
  },
  {
    "objectID": "bigdata_lec2.html#moores-law",
    "href": "bigdata_lec2.html#moores-law",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\n\n\nDefinition (1965)\n\n\nNumber of transistors in an integrated circuit (IC) doubles about every two years."
  },
  {
    "objectID": "bigdata_lec2.html#moores-law-1",
    "href": "bigdata_lec2.html#moores-law-1",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law"
  },
  {
    "objectID": "bigdata_lec2.html#moores-law-2",
    "href": "bigdata_lec2.html#moores-law-2",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law"
  },
  {
    "objectID": "bigdata_lec2.html#moores-law-3",
    "href": "bigdata_lec2.html#moores-law-3",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\n\n\nTrends\n\n\n\nGordon Moore: Moore’s law will end by around 2025.\nNvidia CEO Jensen Huang: declared Moore’s law dead in 2022.\n\n\n\n\n\n\n\nPat Gelsinger, Intel CEO, end of 2023\n\n\n\nWe’re no longer in the golden era of Moore’s Law, it’s much, much harder now, so we’re probably doubling effectively closer to every three years now, so we’ve definitely seen a slowing."
  },
  {
    "objectID": "bigdata_lec2.html#edholms-law",
    "href": "bigdata_lec2.html#edholms-law",
    "title": "Big Data: Speeding up computation",
    "section": "Edholm’s law",
    "text": "Edholm’s law\n\n\n\nDefinition (2004)\n\n\n\nthree categories of telecommunication, namely\n\nwireless (mobile),\nnomadic (wireless without mobility)\nand wired networks (fixed),\n\nare in lockstep and gradually converging\nthe bandwidth and data rates double every 18 months, which has proven to be true since the 1970s"
  },
  {
    "objectID": "bigdata_lec2.html#edholms-law-1",
    "href": "bigdata_lec2.html#edholms-law-1",
    "title": "Big Data: Speeding up computation",
    "section": "Edholm’s law",
    "text": "Edholm’s law"
  },
  {
    "objectID": "bigdata_lec2.html#edholms-law-2",
    "href": "bigdata_lec2.html#edholms-law-2",
    "title": "Big Data: Speeding up computation",
    "section": "Edholm’s law",
    "text": "Edholm’s law\n\n\n\n\nData deluge\n\n\n\n90% of data humankind has produced happened in the last two years.\n80% of data could be unstructured\n99% of data produced is never analyzed"
  },
  {
    "objectID": "bigdata_lec2.html#core-counts",
    "href": "bigdata_lec2.html#core-counts",
    "title": "Big Data: Speeding up computation",
    "section": "Core counts",
    "text": "Core counts"
  },
  {
    "objectID": "bigdata_lec2.html#concurrency-vs-parallelism",
    "href": "bigdata_lec2.html#concurrency-vs-parallelism",
    "title": "Big Data: Speeding up computation",
    "section": "Concurrency vs Parallelism",
    "text": "Concurrency vs Parallelism\n\n\n\n\n\nConcurrency Parallelism\n\n\nIndividual steps of both tasks are executed in an interleaved fashion\n\n\n\n\n\n\n\nParallelism\n\n\nTask statements are executed at the same time."
  },
  {
    "objectID": "bigdata_lec2.html#process",
    "href": "bigdata_lec2.html#process",
    "title": "Big Data: Speeding up computation",
    "section": "Process",
    "text": "Process\n\n\n\nDefinition\n\n\nA process can be defined as an instance of a running program with its own memory.\nAlternatively: a context maintained for an executing program.\nProcesses have:\n\nlifetimes\nparents\nchildren.\nmemory/resources allocated."
  },
  {
    "objectID": "bigdata_lec2.html#process-1",
    "href": "bigdata_lec2.html#process-1",
    "title": "Big Data: Speeding up computation",
    "section": "Process",
    "text": "Process"
  },
  {
    "objectID": "bigdata_lec2.html#threads",
    "href": "bigdata_lec2.html#threads",
    "title": "Big Data: Speeding up computation",
    "section": "Threads",
    "text": "Threads\n\n\n\nDefinition\n\n\nA lightweight unit of execution within a process that can operate independently.\nThread is a basic unit to which the operating system allocates processor time.\nManaged with help of thread context, which consists of processor context and information required for thread management."
  },
  {
    "objectID": "bigdata_lec2.html#threads-1",
    "href": "bigdata_lec2.html#threads-1",
    "title": "Big Data: Speeding up computation",
    "section": "Threads",
    "text": "Threads"
  },
  {
    "objectID": "bigdata_lec2.html#green-threadscoroutines",
    "href": "bigdata_lec2.html#green-threadscoroutines",
    "title": "Big Data: Speeding up computation",
    "section": "Green threads/Coroutines",
    "text": "Green threads/Coroutines\n\n\n\nDefinition\n\n\nThese are threads managed by the process runtime, multiplexed onto OS threads."
  },
  {
    "objectID": "bigdata_lec2.html#comparison-table",
    "href": "bigdata_lec2.html#comparison-table",
    "title": "Big Data: Speeding up computation",
    "section": "Comparison table",
    "text": "Comparison table"
  },
  {
    "objectID": "bigdata_lec2.html#imperative",
    "href": "bigdata_lec2.html#imperative",
    "title": "Big Data: Speeding up computation",
    "section": "Imperative",
    "text": "Imperative\n\n\n\nWikipedia definition\n\n\nA programming paradigm of software that uses statements that change a program’s state.\nImperative program is a step-by-step description of program’s algorithm.\n\n\n\n\n\n\nExamples\n\n\n\nFortran\nCOBOL\nC\nPython\nGo"
  },
  {
    "objectID": "bigdata_lec2.html#imperative-1",
    "href": "bigdata_lec2.html#imperative-1",
    "title": "Big Data: Speeding up computation",
    "section": "Imperative",
    "text": "Imperative\n\n\n\nCons\n\n\n\nDifficult to parallelize\nLots of state management\n\n\n\n\n\n\n\nPros\n\n\n\nIntuitive concept, maps well to how people think, program as a recipe.\nEasy to optimize by the compiler"
  },
  {
    "objectID": "bigdata_lec2.html#functional",
    "href": "bigdata_lec2.html#functional",
    "title": "Big Data: Speeding up computation",
    "section": "Functional",
    "text": "Functional\n\n\n\nWikipedia definition\n\n\nA programming paradigm where programs are constructed by applying and composing functions.\n\n\n\n\n\n\nExamples\n\n\n\nML (Ocaml)\nLisp\nHaskell\nJulia"
  },
  {
    "objectID": "bigdata_lec2.html#functional-1",
    "href": "bigdata_lec2.html#functional-1",
    "title": "Big Data: Speeding up computation",
    "section": "Functional",
    "text": "Functional\n\n\n\nCons\n\n\n\nOften non-intuitive to reason about\nDepending on a specific algorithm, might be slower\n\n\n\n\n\n\n\nPros\n\n\n\nEasier to parallelize\nLend themselves beautifully to certain types of problems"
  },
  {
    "objectID": "bigdata_lec2.html#object-oriented",
    "href": "bigdata_lec2.html#object-oriented",
    "title": "Big Data: Speeding up computation",
    "section": "Object-oriented",
    "text": "Object-oriented\n\n\n\nWikipedia definition\n\n\nA programming paradigm based on the concept of objects, which can contain data and code: data in the form of fields (often known as attributes or properties), and code in the form of procedures (often known as methods).\n\n\n\n\n\n\nExamples\n\n\n\nSmalltalk\nJava\nC++\nPython\nC#"
  },
  {
    "objectID": "bigdata_lec2.html#object-oriented-1",
    "href": "bigdata_lec2.html#object-oriented-1",
    "title": "Big Data: Speeding up computation",
    "section": "Object-oriented",
    "text": "Object-oriented"
  },
  {
    "objectID": "bigdata_lec2.html#object-oriented-2",
    "href": "bigdata_lec2.html#object-oriented-2",
    "title": "Big Data: Speeding up computation",
    "section": "Object-oriented",
    "text": "Object-oriented\n\n\n\nCons\n\n\n\nDoes not map well to many problems\nLots of state management\n\n\n\n\n\n\n\nPros\n\n\n\nIntuitively easy to grasp, as human thinking is largely noun-oriented\nUseful for UIs"
  },
  {
    "objectID": "bigdata_lec2.html#symbolic",
    "href": "bigdata_lec2.html#symbolic",
    "title": "Big Data: Speeding up computation",
    "section": "Symbolic",
    "text": "Symbolic\n\n\n\nWikipedia definition\n\n\nA programming paradigm in which the program can manipulate its own formulas and program components as if they were plain data.\n\n\n\n\n\n\nExamples\n\n\n\nLisp\nProlog\nJulia"
  },
  {
    "objectID": "bigdata_lec2.html#lisp",
    "href": "bigdata_lec2.html#lisp",
    "title": "Big Data: Speeding up computation",
    "section": "Lisp",
    "text": "Lisp"
  },
  {
    "objectID": "bigdata_lec2.html#prolog",
    "href": "bigdata_lec2.html#prolog",
    "title": "Big Data: Speeding up computation",
    "section": "Prolog",
    "text": "Prolog\n :::"
  },
  {
    "objectID": "bigdata_lec2.html#typing-1",
    "href": "bigdata_lec2.html#typing-1",
    "title": "Big Data: Speeding up computation",
    "section": "Typing",
    "text": "Typing\n\n\n\nStatic vs dynamic\n\n\n\nstatic: types are known and checked before running the program\ndynamic: types become known when the program is running\n\n\n\n\n\n\n\nStrong vs weak\n\n\n\nstrong: variable types are not changed easily\nweak: types can be changed by the compiler if necessary"
  },
  {
    "objectID": "bigdata_lec2.html#typing-2",
    "href": "bigdata_lec2.html#typing-2",
    "title": "Big Data: Speeding up computation",
    "section": "Typing",
    "text": "Typing"
  },
  {
    "objectID": "bigdata_lab3.html",
    "href": "bigdata_lab3.html",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "Suppose we want to apply several transformations. What can we do?\n\nSeveral maps.\nChain functions with compose.\nCreate a function pipeline with pipe.\n\nSuppose we want to apply \\(f(x) = \\exp(\\sin(x))\\) to a list of numbers.\nOption 1:\n\nimport math\n\nfloat_list = [0.234, -1.96, 5.071]\n\n# several maps\nlst1 = map(math.sin, float_list)\nlst2 = map(math.exp, lst1)\n\nprint(list(lst2))\n\n[1.260956241447609, 0.396447553306686, 0.3920424231508857]\n\n\nOption 2: we’ll use the toolz package we installed in lab2.\n\nfrom toolz.functoolz import compose\n\ncomposed = compose(math.exp, math.sin) # note reverse order\n\nprint(list(map(composed, float_list)))\n\n[1.260956241447609, 0.396447553306686, 0.3920424231508857]\n\n\nOption 3:\nAnd there is also pipe:\n\nfrom toolz.functoolz import pipe\n\npiped = lambda x: pipe(x, math.sin, math.exp) # now the order is proper\n\nprint(list(map(piped, float_list)))\n\n[1.260956241447609, 0.396447553306686, 0.3920424231508857]\n\n\n\n\n\nmap is lazy! This is the opposite to being eager. That’s why we apply list first to materialize results of map application.\nLaziness provides memory efficiency.\n\n\n\nIn addition to map, there are also filter and zip.\nVariations:\n\nfrom itertools import filterfalse\nfrom toolz.dicttoolz import keyfilter, valfilter, itemfilter\n\ndef is_even(x):\n    if x % 2 == 0: return True\n    else: return False\n        \ndef both_are_even(x):\n    k,v = x\n    if is_even(k) and is_even(v): return True\n    else: return False\n        \nprint(list(filterfalse(is_even, range(10))))\n# [1, 3, 5, 7, 9]\nprint(list(keyfilter(is_even, {1:2, 2:3, 3:4, 4:5, 5:6})))\n# [2, 4]\nprint(list(valfilter(is_even, {1:2, 2:3, 3:4, 4:5, 5:6})))\n# [1, 3, 5]\nprint(list(itemfilter(both_are_even, {1:5, 2:4, 3:3, 4:2, 5:1}))) # [2, 4]\n\n[1, 3, 5, 7, 9]\n[2, 4]\n[1, 3, 5]\n[2, 4]\n\n\n\n\n\nprices = [25, 20, 40]\nitems = [\"peas\", \"chicken\", \"beef\"]\n\nzipped = list(zip(prices, items))\n\nimport pandas as pd\n\ndf = pd.DataFrame(list(zipped), columns=['price', 'name'])\n\ndf\n\n\n\n\n\n\n\n\nprice\nname\n\n\n\n\n0\n25\npeas\n\n\n1\n20\nchicken\n\n\n2\n40\nbeef\n\n\n\n\n\n\n\n\n\n\n\nPython documentation defines iterator as:\nAn iterator is an object representing a stream of data; this object returns the data one element at a time.\n\nAn object is called iterable if you can get an iterator for it.\nIterator classes implement __iter__() and __next__() methods. This means one can create their own iterator, like this:\n\nclass SequenceIterator:\n    def __init__(self, sequence):\n        self._sequence = sequence\n        self._index = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self._index &lt; len(self._sequence):\n            item = self._sequence[self._index]\n            self._index += 1\n            return item\n        else:\n            raise StopIteration\n\nAnd then use it:\n\nfor el in SequenceIterator([1,2,3]):\n    print(el)\n\n1\n2\n3\n\n\n\n\n\nGenerators are simply an easy way to create iterators.\nA generator example, using yield keyword:\n\n\n\nGenerators are simply an easy way to create iterators.\nA generator example, using yield keyword:\n\ndef even_numbers(n):\n    i=1\n    while i &lt;= n:\n        yield i*2\n        i += 1\n\nAnd that’s how we use it:\n\nfor i in even_numbers(20):\n    print(i)\n\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\n30\n32\n34\n36\n38\n40\n\n\nA generator example, using an ad-hoc syntax similar to list comprehensions (generator expression):\n\nfirst_100_even = (i*2 for i in range(1,20))\nprint(list(first_100_even))\n\n[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38]\n\n\nMemory usage comparison:\n\n# Generator function\ndef generate_numbers(n):\n    for i in range(n):\n        yield i\n\n# For Loop Example\ndef generate_numbers_list(n):\n    numbers = []\n    for i in range(n):\n        numbers.append(i)\n    return numbers\n\n# comparing memory usage\nimport sys\n\nn = 1000000  # generating 1 million numbers\n\n# memory usage for Generator\ngenerator_memory = sys.getsizeof(generate_numbers(n))\n\n# memory usage for For Loop\nfor_loop_memory = sys.getsizeof(generate_numbers_list(n))\n\nprint(\"memory usage for Generator:\", generator_memory, \"bytes\")\nprint(\"memory usage for For Loop:\", for_loop_memory, \"bytes\")\n\nmemory usage for Generator: 200 bytes\nmemory usage for For Loop: 8448728 bytes\n\n\n\n\n\n\n\n\nNoteWhat to remember\n\n\n\n\nGenerators optimize for memory.\nSometimes lists might be faster (but still use more memory)\nUsed in streaming applications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd now, what is all the fuss about? We can actually use map, reduce, and friends, on iterables, not just on lists:\n\n# map example\ngen_mapped = map(lambda x: x*x, even_numbers(30))\nprint(list(gen_mapped))\n\n[4, 16, 36, 64, 100, 144, 196, 256, 324, 400, 484, 576, 676, 784, 900, 1024, 1156, 1296, 1444, 1600, 1764, 1936, 2116, 2304, 2500, 2704, 2916, 3136, 3364, 3600]\n\n\n\n# reduce example\nfrom functools import reduce\ngen_reduced = reduce(lambda acc, el: acc+el, even_numbers(5), 0)\nprint(gen_reduced)\n\n30\n\n\n\n\n\n\nImplement versions of map, filter, and compose using reduce.\nUsing a csv file you’ve downloaded in lab1, create a generator that will yield first 3 columns of the file. And use this generator to create a Pandas.DataFrame. Hint: use https://docs.python.org/3/library/csv.html\nRead the Pathway tutorial for web scraping (https://pathway.com/developers/user-guide/connect/python-web-scraping/#simple-web-scraping-with-python) that uses Python Generators. Run it locally with Docker. Additional reading: https://docs.python.org/3/howto/functional.html"
  },
  {
    "objectID": "bigdata_lab3.html#function-pipelines",
    "href": "bigdata_lab3.html#function-pipelines",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "Suppose we want to apply several transformations. What can we do?\n\nSeveral maps.\nChain functions with compose.\nCreate a function pipeline with pipe.\n\nSuppose we want to apply \\(f(x) = \\exp(\\sin(x))\\) to a list of numbers.\nOption 1:\n\nimport math\n\nfloat_list = [0.234, -1.96, 5.071]\n\n# several maps\nlst1 = map(math.sin, float_list)\nlst2 = map(math.exp, lst1)\n\nprint(list(lst2))\n\n[1.260956241447609, 0.396447553306686, 0.3920424231508857]\n\n\nOption 2: we’ll use the toolz package we installed in lab2.\n\nfrom toolz.functoolz import compose\n\ncomposed = compose(math.exp, math.sin) # note reverse order\n\nprint(list(map(composed, float_list)))\n\n[1.260956241447609, 0.396447553306686, 0.3920424231508857]\n\n\nOption 3:\nAnd there is also pipe:\n\nfrom toolz.functoolz import pipe\n\npiped = lambda x: pipe(x, math.sin, math.exp) # now the order is proper\n\nprint(list(map(piped, float_list)))\n\n[1.260956241447609, 0.396447553306686, 0.3920424231508857]"
  },
  {
    "objectID": "bigdata_lab3.html#laziness",
    "href": "bigdata_lab3.html#laziness",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "map is lazy! This is the opposite to being eager. That’s why we apply list first to materialize results of map application.\nLaziness provides memory efficiency."
  },
  {
    "objectID": "bigdata_lab3.html#filter",
    "href": "bigdata_lab3.html#filter",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "In addition to map, there are also filter and zip.\nVariations:\n\nfrom itertools import filterfalse\nfrom toolz.dicttoolz import keyfilter, valfilter, itemfilter\n\ndef is_even(x):\n    if x % 2 == 0: return True\n    else: return False\n        \ndef both_are_even(x):\n    k,v = x\n    if is_even(k) and is_even(v): return True\n    else: return False\n        \nprint(list(filterfalse(is_even, range(10))))\n# [1, 3, 5, 7, 9]\nprint(list(keyfilter(is_even, {1:2, 2:3, 3:4, 4:5, 5:6})))\n# [2, 4]\nprint(list(valfilter(is_even, {1:2, 2:3, 3:4, 4:5, 5:6})))\n# [1, 3, 5]\nprint(list(itemfilter(both_are_even, {1:5, 2:4, 3:3, 4:2, 5:1}))) # [2, 4]\n\n[1, 3, 5, 7, 9]\n[2, 4]\n[1, 3, 5]\n[2, 4]\n\n\n\n\n\nprices = [25, 20, 40]\nitems = [\"peas\", \"chicken\", \"beef\"]\n\nzipped = list(zip(prices, items))\n\nimport pandas as pd\n\ndf = pd.DataFrame(list(zipped), columns=['price', 'name'])\n\ndf\n\n\n\n\n\n\n\n\nprice\nname\n\n\n\n\n0\n25\npeas\n\n\n1\n20\nchicken\n\n\n2\n40\nbeef"
  },
  {
    "objectID": "bigdata_lab3.html#iterators",
    "href": "bigdata_lab3.html#iterators",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "Python documentation defines iterator as:\nAn iterator is an object representing a stream of data; this object returns the data one element at a time.\n\nAn object is called iterable if you can get an iterator for it.\nIterator classes implement __iter__() and __next__() methods. This means one can create their own iterator, like this:\n\nclass SequenceIterator:\n    def __init__(self, sequence):\n        self._sequence = sequence\n        self._index = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self._index &lt; len(self._sequence):\n            item = self._sequence[self._index]\n            self._index += 1\n            return item\n        else:\n            raise StopIteration\n\nAnd then use it:\n\nfor el in SequenceIterator([1,2,3]):\n    print(el)\n\n1\n2\n3"
  },
  {
    "objectID": "bigdata_lab3.html#generators",
    "href": "bigdata_lab3.html#generators",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "Generators are simply an easy way to create iterators.\nA generator example, using yield keyword:"
  },
  {
    "objectID": "bigdata_lab3.html#generators-1",
    "href": "bigdata_lab3.html#generators-1",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "Generators are simply an easy way to create iterators.\nA generator example, using yield keyword:\n\ndef even_numbers(n):\n    i=1\n    while i &lt;= n:\n        yield i*2\n        i += 1\n\nAnd that’s how we use it:\n\nfor i in even_numbers(20):\n    print(i)\n\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\n30\n32\n34\n36\n38\n40\n\n\nA generator example, using an ad-hoc syntax similar to list comprehensions (generator expression):\n\nfirst_100_even = (i*2 for i in range(1,20))\nprint(list(first_100_even))\n\n[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38]\n\n\nMemory usage comparison:\n\n# Generator function\ndef generate_numbers(n):\n    for i in range(n):\n        yield i\n\n# For Loop Example\ndef generate_numbers_list(n):\n    numbers = []\n    for i in range(n):\n        numbers.append(i)\n    return numbers\n\n# comparing memory usage\nimport sys\n\nn = 1000000  # generating 1 million numbers\n\n# memory usage for Generator\ngenerator_memory = sys.getsizeof(generate_numbers(n))\n\n# memory usage for For Loop\nfor_loop_memory = sys.getsizeof(generate_numbers_list(n))\n\nprint(\"memory usage for Generator:\", generator_memory, \"bytes\")\nprint(\"memory usage for For Loop:\", for_loop_memory, \"bytes\")\n\nmemory usage for Generator: 200 bytes\nmemory usage for For Loop: 8448728 bytes\n\n\n\n\n\n\n\n\nNoteWhat to remember\n\n\n\n\nGenerators optimize for memory.\nSometimes lists might be faster (but still use more memory)\nUsed in streaming applications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd now, what is all the fuss about? We can actually use map, reduce, and friends, on iterables, not just on lists:\n\n# map example\ngen_mapped = map(lambda x: x*x, even_numbers(30))\nprint(list(gen_mapped))\n\n[4, 16, 36, 64, 100, 144, 196, 256, 324, 400, 484, 576, 676, 784, 900, 1024, 1156, 1296, 1444, 1600, 1764, 1936, 2116, 2304, 2500, 2704, 2916, 3136, 3364, 3600]\n\n\n\n# reduce example\nfrom functools import reduce\ngen_reduced = reduce(lambda acc, el: acc+el, even_numbers(5), 0)\nprint(gen_reduced)\n\n30"
  },
  {
    "objectID": "bigdata_lab3.html#exercises",
    "href": "bigdata_lab3.html#exercises",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "Implement versions of map, filter, and compose using reduce.\nUsing a csv file you’ve downloaded in lab1, create a generator that will yield first 3 columns of the file. And use this generator to create a Pandas.DataFrame. Hint: use https://docs.python.org/3/library/csv.html\nRead the Pathway tutorial for web scraping (https://pathway.com/developers/user-guide/connect/python-web-scraping/#simple-web-scraping-with-python) that uses Python Generators. Run it locally with Docker. Additional reading: https://docs.python.org/3/howto/functional.html"
  },
  {
    "objectID": "bigdata_lec7.html#why-to-use-python-for-geoscience",
    "href": "bigdata_lec7.html#why-to-use-python-for-geoscience",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Why to use Python for Geoscience?",
    "text": "Why to use Python for Geoscience?\n\n\n\nWhy?\n\n\n\nPython has an extensive ecosystem of libraries and tools for GIS: you can do almost anything with spatial data. In fact, there is an enormous geospatial industry that is heavily relying on open source Python libraries.\nPython GIS libraries are easy to learn: there are tons of good and free resources available to learn doing GIS analysis with Python for free (the online version of this book is one example!).\nPython is highly efficient: used for analysing big (and small) geospatial data.\nPython is highly flexible: supports hundreds of spatial data formats (most likely many which you, or us, have never heard of)."
  },
  {
    "objectID": "bigdata_lec7.html#why-to-use-python-for-geoscience-1",
    "href": "bigdata_lec7.html#why-to-use-python-for-geoscience-1",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Why to use Python for Geoscience?",
    "text": "Why to use Python for Geoscience?\n\n\n\nWhy?\n\n\n\nPython allows to mix and match various libraries: You can combine different libraries and methodological approaches together and create sophisticated analytical pipelines.\nEverything is free: you don’t need to buy a license for using the tools.\nYou will learn and understand more deeply how different geoprocessing operations work.\nBy using Python’s open source libraries and codes, you support open science by making it possible for everyone to reproduce your work, free-of-charge.\nYou can plug-in and chain different third-party software (not necessarily written in Python) which allow you to build e.g. fancy web-GIS applications (using e.g. KeplerGL, Dask or GeoDjango for developing the user interface and having PostGIS as a back-end database)."
  },
  {
    "objectID": "bigdata_lec7.html#why-to-use-python-for-geoscience-2",
    "href": "bigdata_lec7.html#why-to-use-python-for-geoscience-2",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Why to use Python for Geoscience?",
    "text": "Why to use Python for Geoscience?\n\n\n\n\n\n\nImportant\n\n\n\nPython libraries are developing fast: an active open source community is continuously improving existing libraries, or creating new libraries for tasks that require new tools. However, it is good to be aware that fast development is not necessarily a guarantee of stability or quality. Hence, as for any open source library (with any programming language), it is good to investigate a bit before starting to use a new open source GIS library whether it has been actively maintained and if it seems to have an active developer community.\nOne drawback of such a broad open source ecosystem compared to using a specific GIS software such as ArcGIS, is that open source tools and their documentation are spread under different Python modules and created by different developers."
  },
  {
    "objectID": "bigdata_lec7.html#core-libraries",
    "href": "bigdata_lec7.html#core-libraries",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Core libraries",
    "text": "Core libraries\n\n\n\nCore libraries\n\n\n\nCore vector libraries: geopandas, pyproj, shapely\nCore raster libraries: xarray, rioxarray, rasterio\n\n\n\n\n\n\nMore info: https://ecosystem.pythongis.org/"
  },
  {
    "objectID": "bigdata_lec7.html#python-ecosystem",
    "href": "bigdata_lec7.html#python-ecosystem",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Python ecosystem",
    "text": "Python ecosystem\n\n\n\nPython ecosystem for GIS and Earth Observation\n\n\n\n\n\nObjectives\n\n\n\nHow can we represent geographic features such as roads, buildings, lakes or mountains on a computer?\nHow can we read an image taken with a satellite sensor and use that in our analyses?\nHow can we create a spatial network that can be used for navigation and finding the fastest route from A to B?"
  },
  {
    "objectID": "bigdata_lec7.html#ways-to-represent-spatial-data",
    "href": "bigdata_lec7.html#ways-to-represent-spatial-data",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Ways to represent spatial data",
    "text": "Ways to represent spatial data\n\n\n\nData model\n\n\n\nvector\nraster\nspatio-temporal\ntopological"
  },
  {
    "objectID": "bigdata_lec7.html#data-models",
    "href": "bigdata_lec7.html#data-models",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Data models",
    "text": "Data models\n\n\n\nConstruction\n\n\n\nvector: constructed from points in geographical space which are connected to each other forming lines and polygons.\nraster: constructed from rectangular cells (also called as pixels) that form a uniform grid, i.e. a raster. The grid is associated to specific geographical location and each cell of the grid contains a value representing some information, such as elevation, temperature or presence/absence."
  },
  {
    "objectID": "bigdata_lec7.html#data-models-1",
    "href": "bigdata_lec7.html#data-models-1",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Data models",
    "text": "Data models"
  },
  {
    "objectID": "bigdata_lec7.html#data-models-2",
    "href": "bigdata_lec7.html#data-models-2",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Data models",
    "text": "Data models\n\n\n\n\n\n\nNote\n\n\nThe roads on the top-left could be represented as a network, which is a vector-based data structure consisting of intersections (called nodes) that are represented as points, and streets connecting the nodes that are represented as lines (called edges).\nBecause the vector and raster data models are very different, there are typically a different set of GIS tools and methodologies applied for raster and vector data. However, the vector and raster worlds are not totally isolated from each other, as in many cases it is useful to convert the data from one format to another for specific operations."
  },
  {
    "objectID": "bigdata_lec7.html#vector-data",
    "href": "bigdata_lec7.html#vector-data",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector data",
    "text": "Vector data\n\n\n\nGeometry types\n\n\n\npoints\nlines\nareas.\n\n\n\n\n\n\n\nSpecification\n\n\nDefined in Simple Features Access Specification, which is a standard (ISO 19125-1) formalized by the Open Geospatial Consortium and International Organization for Standardization.\nText representation follows a specification called {term}Well-known text (WKT).\nThe geometries can also be represented in binary format, which is called {term}Well-known binary (WKB)."
  },
  {
    "objectID": "bigdata_lec7.html#vector-data-1",
    "href": "bigdata_lec7.html#vector-data-1",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector data",
    "text": "Vector data\n\n\n\nPoint\n\n\nPoint-object represents a single point in geographic space and the location of the point in space is determined with coordinates.\nPoints can be either two-dimensional. A single pair of coordinates forming a point is commonly called as a coordinate tuple.\n\n\n\n\n\n\nLineString\n\n\nLineString-object (i.e. a line) represents a sequence of points joined together to form a line. Hence, a line consist of a list of at least two coordinate tuples.\n\n\n\n\n\n\nPolygon\n\n\nPolygon-object represents a filled area that consists of a list of at least 3 coordinate tuples that forms the outerior ring (LinearRing) and a possible list of holes."
  },
  {
    "objectID": "bigdata_lec7.html#vector-data-2",
    "href": "bigdata_lec7.html#vector-data-2",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector data",
    "text": "Vector data\n\n\n\nCollections\n\n\nIt is also possible to have a collection of geometric objects (i.e. multiple points, lines or areas) represented as MultiPoint, MultiLineString and MultiPolygon.\nGeometry collections can be useful for example when you want to present multiple building polygons belonging to the same property as a single entity."
  },
  {
    "objectID": "bigdata_lec7.html#vector-data-3",
    "href": "bigdata_lec7.html#vector-data-3",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector data",
    "text": "Vector data"
  },
  {
    "objectID": "bigdata_lec7.html#attribute-data",
    "href": "bigdata_lec7.html#attribute-data",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Attribute data",
    "text": "Attribute data\n\n\n\nDefinition\n\n\n\ninformation associated with geometry\ndescribe the given entity with various possible characteristics.\nbuilding example:\n\nnumber of floors\nheight of the building\nhow many people live there.\n\nalways linked to the geometries via a unique feature identifier (Shapefiles) or structurally (GeoJSON)"
  },
  {
    "objectID": "bigdata_lec7.html#vector-data-formats",
    "href": "bigdata_lec7.html#vector-data-formats",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector data formats",
    "text": "Vector data formats\n\n\n\nMain options\n\n\n\nfile saved to disk\nspatially-aware database, such as PostGIS.\n\nAround 80 file formats are supported the Geospatial Data Abstraction Library (GDAL) [^GDAL]."
  },
  {
    "objectID": "bigdata_lec7.html#vector-data-formats-1",
    "href": "bigdata_lec7.html#vector-data-formats-1",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector data formats",
    "text": "Vector data formats\n\n\n\nData formats\n\n\n\nShapefile: Still a widely used data format for storing geospatial vector data, developed and introduced by ESRI in the early 1990s.\n\n.shp: feature geometries\n.shx positional index for the feature geometries\n.dbf attribute information.\n.prj information about the coordinate reference system of the dataset."
  },
  {
    "objectID": "bigdata_lec7.html#vector-data-formats-2",
    "href": "bigdata_lec7.html#vector-data-formats-2",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector data formats",
    "text": "Vector data formats\n\n\n\nData formats\n\n\n\nGeoJSON: GeoJSON [^geojson] is an open standard format for encoding a variety of geographic data structures along with their attribute data which can be stored into a simple text file. Variation: TopoJSON\n\n{\"type\": \"FeatureCollection\",\n  \"features\": [\n    {\"type\": \"Feature\", \"properties\": {\"id\": 75553155, \"timestamp\": 1494181812},\n      \"geometry\": {\"type\": \"MultiLineString\",\n        \"coordinates\": [[[26.938, 60.520], [26.938, 60.520]], [[26.937, 60.521],\n                         [26.937, 60.521]], [[26.937, 60.521], [26.936, 60.522]]]\n      }\n    }, \n    {\"type\": \"Feature\", \"properties\": {\"id\": 424099695, \"timestamp\": 1465572910}, \n      \"geometry\": {\"type\": \"Polygon\",\n        \"coordinates\": [[[26.935, 60.521], [26.935, 60.521], [26.935, 60.521],\n                         [26.935, 60.521], [26.935, 60.521]]]\n      }\n    }\n  ]\n}"
  },
  {
    "objectID": "bigdata_lec7.html#vector-data-formats-3",
    "href": "bigdata_lec7.html#vector-data-formats-3",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector data formats",
    "text": "Vector data formats\n\n\n\nData formats\n\n\n\nGeoPackage: GeoPackage uses a SQLite database container to store the data. Can be used for both vector and raster (limited support).\nGeoParquet: GeoParquet is one of the newest file formats to store geographic data. In this format, the data is stored in Apache Parquet which is a popular open source, column-oriented data file format designed for efficient data storage and retrieval.\nGML: Geography Markup Language (GML) is an XML based data format. GML serves as a modeling language for geographic systems as well as an open interchange format for geographic transactions on the Internet. Able to integrate all forms of geographic information, including not only conventional “vector” or discrete objects, but also coverages and sensor data.\n\nCityGML\nIndoorGML"
  },
  {
    "objectID": "bigdata_lec7.html#raster-data-and-arrays",
    "href": "bigdata_lec7.html#raster-data-and-arrays",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Raster data and arrays",
    "text": "Raster data and arrays\n\n\n\nBasics\n\n\n\nthe data is represented as arrays of cells, also called as pixels, to represent real-world objects or continuous phenomena.\ndigital cameras typically capture the world using Red, Green and Blue (RGB) colors and stores this information in pixels as separate layers (as called as {term}bands or channels) for each color.\nIn a similar manner, we can store other information to pixels, such as elevation or temperature data (which have only one layer or band), or more detailed spectral information that capture how the light reflects from objects on earth at different wave-lengths (which is what e.g. satellite sensors do).\nwe might have tens or even hundreds of different bands (as with hyperspectral imaging)"
  },
  {
    "objectID": "bigdata_lec7.html#raster-data-and-arrays-1",
    "href": "bigdata_lec7.html#raster-data-and-arrays-1",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Raster data and arrays",
    "text": "Raster data and arrays\n\n\n\nVariety\n\n\n\nspatial resolution, i.e. the size of a single pixel\ntemporal resolution, i.e. how frequently the data is captured from the same area of the Earth\nspectral resolution, i.e. the number and location of spectral bands in the electromagnetic spectrum\nradiometric resolution, i.e. the range of available brightness values (bit depth), usually measured in bits (binary digits)\nspatial extent, i.e. how large area of the world a single image represents\ncoordinate reference system, i.e. in what CRS the data is represented"
  },
  {
    "objectID": "bigdata_lec7.html#raster-data-and-arrays-2",
    "href": "bigdata_lec7.html#raster-data-and-arrays-2",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Raster data and arrays",
    "text": "Raster data and arrays"
  },
  {
    "objectID": "bigdata_lec7.html#raster-data-and-arrays-3",
    "href": "bigdata_lec7.html#raster-data-and-arrays-3",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Raster data and arrays",
    "text": "Raster data and arrays\n\n\n\nBit depth\n\n\n\nOne fundamental way to characterize a raster dataset is based on their bit depth (also called pixel depth) that provides information about the {term}radiometric resolution of the data.\nThe bit depth defines the range of distinct values that the raster can store. For example, a 1-bit raster can only store 2 distinct values: 0 and 1, whereas 8-bit raster can have 256 different values that range between 0 to 255.5."
  },
  {
    "objectID": "bigdata_lec7.html#raster-data-and-arrays-4",
    "href": "bigdata_lec7.html#raster-data-and-arrays-4",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Raster data and arrays",
    "text": "Raster data and arrays\n\nExamples of raster bit depths."
  },
  {
    "objectID": "bigdata_lec7.html#raster-data-formats",
    "href": "bigdata_lec7.html#raster-data-formats",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Raster data formats",
    "text": "Raster data formats\n\n\n\nFormats\n\n\n\nGeoTIFF: GeoTIFF is a popular open, non-proprietary raster data format based on TIFF format originally developed at NASA in the early 90’s.\nCOG: A Cloud Optimized GeoTIFF (COG) is a file format based on GeoTIFF. COG files are typically hosted on HTTP file servers. The format can enable more efficient workflows with large data files by leveraging the ability of users to retrieve just the parts of the file that they need, instead of downloading the whole file every time. The file extension of COG is .tif (i.e. the same as with GeoTIFF).\nNetCDF: Network Common Data Form (NetCDF) is a portable, self-describing and scalable file format for storing array-oriented multidimensional scientific data, commonly used for storing earth science data. Variables stored in NetCDF are often measured multiple times per day over large (e.g. continental) areas. The file extension of NetCDF is .nc4."
  },
  {
    "objectID": "bigdata_lec7.html#raster-data-formats-1",
    "href": "bigdata_lec7.html#raster-data-formats-1",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Raster data formats",
    "text": "Raster data formats\n\n\n\nFormats\n\n\n\nASCII Grid: The ASCII Raster File format is a simple format that can be used to transfer raster data between various applications. The file format was originally developed by ESRI and it is also known as Arc/Info ASCII grid. The file extension of ASCII Raster File is .asc.\nIMG: The ERDAS Imagine file format (IMG) is a proprietary file format that was originally created by an image processing software company called ERDAS. The file can be accompanied with an .xml file which stores metadata information about the raster layer. The file extension of Imagine file format is .img."
  },
  {
    "objectID": "bigdata_lec7.html#representing-spatial-networks",
    "href": "bigdata_lec7.html#representing-spatial-networks",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Representing spatial networks",
    "text": "Representing spatial networks\n\n\n\nNetworks\n\n\n\nStudy of networks is based on graph theory\na very useful data structure because it allows to define and construct neighborhood relationships that are central to geographic data science / GIScience.\n\nA network basically consists of couple of core elements:\n\nnodes (e.g. intersections on a street, or a person in social network), and\nedges (a link that connects the nodes to each other)"
  },
  {
    "objectID": "bigdata_lec7.html#representing-spatial-networks-1",
    "href": "bigdata_lec7.html#representing-spatial-networks-1",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Representing spatial networks",
    "text": "Representing spatial networks"
  },
  {
    "objectID": "bigdata_lec7.html#representing-spatial-networks-2",
    "href": "bigdata_lec7.html#representing-spatial-networks-2",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Representing spatial networks",
    "text": "Representing spatial networks\n\n\n\nNetwork attribute data\n\n\n\nnode attributes: typically contain the geographical information associated with the graph, such as the coordinates of the intersections.\nedge attributes: contain information about which nodes are connected to each other, and what is the cost to travel between the nodes measured e.g. as time or distance."
  },
  {
    "objectID": "bigdata_lec7.html#representing-spatial-networks-3",
    "href": "bigdata_lec7.html#representing-spatial-networks-3",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Representing spatial networks",
    "text": "Representing spatial networks"
  },
  {
    "objectID": "bigdata_lec7.html#representing-spatial-networks-4",
    "href": "bigdata_lec7.html#representing-spatial-networks-4",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Representing spatial networks",
    "text": "Representing spatial networks\n\n\n\nDirectionality\n\n\n\ndirected\nundirected"
  },
  {
    "objectID": "bigdata_lec7.html#representing-spatial-networks-5",
    "href": "bigdata_lec7.html#representing-spatial-networks-5",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Representing spatial networks",
    "text": "Representing spatial networks\n\n\n\n\nedge_id\nfrom_node\nto_node\ndescription\n\n\n\n\n1\nA\nC\nedge for direction 1\n\n\n2\nC\nA\nedge for direction 2"
  },
  {
    "objectID": "bigdata_lec7.html#representing-spatial-networks-6",
    "href": "bigdata_lec7.html#representing-spatial-networks-6",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Representing spatial networks",
    "text": "Representing spatial networks\n\n\n\nCommonly used network data formats\n\n\n\nGML: Graph Modelling Language\nGraphML: consists of a language core to describe the structural properties of a graph and extension mechanism to add application-specific data. The file extension of GraphML is .graphml."
  },
  {
    "objectID": "bigdata_lec7.html#crs",
    "href": "bigdata_lec7.html#crs",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "CRS",
    "text": "CRS\n\n\n\nGeoreferencing\n\n\nMajority of all the information in the world can be traced to a specific location: This process of attaching information about a location to a piece of information is commonly referred as georeferencing or geocoding.\nExamples:\n\nplace names (varying accuracy)\npostal codes (accuracy at areal level)\npostal addresses (accuracy at door/mailbox level)\nlatitude and longitude coordinates (up to millimeter level accuracy)."
  },
  {
    "objectID": "bigdata_lec7.html#crs-1",
    "href": "bigdata_lec7.html#crs-1",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "CRS",
    "text": "CRS\n\n\n\nDefinition\n\n\nA CRS tells the GIS tools how the coordinates or geometries are related to the places on Earth. In principle, every geographic dataset should have specific CRS-related information attached to it (if not, it might be difficult or impossible to use the data in GIS). This kind of additional information is called metadata.\nA CRS can be either:\n\ngeographic (based on latitude and longitude)\nor projected (based on a Cartesian coordinate system)."
  },
  {
    "objectID": "bigdata_lec7.html#coordinate-reference-system-crs",
    "href": "bigdata_lec7.html#coordinate-reference-system-crs",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Coordinate reference system (CRS)",
    "text": "Coordinate reference system (CRS)\n\n\n\nMain components\n\n\n\nDatum: A datum specification consists of a model for Earth’s size and shape, such as a reference ellipsoid or a geoid, which describes the average sea level surface of the Earth. One of the most commonly used datums is the World Geodetic System (WGS84). Datum also contains:\n\n\ninformation about the origin of the coordinate system, i.e. the reference point at which the ellipsoid/geoid is tied to a known location on Earth\norientation parameters, which describe the orientation of the coordinate system with respect to the Earth’s surface. They contain information about the tilt of the axis and the position of the origin relative to the Earth’s surface.\n\n\nMap projection: The projection defines the mathematical transformation used to map the Earth’s surface onto a two-dimensional plane."
  },
  {
    "objectID": "bigdata_lec7.html#coordinate-reference-system-crs-1",
    "href": "bigdata_lec7.html#coordinate-reference-system-crs-1",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Coordinate reference system (CRS)",
    "text": "Coordinate reference system (CRS)\n\n\n\nMain components\n\n\n\nAdditional Parameters:\n\n\nthe central meridian\nstandard parallel\nscale factor\norigin and orientation of the coordinate system\nthe conversion between the projected coordinates and the geographic coordinates."
  },
  {
    "objectID": "bigdata_lec7.html#map-projection",
    "href": "bigdata_lec7.html#map-projection",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Map projection",
    "text": "Map projection"
  },
  {
    "objectID": "bigdata_lec7.html#map-projection-1",
    "href": "bigdata_lec7.html#map-projection-1",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Map projection",
    "text": "Map projection"
  },
  {
    "objectID": "bigdata_lec7.html#map-projection-2",
    "href": "bigdata_lec7.html#map-projection-2",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Map projection",
    "text": "Map projection\n\n\n\nDistortions\n\n\nWe always make approximations and hence lose something.\nprojection needs to be chosen according to the purpose of use, in order to preserve specific aspects of the map that are the most important to the user:\n\nshapes of objects\ntheir surface areas\ncorrect distances between locations\nthe compass direction"
  },
  {
    "objectID": "bigdata_lec7.html#map-projection-3",
    "href": "bigdata_lec7.html#map-projection-3",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Map projection",
    "text": "Map projection\n\n\n\nTypes\n\n\n\nIf we want to preserve angles and ratios of distances, we speak of a conformal, or angle-preserving projections (such as Mercator projection). These kind of projections has historically been important e.g. for ship navigation. With conformal map projection, both the distances and surface areas are distorted, except at some special points of the map.\nIf we want to preserve correct area measure, we speak of an equivalent or equal-area projection (such as Mollweide or Albers equal area). Here, the angles and shapes are distorted, again with the exception of special points or lines.\nIf we want to preserve correct distances, we speak of an equidistant projection (such as Azimuthal equidistant). A projection can be equidistant only for a certain point (i.e. it is centered to specific location) or along certain lines, never everywhere."
  },
  {
    "objectID": "bigdata_lec7.html#map-projection-4",
    "href": "bigdata_lec7.html#map-projection-4",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Map projection",
    "text": "Map projection"
  },
  {
    "objectID": "bigdata_lec7.html#crs-2",
    "href": "bigdata_lec7.html#crs-2",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "CRS",
    "text": "CRS\n\n\n\nStoring CRS information\n\n\n\nEPSG registry (Geodetic Parameter Dataset): This is a public registry maintained by the European Petroleum Survey Group (EPSG) that contains information on a wide range of coordinate reference systems, including geographic and projected CRSs, and datums. It is widely used in GIS software and is an open and freely accessible resource.\n\nEPSG codes are numbers that identify different CRS from each other. These numbers, such as 4326 for WGS84 coordinate system, are commonly used to determine a CRS for a specific dataset or when reprojecting the data, because these simple numbers are easy to remember and use."
  },
  {
    "objectID": "bigdata_lec7.html#crs-3",
    "href": "bigdata_lec7.html#crs-3",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "CRS",
    "text": "CRS\n\n\n\nStoring CRS information\n\n\n\nPROJ system: PROJ is another system and open source library for storing and transforming CRS information from one system to another. The CRS information in this one is commonly stored as a “proj-strings”, which includes information about the coordinate system as a plain text following specific naming conventions for the parameters.\n\nAs an example, PROJ4 text for the ETRS-TM35FIN coordinate system (EPSG:3067) commonly used in Finland, is described with following text:\n+proj=utm +zone=35 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs +type=crs"
  },
  {
    "objectID": "bigdata_lec7.html#crs-4",
    "href": "bigdata_lec7.html#crs-4",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "CRS",
    "text": "CRS\n\n\n\nStoring CRS information\n\n\n\nOGC WKT: Open Geospatial Consortium’s (OGC) well-known text (WKT) representation.\n\nThe following shows how the ETRS-TM35FIN coordinate system (EPSG:3067) would be presented as OGW WKT:\nPROJCS[\"ETRS89 / TM35FIN(E,N)\",\n    GEOGCS[\"ETRS89\",\n        DATUM[\"European_Terrestrial_Reference_System_1989\",\n            SPHEROID[\"GRS 1980\",6378137,298.257222101,\n                AUTHORITY[\"EPSG\",\"7019\"]],\n            TOWGS84[0,0,0,0,0,0,0],\n            AUTHORITY[\"EPSG\",\"6258\"]],\n        PRIMEM[\"Greenwich\",0,\n            AUTHORITY[\"EPSG\",\"8901\"]],\n        UNIT[\"degree\",0.0174532925199433,\n            AUTHORITY[\"EPSG\",\"9122\"]],\n        AUTHORITY[\"EPSG\",\"4258\"]],\n    PROJECTION[\"Transverse_Mercator\"],\n    PARAMETER[\"latitude_of_origin\",0],\n    PARAMETER[\"central_meridian\",27],\n    PARAMETER[\"scale_factor\",0.9996],\n    PARAMETER[\"false_easting\",500000],\n    PARAMETER[\"false_northing\",0],\n    UNIT[\"metre\",1,\n        AUTHORITY[\"EPSG\",\"9001\"]],\n    AXIS[\"Easting\",EAST],\n    AXIS[\"Northing\",NORTH],\n    AUTHORITY[\"EPSG\",\"3067\"]]"
  },
  {
    "objectID": "bigdata_lec7.html#vector-format",
    "href": "bigdata_lec7.html#vector-format",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nShapely\n\n\n\nCreating and representing vector-based geometric objects is most commonly done using the shapely library which is one of the fundamental libraries in Python GIS ecosystem when working with geographic data.\nUnder the hood shapely uses a C++ library called GEOS to construct the geometries.\nGEOS is one of the standard libraries behind various GIS software, such as PostGIS and QGIS.\nObjects and methods available in shapely adhere mainly to the Open Geospatial Consortium’s Simple Features Access Specification\n\nuv add shapely"
  },
  {
    "objectID": "bigdata_lec7.html#vector-format-1",
    "href": "bigdata_lec7.html#vector-format-1",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nCreating Point geometries\n\n\nLet’s start by creating a simple Point object.\n\nfrom shapely import Point\n\npoint = Point(2.2, 4.2)\npoint3D = Point(9.26, -2.456, 0.57)\npoint"
  },
  {
    "objectID": "bigdata_lec7.html#vector-format-2",
    "href": "bigdata_lec7.html#vector-format-2",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nPrint\n\n\nWe can use the print() command to get the text representation of the point geometry as Well Known Text (WKT).\nIn the output, the letter Z after the POINT indicates that the geometry contains coordinates in three dimensions (x, y, z): \n\nprint(point)\nprint(point3D)\n\nPOINT (2.2 4.2)\nPOINT Z (9.26 -2.456 0.57)\n\n\nIt is also possible to access the WKT character string representation of the geometry using the .wkt attribute:\n\npoint.wkt\n\n'POINT (2.2 4.2)'"
  },
  {
    "objectID": "bigdata_lec7.html#vector-format-3",
    "href": "bigdata_lec7.html#vector-format-3",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nPoint Attributes\n\n\n\ntype(point.coords)\n\nshapely.coords.CoordinateSequence\n\n\n\nlist(point.coords)\n\n[(2.2, 4.2)]\n\n\nIt is also possible to access the coordinates directly using the x and y properties of the Point object:\n\nprint(point.x)\nprint(point.y)\n\n2.2\n4.2\n\n\n\n\n\n\n\nFor a full list of general attributes and methods for shapely objects, see shapely documentation."
  },
  {
    "objectID": "bigdata_lec7.html#vector-format-4",
    "href": "bigdata_lec7.html#vector-format-4",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nCreating LineString geometries\n\n\n\nfrom shapely import Point, LineString\n\npoint1 = Point(2.2, 4.2)\npoint2 = Point(7.2, -25.1)\npoint3 = Point(9.26, -2.456)\n\nline = LineString([point1, point2, point3])\nline_from_tuples = LineString([(2.2, 4.2), (7.2, -25.1), (9.26, -2.456)])\nline\n\n\n\n\n\n\n\n\n\nline.wkt\n\n'LINESTRING (2.2 4.2, 7.2 -25.1, 9.26 -2.456)'"
  },
  {
    "objectID": "bigdata_lec7.html#vector-format-5",
    "href": "bigdata_lec7.html#vector-format-5",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nLine Attributes\n\n\n\nlist(line.coords)\n\n[(2.2, 4.2), (7.2, -25.1), (9.26, -2.456)]\n\n\nAs a result, we have a list of coordinate tuples (x,y) inside a list. If you need to access all x -coordinates or all y -coordinates of the line, you can do it directly using the xy attribute:\n\nxcoords = list(line.xy[0])\nycoords = list(line.xy[1])\n\nprint(xcoords)\nprint(ycoords)\n\n[2.2, 7.2, 9.26]\n[4.2, -25.1, -2.456]"
  },
  {
    "objectID": "bigdata_lec7.html#vector-format-6",
    "href": "bigdata_lec7.html#vector-format-6",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nLine Attributes\n\n\nIt is possible to retrieve specific attributes such as length of the line and the center of the line (centroid) straight from the LineString object itself:\n\nlength = line.length\ncentroid = line.centroid\nprint(f\"Length of our line: {length:.2f} units\")\nprint(f\"Centroid: {centroid}\")\n\nLength of our line: 52.46 units\nCentroid: POINT (6.229961354035622 -11.892411157572392)"
  },
  {
    "objectID": "bigdata_lec7.html#vector-format-7",
    "href": "bigdata_lec7.html#vector-format-7",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nCreating Polygon geometries\n\n\nPolygon needs at least three coordinate-tuples to form a surface.\n\nfrom shapely import Polygon\n\npoly = Polygon([point1, point2, point3])\npoly\n\n\n\n\n\n\n\n\n\npoly.wkt\n\n'POLYGON ((2.2 4.2, 7.2 -25.1, 9.26 -2.456, 2.2 4.2))'"
  },
  {
    "objectID": "bigdata_lec7.html#vector-format-8",
    "href": "bigdata_lec7.html#vector-format-8",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nPolygon info\n\n\nYou can get more information about the Polygon object by running help(poly) of from the shapely online documentation. Here is a simplified extract from the output of help(Polygon):\nclass Polygon(shapely.geometry.base.BaseGeometry)\n |  Polygon(shell=None, holes=None)\n |  \n |  A two-dimensional figure bounded by a linear ring\n |  \n |  A polygon has a non-zero area. It may have one or more negative-space\n |  \"holes\" which are also bounded by linear rings. If any rings cross each\n |  other, the feature is invalid and operations on it may fail.\n |  \n |  Attributes\n |  ----------\n |  exterior : LinearRing\n |      The ring which bounds the positive space of the polygon.\n |  interiors : sequence\n |      A sequence of rings which bound all existing holes.\n |  \n |  Parameters\n |  ----------\n |  shell : sequence\n |     A sequence of (x, y [,z]) numeric coordinate pairs or triples.\n |     Also can be a sequence of Point objects.\n |  holes : sequence\n |      A sequence of objects which satisfy the same requirements as the\n |      shell parameters above"
  },
  {
    "objectID": "bigdata_lec7.html#vector-format-9",
    "href": "bigdata_lec7.html#vector-format-9",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nPolygon holes\n\n\n\n# Define the exterior coordinates\nexterior = [(-180, 90), (-180, -90), (180, -90), (180, 90)]\n\n# Define the hole coordinates (a single hole in this case)\nholes_coordinates = [[(-170, 80), (-170, -80), (170, -80), (170, 80)]]\n\nWith the four coordinate tuples of the exterior, we can first create a polygon without a hole:\n\npoly_without_hole = Polygon(shell=exterior)\npoly_without_hole"
  },
  {
    "objectID": "bigdata_lec7.html#vector-format-10",
    "href": "bigdata_lec7.html#vector-format-10",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nPolygon holes\n\n\nIn a similar manner, we can make a Polygon with holes by passing the holes_coordinates variable into the parameter holes:\n\npoly_with_hole = Polygon(shell=exterior, holes=holes_coordinates)\npoly_with_hole\n\n\n\n\n\n\n\n\npoly_with_hole.wkt:\nPOLYGON ((-180 90, -180 -90, 180 -90, 180 90, -180 90),\n         (-170 80, -170 -80, 170 -80, 170 80, -170 80))"
  },
  {
    "objectID": "bigdata_lec7.html#vector-format-11",
    "href": "bigdata_lec7.html#vector-format-11",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nPolygon attributes\n\n\n\nprint(\"Polygon centroid: \", poly.centroid)\nprint(\"Polygon Area: \", poly.area)\nprint(\"Polygon Bounding Box: \", poly.bounds)\nprint(\"Polygon Exterior: \", poly.exterior)\nprint(\"Polygon Exterior Length: \", poly.exterior.length)\n\nPolygon centroid:  POINT (6.22 -7.785333333333334)\nPolygon Area:  86.789\nPolygon Bounding Box:  (2.2, -25.1, 9.26, 4.2)\nPolygon Exterior:  LINEARRING (2.2 4.2, 7.2 -25.1, 9.26 -2.456, 2.2 4.2)\nPolygon Exterior Length:  62.16395199996553"
  },
  {
    "objectID": "bigdata_lec7.html#vector-format-12",
    "href": "bigdata_lec7.html#vector-format-12",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nBox polygons\n\n\n\nfrom shapely.geometry import box\n\nmin_x, min_y = -180, -90\nmax_x, max_y = 180, 90\nbox_poly = box(minx=min_x, miny=min_y, maxx=max_x, maxy=max_y)\nbox_poly\n\n\n\n\n\n\n\n\n\nbox_poly.wkt\n\n'POLYGON ((180 -90, 180 90, -180 90, -180 -90, 180 -90))'"
  },
  {
    "objectID": "bigdata_lec7.html#vector-format-13",
    "href": "bigdata_lec7.html#vector-format-13",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nMulti geometries\n\n\n\nfrom shapely import MultiPoint, MultiLineString, MultiPolygon\n\nmultipoint = MultiPoint([Point(2, 2), Point(3, 3)])\nmultipoint\n\n\n\n\n\n\n\n\n\nmultiline = MultiLineString(\n    [LineString([(2, 2), (3, 3)]), LineString([(4, 3), (6, 4)])]\n)\nmultiline"
  },
  {
    "objectID": "bigdata_lec7.html#vector-format-14",
    "href": "bigdata_lec7.html#vector-format-14",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nMulti geometries\n\n\n\nmultipoly = MultiPolygon(\n    [Polygon([(0, 0), (0, 4), (4, 4)]), Polygon([(6, 6), (6, 12), (12, 12)])]\n)\nmultipoly"
  },
  {
    "objectID": "bigdata_lec7.html#vector-format-15",
    "href": "bigdata_lec7.html#vector-format-15",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "Vector format",
    "text": "Vector format\n\n\n\nExercise\n\n\nCreate examples of these shapes:\n\nTriangle\n\nSquare\n\nCircle"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-1",
    "href": "bigdata_lec7.html#geopandas-intro-1",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nDescription\n\n\n\ngeopandas library extends the data structures and data analysis tools available in the pandas library to allow geospatial operations and the handling of coordinate reference systems.\ngeopandas provides a high-level, easy-to-use interface for vector data (like points, lines, and polygons) that integrates with the existing pandas data analysis framework, as well as the extensive Python GIS ecosystem .\n\nuv add geopandas"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-2",
    "href": "bigdata_lec7.html#geopandas-intro-2",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nWhat is a GeoDataFrame?\n\n\n\nThe main data structures in geopandas are GeoSeries and GeoDataFrame.\nGeoDataFrame is basically like a pandas DataFrame that contains at least one dedicated column for storing geometries.\nThe geometry column is a GeoSeries which contains the geometries as shapely objects (points, lines, polygons, multipolygons etc.)."
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-3",
    "href": "bigdata_lec7.html#geopandas-intro-3",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nReading a file\n\n\n\n\nfrom pathlib import Path\n\ndata_folder = Path(\"_data\", \"Austin\")\nfp = data_folder / \"austin_pop_2019.gpkg\"\nprint(fp)\n\n_data/Austin/austin_pop_2019.gpkg\n\n\nSimilar to importing pandas, we will first import geopandas as gpd which allows us to start using the library. Then we will read the file by passing the filepath to .read_file() function of geopandas: \n\nimport geopandas as gpd\n\ndata = gpd.read_file(fp)\n\nLet’s take a look at the data type of our data variable:\n\ntype(data)\n\ngeopandas.geodataframe.GeoDataFrame"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-4",
    "href": "bigdata_lec7.html#geopandas-intro-4",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nExploration\n\n\nLet’s have a closer look at the first rows of the data:\n\ndata.head()\n\n\n\n\n\n\n\n\npop2019\ntract\ngeometry\n\n\n\n\n0\n6070.0\n002422\nPOLYGON ((615643.487 3338728.496, 615645.477 3...\n\n\n1\n2203.0\n001751\nPOLYGON ((618576.586 3359381.053, 618614.33 33...\n\n\n2\n7419.0\n002411\nPOLYGON ((619200.163 3341784.654, 619270.849 3...\n\n\n3\n4229.0\n000401\nPOLYGON ((621623.757 3350508.165, 621656.294 3...\n\n\n4\n4589.0\n002313\nPOLYGON ((621630.247 3345130.744, 621717.926 3...\n\n\n\n\n\n\n\nThe columns pop2019 and tract represent attribute information in our data, namely the number of people living on a given census tract and a unique id-number for the tract, wherease the column geometry contains the geographic data (polygons) for each census tract."
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-5",
    "href": "bigdata_lec7.html#geopandas-intro-5",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nExploration\n\n\nIt is always a good idea to explore your geographic data on a map.\n\ndata.plot()"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-6",
    "href": "bigdata_lec7.html#geopandas-intro-6",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nExercise\n\n\nFigure out the following information from input data:\n\nNumber of rows?\nNumber of census tracts (based on column tract)?\nTotal population (based on column pop2019)?"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-7",
    "href": "bigdata_lec7.html#geopandas-intro-7",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nGeometries in geopandas\n\n\nA geodataframe can have multiple columns containing geometries, and you can change which column should be considered as the “active geometry”. Let’s have a closer look at the .geometry attribute of our data:\n\ndata.geometry.head()\n\n0    POLYGON ((615643.487 3338728.496, 615645.477 3...\n1    POLYGON ((618576.586 3359381.053, 618614.33 33...\n2    POLYGON ((619200.163 3341784.654, 619270.849 3...\n3    POLYGON ((621623.757 3350508.165, 621656.294 3...\n4    POLYGON ((621630.247 3345130.744, 621717.926 3...\nName: geometry, dtype: geometry\n\n\ntype(data.geometry)"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-8",
    "href": "bigdata_lec7.html#geopandas-intro-8",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nGeometries in geopandas\n\n\n\ndata.geometry.area\n\n0      4.029772e+06\n1      1.532030e+06\n2      3.960344e+06\n3      2.181762e+06\n4      2.431208e+06\n           ...     \n125    2.321182e+06\n126    4.388407e+06\n127    1.702764e+06\n128    3.540893e+06\n129    2.054702e+06\nLength: 130, dtype: float64"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-9",
    "href": "bigdata_lec7.html#geopandas-intro-9",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nGeometries in geopandas\n\n\nThe same result can be achieved by using the syntax data.area. Let’s convert the area values from square meters to square kilometers and store them into a new column called area_km2:\n\ndata[\"area_km2\"] = data.area / 1000000\ndata.head()\n\n\n\n\n\n\n\n\npop2019\ntract\ngeometry\narea_km2\n\n\n\n\n0\n6070.0\n002422\nPOLYGON ((615643.487 3338728.496, 615645.477 3...\n4.029772\n\n\n1\n2203.0\n001751\nPOLYGON ((618576.586 3359381.053, 618614.33 33...\n1.532030\n\n\n2\n7419.0\n002411\nPOLYGON ((619200.163 3341784.654, 619270.849 3...\n3.960344\n\n\n3\n4229.0\n000401\nPOLYGON ((621623.757 3350508.165, 621656.294 3...\n2.181762\n\n\n4\n4589.0\n002313\nPOLYGON ((621630.247 3345130.744, 621717.926 3...\n2.431208"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-10",
    "href": "bigdata_lec7.html#geopandas-intro-10",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nGeometries in geopandas\n\n\nVisualize a choropleth map:\n\ndata.plot(column=\"area_km2\")"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-11",
    "href": "bigdata_lec7.html#geopandas-intro-11",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nExercise\n\n\nCreate a new column pop_density_km2 and populate it with population density values (population / km2) calculated based on columns pop2019 and area_km2. Print out answers to the following questions:\n\nWhat was the average population density in 2019?\nWhat was the maximum population density per census tract?"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-12",
    "href": "bigdata_lec7.html#geopandas-intro-12",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nWriting data into a file\n\n\nIt is possible to export the GeoDataFrame into various data formats using the .to_file() method. Let’s start by learning how we can write data into a GeoPackage. Before proceeding, let’s see how the data looks like at this point: \n\ndata.head()\n\n\n\n\n\n\n\n\npop2019\ntract\ngeometry\narea_km2\n\n\n\n\n0\n6070.0\n002422\nPOLYGON ((615643.487 3338728.496, 615645.477 3...\n4.029772\n\n\n1\n2203.0\n001751\nPOLYGON ((618576.586 3359381.053, 618614.33 33...\n1.532030\n\n\n2\n7419.0\n002411\nPOLYGON ((619200.163 3341784.654, 619270.849 3...\n3.960344\n\n\n3\n4229.0\n000401\nPOLYGON ((621623.757 3350508.165, 621656.294 3...\n2.181762\n\n\n4\n4589.0\n002313\nPOLYGON ((621630.247 3345130.744, 621717.926 3...\n2.431208"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-13",
    "href": "bigdata_lec7.html#geopandas-intro-13",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nWriting data into a file\n\n\n\n# Create a output path for the data\noutput_fp = \"_data/Temp/austin_pop_density_2019.gpkg\"\n\n# Write the file\ndata.to_file(output_fp)\n\nTo be more explicit, you can use the driver parameter to specify the output file format for the data allowing you to write the data into numerous data formats (some lesser-known ones as well) supported by a software called GDAL which is used by geopandas under the hood for reading and writing data.\n\n\n\n\noutput_fp = \"_data/Temp/austin_pop_density_2019.fgb\"\n\n# Write the file\ndata.to_file(output_fp, driver=\"FlatGeobuf\")"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-14",
    "href": "bigdata_lec7.html#geopandas-intro-14",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nExercise\n\n\nRead the output file using geopandas and check that the data looks ok."
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-15",
    "href": "bigdata_lec7.html#geopandas-intro-15",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nReading vector data\n\n\nWhen we use the .read_file() function for reading data in various formats, geopandas actually uses a Python library called pyogrio under the hood to read the data (which again depends on GDAL). It is easy to find out all the data formats that are supported by default by calling pyogrio.list_drivers(): \n\nimport geopandas as gpd\nimport pyogrio\n\navailable_drivers = pyogrio.list_drivers()\n\nprint(available_drivers)\nlen(available_drivers)\n\n{'PCIDSK': 'rw', 'PDS4': 'rw', 'VICAR': 'rw', 'PDF': 'rw', 'MBTiles': 'rw', 'EEDA': 'r', 'OGCAPI': 'r', 'ESRI Shapefile': 'rw', 'MapInfo File': 'rw', 'UK .NTF': 'r', 'LVBAG': 'r', 'OGR_SDTS': 'r', 'S57': 'rw', 'DGN': 'rw', 'OGR_VRT': 'r', 'Memory': 'rw', 'CSV': 'rw', 'GML': 'rw', 'GPX': 'rw', 'KML': 'rw', 'GeoJSON': 'rw', 'GeoJSONSeq': 'rw', 'ESRIJSON': 'r', 'TopoJSON': 'r', 'OGR_GMT': 'rw', 'GPKG': 'rw', 'SQLite': 'rw', 'WAsP': 'rw', 'OpenFileGDB': 'rw', 'DXF': 'rw', 'FlatGeobuf': 'rw', 'Geoconcept': 'rw', 'GeoRSS': 'rw', 'VFK': 'r', 'PGDUMP': 'rw', 'OSM': 'r', 'GPSBabel': 'rw', 'OGR_PDS': 'r', 'WFS': 'r', 'OAPIF': 'r', 'EDIGEO': 'r', 'SVG': 'r', 'Idrisi': 'r', 'ODS': 'rw', 'XLSX': 'rw', 'Elasticsearch': 'rw', 'Carto': 'rw', 'AmigoCloud': 'rw', 'SXF': 'r', 'Selafin': 'rw', 'JML': 'rw', 'PLSCENES': 'r', 'CSW': 'r', 'VDV': 'rw', 'MVT': 'rw', 'NGW': 'rw', 'MapML': 'rw', 'GTFS': 'r', 'PMTiles': 'rw', 'JSONFG': 'rw', 'MiraMonVector': 'rw', 'TIGER': 'r', 'AVCBin': 'r', 'AVCE00': 'r', 'HTTP': 'r'}\n\n\n65"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-16",
    "href": "bigdata_lec7.html#geopandas-intro-16",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nReading vector data\n\n\n\n# Read Esri Shapefile\nfp = data_folder / \"austin_pop_2019.shp\"\ndata = gpd.read_file(fp)\ndata.head()\n\n\n\n\n\n\n\n\nfid\npop2019\ntract\ngeometry\n\n\n\n\n0\n1.0\n6070.0\n002422\nPOLYGON ((615643.487 3338728.496, 615645.477 3...\n\n\n1\n2.0\n2203.0\n001751\nPOLYGON ((618576.586 3359381.053, 618614.33 33...\n\n\n2\n3.0\n7419.0\n002411\nPOLYGON ((619200.163 3341784.654, 619270.849 3...\n\n\n3\n4.0\n4229.0\n000401\nPOLYGON ((621623.757 3350508.165, 621656.294 3...\n\n\n4\n5.0\n4589.0\n002313\nPOLYGON ((621630.247 3345130.744, 621717.926 3..."
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-17",
    "href": "bigdata_lec7.html#geopandas-intro-17",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nAnother format\n\n\n\nAs you can see, nothing except the file extension at the end of the filename changed, from .gpkg into .shp. The same syntax works for other common vector data formats, such as GeoJSON or MapInfo TAB format: \n\n# Read file from Geopackage\nfp = data_folder / \"austin_pop_2019.gpkg\"\ndata = gpd.read_file(fp)\n\n# Read file from GeoJSON\nfp = data_folder / \"austin_pop_2019.geojson\"\ndata = gpd.read_file(fp)\n\n# Read file from Geodatabase\nfp = data_folder / \"austin_pop_2019.gdb\"\ndata = gpd.read_file(fp)\n\n# Read file from KML\nfp = \"_data/Austin/austin_pop_2019.kml\"\ndata_kml = gpd.read_file(fp)\n\n# Read file from MapInfo TAB\nfp = data_folder / \"austin_pop_2019.tab\"\ndata = gpd.read_file(fp)\n\ndata.head()\n\n\n\n\n\n\n\n\nfid\npop2019\ntract\ngeometry\n\n\n\n\n0\n1.0\n6070.0\n002422\nPOLYGON ((615643.49 3338728.49, 615645.48 3338...\n\n\n1\n2.0\n2203.0\n001751\nPOLYGON ((618576.59 3359381.06, 618614.33 3359...\n\n\n2\n3.0\n7419.0\n002411\nPOLYGON ((619200.16 3341784.65, 619270.85 3341...\n\n\n3\n4.0\n4229.0\n000401\nPOLYGON ((621623.75 3350508.16, 621656.29 3350...\n\n\n4\n5.0\n4589.0\n002313\nPOLYGON ((621630.24 3345130.75, 621717.92 3345..."
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-18",
    "href": "bigdata_lec7.html#geopandas-intro-18",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nMultiple layers\n\n\nSome file formats such as GeoPackage may contain multiple layers with different names which can be speficied using the layer -parameter.\n\n# Read spesific layer from Geopackage\nfp = data_folder / \"austin_pop_2019.gpkg\"\ndata = gpd.read_file(fp, layer=\"austin_pop_2019\")\n\nNote that the KML file format is a bit of a special case as it is designed for Google Earth rendering in 3D, so there may be additional data columns read into the geodataframe:\n#| echo: true\ndata_kml.head()\nHowever, it easy to filter the extra columns and only keep the ones that we are interested in:\n#| echo: true\ndata = data_kml[[\"pop2019\", \"tract\", \"geometry\"]].copy()\ndata.head(2)"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-19",
    "href": "bigdata_lec7.html#geopandas-intro-19",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nReading from a ZIP file\n\n\n\nfrom zipfile import ZipFile\n\nfp = \"_data/Helsinki/building_points_helsinki.zip\"\n\nwith ZipFile(fp) as z:\n    print(z.namelist())\n\n['building_points_helsinki.gpkg']\n\n\n\nbuildings = gpd.read_file(fp)\nbuildings.head(2)\n\n\n\n\n\n\n\n\nname\ngeometry\n\n\n\n\n0\nNone\nPOINT (24.85584 60.20727)\n\n\n1\nUimastadion\nPOINT (24.93045 60.18882)"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-20",
    "href": "bigdata_lec7.html#geopandas-intro-20",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nReading from a ZIP file\n\n\nHowever, sometimes you might have multiple files and folders stored inside a ZIP archive as in the example below:\n\nfp = \"_data/Helsinki/Kamppi_layers.zip\"\n\nwith ZipFile(fp) as z:\n    print(z.namelist())\n\n['natural/', 'natural/Kamppi_parks.gpkg', 'built_environment/', 'built_environment/Kamppi_buildings.gpkg', 'built_environment/Kamppi_roads.gpkg']\n\n\nWe need a special filepath:\n\nparks_fp = \"_data/Helsinki/Kamppi_layers.zip!natural/Kamppi_parks.gpkg\"\n\nparks = gpd.read_file(parks_fp)\nparks.head(2)\n\n\n\n\n\n\n\n\nosmid\nleisure\nname\ngeometry\n\n\n\n\n0\n8042256\npark\nPikkuparlamentin puisto\nPOLYGON ((385464.223 6672281.159, 385464.274 6...\n\n\n1\n8042613\npark\nSimonpuistikko\nPOLYGON ((385532.543 6672073.355, 385490.887 6..."
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-21",
    "href": "bigdata_lec7.html#geopandas-intro-21",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nWriting to different vector data formats\n\n\n# Write to Shapefile\noutfp = \"data/Temp/austin_pop_2019.shp\"\ndata.to_file(outfp, driver=\"ESRI Shapefile\")\n\n# Write to MapInfo Tab\noutfp = \"data/Temp/austin_pop_2019.tab\"\ndata.to_file(outfp, driver=\"MapInfo File\")\n\n# Write to Geopackage\noutfp = \"data/Temp/austin_pop_2019.gpkg\"\ndata.to_file(outfp, driver=\"GPKG\")\n\n# Write to GeoJSON\noutfp = \"data/Temp/austin_pop_2019.geojson\"\ndata.to_file(outfp, driver=\"GeoJSON\")\n\n# Write to KML\noutfp = \"data/Temp/austin_pop_2019.kml\"\ndata.to_file(outfp, driver=\"LIBKML\")\n\n# Write to File Geodatabase\noutfp = \"data/Temp/austin_pop_2019.gdb\"\ndata.to_file(outfp, driver=\"OpenFileGDB\")"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-22",
    "href": "bigdata_lec7.html#geopandas-intro-22",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nCreating a GeoDataFrame from scratch\n\n\n\nfrom shapely.geometry import Polygon\n\ncoordinates = [\n    (24.950899, 60.169158),\n    (24.953492, 60.169158),\n    (24.953510, 60.170104),\n    (24.950958, 60.169990),\n]\n\npoly = Polygon(coordinates)"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-23",
    "href": "bigdata_lec7.html#geopandas-intro-23",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nCreating a GeoDataFrame from scratch\n\n\nNow we can use this polygon and create a GeoDataFrame from scratch with geopandas.\n\nnewdata = gpd.GeoDataFrame(geometry=[poly])\nnewdata\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOLYGON ((24.9509 60.16916, 24.95349 60.16916,...\n\n\n\n\n\n\n\n\nnewdata.plot();"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-24",
    "href": "bigdata_lec7.html#geopandas-intro-24",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nCreating a GeoDataFrame from scratch\n\n\nWe can create a GeoDataFrame from scratch with multiple attributes by passing a Python dictionary into the GeoDataFrame object as follows:\n\ngdf_data = {\n    \"geometry\": [poly],\n    \"name\": \"Senate Square\",\n    \"city\": \"Helsinki\",\n    \"year\": 2023,\n}\nnew_data_extended = gpd.GeoDataFrame(gdf_data)\nnew_data_extended\n\n\n\n\n\n\n\n\ngeometry\nname\ncity\nyear\n\n\n\n\n0\nPOLYGON ((24.9509 60.16916, 24.95349 60.16916,...\nSenate Square\nHelsinki\n2023"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-25",
    "href": "bigdata_lec7.html#geopandas-intro-25",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nCreating a GeoDataFrame from a text file\n\n\nTo demonstrate this, we have some example data below that contains point coordinates of airports derived from openflights.org. The operation of turning this data into a GeoDataFrame begins with reading the data with pandas into a DataFrame.\n\nimport pandas as pd\n\nairports = pd.read_csv(\n    \"_data/Airports/airports.txt\",\n    usecols=[\"Airport ID\", \"Name\", \"City\", \"Country\", \"Latitude\", \"Longitude\"],\n)\nairports.head()\n\n\n\n\n\n\n\n\nAirport ID\nName\nCity\nCountry\nLatitude\nLongitude\n\n\n\n\n0\n1\nGoroka Airport\nGoroka\nPapua New Guinea\n-6.081690\n145.391998\n\n\n1\n2\nMadang Airport\nMadang\nPapua New Guinea\n-5.207080\n145.789001\n\n\n2\n3\nMount Hagen Kagamuga Airport\nMount Hagen\nPapua New Guinea\n-5.826790\n144.296005\n\n\n3\n4\nNadzab Airport\nNadzab\nPapua New Guinea\n-6.569803\n146.725977\n\n\n4\n5\nPort Moresby Jacksons International Airport\nPort Moresby\nPapua New Guinea\n-9.443380\n147.220001\n\n\n\n\n\n\n\n\ntype(airports)\n\npandas.core.frame.DataFrame\n\n\n\nlen(airports)\n\n7698"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-26",
    "href": "bigdata_lec7.html#geopandas-intro-26",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nCreating a GeoDataFrame from a text file\n\n\nWe can use this coordinate information for turning this data into a GeoDataFrame and ultimately visualizing the data on a map. There is a handy function in geopandas called .points_from_xy() for generating an array of Point objects based on x and y coordinates.\n\nairports[\"geometry\"] = gpd.points_from_xy(\n    x=airports[\"Longitude\"], y=airports[\"Latitude\"]\n)\n\nairports = gpd.GeoDataFrame(airports)\nairports.head()\n\n\n\n\n\n\n\n\nAirport ID\nName\nCity\nCountry\nLatitude\nLongitude\ngeometry\n\n\n\n\n0\n1\nGoroka Airport\nGoroka\nPapua New Guinea\n-6.081690\n145.391998\nPOINT (145.392 -6.08169)\n\n\n1\n2\nMadang Airport\nMadang\nPapua New Guinea\n-5.207080\n145.789001\nPOINT (145.789 -5.20708)\n\n\n2\n3\nMount Hagen Kagamuga Airport\nMount Hagen\nPapua New Guinea\n-5.826790\n144.296005\nPOINT (144.29601 -5.82679)\n\n\n3\n4\nNadzab Airport\nNadzab\nPapua New Guinea\n-6.569803\n146.725977\nPOINT (146.72598 -6.5698)\n\n\n4\n5\nPort Moresby Jacksons International Airport\nPort Moresby\nPapua New Guinea\n-9.443380\n147.220001\nPOINT (147.22 -9.44338)\n\n\n\n\n\n\n\n\n\ngeopandas.geodataframe.GeoDataFrame"
  },
  {
    "objectID": "bigdata_lec7.html#geopandas-intro-27",
    "href": "bigdata_lec7.html#geopandas-intro-27",
    "title": "Big Data: Intro to GeoData Analysis",
    "section": "GeoPandas Intro",
    "text": "GeoPandas Intro\n\n\n\nCreating a GeoDataFrame from a text file\n\n\nThe GeoDataFrame was created with a couple of steps.\n\nWe created a new column called geometry into the DataFrame and used the .points_from_xy() function to turn the coordinates into shapely Point objects. At this stage, the data is still in a DataFrame format.\nThe second command in the code snippet converts the pandas DataFrame into a GeoDataFrame which then has all the capabilities and tools bundled with geopandas. After these two steps, we have succesfully turned the data into geospatial format and we can for example plot the data on a map:\n\n\nairports.plot(markersize=0.1);"
  },
  {
    "objectID": "bigdata_lab6.html",
    "href": "bigdata_lab6.html",
    "title": "Big Data Analytics: Lab 6",
    "section": "",
    "text": "Working with shapes\nDownload GADM data for the country of your choice (https://gadm.org/download_country.html).\nData should be in Shapefile format.\nBelow example is for Ukraine.\nFirst, parse GADM files.\n\nimport os\nimport geopandas as gpd\n\ngadm_files = [os.path.join(root, file)\n              for root, dirs, files in os.walk(\"_data/gadm/\")\n              for file in files if file.startswith(\"gadm\")]\n\nprint(gadm_files)\n\n# Select regional level 2 files\ngadm_files_level2 = [file for file in gadm_files if \"2.shp\" in file]\n#print(gadm_files_level2)\n\n# Load the shapefiles\nshps = [gpd.read_file(shp) for shp in gadm_files_level2]\n#print(shps)\n\n['_data/gadm/gadm41_UKR_2.dbf', '_data/gadm/gadm41_UKR_1.dbf', '_data/gadm/gadm41_UKR_0.dbf', '_data/gadm/gadm41_UKR_0.cpg', '_data/gadm/gadm41_UKR_0.shp', '_data/gadm/gadm41_UKR_1.shp', '_data/gadm/gadm41_UKR_2.shx', '_data/gadm/gadm41_UKR_1.cpg', '_data/gadm/gadm41_UKR_0.shx', '_data/gadm/gadm41_UKR_2.shp', '_data/gadm/gadm41_UKR_1.shx', '_data/gadm/gadm41_UKR_2.cpg', '_data/gadm/gadm41_UKR_shp.zip', '_data/gadm/gadm41_UKR_0.prj', '_data/gadm/gadm41_UKR_1.prj', '_data/gadm/gadm41_UKR_2.prj']\n\n\nConcatenated:\n\nimport pandas as pd\n\nukr_shp = gpd.GeoDataFrame(pd.concat(shps, ignore_index=True))\nukr_shp.plot()\n\n\n\n\n\n\n\n\nOr another version:\n\nimport matplotlib.pyplot as plt\n\n# Plot all shapefiles\nfig, ax = plt.subplots(figsize=(10, 10))\n\nukr_shp.plot(ax=ax, edgecolor='k', facecolor='none', linewidth=1)  # No fill color\n\n\n# Set plot title and labels\nax.set_title('Regional Level 2 Shapefiles')\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\nOutline:\n\nukr_shp.head()\n\n\n\n\n\n\n\n\nGID_2\nGID_0\nCOUNTRY\nGID_1\nNAME_1\nNL_NAME_1\nNAME_2\nVARNAME_2\nNL_NAME_2\nTYPE_2\nENGTYPE_2\nCC_2\nHASC_2\ngeometry\n\n\n\n\n0\n?\nUKR\nUkraine\n?\n?\n?\n?\n?\nNA\n?\nNA\nNA\n?\nPOLYGON ((30.59167 50.41236, 30.60611 50.41604...\n\n\n1\nUKR.1.1_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nCherkas'ka\nNA\nNA\nMis'ka Rada\nCity of Regional Significance\nNA\nUA.CK.CM\nPOLYGON ((32.1715 49.43881, 32.16858 49.41685,...\n\n\n2\nUKR.1.2_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nCherkas'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.CR\nPOLYGON ((32.0388 49.49877, 32.02555 49.48022,...\n\n\n3\nUKR.1.3_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nChornobaivs'kyi\nChornobayivskyi\nNA\nRaion\nDistrict\nNA\nUA.CK.CB\nPOLYGON ((32.1715 49.43881, 32.1708 49.43904, ...\n\n\n4\nUKR.1.4_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nChyhyryns'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.CY\nPOLYGON ((32.26144 49.20893, 32.2705 49.20885,...\n\n\n\n\n\n\n\n\n\nExercises\n\nMerge regions based on historic provinces (e.g. Галичина, Буковина, etc.) or other criteria.\nGeocode centroids for each region and the whole country.\nPlot a color map based on average GRDP of the region. E.g., for Ukraine, this data can be fetched from https://www.ukrstat.gov.ua/operativ/operativ2021/vvp/kvartal_new/vrp/arh_vrp_u.html."
  },
  {
    "objectID": "bigdata_lab4.html",
    "href": "bigdata_lab4.html",
    "title": "Big Data Analytics: Lab 4",
    "section": "",
    "text": "Intro to Dask\nComplete exercises from related lecture ."
  },
  {
    "objectID": "bigdata_lec5.html#dask",
    "href": "bigdata_lec5.html#dask",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nParallel\n\n\nWe refer to algorithms that use multiple cores simultaneously as parallel.\n\n\n\n\n\n\nOut-of-core\n\n\nWe refer to systems that efficiently use disk as extensions of memory as out-of-core."
  },
  {
    "objectID": "bigdata_lec5.html#dask-1",
    "href": "bigdata_lec5.html#dask-1",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nHow to execute parallel code?\n\n\n\nrepresent the structure of our program explicitly as data within the program itself\nencode task schedules programmatically within a framework"
  },
  {
    "objectID": "bigdata_lec5.html#dask-2",
    "href": "bigdata_lec5.html#dask-2",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask Graph definition\n\n\n\nA Python dictionary mapping keys to tasks or values.\nA key is any Python hashable\na value is any Python object that is not a task\na task is a Python tuple with a callable first element."
  },
  {
    "objectID": "bigdata_lec5.html#dask-3",
    "href": "bigdata_lec5.html#dask-3",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\ndef inc(i):\n  return i + 1\n\ndef add(a, b):\n  return a + b\n\nx = 1\ny = inc(x)\nz = add(y, 10)"
  },
  {
    "objectID": "bigdata_lec5.html#dask-4",
    "href": "bigdata_lec5.html#dask-4",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask"
  },
  {
    "objectID": "bigdata_lec5.html#dask-5",
    "href": "bigdata_lec5.html#dask-5",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDictionary representation\n\n\nd = {'x': 1,\n     'y': (inc, 'x'),\n     'z': (add, 'y', 10)}"
  },
  {
    "objectID": "bigdata_lec5.html#dask-6",
    "href": "bigdata_lec5.html#dask-6",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask computation\n\n\nDask represents a computation as a directed acyclic graph of tasks with data dependencies.\nIt can be said that Dask is a specification to encode such a graph using ordinary Python data structures, namely dicts, tuples, functions, and arbitrary Python values."
  },
  {
    "objectID": "bigdata_lec5.html#dask-7",
    "href": "bigdata_lec5.html#dask-7",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n{'x': 1,\n 'y': 2,\n 'z': (add, 'x', 'y'),\n 'w': (sum, ['x', 'y', 'z'])}\n\n\n\nExamples\n\n\n\nkey: 'x', ('x', 2, 3)\ntask: (add, 'x', 'y')\ntask argument: 'x', 1, (inc, 'x'), [1, 'x', (inc, 'x')]"
  },
  {
    "objectID": "bigdata_lec5.html#dask-8",
    "href": "bigdata_lec5.html#dask-8",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nValid tasks in a Dask graph\n\n\n(add, 1, 2)\n(add, 'x' , 2)\n(add, (inc, 'x'), 2)\n(sum, [1, 2])\n(sum, ['x', (inc, 'x')])\n(np.dot, np.array([...]), np.array([...]))"
  },
  {
    "objectID": "bigdata_lec5.html#dask-9",
    "href": "bigdata_lec5.html#dask-9",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask Array\n\n\nThe dask.array submodule uses dask graphs to create a NumPy-like library that uses all of your cores and operates on datasets that do not fit in memory.\nIt does this by building up a dask graph of blocked array algorithms.\nDask array functions produce Array objects that hold on to Dask graphs. These Dask graphs use several NumPy functions to achieve the full result."
  },
  {
    "objectID": "bigdata_lec5.html#dask-10",
    "href": "bigdata_lec5.html#dask-10",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nBlocked Array Algorithms\n\n\nBlocked algorithms compute a large result like\n\n“take the sum of these trillion numbers”\n\nwith many small computations like\n\n“break up the trillion numbers into one million chunks of size one million”,\n“sum each chunk”,\n“then sum all of the intermediate sums.”\n\nThrough tricks like this we can evaluate one large problem by solving very many small problems.\nBlocked algorithm organizes a computation so that it works on contiguous chunks of data."
  },
  {
    "objectID": "bigdata_lec5.html#dask-11",
    "href": "bigdata_lec5.html#dask-11",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nBlocked Array Algorithms"
  },
  {
    "objectID": "bigdata_lec5.html#dask-12",
    "href": "bigdata_lec5.html#dask-12",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nUnblocked\n\n\nfor i in range(N):\n for k in range(N):\n   r = X[i,k]\n   for j in range(N):\n     Z[i,j] += r*Y[k,j]\n\n\n\n\n\n\nBlocked\n\n\nfor kk in range(N/B):\n for jj in range(N/B): \n   for i in range(N):\n     for k in range(kk, min(kk+B-1, N)):\n       r = X[i,k]\n       for j in range(jj, min(jj+B-1,N)):\n         Z[i,j] += r*Y[k,j]"
  },
  {
    "objectID": "bigdata_lec5.html#dask-13",
    "href": "bigdata_lec5.html#dask-13",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\n\n\n\nNote\n\n\nIt is well known that the memory hierarchy can be better utilized if scientific algorithms are blocked.\nBlocking is also known as tiling.\nIn matrix multiplication example: instead of operating on individual matrix entries, the calculation is performed on submatrices.\nB is a blocking factor."
  },
  {
    "objectID": "bigdata_lec5.html#dask-14",
    "href": "bigdata_lec5.html#dask-14",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nBlocking features\n\n\n\nBlocking is a general optimization technique for increasing the effectiveness of a memory hierarchy.\nBy reusing data in the faster level of the hierarchy, it cuts down the average access latency.\nIt also reduces the number of references made to slower levels of the hierarchy.\nBlocking is superior to optimization such as prefetching, which hides the latency but does not reduce the memory bandwidth requirement.\nThis reduction is especially im portant for multiprocessors since memory bandwidth is often the bottleneck of the system."
  },
  {
    "objectID": "bigdata_lec5.html#dask-15",
    "href": "bigdata_lec5.html#dask-15",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\nimport dask.array as da\nx = da.arange(15, chunks=(5,))"
  },
  {
    "objectID": "bigdata_lec5.html#dask-16",
    "href": "bigdata_lec5.html#dask-16",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nMetadata\n\n\nx # Array object metadata\ndask.array&lt;x-1, shape=(15,), chunks=((5, 5, 5)), dtype=int64&gt;\n\n\n\n\n\n\nDask Graph\n\n\nx.dask # Every dask array holds a dask graph\n{('x' , 0): (np.arange, 0, 5),\n ('x', 1): (np.arange, 5, 10),\n ('x' , 2): (np.arange, 10, 15)}"
  },
  {
    "objectID": "bigdata_lec5.html#dask-17",
    "href": "bigdata_lec5.html#dask-17",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nMore complex graph\n\n\nz = (x + 100).sum()\nz.dask\n{('x', 0): (np.arange, 0, 5),\n('x', 1): (np.arange, 5, 10),\n('x', 2): (np.arange, 10, 15),\n('y', 0): (add, ('x', 0), 100),\n('y', 1): (add, ('x', 1), 100),\n('y', 2): (add, ('x', 2), 100),\n('z', 0): (np.sum, ('y', 0)),\n('z', 1): (np.sum, ('y', 1)),\n('z', 2): (np.sum, ('y', 2)),\n('z',): (sum, [('z', 0), ('z', 1), ('z', 2)])}\n\n\n\n\n\n\nExecute the Graph\n\n\nz.compute()\n1605"
  },
  {
    "objectID": "bigdata_lec5.html#dask-18",
    "href": "bigdata_lec5.html#dask-18",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\ndask.array.Array objects\n\n\nx and z are both dask.array.Array objects containing:\n\nDask graph .dask\narray shape and chunk shape .chunks\na name identifying which keys in the graph correspond to the result, .name\na dtype"
  },
  {
    "objectID": "bigdata_lec5.html#dask-19",
    "href": "bigdata_lec5.html#dask-19",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nChunks\n\n\nA normal NumPy array knows its shape, a dask array must know its shape and the shape of all of the internal NumPy blocks that make up the larger array.\nThese shapes can be concisely described by a tuple of tuples of integers, where each internal tuple corresponds to the lengths along a single dimension.\nIn the example above we have a 20 by 24 array cut into uniform blocks of size 5 by 8. The chunks attribute describing this array is the following:\nchunks = ((5, 5, 5, 5), (8, 8, 8))"
  },
  {
    "objectID": "bigdata_lec5.html#dask-20",
    "href": "bigdata_lec5.html#dask-20",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nChunks need not be uniform!\n\n\nx[::2].chunks\n((3, 2, 3, 2), (8, 8, 8))\nx[::2].T.chunks\n((8, 8, 8), (3, 2, 3, 2))"
  },
  {
    "objectID": "bigdata_lec5.html#dask-21",
    "href": "bigdata_lec5.html#dask-21",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask Array operations\n\n\n\narithmetic and scalar math: +, *, exp, log\nreductions along axes: sum(), mean(), std(), sum(axis=0)\ntensor contractions / dot products / matrix multiplication: tensordot\naxis reordering / transposition: transpose\nslicing: x[:100, 500:100:-2]\nutility functions: bincount, where"
  },
  {
    "objectID": "bigdata_lec5.html#dask-22",
    "href": "bigdata_lec5.html#dask-22",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nAhead-of-time shape limitations\n\n\nx[x &gt; 0]"
  },
  {
    "objectID": "bigdata_lec5.html#dask-23",
    "href": "bigdata_lec5.html#dask-23",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\nGraph creation and graph scheduling are separate problems!\nCurrent Dask scheduler is dynamic.\n\n\n\nCurrent Dask scheduler logic\n\n\n\nA worker reports that it has completed a task and that it is ready for another.\nWe update runtime state to record the finished task,\nmark which new tasks can be run, which data can be released, etc.\nWe then choose a task to give to this worker from among the set of ready-to-run tasks. This small choice governs the macroscale performance of the scheduler."
  },
  {
    "objectID": "bigdata_lec5.html#dask-24",
    "href": "bigdata_lec5.html#dask-24",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nOut-of-core computation - which task to choose?\n\n\n\nlast in, first out\nselect tasks whose data dependencies were most recently made available.\nthis causes a behavior where long chains of related tasks trigger each other\nit forces the scheduler to finish related tasks before starting new ones.\nimplementation: a simple stack, which can operate in constant time."
  },
  {
    "objectID": "bigdata_lec5.html#dask-25",
    "href": "bigdata_lec5.html#dask-25",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask"
  },
  {
    "objectID": "bigdata_lec5.html#dask-26",
    "href": "bigdata_lec5.html#dask-26",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask"
  },
  {
    "objectID": "bigdata_lec5.html#taskvine",
    "href": "bigdata_lec5.html#taskvine",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nDescription\n\n\nTaskVine is a framework for building large scale data intensive dynamic workflows for:\n\nhigh performance computing (HPC) clusters\nGPU clusters\ncloud service providers\nand other distributed computing systems.\n\nTaskVine is our third-generation workflow system, built on our twenty years of experience creating scalable applications in fields such as\n\nhigh energy physics\nbioinformatics\nmolecular dynamics\nand machine learning."
  },
  {
    "objectID": "bigdata_lec5.html#taskvine-1",
    "href": "bigdata_lec5.html#taskvine-1",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nWorkflow\n\n\nA workflow is a collection of programs and files that are organized in a graph structure, allowing parts of the workflow to run in a parallel, reproducible way."
  },
  {
    "objectID": "bigdata_lec5.html#taskvine-2",
    "href": "bigdata_lec5.html#taskvine-2",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nSteps\n\n\n\nA TaskVine workflow requires:\n\na manager\nand a large number of worker processes.\n\nThe application generates a large number of small tasks, which are distributed to workers.\nAs tasks access external data sources and produce their own outputs, more and more data is pulled into local storage on cluster nodes.\nThis data is used to accelerate future tasks and avoid re-computing existing results. The application gradually grows “like a vine” through the cluster."
  },
  {
    "objectID": "bigdata_lec5.html#taskvine-3",
    "href": "bigdata_lec5.html#taskvine-3",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nArchitecture"
  },
  {
    "objectID": "bigdata_lec5.html#taskvine-4",
    "href": "bigdata_lec5.html#taskvine-4",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nFeatures and error handling\n\n\n\nWhile an application is running, workers may be added or removed as computing resources become available. (elasticity)\nNewly added workers will gradually accumulate data within the cluster.\nRemoved (or failed) workers are handled gracefully, and tasks will be retried elsewhere as needed.\nIf a worker failure results in the loss of files, tasks will be re-executed as necessary to re-create them."
  },
  {
    "objectID": "bigdata_lec5.html#taskvine-5",
    "href": "bigdata_lec5.html#taskvine-5",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nCoding it\n\n\n\nIndividual tasks can be simple Python functions, complex Unix applications, or serverless function invocations.\nThe key idea is that you declare file objects, and then declare tasks that consume them and produce new file objects. \n\nf = m.declare_url(\"https://www.gutenberg.org/cache/epub/2600/pg2600.txt\")\ng = m.declare_file(\"myoutput.txt\")\n\nt = Task(\"grep needle warandpeace.txt &gt; output.txt\")\nt.add_input(f, \"warandpeace.txt\")\nt.add_output(g, \"output.txt\")"
  },
  {
    "objectID": "bigdata_lec5.html#taskvine-6",
    "href": "bigdata_lec5.html#taskvine-6",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nTask options\n\n\n\nEach task can be labelled with the resources (CPU cores, GPU devices, memory, disk space) that it needs to execute.\nIf you don’t know the resources needed, you can enable a resource monitor to automatically track, report, and allocate what each task uses.\nThis allows each worker to pack the appropriate number of tasks. For example, a worker running on a 64-core machine could run 32 dual-core tasks, 16 four-core tasks, or any other combination that adds up to 64 cores."
  },
  {
    "objectID": "bigdata_lec5.html#taskvine-7",
    "href": "bigdata_lec5.html#taskvine-7",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nAs Dask Scheduler\n\n\nTaskVine manager can be used as Dask scheduler:\ntry:\n    import dask\n    import awkward as ak\n    import dask_awkward as dak\n    import numpy as np\nexcept ImportError:\n    print(\"You need dask, awkward, and numpy installed\")\n    print(\"(e.g. conda install -c conda-forge dask dask-awkward numpy) to run this example.\")\n\nbehavior: dict = {}\n\n@ak.mixin_class(behavior)\nclass Point:\n    def distance(self, other):\n    return np.sqrt((self.x - other.x) ** 2 + (self.y - other.y) ** 2)\n\n\nif __name__ == \"__main__\":\n    # data arrays\n    points1 = ak.Array([\n    [{\"x\": 1.0, \"y\": 1.1}, {\"x\": 2.0, \"y\": 2.2}, {\"x\": 3, \"y\": 3.3}],\n    [],\n    [{\"x\": 4.0, \"y\": 4.4}, {\"x\": 5.0, \"y\": 5.5}],\n    [{\"x\": 6.0, \"y\": 6.6}],\n    [{\"x\": 7.0, \"y\": 7.7}, {\"x\": 8.0, \"y\": 8.8}, {\"x\": 9, \"y\": 9.9}],\n    ])\n\n    points2 = ak.Array([\n    [{\"x\": 0.9, \"y\": 1.0}, {\"x\": 2.0, \"y\": 2.2}, {\"x\": 2.9, \"y\": 3.0}],\n    [],\n    [{\"x\": 3.9, \"y\": 4.0}, {\"x\": 5.0, \"y\": 5.5}],\n    [{\"x\": 5.9, \"y\": 6.0}],\n    [{\"x\": 6.9, \"y\": 7.0}, {\"x\": 8.0, \"y\": 8.8}, {\"x\": 8.9, \"y\": 9.0}],\n    ])\n    array1 = dak.from_awkward(points1, npartitions=3)\n    array2 = dak.from_awkward(points2, npartitions=3)\n\n    array1 = dak.with_name(array1, name=\"Point\", behavior=behavior)\n    array2 = dak.with_name(array2, name=\"Point\", behavior=behavior)\n\n    distance = array1.distance(array2)\n\n    m = vine.DaskVine(port=9123, ssl=True)\n    m.set_name(\"test_manager\")\n    print(f\"Listening for workers at port: {m.port}\")\n\n    f = vine.Factory(manager=m)\n    f.cores = 4\n    f.max_workers = 1\n    f.min_workers = 1\n    with f:\n        with dask.config.set(scheduler=m.get):\n            result = distance.compute(resources={\"cores\": 1}, resources_mode=\"max\", lazy_transfers=True)\n            print(f\"distance = {result}\")\n        print(\"Terminating workers...\", end=\"\")\n    print(\"done!\")"
  },
  {
    "objectID": "bigdata_lec5.html#dask-27",
    "href": "bigdata_lec5.html#dask-27",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nCollections\n\n\n\ndask.array = numpy+ threading\ndask.bag = toolz+ multiprocessing\ndask.dataframe = pandas+ threading"
  },
  {
    "objectID": "bigdata_lec5.html#dask-28",
    "href": "bigdata_lec5.html#dask-28",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask Bag - Definition\n\n\nA bag is an unordered collection with repeats.\nIt is like a Python list but does not guarantee the order of elements.\nThe dask.bag API contains functions like map and filter and generally follows the PyToolz API."
  },
  {
    "objectID": "bigdata_lec5.html#dask-29",
    "href": "bigdata_lec5.html#dask-29",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n&gt;&gt;&gt; import dask.bag as db\n&gt;&gt;&gt; import json\n&gt;&gt;&gt; b = db.from_filenames('2014-*.json.gz').map(json.loads)\n&gt;&gt;&gt; alices = b.filter(lambda d: d['name'] == 'Alice')\n&gt;&gt;&gt; alices.take(3)\n({'name': 'Alice', 'city': 'LA', 'balance': 100},\n{'name': 'Alice', 'city': 'LA', 'balance': 200},\n{'name': 'Alice', 'city': 'NYC', 'balance': 300},)\n\n&gt;&gt;&gt; dict(alices.pluck('city').frequencies())\n{'LA': 10000, 'NYC': 20000, ...}"
  },
  {
    "objectID": "bigdata_lec5.html#dask-30",
    "href": "bigdata_lec5.html#dask-30",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nS3 example\n\n\n&gt;&gt;&gt; import dask.bag as db\n&gt;&gt;&gt; b = db.from_s3('githubarchive-data', '2015-01-01-*.json.gz')\n          .map(json.loads)\n          .map(lambda d: d['type'] == 'PushEvent')\n          .count()"
  },
  {
    "objectID": "bigdata_lec5.html#dask-31",
    "href": "bigdata_lec5.html#dask-31",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask"
  },
  {
    "objectID": "bigdata_lec5.html#dask-32",
    "href": "bigdata_lec5.html#dask-32",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask DataFrame - Definition\n\n\nThe dask.dataframe module implements a large dataframe out of many Pandas DataFrames.\nIt uses a threaded scheduler."
  },
  {
    "objectID": "bigdata_lec5.html#dask-33",
    "href": "bigdata_lec5.html#dask-33",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nPartitioned datasets\n\n\nThe dask dataframe can compute efficiently on partitioned datasets where the different blocks are well separated along an index.\nFor example in time series data we may know that all of January is in one block while all of February is in another.\nJoin, groupby, and range queries along this index are significantly faster when working on partitioned datasets."
  },
  {
    "objectID": "bigdata_lec5.html#dask-34",
    "href": "bigdata_lec5.html#dask-34",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask DataFrame join"
  },
  {
    "objectID": "bigdata_lec5.html#dask-35",
    "href": "bigdata_lec5.html#dask-35",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nOut-of-core parallel SVD example\n\n\n&gt;&gt;&gt; import dask.array as da\n&gt;&gt;&gt; x = da.ones((5000, 1000), chunks=(1000, 1000))\n&gt;&gt;&gt; u, s, v = da.svd(x)\nOut-of-core parallel non-negative matrix factorizations on top of dask.array."
  },
  {
    "objectID": "bigdata_lec5.html#dask-36",
    "href": "bigdata_lec5.html#dask-36",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nOut-of-core parallel SVD"
  },
  {
    "objectID": "bigdata_lec5.html#usage-1",
    "href": "bigdata_lec5.html#usage-1",
    "title": "Big Data: Dask intro part2",
    "section": "Usage",
    "text": "Usage\n\nscida.io - astrophysical simulations"
  },
  {
    "objectID": "bigdata_lec5.html#usage-2",
    "href": "bigdata_lec5.html#usage-2",
    "title": "Big Data: Dask intro part2",
    "section": "Usage",
    "text": "Usage\n\nPangeo - open, reproducible, scalable geoscience. A global slice of Sea Water Temperature"
  },
  {
    "objectID": "bigdata_lec5.html#usage-3",
    "href": "bigdata_lec5.html#usage-3",
    "title": "Big Data: Dask intro part2",
    "section": "Usage",
    "text": "Usage"
  },
  {
    "objectID": "bigdata_lec5.html#comparison-with-spark",
    "href": "bigdata_lec5.html#comparison-with-spark",
    "title": "Big Data: Dask intro part2",
    "section": "Comparison with Spark",
    "text": "Comparison with Spark\n\n\n\nSetup\n\n\n\nBigBrain20, a 3-D image of the human brain, total data size of 606 GiB.\ndataset provided by the Consortium for Reliability and Reproducibility, entire dataset is 379.83 GiB, used all 3,491 anatomical images, representing 26.67 GiB overall."
  },
  {
    "objectID": "bigdata_lec5.html#comparison-with-spark-1",
    "href": "bigdata_lec5.html#comparison-with-spark-1",
    "title": "Big Data: Dask intro part2",
    "section": "Comparison with Spark",
    "text": "Comparison with Spark"
  },
  {
    "objectID": "bigdata_lec5.html#comparison-with-spark-2",
    "href": "bigdata_lec5.html#comparison-with-spark-2",
    "title": "Big Data: Dask intro part2",
    "section": "Comparison with Spark",
    "text": "Comparison with Spark"
  },
  {
    "objectID": "bigdata_lec5.html#comparison-with-spark-3",
    "href": "bigdata_lec5.html#comparison-with-spark-3",
    "title": "Big Data: Dask intro part2",
    "section": "Comparison with Spark",
    "text": "Comparison with Spark"
  },
  {
    "objectID": "bigdata_lab5.html",
    "href": "bigdata_lab5.html",
    "title": "Big Data Analytics: Lab 5",
    "section": "",
    "text": "Dask Delayed and Futures\n\nComplete exercises from \nComplete tutorials:\n\n\nhttps://tutorial.dask.org/03_dask.delayed.html\nhttps://tutorial.dask.org/05_futures.html"
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag",
    "href": "bigdata_lec4.html#program-as-dag",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nRepresentation\n\n\nA parallel program can be represented by a node- and edge-weighted directed acyclic graph (DAG), in which:\n\nnode weights represent task processing times\nedge weights represent data dependencies as well as the communication times between tasks."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-1",
    "href": "bigdata_lec4.html#program-as-dag-1",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG"
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-2",
    "href": "bigdata_lec4.html#program-as-dag-2",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nGeneralization\n\n\nMultithreaded computing can be viewed as a natural generalization of sequential computing in the following sense:\n\nin sequential computing, a computation can be defined as a totally ordered set of instructions, where the ordering corresponds to the sequential execution order;\nin multithreaded computing, a computation can be viewed as a partially ordered set of instructions (as specified by the DAG), where the instructions may be executed in any order compatible with the specified partial order."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-3",
    "href": "bigdata_lec4.html#program-as-dag-3",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nOrdering re-cap\n\n\nA binary relation \\(\\preccurlyeq\\) on some set \\(X\\) is called a partial order if \\(\\forall a,b,c \\in X\\) the following is true:\n\nReflexivity: \\(a \\preccurlyeq a\\)\nTransitiity: \\(a \\preccurlyeq b, b \\preccurlyeq c \\Rightarrow a \\preccurlyeq c\\)\nAntisymmetricity: \\(a \\preccurlyeq b, b \\preccurlyeq a \\Rightarrow a = b\\)\n\nIf, additionally, \\(\\forall a,b \\ in X\\) either \\(a \\preccurlyeq b\\) or \\(b \\preccurlyeq a\\), then the order is total."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-4",
    "href": "bigdata_lec4.html#program-as-dag-4",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nDefinitions\n\n\nA parallel program can be represented by a directed acyclic graph (DAG) \\[\nG=(V,E),\n\\]\nwhere \\(V\\) is a set of \\(v\\) nodes and \\(E\\) is a set of \\(e\\) directed edges.\nA node in the DAG represents a task which in turn is a set of instructions which must be executed sequentially without preemption in the same processor.\nThe weight of a node \\(n_i\\) is called the computation cost and is denoted by \\(w(n_i)\\)."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-5",
    "href": "bigdata_lec4.html#program-as-dag-5",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nDefinitions\n\n\nThe edges in the DAG, each of which is denoted by \\((n_i,n_j)\\), correspond to the communication messages and precedence constraints among the nodes.\nThe weight of an edge is called the communication cost of the edge and is denoted by \\(c(n_i, n_j)\\).\nThe source node of an edge is called the parent node while the sink node is called the child node.\nA node with no parent is called an entry node and a node with no child is called an exit node.\nThe communication-to-computation-ratio (CCR) of a parallel program is defined as its average edge weight divided by its average node weight."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-6",
    "href": "bigdata_lec4.html#program-as-dag-6",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nDefinition\n\n\nScheduling involves executing a parallel program by mapping the computation over the processors so that:\n\ncompletion time is minimized\nuse of other resources such as storage as energy is optimal."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-7",
    "href": "bigdata_lec4.html#program-as-dag-7",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nDefinitions\n\n\n\\(ST(n_i)\\) and \\(FT(n_i)\\) denote start time and finish time at some processor.\nAfter all the nodes have been scheduled, the schedule length is defined as \\(\\max_i\\left\\{FT(n_i)\\right\\}\\) across all processors.\nThe goal of scheduling is to minimize \\(\\max_i\\left\\{FT(n_i)\\right\\}\\).\nScheduling is done in such a manner that the precedence constraints among the program tasks are preserved."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-8",
    "href": "bigdata_lec4.html#program-as-dag-8",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG"
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-9",
    "href": "bigdata_lec4.html#program-as-dag-9",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG"
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-10",
    "href": "bigdata_lec4.html#program-as-dag-10",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG"
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-11",
    "href": "bigdata_lec4.html#program-as-dag-11",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nWork\n\n\nWork is defined as the number of vertices in the DAG.\nWork of a computation corresponds to the total number of operations it performs.\n\n\n\n\n\n\nSpan\n\n\nSpan is the length of the longest path in the DAG.\nSpan corresponds to the longest chain of dependencies in the computation.\n\n\n\n\n\n\nWork[make]span\n\n\nThe overall finish-time of a parallel program is commonly called the schedule length or makespan."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-1",
    "href": "bigdata_lec4.html#scheduling-1",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nTaxonomy"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-2",
    "href": "bigdata_lec4.html#scheduling-2",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nTypes\n\n\n\nStatic: the characteristics of a parallel program (such as task processing times, communication, data dependencies, and synchronization requirements) are known before program execution\nDynamic: a few assumptions about the parallel program can be made before execution, and thus, scheduling decisions have to be made on-the-fly."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-3",
    "href": "bigdata_lec4.html#scheduling-3",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nCategories\n\n\n\nJob scheduling: independent jobs are to be scheduled among the processors of a distributed computing system to optimize overall system performance\nScheduling and mapping: allocation of multiple interacting tasks of a single parallel program in order to minimize the completion time on the parallel computer system."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-model-variations",
    "href": "bigdata_lec4.html#scheduling-model-variations",
    "title": "Big Data: Dask intro",
    "section": "Scheduling model variations",
    "text": "Scheduling model variations\n\n\n\nPreemptive vs non-preemptive\n\n\n\npreemptive: execution of the task might be interrupted so that it’s allocated to a different processor\nnon-preemptive: execution must complete on a single processor\n\n\n\n\n\n\n\nParallel vs non-parallel\n\n\nParallel task requires more than one processor for its execution.\n\n\n\n\n\n\nWith vs without conditional branches\n\n\nIn conditional model, each edge in the DAG is associated with a non-zero probability that the child will be executed immediately after the parent."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-4",
    "href": "bigdata_lec4.html#scheduling-4",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nList scheduling\n\n\nThe basic idea of list scheduling is to make a scheduling list (a sequence of nodes for scheduling) by assigning them some priorities, and then repeatedly execute the following two steps until all the nodes in the graph are scheduled:\n\nRemove the first node from the scheduling list;\nAllocate the node to a processor which allows the earliest start-time."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-5",
    "href": "bigdata_lec4.html#scheduling-5",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nGreedy Scheduler\n\n\nWe say that a scheduler is greedy if whenever there is a processor available and a task ready to execute, then it assigns the task to the processor and starts running it immediately. Greedy schedulers have an important property that is summarized by the greedy scheduling principle.\n\n\n\n\n\n\nGreedy Scheduling Principle\n\n\nThe greedy scheduling principle postulates that if a computation is run on \\(P\\) processors using a greedy scheduler, then the total time (clock cycles) for running the computation is bounded by \\[\nT_P &lt; \\frac{W}{P} + S\n\\] where \\(W\\) is the work of the computation, and \\(S\\) is the span of the computation (both measured in units of clock cycles)."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-6",
    "href": "bigdata_lec4.html#scheduling-6",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nOptimality of Greedy Schedulers\n\n\nFirstly, the time to execute the computation cannot be less than \\(\\frac{W}{P}\\) clock cycles since we have a total of \\(W\\) clock cycles of work to do and the best we can possibly do is divide it evenly among the processors. Secondly, the time to execute the computation cannot be any less than \\(S\\) clock cycles, because \\(S\\) represents the longest chain of sequential dependencies. Therefore we have \\[\nT_P \\geq \\max\\left(\\frac{W}{P},S\\right).\n\\] We therefore see that a greedy scheduler does reasonably close to the best possible. In particular, \\(\\frac{W}{P} +S\\) is never more than twice \\(\\max\\left(\\frac{W}{P} ,S\\right)\\)."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-1",
    "href": "bigdata_lec4.html#scheduling-algorithms-1",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-2",
    "href": "bigdata_lec4.html#scheduling-algorithms-2",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms\n\n\n\nWhat’s a heuristic algorithm?\n\n\nAlgorithm used when classic methods are too slow for finding an exact or approximate solution, or when classic methods fail to find any exact solution in a search space.\nThis is achieved by trading\n\noptimality,\ncompleteness,\naccuracy,\nor precision\n\nfor speed.\nIn a way, it can be considered a shortcut."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-3",
    "href": "bigdata_lec4.html#scheduling-algorithms-3",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms\n\n\n\nList scheduling\n\n\nA list-scheduling heuristic maintains a list of all tasks of a given graph according to their priorities. It has two phases:\n\nthe task prioritizing or task selection phase for selecting the highest-priority ready task\nand the processor selection phase for selecting a suitable processor that minimizes a predefined cost function which can be the execution start time.\n\n\n\n\n\n\n\nFeatures\n\n\n\nfor a bounded number of fully connected homogeneous processors\nprovide better performance results at a lower scheduling time than the other groups"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-4",
    "href": "bigdata_lec4.html#scheduling-algorithms-4",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms\n\n\n\nClustering\n\n\n\nMaps the tasks to unlimited number of clusters. The selected tasks for clustering can be any task, not necessarily a ready task.\nEach iteration refines the previous clustering by merging some clusters.\nIf two tasks are assigned to the same cluster, they will be executed on the same processor."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-5",
    "href": "bigdata_lec4.html#scheduling-algorithms-5",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms\n\n\n\nClustering: Extra final steps\n\n\n\na cluster merging step for merging the clusters so that the remaining number of clusters equal the number of processors\na cluster mapping step for mapping the clusters on the available processors\na task ordering step for ordering the mapped tasks within each processor"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-6",
    "href": "bigdata_lec4.html#scheduling-algorithms-6",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms\n\n\n\nGuided random search\n\n\nGuided random search techniques (or randomized search techniques) use random choice to guide themselves through the problem space, which is not the same as performing merely random walks as in the random search methods.\nThese techniques combine the knowledge gained from previous search results with some randomizing features to generate new results."
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop",
    "href": "bigdata_lec4.html#heftcpop",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nDefinition\n\n\nHeterogeneous earliest finish time (HEFT) is a heuristic algorithm to schedule a set of dependent tasks onto a network of heterogeneous workers taking communication time into account.\nCritical-Path-On-a-Processor (CPOP) algorithm uses the summation of upward and downward rank values for prioritizing tasks.\n\n\n\n\n\nH. Topcuoglu, S. Hariri and Min-You Wu, “Performance-effective and low-complexity task scheduling for heterogeneous computing,” in IEEE Transactions on Parallel and Distributed Systems, vol. 13, no. 3, pp. 260-274, March 2002, doi: 10.1109/71.993206."
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-1",
    "href": "bigdata_lec4.html#heftcpop-1",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nDefinitions\n\n\n\n\\(V\\) – set of \\(v\\) nodes\n\\(E\\) – set of \\(e\\) edges\n\\(data\\) – a \\(v \\times v\\) matrix of communication data\n\\(data_{i,k}\\) – amount of data to be transmitted from \\(n_i\\) to \\(n_k\\)\n\\(Q\\) – set of \\(q\\) processors\n\\(W\\) – a \\(v \\times q\\) computation cost matrix, in which each \\(w_{i,j}\\) gives the estimated execution cost to complete task \\(n_i\\) on processor \\(p_j\\)\n\\(B\\) – \\(q \\times q\\) data transfer rates matrix\n\\(L\\) – \\(q\\)-dimensional vector of communication startup costs"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-2",
    "href": "bigdata_lec4.html#heftcpop-2",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nCosts\n\n\nAverage execution cost: \\[\n  \\overline{w_i} = \\sum\\limits_{j=1}^q \\frac{w_{i,j}}{q}.\n\\] Communication cost of the edge \\((i,k)\\) for transfering data from task \\(n_i\\) scheduled on processor \\(p_m\\) to task \\(n_k\\) scheduled on processor \\(p_n\\): \\[\nc_{i,k} = L_m + \\frac{data_{i,k}}{B_{m,n}}\n\\] Average communication cost: \\[\n\\overline{c_{i,k}} = \\overline{L} + \\frac{data_{i,k}}{\\overline{B}}\n\\]"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-3",
    "href": "bigdata_lec4.html#heftcpop-3",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nTimes\n\n\nEarliest execution start time of task \\(n_i\\) on processor \\(p_j\\): \\[\n\\begin{align*}\n& EST(n_{entry}, p_j) = 0, \\\\\n& EST(n_i, p_j) = \\max\\left\\{avail[j], \\max\\limits_{n_m \\in pred(n_i)} (AFT(n_m)+c_{m,i})\\right\\},\n\\end{align*}\n\\] where \\(avail[j]\\) is the earliest time at which processor \\(p_j\\) is ready for task execution.\nEarliest execution finish time of task \\(n_i\\) on processor \\(p_j\\): \\[\nEFT(n_i, p_j) = w_{i,j} + EST(n_i,p_j)\n\\]\nAfter a task \\(n_m\\) is scheduled on processor \\(p_j\\), the earliest start time and the earliest finish time of \\(n_m\\) on processor \\(p_j\\) is equal to the actual start time \\(AST(n_m)\\) and the actual finish time \\(AFT(n_m)\\), respectively."
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-4",
    "href": "bigdata_lec4.html#heftcpop-4",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nWorkspan\n\n\nAfter all tasks in the graph have been scheduled, the schedule length (makespan) will be equal to the actual finish time of the exit task \\(n_{exit}\\). In case of several exits: \\[\nmakespan = \\max \\left\\{AFT(n_{exit})\\right\\}\n\\]\n\n\n\n\n\n\nDefinition\n\n\nThe objective function of the task scheduling problem is to determine the assignment of tasks to processors such that its makespan is minimized."
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-5",
    "href": "bigdata_lec4.html#heftcpop-5",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nUpward rank\n\n\n\\[\n\\begin{align*}\n  rank_u(n_i) = \\overline{w_i} + \\max\\limits_{n_j \\in succ(n_i)} \\left(\\overline{c_{i,j}} + rank_u(n_j)\\right)\n\\end{align*}\n\\]\n\n\\(succ(n_i)\\) – set of immediate successors of \\(n_i\\)\n\\(\\overline{c_{i,j}}\\) – average communication cost of edge \\((i,j)\\)\n\\(\\overline{w_i}\\) – average computation cost of task \\(n_i\\)\n\n\\[\nrank_u(n_{exit}) = \\overline{w_{exit}}\n\\]"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-6",
    "href": "bigdata_lec4.html#heftcpop-6",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nDownward rank\n\n\n\\[\n\\begin{align*}\n  rank_d(n_i) = \\max\\limits_{n_j \\in pred(n_i)} \\left(rank_d(n_j)+ \\overline{w_j} + \\overline{c_{j,i}}\\right)\n  \\end{align*}\n\\]\n\n\\(pred(n_i)\\) – set of immediate predecessors of \\(n_i\\)\n\\(rank_d(n_{entry}) = 0\\)\ncan be thought of as the longest distance from the entry task to \\(n_i\\) without computation costs"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-7",
    "href": "bigdata_lec4.html#heftcpop-7",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nHEFT\n\n\n\nSet the computation costs of tasks and communication costs of edges with mean values\nCompute \\(rank_u\\) for all tasks by traversing graph upward, starting from the exit task\nSort the tasks in the scheduling list by non-increasing order of \\(rank_u\\) values\nwhile there are unscheduled tasks in the list do:\n   select the task \\(n_i\\) from the scheduling list\n   for each processor \\(p_k\\) in the processor set \\(Q\\) do:\n     Compute \\(EFT(n_i,p_k)\\) using the insertion-based scheduling policy\n   Assign task \\(n_i\\) to the processor \\(p_j\\) minimizing \\(EFT(n_i, p_j)\\)"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-8",
    "href": "bigdata_lec4.html#heftcpop-8",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nCPOP\n\n\n\nSet the computation costs of tasks and communication costs of edges with mean values\nCompute \\(rank_u\\) for all tasks by traversing graph upward, starting from the exit task\nCompute \\(rank_d\\) for all tasks by traversing graph downward, starting from the entry task\nCompute \\(priority(n_i) = rank_d(n_i) + rank_u(n_i) \\; \\forall n_i\\)\n\\(|CP| = priority(n_{entry})\\)\n\\(SET_{CP} = \\{n_{entry}\\}\\), where \\(SET_{CP}\\) is a set of tasks on a critical path\n\\(n_k \\leftarrow n_{entry}\\)"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-9",
    "href": "bigdata_lec4.html#heftcpop-9",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nCPOP\n\n\n\nwhile \\(n_k \\neq n_{exit}\\) do\n   select \\(n_j: n_j \\in succ(n_k) \\textbf{ and } priority(n_j) = |CP|\\)\n   \\(SET_{CP} = SET_{CP} \\cup {n_j}\\)\n   \\(n_k \\leftarrow n_j\\)\nselect \\(p_{CP}\\) minimizing \\(\\sum\\limits_{n_i \\in SET_{CP}} w_{i,j} \\; \\forall p_j \\in Q\\)\ninitialize priority queue \\(PQ\\) with the entry task"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-10",
    "href": "bigdata_lec4.html#heftcpop-10",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nCPOP\n\n\n\nwhile there are unscheduled tasks in the \\(PQ\\)do\n   select the highest priority task \\(n_i\\) from \\(PQ\\)\n   if \\(n_i \\ in SET_{CP}\\) then\n     assign \\(n_i\\) to \\(p_{CP}\\)\n   else\n     assign \\(n_i\\) to \\(p_j\\) minimizing \\(EFT(n_i,p_j)\\)\n   update \\(PQ\\) with the successors of \\(n_i\\) if ready"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-11",
    "href": "bigdata_lec4.html#heftcpop-11",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-12",
    "href": "bigdata_lec4.html#heftcpop-12",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\nHEFT - (a), CPOP - (b)"
  },
  {
    "objectID": "bigdata_lec4.html#dask",
    "href": "bigdata_lec4.html#dask",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\nDask is a library to perform parallel computation for analytics."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-7",
    "href": "bigdata_lec4.html#scheduling-7",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nWhat does Dask scheduler do?\n\n\n\nexecute DAGs on parallel hardware\nmanage resource allocation across DAG nodes"
  },
  {
    "objectID": "bigdata_lec4.html#dask-scheduling-1",
    "href": "bigdata_lec4.html#dask-scheduling-1",
    "title": "Big Data: Dask intro",
    "section": "Dask Scheduling",
    "text": "Dask Scheduling\n\n\n\nDask Scheduler types\n\n\n\nSingle-machine scheduler: This scheduler provides basic features on a local process or thread pool. This scheduler was made first and is the default. It is simple and cheap to use, although it can only be used on a single machine and does not scale\nDistributed scheduler: This scheduler is more sophisticated, offers more features, but also requires a bit more effort to set up. It can run locally or distributed across a cluster"
  },
  {
    "objectID": "bigdata_lec4.html#dag",
    "href": "bigdata_lec4.html#dag",
    "title": "Big Data: Dask intro",
    "section": "DAG",
    "text": "DAG"
  },
  {
    "objectID": "bigdata_lec4.html#dask-scheduling-2",
    "href": "bigdata_lec4.html#dask-scheduling-2",
    "title": "Big Data: Dask intro",
    "section": "Dask Scheduling",
    "text": "Dask Scheduling"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-8",
    "href": "bigdata_lec4.html#scheduling-8",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nSingle-thread scheduler\n\n\nimport dask\ndask.config.set(scheduler='synchronous')  # overwrite default with single-threaded scheduler\n\n\n\n\n\n\nNotes\n\n\n\nUseful for debugging or profiling\nNo parallelism at all"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-9",
    "href": "bigdata_lec4.html#scheduling-9",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nThread scheduler\n\n\nimport dask\ndask.config.set(scheduler='threads')  # overwrite default with threaded scheduler \n\n\n\n\n\n\nNotes\n\n\n\nSmall overhead of 50 microseconds per task\nOnly provides parallelism when executing non-Python code (because of GIL)"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-10",
    "href": "bigdata_lec4.html#scheduling-10",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nProcess scheduler\n\n\nimport dask\ndask.config.set(scheduler='processes')  # overwrite default with multiprocessing scheduler\n\n\n\n\n\n\nNotes\n\n\n\nPerformance penalties when inter-process communication is heavy\nCan provide parallelism when executing Python code"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-11",
    "href": "bigdata_lec4.html#scheduling-11",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nDistributed (local) scheduler\n\n\nfrom dask.distributed import Client\nclient = Client()\n# or\nclient = Client(processes=False)\n\n\n\n\n\n\nNotes\n\n\n\nCan be more efficient than the multiprocessing scheduler on workloads that require multiple processes\nDiagnostic dashboard and async API"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-12",
    "href": "bigdata_lec4.html#scheduling-12",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nDistributed (cluster) scheduler\n\n\n# You can swap out LocalCluster for other cluster types\n\nfrom dask.distributed import LocalCluster\nfrom dask_kubernetes import KubeCluster\n\n# cluster = LocalCluster()\ncluster = KubeCluster()  # example, you can swap out for Kubernetes\n\nclient = cluster.get_client()\n\n\n\n\n\n\nNotes\n\n\n\nCan be setup either locally, or e.g. on a pre-existing Kubernetes cluster\nDifferent cluster backends easy to swap"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-13",
    "href": "bigdata_lec4.html#scheduling-13",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling"
  },
  {
    "objectID": "bigdata_lec4.html#issues",
    "href": "bigdata_lec4.html#issues",
    "title": "Big Data: Dask intro",
    "section": "Issues",
    "text": "Issues\n\n\n\nIssues\n\n\n\nResource starvation\nWorker failures\nData loss"
  },
  {
    "objectID": "applied.html",
    "href": "applied.html",
    "title": "Applied Data Analytics",
    "section": "",
    "text": "Course will cover the following:\n\ndata fetching/preprocessing\nspecialized data types:\n\ngeospatial\nfinancial\nsupply chain management\nmore\n\nvisualization/presentation (e.g. Gradio)\nLLM toolkits (LangChain/LangFlow)\nML Pipelines (MLFlow/KubeFlow/AirFlow/Prefect/Metaflow)\nMLOps\nAmazon Web Services\nDocker"
  },
  {
    "objectID": "applied.html#overview",
    "href": "applied.html#overview",
    "title": "Applied Data Analytics",
    "section": "",
    "text": "Course will cover the following:\n\ndata fetching/preprocessing\nspecialized data types:\n\ngeospatial\nfinancial\nsupply chain management\nmore\n\nvisualization/presentation (e.g. Gradio)\nLLM toolkits (LangChain/LangFlow)\nML Pipelines (MLFlow/KubeFlow/AirFlow/Prefect/Metaflow)\nMLOps\nAmazon Web Services\nDocker"
  },
  {
    "objectID": "applied.html#lectures",
    "href": "applied.html#lectures",
    "title": "Applied Data Analytics",
    "section": "Lectures",
    "text": "Lectures\nLecture 1: Intro to Docker\nLecture 2: Dockerfiles\nLecture 3: Docker Networking\nLecture 4: AWS intro\nLecture 5: AWS part 2\nLecture 6: Docker Compose\nLecture 7: Kubernetes part 1"
  },
  {
    "objectID": "applied.html#labs",
    "href": "applied.html#labs",
    "title": "Applied Data Analytics",
    "section": "Labs",
    "text": "Labs\nLab 1: Environment setup\nLab 2: Parallelisation and functional programming\nLab 3: Exercises from Docker intro lecture\nLab 4: Exercises from Dockerfile lecture\nLab 5: Python environment in Docker\nLab 6: Docker on AWS"
  },
  {
    "objectID": "bigdata_lec6.html#intro",
    "href": "bigdata_lec6.html#intro",
    "title": "Big Data: Dask tutorial",
    "section": "Intro",
    "text": "Intro\n\n\nDask is a parallel and distributed computing library that scales the existing Python and PyData ecosystem.\nDask can scale up to your full laptop capacity and out to a cloud cluster."
  },
  {
    "objectID": "bigdata_lec6.html#installation",
    "href": "bigdata_lec6.html#installation",
    "title": "Big Data: Dask tutorial",
    "section": "Installation",
    "text": "Installation\n\n\n\n\n\n\nWith uv\n\n\nuv add \"dask[complete]\"\nuv add s3fs\nuv add pyarrow"
  },
  {
    "objectID": "bigdata_lec6.html#an-example-dask-computation",
    "href": "bigdata_lec6.html#an-example-dask-computation",
    "title": "Big Data: Dask tutorial",
    "section": "An example Dask computation",
    "text": "An example Dask computation\n\n\n\nExample\n\n\nIn the following lines of code, we’re reading the NYC taxi cab data from 2015 and finding the mean tip amount.\n\nImport\n\nimport dask.dataframe as dd\nfrom dask.distributed import Client\n\nCreate client.\n\nclient = Client()\nclient\n\nRead data.\n\nddf = dd.read_parquet(\n    \"s3://dask-data/nyc-taxi/nyc-2015.parquet/part.*.parquet\",\n    columns=[\"passenger_count\", \"tip_amount\"],\n    storage_options={\"anon\": True},\n)\n\nGroupby and mean\n\nresult = ddf.groupby(\"passenger_count\").tip_amount.mean().compute()\nresult"
  },
  {
    "objectID": "bigdata_lec6.html#what-is-dask",
    "href": "bigdata_lec6.html#what-is-dask",
    "title": "Big Data: Dask tutorial",
    "section": "What is Dask?",
    "text": "What is Dask?\n\n\n\nMany parts\n\n\nThere are many parts to the “Dask” the project:\n\nCollections/API also known as “core-library”.\nDistributed – to create clusters\nIntegrations and broader ecosystem"
  },
  {
    "objectID": "bigdata_lec6.html#dask-collections",
    "href": "bigdata_lec6.html#dask-collections",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Collections",
    "text": "Dask Collections\n\n\n\nOverview\n\n\nDask provides multi-core and distributed+parallel execution on larger-than-memory datasets\n\n\nHigh-level collections: Dask provides high-level Array, Bag, and DataFrame collections that mimic NumPy, lists, and pandas but can operate in parallel on datasets that don’t fit into memory.\nLow-level collections: Dask also provides low-level Delayed and Futures collections that give you finer control to build custom parallel and distributed computations."
  },
  {
    "objectID": "bigdata_lec6.html#dask-cluster",
    "href": "bigdata_lec6.html#dask-cluster",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Cluster",
    "text": "Dask Cluster\n\n\n\nCluster\n\n\nMost of the times when you are using Dask, you will be using a distributed scheduler, which exists in the context of a Dask cluster. The Dask cluster is structured as:"
  },
  {
    "objectID": "bigdata_lec6.html#dask-ecosystem",
    "href": "bigdata_lec6.html#dask-ecosystem",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Ecosystem",
    "text": "Dask Ecosystem\n\n\n\nLibraries\n\n\nIn addition to the core Dask library and its distributed scheduler, the Dask ecosystem connects several additional initiatives, including:\n\nDask-ML (parallel scikit-learn-style API)\nDask-image\nDask-cuDF\nDask-sql\nDask-snowflake\nDask-mongo\nDask-bigquery"
  },
  {
    "objectID": "bigdata_lec6.html#dask-ecosystem-1",
    "href": "bigdata_lec6.html#dask-ecosystem-1",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Ecosystem",
    "text": "Dask Ecosystem\n\n\n\nCommunity ibraries\n\n\nCommunity libraries that have built-in dask integrations like:\n\nXarray\nXGBoost\nPrefect\nAirflow\n\nDask deployment libraries\n\nDask-kubernetes\nDask-YARN\nDask-gateway\nDask-cloudprovider\njobqueue"
  },
  {
    "objectID": "bigdata_lec6.html#dask-use-cases",
    "href": "bigdata_lec6.html#dask-use-cases",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Use Cases",
    "text": "Dask Use Cases\n\n\n\nUse cases\n\n\nDask is used in multiple fields such as:\n\nGeospatial\nFinance\nAstrophysics\nMicrobiology\nEnvironmental science\n\nCheck out the Dask use cases page that provides a number of sample workflows."
  },
  {
    "objectID": "bigdata_lec6.html#dask-dataframe---parallelized-pandas",
    "href": "bigdata_lec6.html#dask-dataframe---parallelized-pandas",
    "title": "Big Data: Dask tutorial",
    "section": "Dask DataFrame - parallelized pandas",
    "text": "Dask DataFrame - parallelized pandas\n\n\n\nDescription\n\n\nLooks and feels like the pandas API, but for parallel and distributed workflows.\n\nAt its core, the dask.dataframe module implements a “blocked parallel” DataFrame object that looks and feels like the pandas API, but for parallel and distributed workflows.\nOne Dask DataFrame is comprised of many in-memory pandas DataFrames separated along the index.\nOne operation on a Dask DataFrame triggers many pandas operations on the constituent pandas DataFrames in a way that is mindful of potential parallelism and memory constraints."
  },
  {
    "objectID": "bigdata_lec6.html#dask-dataframe",
    "href": "bigdata_lec6.html#dask-dataframe",
    "title": "Big Data: Dask tutorial",
    "section": "Dask DataFrame",
    "text": "Dask DataFrame"
  },
  {
    "objectID": "bigdata_lec6.html#dask-dataframe-1",
    "href": "bigdata_lec6.html#dask-dataframe-1",
    "title": "Big Data: Dask tutorial",
    "section": "Dask DataFrame",
    "text": "Dask DataFrame\n\n\n\nRelated Documentation\n\n\n\nDataFrame documentation\nDataFrame screencast\nDataFrame API\nDataFrame examples\npandas documentation"
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-1",
    "href": "bigdata_lec6.html#dataframe-1",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nWhen to use dask.dataframe\n\n\npandas is great for tabular datasets that fit in memory. A general rule of thumb for pandas is:\n\n“Have 5 to 10 times as much RAM as the size of your dataset”\n\n\n~ Wes McKinney (2017) in 10 things I hate about pandas\n\nHere “size of dataset” means dataset size on the disk.\nDask becomes useful when the datasets exceed the above rule.\nIn this notebook, you will be working with the New York City Airline data. This dataset is only ~200MB, so that you can download it in a reasonable time, but dask.dataframe will scale to datasets much larger than memory."
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-2",
    "href": "bigdata_lec6.html#dataframe-2",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nCreate datasets\n\n\nCreate the datasets you will be using in this notebook:\n%run prep.py -d flights\n\n\n\n\n\n\nSet up your local cluster\n\n\nCreate a local Dask cluster and connect it to the client.\nfrom dask.distributed import Client\n\nclient = Client(n_workers=4)\nclient"
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-3",
    "href": "bigdata_lec6.html#dataframe-3",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nDask Diagnostic Dashboard\n\n\nDask Distributed provides a useful Dashboard to visualize the state of your cluster and computations.\nClick on the dashboard link displayed in the Client details above: http://127.0.0.1:8787/status. It will open a new browser tab with the Dashboard."
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-4",
    "href": "bigdata_lec6.html#dataframe-4",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nReading and working with datasets\n\n\nLet’s read an extract of flights in the USA across several years. This data is specific to flights out of the three airports in the New York City area.\nimport os\nimport dask\nBy convention, we import the module dask.dataframe as dd, and call the corresponding DataFrame object ddf.\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe term “Dask DataFrame” is slightly overloaded. Depending on the context, it can refer to the module or the DataFrame object. To avoid confusion:\n\ndask.dataframe (note the all lowercase) refers to the API, and\nDataFrame (note the CamelCase) refers to the object."
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-5",
    "href": "bigdata_lec6.html#dataframe-5",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nReading and working with datasets\n\n\nThe following filename includes a glob pattern *, so all files in the path matching that pattern will be read into the same DataFrame.\nimport dask.dataframe as dd\n\nddf = dd.read_csv(\n    os.path.join(\"data\", \"nycflights\", \"*.csv\"), parse_dates={\"Date\": [0, 1, 2]}\n)\nddf"
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-6",
    "href": "bigdata_lec6.html#dataframe-6",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nWhat happened?\n\n\nDask has not loaded the data yet, it has:\n\ninvestigated the input path and found that there are ten matching files\nintelligently created a set of jobs for each chunk – one per original CSV file in this case\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nNotice that the representation of the DataFrame object contains no data - Dask has just done enough to read the start of the first file, and infer the column names and dtypes."
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-7",
    "href": "bigdata_lec6.html#dataframe-7",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nLazy Evaluation\n\n\nMost Dask Collections, including Dask DataFrame are evaluated lazily, which means Dask constructs the logic (called task graph) of your computation immediately but “evaluates” them only when necessary.\nYou can view this task graph using .visualize().\nddf.visualize()\nNote that we need to call .compute() to trigger actual computations."
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-8",
    "href": "bigdata_lec6.html#dataframe-8",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nLazy Evaluation\n\n\nSome functions like len and head also trigger a computation. Specifically, calling len will:\n\nload actual data, (that is, load each file into a pandas DataFrame)\nthen apply the corresponding functions to each pandas DataFrame (also known as a partition)\ncombine the subtotals to give you the final grand total\n\n# load and count number of rows\nlen(ddf)"
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-9",
    "href": "bigdata_lec6.html#dataframe-9",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nLazy Evaluation\n\n\nYou can view the start and end of the data as you would in pandas:\nddf.head()\nddf.tail()\n\n# ValueError: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n# +----------------+---------+----------+\n# | Column         | Found   | Expected |\n# +----------------+---------+----------+\n# | CRSElapsedTime | float64 | int64    |\n# | TailNum        | object  | float64  |\n# +----------------+---------+----------+\n\n# The following columns also raised exceptions on conversion:\n\n# - TailNum\n#   ValueError(\"could not convert string to float: 'N54711'\")\n\n# Usually this is due to dask's dtype inference failing, and\n# *may* be fixed by specifying dtypes manually by adding:\n\n# dtype={'CRSElapsedTime': 'float64',\n#        'TailNum': 'object'}\n\n# to the call to `read_csv`/`read_table`."
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-10",
    "href": "bigdata_lec6.html#dataframe-10",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nHandling conflicting types\n\n\nUnlike pandas.read_csv which reads in the entire file before inferring datatypes, dask.dataframe.read_csv only reads in a sample from the beginning of the file (or first file if using a glob). These inferred datatypes are then enforced when reading all partitions.\nIn this case, the datatypes inferred in the sample are incorrect. The first n rows have no value for CRSElapsedTime (which pandas infers as a float), and later on turn out to be strings (object dtype). When this happens you have a few options:\n\nSpecify dtypes directly using the dtype keyword. This is the recommended solution, as it’s the least error prone (better to be explicit than implicit) and also the most performant.\nIncrease the size of the sample keyword (in bytes)\nUse assume_missing to make dask assume that columns inferred to be int (which don’t allow missing values) are actually floats (which do allow missing values). In our particular case this doesn’t apply."
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-11",
    "href": "bigdata_lec6.html#dataframe-11",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nHandling conflicting types\n\n\nIn our case we’ll use the first option and directly specify the dtypes of the offending columns.\nddf = dd.read_csv(\n    os.path.join(\"data\", \"nycflights\", \"*.csv\"),\n    parse_dates={\"Date\": [0, 1, 2]},\n    dtype={\"TailNum\": str, \"CRSElapsedTime\": float, \"Cancelled\": bool},\n)\nddf.tail()  # now works"
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-12",
    "href": "bigdata_lec6.html#dataframe-12",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nReading from remote storage\n\n\nIf you’re thinking about distributed computing, your data is probably stored remotely on services (like Amazon’s S3 or Google’s cloud storage) and is in a friendlier format (like Parquet). Dask can read data in various formats directly from these remote locations lazily and in parallel.\nHere’s how you can read the NYC taxi cab data from Amazon S3:\nddf = dd.read_parquet(\n    \"s3://nyc-tlc/trip data/yellow_tripdata_2012-*.parquet\",\n)\nYou can also leverage Parquet-specific optimizations like column selection and metadata handling, learn more in the Dask documentation on working with Parquet files."
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-13",
    "href": "bigdata_lec6.html#dataframe-13",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nComputations with dask.dataframe\n\n\nLet’s compute the maximum of the flight delay.\nWith just pandas, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums.\nimport pandas as pd\n\nfiles = os.listdir(os.path.join('data', 'nycflights'))\n\nmaxes = []\n\nfor file in files:\n    df = pd.read_csv(os.path.join('data', 'nycflights', file))\n    maxes.append(df.DepDelay.max())\n    \nfinal_max = max(maxes)"
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-14",
    "href": "bigdata_lec6.html#dataframe-14",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nComputations with dask.dataframe\n\n\ndask.dataframe lets us write pandas-like code, that operates on larger-than-memory datasets in parallel.\n%%time\nresult = ddf.DepDelay.max()\nresult.compute()\nThis creates the lazy computation for us and then runs it."
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-15",
    "href": "bigdata_lec6.html#dataframe-15",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\n\n\n\nComputations with dask.dataframe\n\n\nDask will delete intermediate results (like the full pandas DataFrame for each file) as soon as possible. This means you can handle datasets that are larger than memory but, repeated computations will have to load all of the data in each time. (Run the code above again, is it faster or slower than you would expect?)\nYou can view the underlying task graph using .visualize():\n# notice the parallelism\nresult.visualize()"
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-16",
    "href": "bigdata_lec6.html#dataframe-16",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nExercises\n\n\n\nHow many rows are in our dataset?\n\n# Your code here\n\n\n\n\n\n\nIn total, how many non-canceled flights were taken?\n\nHint: use boolean indexing.\n# Your code here"
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-17",
    "href": "bigdata_lec6.html#dataframe-17",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nExercises\n\n\n\nIn total, how many non-canceled flights were taken from each airport?\n\nHint: use groupby.\n# Your code here\n\n\n\n\n\n\nWhat was the average departure delay from each airport?\n\n# Your code here"
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-18",
    "href": "bigdata_lec6.html#dataframe-18",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nExercises\n\n\n\nWhat day of the week has the worst average departure delay?\n\n# Your code here\n\n\n\n\n\n\nLet’s say the distance column is erroneous and you need to add 1 to all values, how would you do this?\n\n# Your code here"
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-19",
    "href": "bigdata_lec6.html#dataframe-19",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nSharing Intermediate Results\n\n\nWhen computing all of the above, we sometimes did the same operation more than once. For most operations, dask.dataframe stores the arguments, allowing duplicate computations to be shared and only computed once.\nFor example, let’s compute the mean and standard deviation for departure delay of all non-canceled flights.\nIf you compute them with two calls to compute, there is no sharing of intermediate computations.\nnon_canceled = ddf[~ddf.Cancelled]\nmean_delay = non_canceled.DepDelay.mean()\nstd_delay = non_canceled.DepDelay.std()\n#| tags: []\n%%time\n\nmean_delay_res = mean_delay.compute()\nstd_delay_res = std_delay.compute()"
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-20",
    "href": "bigdata_lec6.html#dataframe-20",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\ndask.compute\n\n\nBut let’s try by passing both to a single compute call.\n#| tags: []\n%%time\n\nmean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)\nUsing dask.compute takes roughly 1/2 the time. This is because the task graphs for both results are merged when calling dask.compute, allowing shared operations to only be done once instead of twice. In particular, using dask.compute only does the following once:\n\nthe calls to read_csv\nthe filter (df[~df.Cancelled])\nsome of the necessary reductions (sum, count)"
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-21",
    "href": "bigdata_lec6.html#dataframe-21",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nVisualization\n\n\n\nTo see what the merged task graphs between multiple results look like (and what’s shared), you can use the dask.visualize function\nyou might want to use filename='graph.pdf' to save the graph to disk so that you can zoom in more easily\n\ndask.visualize(mean_delay, std_delay, engine=\"cytoscape\")"
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-22",
    "href": "bigdata_lec6.html#dataframe-22",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\n.persist()\n\n\nWhile using a distributed scheduler (you will learn more about schedulers in the upcoming notebooks), you can keep some data that you want to use often in the distributed memory.\npersist generates “Futures” (more on this later as well) and stores them in the same structure as your output. You can use persist with any data or computation that fits in memory.\nIf you want to analyze data only for non-canceled flights departing from JFK airport, you can either have two compute calls like in the previous section:\nnon_cancelled = ddf[~ddf.Cancelled]\nddf_jfk = non_cancelled[non_cancelled.Origin == \"JFK\"]\n%%time\nddf_jfk.DepDelay.mean().compute()\nddf_jfk.DepDelay.sum().compute()"
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-23",
    "href": "bigdata_lec6.html#dataframe-23",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\n.persist()\n\n\nOr, consider persisting that subset of data in memory.\nSee the “Graph” dashboard plot, the red squares indicate persisted data stored as Futures in memory. You will also notice an increase in Worker Memory (another dashboard plot) consumption.\nddf_jfk = ddf_jfk.persist()  # returns back control immediately\n%%time\nddf_jfk.DepDelay.mean().compute()\nddf_jfk.DepDelay.std().compute()\nAnalyses on this persisted data is faster because we are not repeating the loading and selecting (non-canceled, JFK departure) operations."
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-24",
    "href": "bigdata_lec6.html#dataframe-24",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nCustom code with Dask DataFrame\n\n\ndask.dataframe only covers a small but well-used portion of the pandas API.\nThis limitation is for two reasons:\n\nThe Pandas API is huge\nSome operations are genuinely hard to do in parallel, e.g, sorting.\n\nAdditionally, some important operations like set_index work, but are slower than in pandas because they include substantial shuffling of data, and may write out to disk."
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-25",
    "href": "bigdata_lec6.html#dataframe-25",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nSolutions\n\n\nIn case it’s a custom function or tricky to implement, dask.dataframe provides a few methods to make applying custom functions to Dask DataFrames easier:\n\nmap_partitions: to run a function on each partition (each pandas DataFrame) of the Dask DataFrame\nmap_overlap: to run a function on each partition (each pandas DataFrame) of the Dask DataFrame, with some rows shared between neighboring partitions\nreduction: for custom row-wise reduction operations."
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-26",
    "href": "bigdata_lec6.html#dataframe-26",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nmap_partitions\n\n\nLet’s take a quick look at the map_partitions() function:\n#| tags: []\nhelp(ddf.map_partitions)\nThe “Distance” column in ddf is currently in miles. Let’s say we want to convert the units to kilometers and we have a general helper function as shown below. In this case, we can use map_partitions to apply this function across each of the internal pandas DataFrames in parallel.\ndef my_custom_converter(df, multiplier=1):\n    return df * multiplier\n\n\nmeta = pd.Series(name=\"Distance\", dtype=\"float64\")\n\ndistance_km = ddf.Distance.map_partitions(\n    my_custom_converter, multiplier=0.6, meta=meta\n)\ndistance_km.visualize()\ndistance_km.head()"
  },
  {
    "objectID": "bigdata_lec6.html#dataframe-27",
    "href": "bigdata_lec6.html#dataframe-27",
    "title": "Big Data: Dask tutorial",
    "section": "DataFrame",
    "text": "DataFrame\n\n\n\nWhat is meta?\n\n\nSince Dask operates lazily, it doesn’t always have enough information to infer the output structure (which includes datatypes) of certain operations.\nmeta is a suggestion to Dask about the output of your computation. Importantly, meta never infers with the output structure. Dask uses this meta until it can determine the actual output structure.\nEven though there are many ways to define meta, we suggest using a small pandas Series or DataFrame that matches the structure of your final output."
  },
  {
    "objectID": "bigdata_lec6.html#close-you-local-dask-cluster",
    "href": "bigdata_lec6.html#close-you-local-dask-cluster",
    "title": "Big Data: Dask tutorial",
    "section": "Close you local Dask Cluster",
    "text": "Close you local Dask Cluster\n\n\n\n\n\n\nImportant\n\n\nIt’s good practice to always close any Dask cluster you create:\nclient.shutdown()"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-1",
    "href": "bigdata_lec6.html#dask-arrays-1",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nDask Arrays - parallelized numpy\n\n\nParallel, larger-than-memory, n-dimensional array using blocked algorithms.\n\nParallel: Uses all of the cores on your computer\nLarger-than-memory: Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\nBlocked Algorithms: Perform large computations by performing many smaller computations."
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-2",
    "href": "bigdata_lec6.html#dask-arrays-2",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nDask Arrays - parallelized numpy"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-3",
    "href": "bigdata_lec6.html#dask-arrays-3",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nRelated Documentation\n\n\n\nArray documentation\nArray screencast\nArray API\nArray examples"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-4",
    "href": "bigdata_lec6.html#dask-arrays-4",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nCreate datasets\n\n\nCreate the datasets you will be using in this notebook:\n%run prep.py -d random"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-5",
    "href": "bigdata_lec6.html#dask-arrays-5",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nStart the Client\n\n\nfrom dask.distributed import Client\n\nclient = Client(n_workers=4)\nclient"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-6",
    "href": "bigdata_lec6.html#dask-arrays-6",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nBlocked Algorithms in a nutshell\n\n\nLet’s do side by side the sum of the elements of an array using a NumPy array and a Dask array.\nimport numpy as np\nimport dask.array as da\n# NumPy array\na_np = np.ones(10)\na_np"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-7",
    "href": "bigdata_lec6.html#dask-arrays-7",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nBlocked Algorithms in a nutshell\n\n\nWe know that we can use sum() to compute the sum of the elements of our array, but to show what a blocksized operation would look like, let’s do:\na_np_sum = a_np[:5].sum() + a_np[5:].sum()\na_np_sum\nNow notice that each sum in the computation above is completely independent so they could be done in parallel. To do this with Dask array, we need to define our “slices”, we do this by defining the amount of elements we want per block using the variable chunks.\na_da = da.ones(10, chunks=5)\na_da"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-8",
    "href": "bigdata_lec6.html#dask-arrays-8",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\n\n\n\nImportant\n\n\nNote here that to get two blocks, we specify chunks=5, in other words, we have 5 elements per block.\na_da_sum = a_da.sum()\na_da_sum"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-9",
    "href": "bigdata_lec6.html#dask-arrays-9",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nTask Graphs\n\n\n# visualize the low level Dask graph using cytoscape\na_da_sum.visualize(engine=\"cytoscape\")\nAnd then compute:\na_da_sum.compute()"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-10",
    "href": "bigdata_lec6.html#dask-arrays-10",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nPerformance comparison\n\n\nLet’s try a more interesting example. We will create a 20_000 x 20_000 array with normally distributed values, and take the mean along one of its axis.\nNumpy version\n%%time\nxn = np.random.normal(10, 0.1, size=(30_000, 30_000))\nyn = xn.mean(axis=0)\nyn\nDask array version\nxd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\nxd\nxd.nbytes / 1e9  # Gigabytes of the input processed lazily\nyd = xd.mean(axis=0)\nyd"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-11",
    "href": "bigdata_lec6.html#dask-arrays-11",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nPerformance comparison\n\n\n%%time\nxd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\nyd = xd.mean(axis=0)\nyd.compute()"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-12",
    "href": "bigdata_lec6.html#dask-arrays-12",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nExercise\n\n\n\nWhat happens if the Dask chunks=(10000,10000)?\nWhat happens if the Dask chunks=(30,30)?\nFor Dask arrays, compute the mean along axis=1 of the sum of the x array and its transpose.\n\n# Your code here"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-13",
    "href": "bigdata_lec6.html#dask-arrays-13",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nChoosing good chunk sizes\n\n\nWe can think of Dask arrays as a big structure composed by chunks of a smaller size, where these chunks are typically an a single numpy array, and they are all arranged to form a larger Dask array.\nIf you have a Dask array and want to know more information about chunks and their size, you can use the chunksize and chunks attributes to access this information. If you are in a jupyter notebook you can also visualize the Dask array via its HTML representation.\ndarr = da.random.random((1000, 1000, 1000))\ndarr"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-14",
    "href": "bigdata_lec6.html#dask-arrays-14",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nChoosing good chunk sizes\n\n\nNotice that when we created the Dask array, we did not specify the chunks. Dask has set by default chunks='auto' which accommodates ideal chunk sizes. To learn more on how auto-chunking works you can go to this documentation here.\n\ndarr.chunksize shows the largest chunk size.\nBut if your array have irregular chunks, darr.chunks will show you the explicit sizes of all the chunks along all the dimensions of your dask array.\n\ndarr.chunksize\ndarr.chunks"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-15",
    "href": "bigdata_lec6.html#dask-arrays-15",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nChoosing good chunk sizes\n\n\nLet’s modify our example to see explore chunking a bit more. We can rechunk our array:\ndarr = darr.rechunk({0: -1, 1: 100, 2: \"auto\"})\ndarr\ndarr.chunksize\ndarr.chunks"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-16",
    "href": "bigdata_lec6.html#dask-arrays-16",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nExercise\n\n\n\nWhat does -1 do when specify as the chunk on a certain axis?"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-17",
    "href": "bigdata_lec6.html#dask-arrays-17",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nToo small is a problem\n\n\n\nIf your chunks are too small, the amount of actual work done by every task is very tiny, and the overhead of coordinating all these tasks results in a very inefficient process.\nIn general, the dask scheduler takes approximately one millisecond to coordinate a single task. That means we want the computation time to be comparatively large, i.e in the order of seconds."
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-18",
    "href": "bigdata_lec6.html#dask-arrays-18",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nToo big is a problem\n\n\n\nIf your chunks are too big, this is also a problem because you will likely run out of memory.\nYou will start seeing in the dashboard that data is being spill to disk and this will lead to performance decrements.\nIf we load to much data into memory, Dask workers will start to spill data to disk to avoid crashing.\nTo watch out for this you can look at the worker memory plot on the dashboard.\nOrange bars are a warning you are close to the limit, and gray means data is being spilled to disk."
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-19",
    "href": "bigdata_lec6.html#dask-arrays-19",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nRules of thumb\n\n\n\nUsers have reported that chunk sizes smaller than 1MB tend to be bad. In general, a chunk size between 100MB and 1GB is good, while going over 1 or 2GB means you have a really big dataset and/or a lot of memory available per worker.\nUpper bound: Avoid very large task graphs. More than 10,000 or 100,000 chunks may start to perform poorly.\nLower bound: To get the advantage of parallelization, you need the number of chunks to at least equal the number of worker cores available (or better, the number of worker cores times 2). Otherwise, some workers will stay idle.\nThe time taken to compute each task should be much larger than the time needed to schedule the task. The Dask scheduler takes roughly 1 millisecond to coordinate a single task, so a good task computation time would be in the order of seconds (not milliseconds).\nChunks should be aligned with array storage on disk. Modern NDArray storage formats (HDF5, NetCDF, TIFF, Zarr) allow arrays to be stored in chunks so that the blocks of data can be pulled efficiently. However, data stores often chunk more finely than is ideal for Dask array, so it is common to choose a chunking that is a multiple of your storage chunk size, otherwise you might incur high overhead. For example, if you are loading data that is chunked in blocks of (100, 100), the you might might choose a chunking strategy more like (1000, 2000) that is larger but still divisible by (100, 100).\n\nFor more more advice on chunking see https://docs.dask.org/en/stable/array-chunks.html"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-20",
    "href": "bigdata_lec6.html#dask-arrays-20",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nExample of chunked data with Zarr\n\n\nZarr is a format for the storage of chunked, compressed, N-dimensional arrays. Zarr provides classes and functions for working with N-dimensional arrays that behave like NumPy arrays (Dask array behave like Numpy arrays) but whose data is divided into chunks and each chunk is compressed. If you are already familiar with HDF5 then Zarr arrays provide similar functionality, but with some additional flexibility.\nFor extra material check the Zarr tutorial\nLet’s read an array from zarr:\nimport zarr\na = da.from_zarr(\"data/random.zarr\")\na"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-21",
    "href": "bigdata_lec6.html#dask-arrays-21",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nExample of chunked data with Zarr\n\n\nNotice that the array is already chunked, and we didn’t specify anything when loading it. Now notice that the chunks have a nice chunk size, let’s compute the mean and see how long it takes to run\n%%time\na.mean().compute()\nLet’s load a separate example where the chunksize is much smaller, and see what happen\nb = da.from_zarr(\"data/random_sc.zarr\")\nb\n%%time\nb.mean().compute()"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-22",
    "href": "bigdata_lec6.html#dask-arrays-22",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nExercise\n\n\nProvide a chunksize when reading b that will improve the time of computation of the mean. Try multiple chunks values and see what happens.\n# Your code here\n\n\n\n\n\n\n\n%%time\nc.mean().compute()"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-23",
    "href": "bigdata_lec6.html#dask-arrays-23",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nXarray\n\n\nIn some applications we have multidimensional data, and sometimes working with all this dimensions can be confusing. Xarray is an open source project and Python package that makes working with labeled multi-dimensional arrays easier.\nLet’s learn how to use xarray and Dask together:\nimport xarray as xr\nds = xr.tutorial.open_dataset(\n    \"air_temperature\",\n    chunks={  # this tells xarray to open the dataset as a dask array\n        \"lat\": 25,\n        \"lon\": 25,\n        \"time\": -1,\n    },\n)\nds\nds.air\nds.air.chunks\nmean = ds.air.mean(\"time\")  # no activity on dashboard\nmean  # contains a dask array\n# we will see dashboard activity\nmean.load()"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-24",
    "href": "bigdata_lec6.html#dask-arrays-24",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nStandard Xarray Operations\n\n\nLet’s grab the air variable and do some operations. Operations using xarray objects are identical, regardless if the underlying data is stored as a Dask array or a NumPy array.\ndair = ds.air\ndair2 = dair.groupby(\"time.month\").mean(\"time\")\ndair_new = dair - dair2\ndair_new\nCall .compute() or .load() when you want your result as a xarray.DataArray with data stored as NumPy arrays.\n# things happen in the dashboard\ndair_new.load()"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-25",
    "href": "bigdata_lec6.html#dask-arrays-25",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nTime Series Operations with xarray\n\n\nBecause we have a datetime index time-series operations work efficiently, for example we can do a resample and then plot the result.\ndair_resample = dair.resample(time=\"1w\").mean(\"time\").std(\"time\")\ndair_resample.load().plot(figsize=(12, 8))"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-26",
    "href": "bigdata_lec6.html#dask-arrays-26",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nLearn More\n\n\nBoth xarray and zarr have their own tutorials that go into greater depth:\n\nZarr tutorial\nXarray tutorial"
  },
  {
    "objectID": "bigdata_lec6.html#dask-arrays-27",
    "href": "bigdata_lec6.html#dask-arrays-27",
    "title": "Big Data: Dask tutorial",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\n\n\nClose your cluster\n\n\nIt’s good practice to close any Dask cluster you create:\nclient.shutdown()"
  },
  {
    "objectID": "bigdata_lec6.html#useful-links",
    "href": "bigdata_lec6.html#useful-links",
    "title": "Big Data: Dask tutorial",
    "section": "Useful Links",
    "text": "Useful Links\n\nReference\n\nDocs\nExamples\nCode\nBlog\n\nAsk for help\n\ndask tag on Stack Overflow, for usage questions\ngithub issues for bug reports and feature requests\ndiscourse forum for general, non-bug, questions and discussion\nAttend a live tutorial"
  },
  {
    "objectID": "bigdata_lab7.html",
    "href": "bigdata_lab7.html",
    "title": "Big Data Analytics: Lab 7",
    "section": "",
    "text": "Spatial relationships\nFor this lab, we will be fetching data from two resources:\n\nhttps://opendata.city-adm.lviv.ua/dataset/\nhttps://map.city-adm.lviv.ua/dataset/\n\n\n\nExercises\n\nDownload datasets:\n\n\ndistricts dataset: https://opendata.city-adm.lviv.ua/dataset/mikrorayony-lvova.\naddresses dataset: https://opendata.city-adm.lviv.ua/dataset/adreslviv\nmobility dataset: https://opendata.city-adm.lviv.ua/dataset/suburban-mobility\nPOI (point-of-interest) dataset:\n\nshelters: https://map.city-adm.lviv.ua/opendata_page/2765617480184367031\nhistorical monuments: https://map.city-adm.lviv.ua/opendata_page/12727154373001049\nplaygrounds: https://map.city-adm.lviv.ua/opendata_page/2342814049890207007\nbotanical POIs: https://map.city-adm.lviv.ua/opendata_page/2372652871767295987\n\n\n\nCompute:\n\n\nPOI counts per city district\ndistances from each city address to closest POI. Describe stats.\nvisualize 20 most common source locations for visitors (Mobility dataset)\naverage distance traveled (Mobility dataset)"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management",
    "href": "bigdata_lec3.html#memory-management",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\n\n\nManual\n\n\n\nC/C++\nPascal\nForth\nFortran\nZig\n\n\n\n\n\n\n\n\nAutomatic\n\n\n\nLisp\nJava\nPython\nGo\nJulia"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-1",
    "href": "bigdata_lec3.html#memory-management-1",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\nCode\n\n\nimport os\nimport gc\nimport psutil\n\nproc = psutil.Process(os.getpid())\ngc.collect()\ninitial_memory = proc.memory_info().rss\n\n## Allocate memory by creating large lists\nfoo = ['abc' for _ in range(10**7)]\nallocated_memory = proc.memory_info().rss\n\n## Deallocate memory\ndel foo\ngc.collect()\nfinal_memory = proc.memory_info().rss"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-2",
    "href": "bigdata_lec3.html#memory-management-2",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\nPrint memory statistics\n\n\nincrease = lambda x2, x1: 100.0 * (x2 - x1) / initial_memory\nprint(\"Allocated Memory Increase: %0.2f%%\" % increase(allocated_memory, initial_memory))\nprint(\"Memory After Deletion: %0.2f%%\" % increase(final_memory, allocated_memory))\n\n&gt;&gt;&gt; Allocated Memory Increase: 23.35%\n&gt;&gt;&gt; Memory After Deletion: -10.78%"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-3",
    "href": "bigdata_lec3.html#memory-management-3",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\n\n\n\n\nPython internals\n\n\n\npools\nblocks\narenas\n\n\n\n\n\n\n\nhttps://docs.python.org/3/c-api/memory.html\nhttps://realpython.com/python-memory-management"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-4",
    "href": "bigdata_lec3.html#memory-management-4",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management"
  },
  {
    "objectID": "bigdata_lec3.html#python-options",
    "href": "bigdata_lec3.html#python-options",
    "title": "Big Data: Speeding up computation",
    "section": "Python Options",
    "text": "Python Options\n\n\n\n\n\n\n\n\n\nLibraries\nLow-level langs\nAlt Python Impls\nJIT\n\n\n\n\nNumPy,  SciPy\nC, Rust, Cython, PyO3\nPyPy, Jython\nNumba, PyPy\n\n\n\n\n\nOptions above are not mutually exclusive!"
  },
  {
    "objectID": "bigdata_lec3.html#interpreters",
    "href": "bigdata_lec3.html#interpreters",
    "title": "Big Data: Speeding up computation",
    "section": "Interpreters",
    "text": "Interpreters\n\n\n\nWikipedia definition\n\n\nAn interpreter is a computer program that directly executes instructions written in a programming or scripting language, without requiring them previously to have been compiled into a machine language program.\n\n\n\n\n\n\nExamples\n\n\n\nPython\nRuby\nLua\nJavascript"
  },
  {
    "objectID": "bigdata_lec3.html#cpython",
    "href": "bigdata_lec3.html#cpython",
    "title": "Big Data: Speeding up computation",
    "section": "CPython",
    "text": "CPython\n\n\n\n\nFlow\n\n\n\nRead Python code\nConvert Python into bytecode\nExecute bytecode inside a VM\nVM converts bytecode to machine code"
  },
  {
    "objectID": "bigdata_lec3.html#cpython-1",
    "href": "bigdata_lec3.html#cpython-1",
    "title": "Big Data: Speeding up computation",
    "section": "CPython",
    "text": "CPython"
  },
  {
    "objectID": "bigdata_lec3.html#compilers",
    "href": "bigdata_lec3.html#compilers",
    "title": "Big Data: Speeding up computation",
    "section": "Compilers",
    "text": "Compilers\n\n\n\nWikipedia definition\n\n\nSource code is compiled - in this context, translated into machine code for better performance.\n\n\n\n\n\n\nExamples\n\n\n\nC/C++\nGo\nPython (to intermediate VM code)\nJava\nCython"
  },
  {
    "objectID": "bigdata_lec3.html#compilers-1",
    "href": "bigdata_lec3.html#compilers-1",
    "title": "Big Data: Speeding up computation",
    "section": "Compilers",
    "text": "Compilers"
  },
  {
    "objectID": "bigdata_lec3.html#cython",
    "href": "bigdata_lec3.html#cython",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nDefinition\n\n\nCython is an optimising static compiler for the Python programming language.\n\nconverts Python code to C\nsupports static type declarations"
  },
  {
    "objectID": "bigdata_lec3.html#cython-1",
    "href": "bigdata_lec3.html#cython-1",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython"
  },
  {
    "objectID": "bigdata_lec3.html#cython-2",
    "href": "bigdata_lec3.html#cython-2",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython"
  },
  {
    "objectID": "bigdata_lec3.html#cython-3",
    "href": "bigdata_lec3.html#cython-3",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nPython code"
  },
  {
    "objectID": "bigdata_lec3.html#cython-4",
    "href": "bigdata_lec3.html#cython-4",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nAnnotated Python code"
  },
  {
    "objectID": "bigdata_lec3.html#cython-5",
    "href": "bigdata_lec3.html#cython-5",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nCython code"
  },
  {
    "objectID": "bigdata_lec3.html#parallel-cython",
    "href": "bigdata_lec3.html#parallel-cython",
    "title": "Big Data: Speeding up computation",
    "section": "Parallel Cython",
    "text": "Parallel Cython"
  },
  {
    "objectID": "bigdata_lec3.html#parallel-cython-1",
    "href": "bigdata_lec3.html#parallel-cython-1",
    "title": "Big Data: Speeding up computation",
    "section": "Parallel Cython",
    "text": "Parallel Cython"
  },
  {
    "objectID": "bigdata_lec3.html#jit",
    "href": "bigdata_lec3.html#jit",
    "title": "Big Data: Speeding up computation",
    "section": "JIT",
    "text": "JIT\n\n\n\nWikipedia definition\n\n\nA compilation (of computer code) during execution of a program (at run time) rather than before execution.\n\n\n\n\n\n\nFeatures\n\n\n\nwarm-up time: JIT causes a slight to noticeable delay in the initial execution of an application, due to the time taken to load and compile the input code.\nstatistics collection: performed by the system during runtime, shows how the program is actually running in the environment it is in; helps JIT to rearrange and recompile for optimum performance.\nparticularly suited for dynamic programming languages"
  },
  {
    "objectID": "bigdata_lec3.html#jit-1",
    "href": "bigdata_lec3.html#jit-1",
    "title": "Big Data: Speeding up computation",
    "section": "JIT",
    "text": "JIT\n\n\n\nExamples\n\n\n\nHotSpot Java Virtual Machine\nLuaJIT\nNumba\nPyPy"
  },
  {
    "objectID": "bigdata_lec3.html#numba",
    "href": "bigdata_lec3.html#numba",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba"
  },
  {
    "objectID": "bigdata_lec3.html#numba-1",
    "href": "bigdata_lec3.html#numba-1",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nDescription\n\n\n\nNumba translates Python byte-code to machine code immediately before execution to improve the execution speed.\nFor that we add a @jit decorator\nWorks well for numeric operations, NumPy, and loops"
  },
  {
    "objectID": "bigdata_lec3.html#numba-2",
    "href": "bigdata_lec3.html#numba-2",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nSteps\n\n\n\nread the Python bytecode for a decorated function\ncombine it with information about the types of the input arguments to the function\nanalyze and optimize the code\nuse the LLVM compiler library to generate a machine code version of the function, tailored to specific CPU capabilities."
  },
  {
    "objectID": "bigdata_lec3.html#numba-3",
    "href": "bigdata_lec3.html#numba-3",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nWorks great"
  },
  {
    "objectID": "bigdata_lec3.html#numba-4",
    "href": "bigdata_lec3.html#numba-4",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nNope"
  },
  {
    "objectID": "bigdata_lec3.html#numba-5",
    "href": "bigdata_lec3.html#numba-5",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba"
  },
  {
    "objectID": "bigdata_lec3.html#numpy",
    "href": "bigdata_lec3.html#numpy",
    "title": "Big Data: Speeding up computation",
    "section": "Numpy",
    "text": "Numpy\n\n\n\nWhy so fast?\n\n\n\nOptimized C code\nDensely packed arrays\nUses BLAS - Basic Linear Algebra Subroutines."
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-1",
    "href": "bigdata_lec3.html#rustpyo3-example-1",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nDescription\n\n\nWe show an example of a simple algebraic cipher that utilizes PyO3 bindings to speed up encoding/decoding.\n\n\n\n\n\n\nCipher definition\n\n\nThe basic mechanism for encrypting a message using a shared secret key is called a cipher (or encryption scheme)"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-2",
    "href": "bigdata_lec3.html#rustpyo3-example-2",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\n\n\n\nDefinition\n\n\nEncryption and decryption use the same secret key.\n\n\n\n\n\n\nExamples\n\n\n\nAES\nSalsa20\nTwofish\nDES"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-3",
    "href": "bigdata_lec3.html#rustpyo3-example-3",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nTypes\n\n\n\nblock ciphers\nstream ciphers\nhash functions"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-4",
    "href": "bigdata_lec3.html#rustpyo3-example-4",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nOverview\n\n\n\\[\nf: \\mathcal{K}\\times\\mathcal{D} \\rightarrow C\n\\] where\n\n\\(\\mathcal{K}\\) is key space\n\\(\\mathcal{D}\\) is domain (or message space)\n\\(\\mathcal{C}\\) is co-domain (or ciphertext space)"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-5",
    "href": "bigdata_lec3.html#rustpyo3-example-5",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nShannon cipher\n\n\nA Shannon cipher is a pair \\(\\mathcal{E} = (E,D)\\) of functions:\n\nThe function \\(E\\) (the encryption function) takes as input a key \\(k\\) and message \\(m\\) (also called plaintext) and produces as output a ciphertext \\(c):\\)$ c = E(k,m) $$\n\n\\(c\\) is the encryption of \\(m\\) under \\(k\\).\n\nThe function \\(D\\) (the decryption function) takes as input a key \\(k\\) and ciphertext \\(c\\), and produces a message \\(m\\): \\[\nm = D(k,c)\n\\]\n\n\\(m\\) is the decryption of \\(c\\) under \\(k\\)."
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-6",
    "href": "bigdata_lec3.html#rustpyo3-example-6",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nCorrectness property\n\n\nFor all keys \\(k\\) and messages \\(m\\), we have \\[\nD(k, E(k,m)) = m\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-7",
    "href": "bigdata_lec3.html#rustpyo3-example-7",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nParameters\n\n\nNow we describe the cipher.\nFirst, we define cipher parameters:\n\nAn alphabet \\(A\\) with size \\(L \\equiv |A|\\).\nA matrix \\(M\\) with size \\(N \\gg L\\)\n\\(\\sigma_1, \\sigma_2\\) are some permutations \\(N \\rightarrow N\\)\n\\(\\phi\\) is some bit sequence of length \\(P\\): \\(\\phi \\in \\{0,1\\}^P\\)\n\nA triple \\((\\sigma_1, \\sigma_2, \\phi)\\) will be our secret key.\nWe define each symbol \\(z\\) by a corresponding set of diagonals \\(D\\) in the matrix \\(M\\), so that \\(\\forall (x,y) \\in D: x - y = z (\\mod L)\\) (see Figure 1)."
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-8",
    "href": "bigdata_lec3.html#rustpyo3-example-8",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-9",
    "href": "bigdata_lec3.html#rustpyo3-example-9",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nEncoder flow\n\n\nSuppose we receive some text \\(T\\) containing symbols to be encoded.\nFor each \\(t_i \\in T\\), obtain its numeric representation \\(z_i\\). \\[\nz_i \\in [0,L)\n\\] Then we map each \\(z_i\\) to a pair of matrix coordinates \\((x_i, y_i)\\) such that:\n\nFirst, we pick a random \\(x_i \\in [0,N)\\) (e.g., horizontal coordinate in a matrix)\nThen, we randomly pick some \\(y_i \\in [0,N)\\) such that: \\[\nx_i - y_i = z_i (\\mod L)\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-10",
    "href": "bigdata_lec3.html#rustpyo3-example-10",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nEncoder flow\n\n\nHaving thus obtained a sequence \\(\\{(x_i, y_i), \\, i \\in [0, |T|) \\}\\), we now apply permutations \\(\\sigma_k: [0,N) \\rightarrow [0,N), \\, k=1,2\\): \\[\n\\text{ciphertext } (\\xi,\\eta) := (\\sigma_j(x),\\sigma_{j+1}(y))\n\\] where \\[\n\\sigma_j = \\begin{cases}\n\\sigma_1, \\; \\text{if}\\; \\phi_j=0,\\\\\n\\sigma_2\\; \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-11",
    "href": "bigdata_lec3.html#rustpyo3-example-11",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nDecoder flow\n\n\nBelow are steps executing during decoding phase:\n\nReceive encoded ciphertext \\(\\{(\\xi_i, \\eta_i)\\}\\).\nApply inverse permutations \\(\\sigma_j^{-1}, \\sigma_{j+1}^{-1}\\): \\[\n(x_i,y_i) = (\\sigma_j^{-1}(\\xi_i), \\sigma_{j+1}^{-1}(\\eta_i))\n\\]\nFind \\(z_i\\): \\[\nz_i = x_i - y_i \\; (\\mod L)\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-12",
    "href": "bigdata_lec3.html#rustpyo3-example-12",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nImplementation: PyO3\n\n\n\nRust bindings for Python extension modules\n\n\n\n\n\n\n\nUsers\n\n\n\nQiskit https://www.ibm.com/quantum/qiskit\nPython Cryptography package https://github.com/pyca/cryptography\nScallop https://www.scallop-lang.org\nHuggingFace Tokenizers https://github.com/huggingface/tokenizers"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-13",
    "href": "bigdata_lec3.html#rustpyo3-example-13",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nRust wrapper\n\n\n/// A Python module implemented in Rust.\n#[pymodule]\nfn cipher(m: &Bound&lt;'_, PyModule&gt;) -&gt; PyResult&lt;()&gt; {\n    m.add_function(wrap_pyfunction!(encode, m)?)?;\n    m.add_function(wrap_pyfunction!(decode, m)?)?;\n    Ok(())\n}"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-14",
    "href": "bigdata_lec3.html#rustpyo3-example-14",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nEncoding\n\n\n// Generate random permutation of N integers\nlet mut numbers: Vec&lt;usize&gt; = (0..N).collect();\nlet mut rng = thread_rng();\nnumbers.shuffle(&mut rng);\nlet permutation = numbers;\n\n// Generate pairs for each z\nlet mut pairs = Vec::with_capacity(zs.len());\nfor &z in &zs {\n    // Generate random x between 0 and N-1\n    let x = rng.gen_range(0..N);\n\n    // Compute y such that x - y = z (mod L)\n    let y = if x &gt;= z {\n        (x - z) % L\n    } else {\n        ((x + L) - z) % L\n    };\n\n    // Apply permutation to x and y\n    let px = permutation[x];\n    let py = permutation[y];\n    pairs.push((px, py));"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-15",
    "href": "bigdata_lec3.html#rustpyo3-example-15",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nDecoding\n\n\n// Create inverse permutation\nlet mut inverse = vec![0; N];\nfor (i, &p) in permutation.iter().enumerate() {\n    inverse[p] = i;\n}\n\n// Recover z for each pair\nlet mut zs = Vec::with_capacity(pairs.len());\nfor &(px, py) in pairs {\n    // Apply inverse permutation to get x and y\n    let x = inverse[px];\n    let y = inverse[py];\n\n    // Compute z = x - y (mod L)\n    let z = if x &gt;= y {\n        (x - y) % L\n    } else {\n        ((x + L) - y) % L\n    };\n    zs.push(z);\n}"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-16",
    "href": "bigdata_lec3.html#rustpyo3-example-16",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nPython\n\n\nmkdir cipher; cd cipher\npython -m venv venv\n. venv/bin/activate\n!pip install maturin\nmaturin init\nmaturin develop\n\n\n\n\n\n\nUsage\n\n\nimport cipher\nencoded = cipher.encode(\"Whazzuuupppp??!!\")\ndecoded = cipher.decode(encoded)"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-1",
    "href": "bigdata_lec3.html#distributed-computing-1",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nTypes\n\n\n\nCluster computing: collection of similar workstations\nGrid computing: federation of different computer systems\nCloud computing: provide the facilities to dynamically construct an infrastructure and compose what is needed from available services. Not only providing lots of resources."
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-2",
    "href": "bigdata_lec3.html#distributed-computing-2",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nOriginal Beowulf cluster at NASA (1994)"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-3",
    "href": "bigdata_lec3.html#distributed-computing-3",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nBeowulf cluster diagram"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-4",
    "href": "bigdata_lec3.html#distributed-computing-4",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\n\n\nGrid architecture diagram (Foster et al. 2001)\n\n\n\n\n\n\n\n\n\n\nNotation\n\n\n\n\nfabric: interfaces to local resources at a specific site\nconnectivity: communication protocols for supporting grid transactions that span the usage of multiple resources\nresource: responsible for managing a single resource\ncollective: handling access to multiple resources and typically consists of services for resource discovery, allocation and scheduling of tasks onto multiple resources, data replication, and so on\napplication: applications that operate within a virtual organization"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-5",
    "href": "bigdata_lec3.html#distributed-computing-5",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nCloud architecture"
  },
  {
    "objectID": "bigdata_lab2.html",
    "href": "bigdata_lab2.html",
    "title": "Big Data Analytics: Lab 2",
    "section": "",
    "text": "First, install Cython and Numba:\nuv add cython\nuv add numba\nThen we can load Cython extension in Jupyter:\n\n%load_ext Cython\n\nDefine a function that generates a list of first \\(n\\) primes:\n\n\n\n\n\n\nImportantPure Python\n\n\n\n\ndef primes1(nb_primes):\n    p = [0] * 1000\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nImportantPure Python results\n\n\n\nWe can measure its execution time:\n\n%timeit primes1(100)\n\n222 μs ± 4.37 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n\nNow, define a function using Cython type annotations. Note that we use %%cython magic before function definition:\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\n\n%%cython\ndef primes2(nb_primes: cython.int):\n    i: cython.int\n    p: cython.int[1000] = [0] * 1000\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p: cython.int = 0  # The current number of elements in p.\n    n: cython.int = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nNotePython with Cython type hints: results\n\n\n\nWe measure its execution time:\n\n%timeit primes2(100)\n\n85 μs ± 2.04 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nDifferent syntax:\n\n\n\n\n\n\nTipCython syntax\n\n\n\n\n%%cython\ndef primes3(int nb_primes):\n    cdef int n, i, len_p\n    cdef int[1000] p\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nTipCython syntax results\n\n\n\nWe measure its execution time:\n\n%timeit primes3(100)\n\n4.54 μs ± 195 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPassing --annotate flag will produce more relevant info."
  },
  {
    "objectID": "bigdata_lab2.html#cython",
    "href": "bigdata_lab2.html#cython",
    "title": "Big Data Analytics: Lab 2",
    "section": "",
    "text": "First, install Cython and Numba:\nuv add cython\nuv add numba\nThen we can load Cython extension in Jupyter:\n\n%load_ext Cython\n\nDefine a function that generates a list of first \\(n\\) primes:\n\n\n\n\n\n\nImportantPure Python\n\n\n\n\ndef primes1(nb_primes):\n    p = [0] * 1000\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nImportantPure Python results\n\n\n\nWe can measure its execution time:\n\n%timeit primes1(100)\n\n222 μs ± 4.37 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n\nNow, define a function using Cython type annotations. Note that we use %%cython magic before function definition:\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\n\n%%cython\ndef primes2(nb_primes: cython.int):\n    i: cython.int\n    p: cython.int[1000] = [0] * 1000\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p: cython.int = 0  # The current number of elements in p.\n    n: cython.int = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nNotePython with Cython type hints: results\n\n\n\nWe measure its execution time:\n\n%timeit primes2(100)\n\n85 μs ± 2.04 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nDifferent syntax:\n\n\n\n\n\n\nTipCython syntax\n\n\n\n\n%%cython\ndef primes3(int nb_primes):\n    cdef int n, i, len_p\n    cdef int[1000] p\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nTipCython syntax results\n\n\n\nWe measure its execution time:\n\n%timeit primes3(100)\n\n4.54 μs ± 195 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPassing --annotate flag will produce more relevant info."
  },
  {
    "objectID": "bigdata_lab2.html#using-cython-in-pandas",
    "href": "bigdata_lab2.html#using-cython-in-pandas",
    "title": "Big Data Analytics: Lab 2",
    "section": "Using Cython in Pandas",
    "text": "Using Cython in Pandas\nDefine a random DataFrame:\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {\n        \"a\": np.random.randn(1000),\n        \"b\": np.random.randn(1000),\n        \"N\": np.random.randint(100, 1000, (1000)),\n        \"x\": \"x\",\n    }\n)\n\nDefine functions that will be applied to the DataFrame:\n\n\n\n\n\n\nImportantPure Python\n\n\n\n\ndef f(x):\n    return x * (x - 1)\n\n\ndef integrate_f(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f(a + i * dx)\n    return s * dx\n\n\n\n\n\n\n\n\n\nImportantPure Python results\n\n\n\nApply functions to DataFrame:\n\n%timeit df.apply(lambda x: integrate_f(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n\n39 ms ± 539 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nNow let’s use Cython-annotated functions:\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\n\n%%cython\ndef f_typed2(x: cython.double) -&gt; cython.double:\n    return x * (x - 1)\n\ndef integrate_f_typed2(a: cython.double, b: cython.double, N: cython.int) -&gt; cython.double:\n    i: cython.int\n    s: cython.double\n    dx: cython.double\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f_typed2(a + i * dx)\n    return s * dx\n\n\n\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\nApply annotated functions to DataFrame:\n\n%timeit df.apply(lambda x: integrate_f_typed2(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n\n29.9 ms ± 223 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nWith a different syntax:\n\n\n\n\n\n\nTipCython syntax\n\n\n\n\n%%cython\ncdef double f_typed(double x) except? -2:\n   return x * (x - 1)\ncpdef double integrate_f_typed(double a, double b, int N):\n   cdef int i\n   cdef double s, dx\n   s = 0\n   dx = (b - a) / N\n   for i in range(N):\n       s += f_typed(a + i * dx)\n   return s * dx\n\n\n\n\n\n\n\n\n\nTipCython syntax results\n\n\n\n\n%timeit df.apply(lambda x: integrate_f_typed(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n\n3.74 ms ± 101 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n\n\n\n\n\nNotePandas\n\n\n\nRead more about type annotations for Pandas: https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html."
  },
  {
    "objectID": "bigdata_lab2.html#numba",
    "href": "bigdata_lab2.html#numba",
    "title": "Big Data Analytics: Lab 2",
    "section": "Numba",
    "text": "Numba\nEither add engine=\"numba\" parameter to Pandas functions, or add a @jit annotation to Python funcs.\n\n\n\n\n\n\nWarningNumba-annotated code\n\n\n\n\nimport numba\n\n\n@numba.jit\ndef f_numba(x):\n    return x * (x - 1)\n\n\n@numba.jit\ndef integrate_f_numba(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f_numba(a + i * dx)\n    return s * dx\n\n\n@numba.jit\ndef apply_integrate_f_numba(col_a, col_b, col_N):\n    n = len(col_N)\n    result = np.empty(n, dtype=\"float64\")\n    assert len(col_a) == len(col_b) == n\n    for i in range(n):\n        result[i] = integrate_f_numba(col_a[i], col_b[i], col_N[i])\n    return result\n\n\ndef compute_numba(df):\n    result = apply_integrate_f_numba(\n        df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy()\n    )\n    return pd.Series(result, index=df.index, name=\"result\")\n\n\n\n\n\n\n\n\n\nWarningNumba Results\n\n\n\n\n%timeit compute_numba(df)\n\n390 μs ± 52 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "bigdata_lab2.html#exercises",
    "href": "bigdata_lab2.html#exercises",
    "title": "Big Data Analytics: Lab 2",
    "section": "Exercises",
    "text": "Exercises\n\nApply Cython/Numba optimizations to some computations on your dataframe from Lab 1.\nGo through the tutorial on Black-Scholes option pricing (https://louis-finegan.github.io/2024/10/10/Black-Scholes.html). Modify the code so that:\n\n\nit works for some different company\nhas 2 versions: using Cython and Numba\nmeasure results"
  },
  {
    "objectID": "bigdata_lab2.html#references",
    "href": "bigdata_lab2.html#references",
    "title": "Big Data Analytics: Lab 2",
    "section": "References",
    "text": "References\n\nCython tutorial\nCython build instructions\nCython language\nPandas optimization"
  },
  {
    "objectID": "bigdata_lec1.html#what-is-big-data",
    "href": "bigdata_lec1.html#what-is-big-data",
    "title": "Big Data: Intro",
    "section": "What is Big Data?",
    "text": "What is Big Data?\n\n\n\nDefinition\n\n\nBig Data is a set of technologies designed to store, manage and analyze data that is:\n\ntoo large to fit on a single machine\nwhile accommodating for the issue of growing discrepancy between capacity, throughput and latency."
  },
  {
    "objectID": "bigdata_lec1.html#prefixes",
    "href": "bigdata_lec1.html#prefixes",
    "title": "Big Data: Intro",
    "section": "Prefixes",
    "text": "Prefixes\n\n\n\nPrefixes\n\n\n\nkilo (k) 1,000 (3 zeros)\nMega (M) 1,000,000 (6 zeros)\nGiga (G) 1,000,000,000 (9 zeros)\nTera (T) 1,000,000,000,000 (12 zeros)\nPeta (P) 1,000,000,000,000,000 (15 zeros)\nExa (E) 1,000,000,000,000,000,000 (18 zeros)\nZetta (Z) 1,000,000,000,000,000,000,000 (21 zeros)\nYotta (Y) 1,000,000,000,000,000,000,000,000 (24 zeros)\nRonna (R) 1,000,000,000,000,000,000,000,000,000 (27 zeros)\nQuetta (Q) 1,000,000,000,000,000,000,000,000,000,000 (30 zeros)"
  },
  {
    "objectID": "bigdata_lec1.html#total-estimate",
    "href": "bigdata_lec1.html#total-estimate",
    "title": "Big Data: Intro",
    "section": "Total estimate",
    "text": "Total estimate\n\n\n\nEstimate\n\n\nThe total amount of data stored digitally worldwide is estimated to be getting close to 100 ZB as of 2021 (zettabytes)"
  },
  {
    "objectID": "bigdata_lec1.html#three-vs",
    "href": "bigdata_lec1.html#three-vs",
    "title": "Big Data: Intro",
    "section": "Three Vs",
    "text": "Three Vs\n\n\n\nVolume\nVariety\nVelocity"
  },
  {
    "objectID": "bigdata_lec1.html#volume",
    "href": "bigdata_lec1.html#volume",
    "title": "Big Data: Intro",
    "section": "Volume",
    "text": "Volume\n\n\n\nIssue\n\n\nData volume has exponentially increased in recent decades.\n\n\n\n\n\n\nWait but why?\n\n\n\nInternet-of-Things sensor data\nSocial networks\nStorage device progress"
  },
  {
    "objectID": "bigdata_lec1.html#variety",
    "href": "bigdata_lec1.html#variety",
    "title": "Big Data: Intro",
    "section": "Variety",
    "text": "Variety\n\n\n\nTypes\n\n\n\ntrees - XML, JSON, Parquet, Avro, etc\nunstructured - text, pictures, audio, video\ndata cubes\ngraphs"
  },
  {
    "objectID": "bigdata_lec1.html#velocity",
    "href": "bigdata_lec1.html#velocity",
    "title": "Big Data: Intro",
    "section": "Velocity",
    "text": "Velocity\n\n\n\nDefinion\n\n\nSpeed at which data is being generated, collected, and processed.\n\n\n\n\n\n\nAttributes\n\n\n\nCapacity: how much data can we store per unit of volume?\nThroughput: how many bytes can we read per unit of time?\nLatency: how much time do we need to wait until the bytes start arriving?"
  },
  {
    "objectID": "bigdata_lec1.html#velocity-1",
    "href": "bigdata_lec1.html#velocity-1",
    "title": "Big Data: Intro",
    "section": "Velocity",
    "text": "Velocity\nEvolution since 1950s"
  },
  {
    "objectID": "bigdata_lec1.html#features",
    "href": "bigdata_lec1.html#features",
    "title": "Big Data: Intro",
    "section": "Features",
    "text": "Features\n\n\n\nFeatures\n\n\n\nReliability\nScalability\nMaintainability"
  },
  {
    "objectID": "bigdata_lec1.html#reliability",
    "href": "bigdata_lec1.html#reliability",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nKleppmann’s definition\n\n\nThe system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity\n\nhardware faults\nsoftware faults\nand even human error"
  },
  {
    "objectID": "bigdata_lec1.html#reliability-1",
    "href": "bigdata_lec1.html#reliability-1",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nFaults\n\n\nBasically, theses are things that could go wrong.\nSystems that can anticipate faults are called fault-tolerant or resilient.\nFault can be defined as one component of the system deviating from the spec.\n\n\n\n\n\n\nFailures\n\n\nFailures occur when system stops providing services to the user.\n\n\n\nFaults might degenerate into failures."
  },
  {
    "objectID": "bigdata_lec1.html#reliability-2",
    "href": "bigdata_lec1.html#reliability-2",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nTypes of errors\n\n\n\nHardware\nSoftware\nHuman"
  },
  {
    "objectID": "bigdata_lec1.html#reliability-3",
    "href": "bigdata_lec1.html#reliability-3",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability"
  },
  {
    "objectID": "bigdata_lec1.html#scalability",
    "href": "bigdata_lec1.html#scalability",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nKleppmann\n\n\nAs the system grows (in data volume, traffic volume, or complexity), there should be reasonable ways of dealing with that growth.\nIn other words, scalability is a system’s ability to cope with increased load.\n\n\n\n\n\nNote that scalability is a multi-dimensional term. When saying “system scales well”, it’s important to state exactly along which axis."
  },
  {
    "objectID": "bigdata_lec1.html#scalability-1",
    "href": "bigdata_lec1.html#scalability-1",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nWhat is load?\n\n\nLoad is described by load parameters. These might include:\n\ndata set size\ndata write speed\ndata read speed\ncomputational complexity\netc…"
  },
  {
    "objectID": "bigdata_lec1.html#scalability-2",
    "href": "bigdata_lec1.html#scalability-2",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nPerformance\n\n\nIncreasing load affects performance. There are several meanings to this term:\n\nthroughput – time required to process a dataset of certain size\nresponse time – time between sending a request and receiving a response\nlatency – duration of waiting for a request to be processed. Included in response time.\n\n\n\n\n\n\nPerformance might be more strictly defined by service level objectives (SLOs) and service level agreements (SLAs)."
  },
  {
    "objectID": "bigdata_lec1.html#scalability-3",
    "href": "bigdata_lec1.html#scalability-3",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nHow to deal with load\n\n\n\nvertical scaling - scaling up\nhorizontal scaling - scaling out\narchitectural changes\n\n\n\n\n\n\n\nElasticity\n\n\nAn approach to load handling whereby a system automatically adds resources in case of load increase, and can decrease resources if load decreases."
  },
  {
    "objectID": "bigdata_lec1.html#scalability-4",
    "href": "bigdata_lec1.html#scalability-4",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nCommon wisdom\n\n\n\nKeep your database on a single node (scale up) until scaling cost or high-availability requirements forces you to make it distributed.\nOptimize code so that it can run on a single node."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability",
    "href": "bigdata_lec1.html#maintainability",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nKleppmann\n\n\nOver time, many different people will work on the system\n\nengineering\noperations\nboth maintaining current behavior and adapting the system to new use cases),\n\nand they should all be able to work on it productively."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-1",
    "href": "bigdata_lec1.html#maintainability-1",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nPrinciples\n\n\n\nOperability – make it easy for operations teams to keep the system running smoothly.\nSimplicity – make it easy for new engineers to understand the system, by removing as much complexity as possible from the system.\nEvolvability – Make it easy for engineers to make changes to the system in the future, adapting it for unanticipated use cases as requirements change. Also known as extensibility, modifiability, or plasticity."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-2",
    "href": "bigdata_lec1.html#maintainability-2",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nOperability\n\n\n\nHealth monitoring\nGood deployment practices\nConfiguration management\nVisibility into the internals of the system\nKnowledge preservation – documentation (!).\netc…"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-3",
    "href": "bigdata_lec1.html#maintainability-3",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nComplexity symptoms\n\n\n\nLots of hidden state\nLoose cohesion, tight coupling\nBad naming (!)\nUnnecessary hacks\netc…"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity",
    "href": "bigdata_lec1.html#maintainability-complexity",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity-1",
    "href": "bigdata_lec1.html#maintainability-complexity-1",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity\n\n\n\nTypes\n\n\n\nincidental\naccidental"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity-2",
    "href": "bigdata_lec1.html#maintainability-complexity-2",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity\n\n\n\nIncidental\n\n\n\nEasy things can be complex. There can be complex constructs that are succinctly described, familiar, available and easy to use. That is incidental complexity.\n\nRich Hickey talk “Simple made easy”: https://www.youtube.com/watch?v=SxdOUGdseq4"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity-3",
    "href": "bigdata_lec1.html#maintainability-complexity-3",
    "title": "Big Data: Intro",
    "section": "Maintainability: complexity",
    "text": "Maintainability: complexity\nHowever: Complexity is often caused by\n\n\n\nAccidental complexity\n\n\nMoseley and Marks define complexity as accidental if it is not inherent in the problem that the software solves (as seen by the users) but arises only from the implementation.\n\n\n\n\n\n\nHow to remove?\n\n\nBy providing proper abstractions."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-abstractions",
    "href": "bigdata_lec1.html#maintainability-abstractions",
    "title": "Big Data: Intro",
    "section": "Maintainability: abstractions",
    "text": "Maintainability: abstractions\n\n\n\n\n\n\nDefinition (Ousterhout)\n\n\nAn abstraction is a simplified view of an entity, which omits unimportant details.\nIn modular programming, each module provides an abstraction in the form of its interface."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-abstractions-1",
    "href": "bigdata_lec1.html#maintainability-abstractions-1",
    "title": "Big Data: Intro",
    "section": "Maintainability: abstractions",
    "text": "Maintainability: abstractions\n\n\n\n\n\n\nWhat can abstractions do?\n\n\n\nHide implementation details\nProvide reusable building blocks"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-4",
    "href": "bigdata_lec1.html#maintainability-4",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nEvolvability\n\n\nOne needs to adapt their big data system to possible future requirements changes.\nHowever, keep in mind the following:\n\nInability to foresee exact nature of changes\nNeed to strike the balance of flexibility and fitness for a particular task"
  },
  {
    "objectID": "bigdata_lec1.html#types-of-big-data-analytics",
    "href": "bigdata_lec1.html#types-of-big-data-analytics",
    "title": "Big Data: Intro",
    "section": "Types of big data analytics",
    "text": "Types of big data analytics\n\n\n\nTypes\n\n\n\nPrescriptive\nDiagnostic\nDescriptive\nPredictive"
  },
  {
    "objectID": "bigdata_lec1.html#types-prescriptive",
    "href": "bigdata_lec1.html#types-prescriptive",
    "title": "Big Data: Intro",
    "section": "Types: Prescriptive",
    "text": "Types: Prescriptive\n\n\n\nPrescriptive\n\n\n\nForward looking\nOptimal decisions for future situations"
  },
  {
    "objectID": "bigdata_lec1.html#types-diagnostic",
    "href": "bigdata_lec1.html#types-diagnostic",
    "title": "Big Data: Intro",
    "section": "Types: Diagnostic",
    "text": "Types: Diagnostic\n\n\n\nDiagnostic\n\n\n\nBackward looking\nFocused on causal relationships"
  },
  {
    "objectID": "bigdata_lec1.html#types-descriptive",
    "href": "bigdata_lec1.html#types-descriptive",
    "title": "Big Data: Intro",
    "section": "Types: Descriptive",
    "text": "Types: Descriptive\n\n\n\nDescriptive\n\n\n\nBackward looking\nFocused on descriptions and comparisons"
  },
  {
    "objectID": "bigdata_lec1.html#types-descriptive-1",
    "href": "bigdata_lec1.html#types-descriptive-1",
    "title": "Big Data: Intro",
    "section": "Types: Descriptive",
    "text": "Types: Descriptive\n\n\n\nPredictive\n\n\n\nForward looking\nFocused on the prediction of future states, relationship, and patterns"
  },
  {
    "objectID": "bigdata_lec1.html#challenges",
    "href": "bigdata_lec1.html#challenges",
    "title": "Big Data: Intro",
    "section": "Challenges",
    "text": "Challenges\nThere are 2 main challenges associated with Big Data.\n\n\n\nChallenges\n\n\n\nhow do we store and manage such a huge volume of data efficiently?\nhow do we process and extract valuable information from the data within the given time frame?"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-1",
    "href": "bigdata_lec12.html#query-languages-1",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages",
    "text": "Query languages\n\n\n\nSQL History\n\n\n\nDeveloped in 1974. Originally named SEQUEL, for Structured English QUEry Language. Changed to SQL due to trademark issues.\nIn 1977 System R sells SQL to its first customer: Pratt & Whitney (aerospace)\nSoftware Development Laboratories (1977) -&gt; Relational Software (1979) -&gt; Oracle (1982)\n1986: POSTGRES project at the University of California at Berkeley"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-2",
    "href": "bigdata_lec12.html#query-languages-2",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages",
    "text": "Query languages\n\n\n\nDBMS ranking, June 2024"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-sql",
    "href": "bigdata_lec12.html#query-languages-sql",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: SQL",
    "text": "Query languages: SQL\n\n\n\nSQL Features\n\n\n\nDeclarative (IMS and CODASYL where imperative)\nSet-based\nFunctional\n\n\n\n\n\n\n\nPros of declarative approach\n\n\n\nmore concise. Describe the pattern of desired data, not the sequence of steps required to get it\nleaves room for automatic optimizations (e.g. query optimizer)\nlends itself better for parallel execution"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-sql-1",
    "href": "bigdata_lec12.html#query-languages-sql-1",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: SQL",
    "text": "Query languages: SQL\n\n\n\nExample\n\n\nSELECT c.century AS cent,\n       COUNT(c.name) AS num_captains,\n       SUM(s.id) AS ships\nFROM captains c, ships s\nWHERE c.id = s.captain\nGROUP BY century\nHAVING COUNT(c.name) &gt;= 3\nORDER BY century DESC\nLIMIT 3 OFFSET 2"
  },
  {
    "objectID": "bigdata_lec12.html#sql",
    "href": "bigdata_lec12.html#sql",
    "title": "Databases: History, Models and Queries",
    "section": "SQL",
    "text": "SQL\n\n\n\nClauses\n\n\n\nThe FROM clause selects from which tables to read the data, in this case two tables, captains and ships. Implicitly the Cartesian product is computed.\nThe WHERE clause performs a selection: it only keeps the records for which the captain (from the captains table) is the captain of the ship (from the ships table). If you pay attention, you will recognize that this filter together with the Cartesian product is actually a theta join. Any reasonable SQL implementation will be smart enough to detect this and evaluate this query efficiently (joins can be computed in linear time rather than quadratic!).\nThe SELECT clause is also where projections are made: it lists the columns to include in the results. Renames are also made in this clause with AS."
  },
  {
    "objectID": "bigdata_lec12.html#sql-1",
    "href": "bigdata_lec12.html#sql-1",
    "title": "Databases: History, Models and Queries",
    "section": "SQL",
    "text": "SQL\n\n\n\nClauses\n\n\n\nThe GROUP BY clause performs an aggregation, with century as a grouping key. Aggregations on the captain name (COUNT) and the ship id (SUM) are done in the SELECT clause.\nThe HAVING clause is in fact like the WHERE clause, but performs a selection after, rather than before, the grouping.\nThe ORDER BY clause reorders the output rows according to the specified keys.\nThe LIMIT and OFFSET clauses allow pagination of the output: OFFSET specifies how many records to skip, and LIMIT specifies how many records to output after the skipped ones.\nAll clauses are optional except for SELECT and FROM."
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-sql-2",
    "href": "bigdata_lec12.html#query-languages-sql-2",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: SQL",
    "text": "Query languages: SQL"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-sql-3",
    "href": "bigdata_lec12.html#query-languages-sql-3",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: SQL",
    "text": "Query languages: SQL\n\n\n\n\nPostgres example"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-sql-4",
    "href": "bigdata_lec12.html#query-languages-sql-4",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: SQL",
    "text": "Query languages: SQL\n\n\n\n\nPostgres example\n\n\nSELECT id, data FROM tbl_a WHERE id &lt; 300 ORDER BY data;"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-sql-5",
    "href": "bigdata_lec12.html#query-languages-sql-5",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: SQL",
    "text": "Query languages: SQL\n\n\n\n\nPostgres example"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-sql-6",
    "href": "bigdata_lec12.html#query-languages-sql-6",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: SQL",
    "text": "Query languages: SQL\n\n\n\nPostgres example"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-sql-7",
    "href": "bigdata_lec12.html#query-languages-sql-7",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: SQL",
    "text": "Query languages: SQL\n\n\n\nMap-reduce example\n\n\nSELECT date_trunc('month', observation_timestamp) AS observation_month,\nsum(num_animals) AS total_animals\nFROM observations\nWHERE family = 'Sharks'\nGROUP BY observation_month;"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-mongodb-api",
    "href": "bigdata_lec12.html#query-languages-mongodb-api",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: MongoDB API",
    "text": "Query languages: MongoDB API\n\n\n\nMap-reduce example\n\n\ndb.observations.mapReduce(\n  function map() {\n    var year = this.observationTimestamp.getFullYear();\n    var month = this.observationTimestamp.getMonth() + 1;\n    emit(year + \"-\" + month, this.numAnimals);\n  },\n  function reduce(key, values) {\n    return Array.sum(values);\n  },\n  {\n    query: { family: \"Sharks\" },\n    out: \"monthlySharkReport\"\n  }\n);"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-mongodb-api-1",
    "href": "bigdata_lec12.html#query-languages-mongodb-api-1",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: MongoDB API",
    "text": "Query languages: MongoDB API\n\n\n\nMap-reduce example: aggregation pipeline\n\n\ndb.observations.aggregate([\n  { $match: { family: \"Sharks\" } },\n  { $group: {\n    _id: {\n    year: { $year: \"$observationTimestamp\" },\n    month: { $month: \"$observationTimestamp\" }\n    },\n    totalAnimals: { $sum: \"$numAnimals\" }\n  } }\n]);"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-cypher",
    "href": "bigdata_lec12.html#query-languages-cypher",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: Cypher",
    "text": "Query languages: Cypher\n\n\n\nCypher query: create\n\n\nCREATE\n  (NAmerica:Location {name:'North America', type:'continent'}),\n  (USA:Location {name:'United States', type:'country' }),\n  (Idaho:Location {name:'Idaho', type:'state' }),\n  (Lucy:Person {name:'Lucy' }),\n  (Idaho) -[:WITHIN]-&gt; (USA) -[:WITHIN]-&gt; (NAmerica),\n  (Lucy) -[:BORN_IN]-&gt; (Idaho)"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-cypher-1",
    "href": "bigdata_lec12.html#query-languages-cypher-1",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: Cypher",
    "text": "Query languages: Cypher\n\n\n\nCypher query: find\n\n\nMATCH\n  (person) -[:BORN_IN]-&gt; () -[:WITHIN*0..]-&gt; (us:Location {name:'United States'}),\n  (person) -[:LIVES_IN]-&gt; () -[:WITHIN*0..]-&gt; (eu:Location {name:'Europe'})\nRETURN person.name"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-cypher-2",
    "href": "bigdata_lec12.html#query-languages-cypher-2",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: Cypher",
    "text": "Query languages: Cypher\n\n\n\nCypher vs SQL: find\n\n\nWITH RECURSIVE\n  -- in_usa is the set of vertex IDs of all locations within the United States\n  in_usa(vertex_id) AS (\n    SELECT vertex_id FROM vertices WHERE properties-&gt;&gt;'name' = 'United States'\n    UNION\n    SELECT edges.tail_vertex FROM edges\n    JOIN in_usa ON edges.head_vertex = in_usa.vertex_id\n    WHERE edges.label = 'within'\n  ),\n  -- in_europe is the set of vertex IDs of all locations within Europe\n  in_europe(vertex_id) AS (\n    SELECT vertex_id FROM vertices WHERE properties-&gt;&gt;'name' = 'Europe'\n    UNION\n    SELECT edges.tail_vertex FROM edges\n    JOIN in_europe ON edges.head_vertex = in_europe.vertex_id\n    WHERE edges.label = 'within'\n  ),\n  -- born_in_usa is the set of vertex IDs of all people born in the US\n  born_in_usa(vertex_id) AS (\n    SELECT edges.tail_vertex FROM edges\n    JOIN in_usa ON edges.head_vertex = in_usa.vertex_id\n    WHERE edges.label = 'born_in'\n  ),\n  -- lives_in_europe is the set of vertex IDs of all people living in Europe\n  lives_in_europe(vertex_id) AS (\n    SELECT edges.tail_vertex FROM edges\n    JOIN in_europe ON edges.head_vertex = in_europe.vertex_id\n    WHERE edges.label = 'lives_in'\n  )\n  SELECT vertices.properties-&gt;&gt;'name'\n  FROM vertices\n  -- join to find those people who were both born in the US *and* live in Europe\n  JOIN born_in_usa ON vertices.vertex_id = born_in_usa.vertex_id\n  JOIN lives_in_europe ON vertices.vertex_id = lives_in_europe.vertex_id;"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-sparql",
    "href": "bigdata_lec12.html#query-languages-sparql",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: SPARQL",
    "text": "Query languages: SPARQL\n\n\n\nSPARQL query: find\n\n\nSELECT ?personName WHERE {\n  ?person :name ?personName.\n  ?person :bornIn / :within* / :name \"United States\".\n  ?person :livesIn / :within* / :name \"Europe\".\n}\n\n\n\n\n\n\nCypher vs SPARQL\n\n\n(person)-[:BORN_IN]-&gt;()-[:WITHIN*0..]-&gt;(location) #Cypher\n?person :bornIn / :within* ?location. #SPARQL"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-datalog",
    "href": "bigdata_lec12.html#query-languages-datalog",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: Datalog",
    "text": "Query languages: Datalog\n\n\n\nDatalog\n\n\nUses a generalized version of triple-store model: predicate(subject, object)\nname(namerica, 'North America').\ntype(namerica, continent).\nname(usa, 'United States').\ntype(usa, country).\nwithin(usa, namerica).\nname(idaho, 'Idaho').\ntype(idaho, state).\nwithin(idaho, usa).\nname(lucy, 'Lucy').\nborn_in(lucy, idaho)."
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-datalog-1",
    "href": "bigdata_lec12.html#query-languages-datalog-1",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: Datalog",
    "text": "Query languages: Datalog\n\n\n\nDefinition\n\n\nA Datalog program is a collection of Datalog rules, each of which is of the form: \\[\nA :- B_1,B_2,\\ldots,B_n,\n\\] where \\(n \\geq 0\\), A is the head of the rule, and the conjunction of \\(B_1,\\ldots,B_n\\) is the body of the rule.\nThe rule can be read informally as “\\(B_1\\) and \\(B_2\\) and \\(\\ldots\\) and \\(B_n\\) implies \\(A\\)”."
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-datalog-2",
    "href": "bigdata_lec12.html#query-languages-datalog-2",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: Datalog",
    "text": "Query languages: Datalog"
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-datalog-3",
    "href": "bigdata_lec12.html#query-languages-datalog-3",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: Datalog",
    "text": "Query languages: Datalog\n\n\n\nDatalog: define data\n\n\nname(namerica, 'North America').\ntype(namerica, continent).\nname(usa, 'United States').\ntype(usa, country).\nwithin(usa, namerica).\nname(idaho, 'Idaho').\ntype(idaho, state).\nwithin(idaho, usa).\nname(lucy, 'Lucy').\nborn_in(lucy, idaho)."
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-datalog-4",
    "href": "bigdata_lec12.html#query-languages-datalog-4",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: Datalog",
    "text": "Query languages: Datalog\n\n\n\nDatalog query\n\n\nwithin_recursive(Location, Name) :- name(Location, Name). /* Rule 1 */\nwithin_recursive(Location, Name) :- within(Location, Via), within_recursive(Via, Name). /* Rule 2 */\nmigrated(Name, BornIn, LivingIn) :- name(Person, Name), /* Rule 3 */\n            born_in(Person, BornLoc),\n            within_recursive(BornLoc, BornIn),\n            lives_in(Person, LivingLoc),\n            within_recursive(LivingLoc, LivingIn).\n?- migrated(Who, 'United States', 'Europe')."
  },
  {
    "objectID": "bigdata_lec12.html#query-languages-datalog-5",
    "href": "bigdata_lec12.html#query-languages-datalog-5",
    "title": "Databases: History, Models and Queries",
    "section": "Query languages: Datalog",
    "text": "Query languages: Datalog\n\n\n\nDatalog: determine if Idaho is in NA"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-1",
    "href": "bigdata_lec12.html#storage-engines-1",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines",
    "text": "Storage engines\n\n\n\nUser view\n\n\n\ndata model - how to store the data\nquery lang - how to query the data\n\n\n\n\n\n\n\nDB view\n\n\n\nstorage engine internals - how to store the data\nstorage engine/DB API - how to find the data"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-2",
    "href": "bigdata_lec12.html#storage-engines-2",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines",
    "text": "Storage engines\n\n\n\nStorage engine optimizations\n\n\n\nfor transactional workloads\nfor analytics"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-3",
    "href": "bigdata_lec12.html#storage-engines-3",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines",
    "text": "Storage engines\n\n\n\nTwo families of storage engines\n\n\n\nlog-structured\npage-oriented"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs",
    "href": "bigdata_lec12.html#storage-engines-logs",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nLog definition\n\n\nAn append-only sequence of records.\n\n\n\n\n\n\nStill need to think about\n\n\n\nconcurrency control\ndisk space\nand handling errors and partial writes"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-1",
    "href": "bigdata_lec12.html#storage-engines-logs-1",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nSimple log-based db\n\n\n#!/bin/bash\n\ndb_set () {\n  echo \"$1,$2\" &gt;&gt; database\n}\n\ndb_get () {\n  grep \"^$1,\" database | sed -e \"s/^$1,//\" | tail -n 1\n}"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-2",
    "href": "bigdata_lec12.html#storage-engines-logs-2",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nWrite\n\n\n$ db_set 123456 '{\"name\":\"London\",\"attractions\":[\"Big Ben\",\"London Eye\"]}'\n$ db_set 42 '{\"name\":\"San Francisco\",\"attractions\":[\"Golden Gate Bridge\"]}'\n\n\n\n\n\n\nRead\n\n\n$ db_get 42\n{\"name\":\"San Francisco\",\"attractions\":[\"Exploratorium\"]}"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-3",
    "href": "bigdata_lec12.html#storage-engines-logs-3",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nPros and cons\n\n\n\nPros: writes are fast \\(O(1)\\)\nCons: reads are slow \\(O(n)\\)\n\nHow to speed up writes: use an index.\n\n\n\n\n\n\n\n\n\nIndex\n\n\nIndex is an additional structure that is derived from the primary data.\n\nany kind of index slows down writes\nnecessary to choose manually based on typical query patterns."
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-4",
    "href": "bigdata_lec12.html#storage-engines-logs-4",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nIndex structure\n\n\nConsider key-value data type (akin to Python dictionary)."
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-5",
    "href": "bigdata_lec12.html#storage-engines-logs-5",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nLimitations\n\n\n\nall keys should fit into available memory\nwhat if db file becomes too large?\n\n\n\n\n\n\n\nSolution: Segmentation and Compaction\n\n\n\nbreak the log into segments\nwrite only to the newest segment\nperform compaction on older segments"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-6",
    "href": "bigdata_lec12.html#storage-engines-logs-6",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nSolution: Compaction"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-7",
    "href": "bigdata_lec12.html#storage-engines-logs-7",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nSolution: Compaction"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-8",
    "href": "bigdata_lec12.html#storage-engines-logs-8",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nCompaction querying\n\n\n\nseparate hash table for each segment\nfirst check the most recent segment\nif not found, check the second-most-recent segment\nmerging process will make sure there’s not too many segments"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-9",
    "href": "bigdata_lec12.html#storage-engines-logs-9",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nCompaction caveats\n\n\n\ndeletions: use a tombstone record\ncrash recovery: store hash tables in disk files for faster recovery\nlog corruption: use checksums\nconcurrency: single writer, multiple readers"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-10",
    "href": "bigdata_lec12.html#storage-engines-logs-10",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nAppend vs in-place: Pros\n\n\n\nappending and segmenting are sequential writes\nconcurrency and crash recovery become much simpler\nsegment merge does away with fragmentation\n\n\n\n\n\n\n\nAppend vs in-place: Cons\n\n\n\nhash table size limitations\nrange queries become slow\n\nSolutions: SSTable and LSM-tree, B-tree"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-11",
    "href": "bigdata_lec12.html#storage-engines-logs-11",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nDefinition\n\n\nSorted string table (SSTable): sort key-value entries by key.\nAdvantages:\n\nMerge is faster\nNo need to keep hashes for all entries\nRecords can be grouped into blocks and compressed\n\nIntroduced in Bigtable: A Distributed Storage System for Structured Data, Google (2006)."
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-12",
    "href": "bigdata_lec12.html#storage-engines-logs-12",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\nSSTable advantages: merge is faster"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-13",
    "href": "bigdata_lec12.html#storage-engines-logs-13",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\nSSTable advantages: index optimizations"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-14",
    "href": "bigdata_lec12.html#storage-engines-logs-14",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nHow to sort data on disk?\n\n\n\nWhen a write comes in, add it to an in-memory balanced tree data structure (a memtable)\nWhen the memtable gets bigger than some threshold—typically a few megabytes —write it out to disk as an SSTable file.\nIn order to serve a read request, first try to find the key in the memtable, then in the most recent on-disk segment, then in the next-older segment, etc.\nFrom time to time, run a merging and compaction process in the background"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-15",
    "href": "bigdata_lec12.html#storage-engines-logs-15",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nSSTable uses\n\n\n\nLevelDB\nRocksDB\nCassandra\nHBase\nGoogle BigTable users: Google Earth, Google Finance"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-16",
    "href": "bigdata_lec12.html#storage-engines-logs-16",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nLog-Structured Merge-Tree\n\n\nLSM-tree: introduced in The Log-Structured Merge-Tree, Patrick O’Neil et al. (1996).\nStorage engines that are based on this principle of merging and compacting sorted files are often called LSM storage engines.\nBasic idea: keeping a cascade of SSTables that are merged in the background."
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-17",
    "href": "bigdata_lec12.html#storage-engines-logs-17",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nLog-Structured Merge-Tree"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-logs-18",
    "href": "bigdata_lec12.html#storage-engines-logs-18",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: logs",
    "text": "Storage engines: logs\n\n\n\nPerformance optimizations\n\n\n\nUse Bloom filters for segment checks\nsize-tiered compaction: newer and smaller SSTables are successively merged into older and larger SSTables.\nleveled compaction: the key range is split up into smaller SSTables and older data is moved into separate “levels,” which allows the compaction to proceed more incrementally and use less disk space."
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-trees",
    "href": "bigdata_lec12.html#storage-engines-trees",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: trees",
    "text": "Storage engines: trees\n\n\n\nB-trees\n\n\nIntroduced in Organization and Maintenance of Large Ordered Indices, Bayer et al, Boeing Scientific Research Laboratories, 1970.\n\nB-trees break the database down into fixed-size blocks or pages, traditionally 4 KB in size (sometimes bigger), and read or write one page at a time.\nEach page contains several keys and references to child pages.\nNumber of references to child pages in one page of the B-tree is called the branching factor."
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-trees-1",
    "href": "bigdata_lec12.html#storage-engines-trees-1",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: trees",
    "text": "Storage engines: trees\n\n\n\nDefinition\n\n\nLet \\(h \\geq 0\\) be an integer, \\(k\\) a natural number. A directed tree \\(T\\) is in the class \\(\\tau(k,h)\\) of B-trees if \\(T\\) is either empty (\\(h=0\\)) or has the following properties:\n\nEach path from the root to any leaf has the same length \\(h\\), also called the height of \\(T\\), i.e., \\(h\\) = number of nodes in path.\nEach node except the root and the leaves has at least \\(k + 1\\) sons. The root is a leaf or has at least two sons.\nEach node has at most \\(2k + 1\\) sons."
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-trees-2",
    "href": "bigdata_lec12.html#storage-engines-trees-2",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: trees",
    "text": "Storage engines: trees"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-trees-3",
    "href": "bigdata_lec12.html#storage-engines-trees-3",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: trees",
    "text": "Storage engines: trees\n\n\n\nB-tree Update\n\n\n\nsearch for the leaf page containing that key\nchange the value in that page\nwrite the page back to disk (any references to that page remain valid)\n\n\n\n\n\n\n\nB-tree Insert\n\n\n\nfind the page whose range encompasses the new key\nadd it to that page.\nIf there isn’t enough free space in the page to accommodate the new key, it is split into two half-full pages, and the parent page is updated to account for the new subdivision of key ranges"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-trees-4",
    "href": "bigdata_lec12.html#storage-engines-trees-4",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: trees",
    "text": "Storage engines: trees"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-trees-5",
    "href": "bigdata_lec12.html#storage-engines-trees-5",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: trees",
    "text": "Storage engines: trees\n\nPostgreSQL page structure"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-trees-6",
    "href": "bigdata_lec12.html#storage-engines-trees-6",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: trees",
    "text": "Storage engines: trees\n\n\n\nBalancing\n\n\nThis algorithm ensures that the tree remains balanced:\n\na B-tree with \\(n\\) keys always has a depth of \\(O(\\log n)\\)\nmost databases can fit into a B-tree that is three or four levels deep\nfour-level tree of 4 KB pages with a branching factor of 500 can store up to 256 TB."
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-trees-7",
    "href": "bigdata_lec12.html#storage-engines-trees-7",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: trees",
    "text": "Storage engines: trees\n\n\n\nB-tree optimizations\n\n\n\nwrite-ahead log (WAL). This is an append-only file to which every B-tree modification must be written before it can be applied to the pages of the tree itself.\nlatches (lightweight locks): solve concurrency issues by protecting the tree’s data structures\ncopy-on-write for new page creation\nkey abbreviation. Especially in pages on the interior of the tree, keys only need to provide enough information to act as boundaries between key ranges. Packing more keys into a page allows the tree to have a higher branching factor, and thus fewer levels\nadditional tree pointers, e.g. siblings"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-index-comparison",
    "href": "bigdata_lec12.html#storage-engines-index-comparison",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: index comparison",
    "text": "Storage engines: index comparison\n\n\n\n\n\n\nLSM-trees vs B-trees\n\n\n\nLSM-trees are typically faster for writes\nB-trees are thought to be faster for reads\nCompaction process interferes with ongoing read/write performance\nLSM-trees: lower storage overhead\nLSM-trees: issue of write amplification\nB-trees: each key exists exactly in one place in the index"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-index-comparison-1",
    "href": "bigdata_lec12.html#storage-engines-index-comparison-1",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: index comparison",
    "text": "Storage engines: index comparison\n\nThe RUM Conjecture: Read, Update, Memory – Optimize Two at the Expense of the Third."
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-index-comparison-2",
    "href": "bigdata_lec12.html#storage-engines-index-comparison-2",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: index comparison",
    "text": "Storage engines: index comparison\n\n\n\nOther index types\n\n\n\nSecondary indexes\nClustered indexes\nCovering indexes\nMulti-column indexes\nFuzzy search indexes"
  },
  {
    "objectID": "bigdata_lec12.html#storage-engines-in-memory-dbs",
    "href": "bigdata_lec12.html#storage-engines-in-memory-dbs",
    "title": "Databases: History, Models and Queries",
    "section": "Storage engines: in-memory DBs",
    "text": "Storage engines: in-memory DBs\n\nAnti-caching: cold data is moved to disk in a transactionally-safe manner as the database grows in size."
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-1",
    "href": "bigdata_lec12.html#oltp-vs-olap-1",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP"
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-2",
    "href": "bigdata_lec12.html#oltp-vs-olap-2",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP\n\n\n\nDefinition\n\n\nData warehousing is a collection of decision support technologies, aimed at enabling the knowledge worker (executive, manager, analyst) to make better and faster decisions.\n\n\n\n\n\n\nEven worse definition\n\n\nA data warehouse is a “subject-oriented, integrated, time- varying, non-volatile collection of data that is used primarily in organizational decision making.”\n\n\n\nWarehouses are OLAP Gen 1."
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-3",
    "href": "bigdata_lec12.html#oltp-vs-olap-3",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP"
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-4",
    "href": "bigdata_lec12.html#oltp-vs-olap-4",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP\n\n\n\nData warehouse\n\n\n\nIt is a separate database that analysts can query without affecting OLTP operations.\nThe data warehouse contains a read-only copy of the data in all the various OLTP systems in the company.\n\n\n\n\n\n\n\nExtract–Transform–Load (ETL)\n\n\n\nData is extracted from OLTP databases (using either a periodic data dump or a continuous stream of updates)\ntransformed into an analysis-friendly schema\ncleaned up\nand then loaded into the data warehouse."
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-5",
    "href": "bigdata_lec12.html#oltp-vs-olap-5",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP\n\n\n\nWhy separate?\n\n\n\ntrying to execute complex OLAP queries against the operational databases would result in unacceptable performance.\ndecision support requires non-OLTP data, such as historical data\ndecision support requires consolidating data from many heterogeneous sources\nsupporting the multidimensional data models and operations typical of OLAP requires special data organization, access, and implementation methods"
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-6",
    "href": "bigdata_lec12.html#oltp-vs-olap-6",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP\n\n\n\nData warehouse rollout\n\n\n\ndefine the architecture + capacity planning\nintegrate disparate servers\ndesign schemas and views\nconnect data sources to the warehouse\ndefine structure of physical storage\nwrite scripts for ETL\nimplement end-user applications"
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-7",
    "href": "bigdata_lec12.html#oltp-vs-olap-7",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP\n\n\n\nETL preparation: cleaning\n\n\n\ndata migration: specify simple transformation rules\ndata scrubbing: use domain-specific knowledge\ndata auditing: discover rules (or violations thereof) by scanning data"
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-8",
    "href": "bigdata_lec12.html#oltp-vs-olap-8",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP\n\n\n\nLoad preparation: preprocessing\n\n\n\nchecking integrity constraints\nsorting, summarization, aggregation, etc.\nindexing and building other access paths\npartitioning to multiple storage areas"
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-9",
    "href": "bigdata_lec12.html#oltp-vs-olap-9",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP\n\n\n\nCommercial\n\n\n\nMicrosoft SQL Server\nSAP HANA (OLTAP)\nAmazon RedShift\n\n\n\n\n\n\n\nOpen-source\n\n\n\nApache Hive\nSpark SQL\nPresto/Trino"
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-10",
    "href": "bigdata_lec12.html#oltp-vs-olap-10",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP\n\n\n\nThe divergence between OLTP DBs and data warehouses\n\n\nData warehouse:\n\nData model: often relational\nQuery language: often SQL-like\nInternals: quite different"
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-11",
    "href": "bigdata_lec12.html#oltp-vs-olap-11",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP\n\n\n\nDimensional model\n\n\n\nMultidimensional data model: a set of numeric measures that are the objects of analysis.\nEach of the numeric measures depends on a set of dimensions\nA measure as a value in the multidimensional space of dimensions.\nEach dimension is described by a set of attributes."
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-12",
    "href": "bigdata_lec12.html#oltp-vs-olap-12",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP"
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-13",
    "href": "bigdata_lec12.html#oltp-vs-olap-13",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP\n\n\n\nStar schema\n\n\nThe name “star schema” comes from the fact that when the table relationships are visualized, the fact table is in the middle, surrounded by its dimension tables; the connections to these tables are like the rays of a star.\n\nfact table – collection of events occurring at particular time\ndimension table – who, what, where, when, how, and why of the event."
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-14",
    "href": "bigdata_lec12.html#oltp-vs-olap-14",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP"
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-15",
    "href": "bigdata_lec12.html#oltp-vs-olap-15",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP"
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-16",
    "href": "bigdata_lec12.html#oltp-vs-olap-16",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP\n\n\n\nSnowflake schema: definition\n\n\nA variation of star schema, where dimensions are further broken down into subdimensions.\n\nmore normalized than star schemas\nhowever, star schemas are often preferred because they are simpler for analysts to work with"
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-17",
    "href": "bigdata_lec12.html#oltp-vs-olap-17",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP"
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-18",
    "href": "bigdata_lec12.html#oltp-vs-olap-18",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP\n\n\n\nOLAP cubes\n\n\nMaterialized aggregates: data warehouse queries often involve an aggregate function, such as COUNT, SUM, AVG, MIN, or MAX in SQL.\nIf the same aggregates are used often, they can be modified to be materialized.\nRelational model offers materialized views."
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-19",
    "href": "bigdata_lec12.html#oltp-vs-olap-19",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP"
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-20",
    "href": "bigdata_lec12.html#oltp-vs-olap-20",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP\n\n\n\nOperations\n\n\n\nrollup: increasing the level of aggregation\ndrill-down: decreasing the level of aggregation or increasing detail along one or more dimension hierarchies\nslice-and-dice: selection and projection\npivot: re-orienting the multidimensional view of data"
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-21",
    "href": "bigdata_lec12.html#oltp-vs-olap-21",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP\n\n\n\nPivoting\n\n\nThe simplest view of pivoting is that it selects two dimensions that are used to aggregate a measure.\n\n\n\n\n\n\nRollup\n\n\nRollup corresponds to taking the current data object and doing a further group-by on one of the dimensions.\nA converse is called drill-down.\n\n\n\n\n\n\nSlice-and-dice\n\n\nSlice-and-dice corresponds to reducing the dimensionality of the data, i.e., taking a projection of the data on a subset of dimensions for selected values of the other dimensions."
  },
  {
    "objectID": "bigdata_lec12.html#oltp-vs-olap-22",
    "href": "bigdata_lec12.html#oltp-vs-olap-22",
    "title": "Databases: History, Models and Queries",
    "section": "OLTP vs OLAP",
    "text": "OLTP vs OLAP\n\n\n\nData warehouse challenges\n\n\n\ncouple compute and storage into an on-premises appliance. This forced enterprises to provision and pay for the peak of user load and data under management, which became very costly as datasets grew.\nmore and more datasets were completely unstructured, e.g., video, audio, and text documents, which data warehouses could not store and query at all.\nschema-on-write"
  },
  {
    "objectID": "bigdata_lec11.html#history",
    "href": "bigdata_lec11.html#history",
    "title": "Databases: History, Models and Queries",
    "section": "History",
    "text": "History\n\n\n\nPlimpton 322 tablet (1800 BC)"
  },
  {
    "objectID": "bigdata_lec11.html#overview-tech-stack",
    "href": "bigdata_lec11.html#overview-tech-stack",
    "title": "Databases: History, Models and Queries",
    "section": "Overview: tech stack",
    "text": "Overview: tech stack"
  },
  {
    "objectID": "bigdata_lec11.html#overview-data-shapes",
    "href": "bigdata_lec11.html#overview-data-shapes",
    "title": "Databases: History, Models and Queries",
    "section": "Overview: data shapes",
    "text": "Overview: data shapes\n\n\n\nData shapes\n\n\n\ntrees\ncubes\ngraphs\nunstructured"
  },
  {
    "objectID": "bigdata_lec11.html#data-models",
    "href": "bigdata_lec11.html#data-models",
    "title": "Databases: History, Models and Queries",
    "section": "Data models",
    "text": "Data models\n\n\n\nOrigins of databases\n\n\nBusiness data processing performed on mainframe computers in the 1960s and ’70s:\n\ntransaction processing (entering sales or banking transactions, airline reservations, stock-keeping in warehouses)\nbatch processing (customer invoicing, payroll, reporting).\n\n\n\n\n\n\n\nHistory outline\n\n\n\n1970-1980s: network and hierarchical models\n1980-1990s: object databases\n1970s-present: relational model\n1990s-present: document model"
  },
  {
    "objectID": "bigdata_lec11.html#hierarchical-model",
    "href": "bigdata_lec11.html#hierarchical-model",
    "title": "Databases: History, Models and Queries",
    "section": "Hierarchical model",
    "text": "Hierarchical model\n\n\n\nInformation Management System (IBM)\n\n\nOriginally released in 1968, developed for stock keeping of the Apollo space program.\nRepresented all data as a tree of records nested within records.\n\nworks well for one-to-many relationships\nnot so well for many-to-many relationships\ndoes not support joins"
  },
  {
    "objectID": "bigdata_lec11.html#hierarchical-model-1",
    "href": "bigdata_lec11.html#hierarchical-model-1",
    "title": "Databases: History, Models and Queries",
    "section": "Hierarchical model",
    "text": "Hierarchical model"
  },
  {
    "objectID": "bigdata_lec11.html#network-model",
    "href": "bigdata_lec11.html#network-model",
    "title": "Databases: History, Models and Queries",
    "section": "Network model",
    "text": "Network model\n\n\n\nConference on Data Systems Languages (CODASYL, 1969)\n\n\n\nchild record can have many parents\nallows modeling of many-to-one and many-to-many relationships\nuses access paths: sequences of links between records\nqueries are constructed by following access paths"
  },
  {
    "objectID": "bigdata_lec11.html#network-model-1",
    "href": "bigdata_lec11.html#network-model-1",
    "title": "Databases: History, Models and Queries",
    "section": "Network model",
    "text": "Network model\n\n\n\nBachman diagram"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model",
    "href": "bigdata_lec11.html#relational-model",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nEdgar Codd. A Relational Model of Data for Large Shared Data Banks (1970)\n\n\n\nA usable database management system should hide all the physical complexity from the user and expose instead a simple, clean model. This model should be based on tables, which gave birth to the relational model and the relational algebra.\n\n\n\n\n\n\n\nData independence\n\n\nData independence - the independence of application programs and terminal activities from growth in data types and changes in data representation.\nLogical view on the data is cleanly separated, decoupled, from its physical storage."
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-1",
    "href": "bigdata_lec11.html#relational-model-1",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nLogical model\n\n\nA relational database management system (RDBMS) exposes this logical model, together with logical building blocks for manipulating it, on top of a physical layer.\nThus, 2 layers:\n\nLogical\nPhysical\n\nIn relational model, logical view is a table.\n\n\n\n\n\nNote: query optimizer handles the construction of access paths (compare with manual effort required in the network model)"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-2",
    "href": "bigdata_lec11.html#relational-model-2",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nRDBMS as a stack\n\n\n\nA logical query language with which the user can query data;\nA logical model for the data;\nA physical compute layer that processes the query on an instance of the model;\nA physical storage layer where the data is physically stored."
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-3",
    "href": "bigdata_lec11.html#relational-model-3",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nDefinitions\n\n\nA relation \\(R\\) on a family of sets \\((A_i)_{i=1,n}\\) is a subset of their Cartesian product: \\(R \\subseteq A_1 \\times \\ldots \\times A_n\\).\nA partial function between two sets \\(A\\) and \\(B\\) is a relation that associates to each element of \\(A\\) at most one element of \\(B\\): \\[\n\\forall (x_A, x_B), (y_A, y_B) \\ in p: x_A = y_A \\Rightarrow x_B = y_B\n\\]\nThe set of all partial functions from \\(A\\) to \\(B\\) is denoted as \\(A \\mapsto B\\).\nThe subset of \\(A\\) with the elements that do get associated to an element of \\(B\\) is denoted \\(support(p)\\): \\[\nsupport(p) = \\left\\{a \\in A: \\exists b \\in B, p(a)=b\\right\\} = p^{-1}(B).\n\\]"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-4",
    "href": "bigdata_lec11.html#relational-model-4",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nSet definitions\n\n\n\n\\(\\mathbb{S}\\) is a set of all strings.\n\\(\\mathbb{V}\\) is a set of all possible values. Contains anything that can be stored on persistent storage or in memory as a sequence of 0s and 1s and according to a convention to interpret these bits."
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-5",
    "href": "bigdata_lec11.html#relational-model-5",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nSet definitions\n\n\n\n\\(\\mathbb{A} \\subset \\mathbb{V}\\) – the set that contains all atomic values. Atomic values are values that are not structured, i.e., this excludes:\n\nobjects,\narrays,\nlists,\nsets,\ntrees,\nbags,\netc.\n\nbut only includes:\n\nstrings,\nintegers,\nBooleans,\ndates\n\nand so on."
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-6",
    "href": "bigdata_lec11.html#relational-model-6",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nSet definitions\n\n\n\nThe table itself, which can be seen as a collection of records. A synonym is a collection.\nThe attribute, which is a property that records can have.\nThe row, which is a record in a collection. A row associates properties with the values applicable for the record it represents. Synonyms of row are:\n\nrecord,\nentity,\ndocument,\nitem.\n\nThe primary key, which is a particular attribute or set of attributes that uniquely identify a record in its table."
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-7",
    "href": "bigdata_lec11.html#relational-model-7",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nTable definition\n\n\nExpressed formally, given a family of attributes \\((A_i)_{1\\leq i \\leq n}\\) and their associated domains \\[\n(Domain(A_i))_{1\\leq i \\leq n}\n\\] a table \\(T\\) is a relation over these domains, i.e. \\[\nT \\subseteq Domain(A_1) \\times Domain(A_2) \\times \\ldots \\times Domain(A_n)\n\\]"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-8",
    "href": "bigdata_lec11.html#relational-model-8",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nTable definition\n\n\nA collection of records is a set of partial functions from \\(\\mathbb{S}\\) to \\(\\mathbb{V}\\). We denote the set of collections as \\(\\mathcal{C}\\): \\[\n\\mathcal{C} = \\mathcal{P}(\\mathbb{S} \\mapsto \\mathbb{V})\n\\] Each record is thus modelled as a partial function mapping strings to values."
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-9",
    "href": "bigdata_lec11.html#relational-model-9",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nTable definition\n\n\nA table is a collection with three constraints:\n\nRelational integrity\nDomain integrity\nAtomic integrity"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-10",
    "href": "bigdata_lec11.html#relational-model-10",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nRelational integrity\n\n\nA collection T fulfils relational integrity if all its records have identical support: \\[\n\\forall t, u \\in T : support(u) = support(t)\n\\] Common support is a property of the table and contains the attributes of the table \\(T\\): \\(Attributes_T\\).\nThe extension of the table, sometimes denoted \\(Extension_T\\) is its actual content, which is \\(T\\) itself."
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-11",
    "href": "bigdata_lec11.html#relational-model-11",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nNo relational integrity"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-12",
    "href": "bigdata_lec11.html#relational-model-12",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nWith relational integrity"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-13",
    "href": "bigdata_lec11.html#relational-model-13",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nDomain integrity\n\n\nA collection \\(T\\) fulfils domain integrity if the values associated with each attribute are restricted to a domain.\nWe define these domains with a function \\(D\\) mapping strings (the attributes) to domains (unused attributes are just associated with empty domains, i.e. this does not need to be a partial function): \\[\nD \\in \\mathcal{P}(\\mathbb{V})^\\mathbb{S}\n\\]\nA collection \\(T\\) fulfils the domain integrity constraint specified by \\(D\\) if, for each row, the values are in the specified domains: \\[\n\\forall t \\in T, \\forall a \\in support(t): t.a \\in D(a)\n\\]\nNote: domain integrity still allows missing values."
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-14",
    "href": "bigdata_lec11.html#relational-model-14",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nSchema\n\n\nDomain mapping \\(D\\) is called a schema.\nSchema contains:\n\nnames of the columns\nthe domain or type of each column (string, integer, date, boolean, and so on)."
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-15",
    "href": "bigdata_lec11.html#relational-model-15",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nNo domain integrity"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-16",
    "href": "bigdata_lec11.html#relational-model-16",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nWith domain integrity"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-17",
    "href": "bigdata_lec11.html#relational-model-17",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nAtomic integrity\n\n\nA collection \\(T\\) fulfils the atomic integrity constraint if the values used in it are only atomic values, i.e. \\[\nT \\subseteq \\mathbb{S} \\mapsto \\mathbb{A}\n\\]\nThis means that the collection does not contain any nested collections or sets or lists or anything that has a structure of its own: it is flat."
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-18",
    "href": "bigdata_lec11.html#relational-model-18",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nNo atomic integrity"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-19",
    "href": "bigdata_lec11.html#relational-model-19",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nWith atomic integrity"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-20",
    "href": "bigdata_lec11.html#relational-model-20",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-21",
    "href": "bigdata_lec11.html#relational-model-21",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\nRelational algebra: a framework for manipulating relational tables"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-22",
    "href": "bigdata_lec11.html#relational-model-22",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nQueries\n\n\n\nSet queries act on relational tables as sets (as previously described): one can take the union or the intersection of two sets, or subtract a set from another. \nFilter queries take a portion of a table: some or all columns, some or all rows, etc. They are known as projection and selection. \nRenaming queries can rename columns.\nJoining queries can take the Cartesian product of two tables, potentially filtering to match values from both sides. The latter is called a join."
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-23",
    "href": "bigdata_lec11.html#relational-model-23",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nSelection\n\n\nA selection takes a subset of the records belonging to the table, taking a parameter, which is a predicate on the attributes.\nThe notation used is the \\(\\sigma\\) letter. \\[\nS = \\sigma_{B \\leq 2}(R)\n\\]"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-24",
    "href": "bigdata_lec11.html#relational-model-24",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nProjection\n\n\nA projection keeps all records, but removes columns specified in the parameter list.\nThe notation used is the \\(\\pi\\) letter: \\[\nS = \\pi_{A,C}(R)\n\\]"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-25",
    "href": "bigdata_lec11.html#relational-model-25",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nGrouping\n\n\nA grouping, also called aggregation, merges records by grouping on some attributes, and aggregating on all others.\nThe notation used is the \\(\\gamma\\) letter: \\[\nS = \\gamma_{G,SUM(A) \\rightarrow A}(R)\n\\] This groups by \\(G\\) and aggregate the values in column \\(A\\) (within the same group) with a sum\nNote: there are grouping and non-grouping attributes.\nExamples: COUNT, SUM, MAX, MIN, AVERAGE."
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-26",
    "href": "bigdata_lec11.html#relational-model-26",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nRelational grouping"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-27",
    "href": "bigdata_lec11.html#relational-model-27",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nRenaming\n\n\nRename a column from \\(A\\) to \\(D\\): \\[\nS = \\rho_{A \\rightarrow D}(R)\n\\]"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-28",
    "href": "bigdata_lec11.html#relational-model-28",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nJoins\n\n\nA natural join is a filtered Cartesian product in which we only combine directly related tuples and omit all other non-matching pairs. The notation used is the \\(\\bowtie\\) symbol: \\[\nT= R \\bowtie S\n\\] joins \\(R\\) and \\(S\\) but only keeps records that coincide on the attributes common to both sides.\nJoins are computationally more expensive than projections or selections."
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-29",
    "href": "bigdata_lec11.html#relational-model-29",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nRelational join"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-30",
    "href": "bigdata_lec11.html#relational-model-30",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nOther joins\n\n\n\nTheta joins explicitly specify the joining criterion instead of matching the common attributes, e.g. \\(T= R \\bowtie_{A=D} S\\). Common attributes should be renamed, e.g. \\(T= R \\bowtie_{B=E \\rho_{B \\rightarrow E}} S\\).\nOuter joins keep records with no match on the other side, keeping the other attributes absent. Note that this breaks relational integrity in its strictest sense\nSemi-outer joins are like (full) outer joins but only keep unmatched records on the left, or only on the right."
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-31",
    "href": "bigdata_lec11.html#relational-model-31",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nNormal forms\n\n\n\nAtomic integrity\nEach column in a record contains information on the entire record\nNo functional dependencies on anything else than the primary key\n\nNormal forms help to address inconsistencies:\n\ndeletion anomalies: deleting a record makes the database inconsistent\ninsertion anomalies: inserting a new record makes the database inconsistent\nupdate anomalies: updating a record makes the database inconsistent"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-32",
    "href": "bigdata_lec11.html#relational-model-32",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nKey idea behind of normalization\n\n\nRemoval of duplication.\n\n\n\n\n\n\nDownsides\n\n\n\nRequires joins\nLoss of locality (also called shredding)"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-33",
    "href": "bigdata_lec11.html#relational-model-33",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nTransactions\n\n\nACID guarantees:\n\nAtomicity: either an update (called a transaction if it consists of several updates) is applied to the database completely, or not at all;\nConsistency: before and after the transactions, the data is in a consistent state (e.g., some values sum to another value, another value is positive, etc);\nIsolation: the system “feels like” the user is the only one using the system, where in fact maybe thousands of people are using it as well concurrently;\nDurability: any data written to the database is durably stored and will not be lost (e.g., if there is an electricity shortage or a disk crash)."
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-34",
    "href": "bigdata_lec11.html#relational-model-34",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nLimitations\n\n\n\nlots of rows: beyond a million records, a system on one machine can start showing signs of weakness; even though more recent systems manage to push it a bit higher on a single machine (e.g., close to a billion)\nlots of columns: beyond 255 columns, a system on one machine can start showing signs of weakness or even not support it at all\nlots of nesting: many systems do not support nested data or, if they do, do so only in a limited fashion and it becomes quickly cumbersome"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-35",
    "href": "bigdata_lec11.html#relational-model-35",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nAlternatives\n\n\n\n\n\nIssue\nSolution\n\n\n\n\nLots of rows\nObject storage\n\n\nLots of rows\nDistributed File Systems\n\n\nLots of nesting\nSyntax\n\n\nLots of rows/columns\nColumn storage\n\n\nLots of nesting\nData models\n\n\nLots of rows\nMassive Parallel Processing\n\n\nLots of nesting\nDocument Stores\n\n\nLots of nesting\nQuerying"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-36",
    "href": "bigdata_lec11.html#relational-model-36",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nDrivers for change\n\n\n\nA need for greater scalability than relational databases can easily achieve, including very large datasets or very high write throughput\nA widespread preference for free and open source software over commercial database products\nSpecialized query operations that are not well supported by the relational model\nFrustration with the restrictiveness of relational schemas, and a desire for a more dynamic and expressive data model"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-37",
    "href": "bigdata_lec11.html#relational-model-37",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nModern systems\n\n\n\nkey-value stores\nwide column stores\ndocument stores\ngraph databases"
  },
  {
    "objectID": "bigdata_lec11.html#relational-model-38",
    "href": "bigdata_lec11.html#relational-model-38",
    "title": "Databases: History, Models and Queries",
    "section": "Relational model",
    "text": "Relational model\n\n\n\nChallenges\n\n\n\nintegrity may not hold (nested/heterogeneous data)\nACID guarantees may not hold"
  },
  {
    "objectID": "bigdata_lec11.html#cap",
    "href": "bigdata_lec11.html#cap",
    "title": "Databases: History, Models and Queries",
    "section": "CAP",
    "text": "CAP\n\n\n\nDefinition\n\n\nCAP: presented as a conjecture by Eric Brewer at the 2000 Symposium on Principles of Distributed Computing and formalized and proven by Gilbert and Lynch in 2002.\n\nConsistent: All replicas of the same data will be the same value across a distributed system.\nAvailable: All live nodes in a distributed system can process operations and respond to queries.\nPartition Tolerant: The system is designed to operate in the face of connectivity loss between replicas.\n\n\n\n\n\n\n\nCAP (Brewer’s) theorem\n\n\nYou can have only two of the above."
  },
  {
    "objectID": "bigdata_lec11.html#document-model",
    "href": "bigdata_lec11.html#document-model",
    "title": "Databases: History, Models and Queries",
    "section": "Document model",
    "text": "Document model\n\n\n\nComparison\n\n\n\nvs hierarchical: child data is stored together with a parent record\nvs relational: use document references instead of foreign keys"
  },
  {
    "objectID": "bigdata_lec11.html#document-model-1",
    "href": "bigdata_lec11.html#document-model-1",
    "title": "Databases: History, Models and Queries",
    "section": "Document model",
    "text": "Document model\n\n\n\nExample\n\n\n{\"user_id\": 251,\n  \"first_name\": \"Bill\",\n  \"last_name\": \"Gates\",\n  \"summary\": \"Co-chair of the Bill and Melinda Gates... Active blogger.\",\n  \"education\": [\n    {\"school_name\": \"Harvard University\", \"start\": 1973, \"end\": 1975},\n    {\"school_name\": \"Lakeside School, Seattle\", \"start\": null, \"end\": null}\n  ],\n  \"contact_info\": {\n    \"blog\": \"http://thegatesnotes.com\",\n    \"twitter\": \"http://twitter.com/BillGates\"\n  }\n}"
  },
  {
    "objectID": "bigdata_lec11.html#document-model-2",
    "href": "bigdata_lec11.html#document-model-2",
    "title": "Databases: History, Models and Queries",
    "section": "Document model",
    "text": "Document model\n\n\n\nPros\n\n\n\nschema flexibility\ndata locality (performance implications!)\n\n\n\n\n\n\n\nCons\n\n\n\nno joins\nmany-to-one and many-to-many relationships"
  },
  {
    "objectID": "bigdata_lec11.html#document-model-3",
    "href": "bigdata_lec11.html#document-model-3",
    "title": "Databases: History, Models and Queries",
    "section": "Document model",
    "text": "Document model\n\n\n\nSchemas\n\n\nDocument DBs are not schemaless. They are schema-on-read.\nRelational DBs are schema-on-write."
  },
  {
    "objectID": "bigdata_lec11.html#document-model-4",
    "href": "bigdata_lec11.html#document-model-4",
    "title": "Databases: History, Models and Queries",
    "section": "Document model",
    "text": "Document model\n\n\n\nHybrids: JSON support in RDBMS\n\n\n\nPostgreSQL 9.3 (2013)\nMySQL 5.7 (2013)\nIBM DB2 10.5 (2013)\n\n\n\n\n\n\n\nHybrids: joins support in document DBs\n\n\n\nRethinkDB: relational-like joins\nMongoDB: client-side joins"
  },
  {
    "objectID": "bigdata_lec11.html#graph-model",
    "href": "bigdata_lec11.html#graph-model",
    "title": "Databases: History, Models and Queries",
    "section": "Graph model",
    "text": "Graph model\n\n\n\nFeatures\n\n\n\nGood for many-to-many relationships\nsuitable for homogeneous and heterogeneous data\nseveral data structuring and querying approaches\nflexible data modeling"
  },
  {
    "objectID": "bigdata_lec11.html#graph-model-1",
    "href": "bigdata_lec11.html#graph-model-1",
    "title": "Databases: History, Models and Queries",
    "section": "Graph model",
    "text": "Graph model"
  },
  {
    "objectID": "bigdata_lec11.html#graph-model-2",
    "href": "bigdata_lec11.html#graph-model-2",
    "title": "Databases: History, Models and Queries",
    "section": "Graph model",
    "text": "Graph model\n\n\n\nProperty graphs (Neo4J, Titan, InfiniteGraph)\n\n\nVertices:\n\nA unique identifier\nA set of outgoing edges\nA set of incoming edges\nA collection of properties (key-value pairs)\n\nEdges:\n\nA unique identifier\nThe vertex at which the edge starts (the tail vertex)\nThe vertex at which the edge ends (the head vertex)\nAn edge label\nA collection of properties (key-value pairs)"
  },
  {
    "objectID": "bigdata_lec11.html#graph-model-3",
    "href": "bigdata_lec11.html#graph-model-3",
    "title": "Databases: History, Models and Queries",
    "section": "Graph model",
    "text": "Graph model\n\n\n\nProperty graph as a pair of relational tables\n\n\nCREATE TABLE vertices (\n  vertex_id integer PRIMARY KEY,\n  properties json\n);\nCREATE TABLE edges (\n  edge_id integer PRIMARY KEY,\n  tail_vertex integer REFERENCES vertices (vertex_id),\n  head_vertex integer REFERENCES vertices (vertex_id),\n  label text,\n  properties json\n);\nCREATE INDEX edges_tails ON edges (tail_vertex);\nCREATE INDEX edges_heads ON edges (head_vertex);"
  },
  {
    "objectID": "bigdata_lec11.html#graph-model-4",
    "href": "bigdata_lec11.html#graph-model-4",
    "title": "Databases: History, Models and Queries",
    "section": "Graph model",
    "text": "Graph model\n\n\n\nTriple stores (Datomic, AllegroGraph)\n\n\nAll information is stored in the form of very simple three-part statements: (subject, predicate, object):\n\nsubject: equivalent to a vertex in a graph\nobject: either a primitive datatype or a vertex"
  },
  {
    "objectID": "bigdata_lec11.html#graph-model-5",
    "href": "bigdata_lec11.html#graph-model-5",
    "title": "Databases: History, Models and Queries",
    "section": "Graph model",
    "text": "Graph model\n\n\n\nTurtle triple format\n\n\n_:lucy a :Person; :name \"Lucy\"; :bornIn _:idaho.\n_:idaho a :Location; :name \"Idaho\"; :type \"state\"; :within _:usa.\n_:usa a :Location; :name \"United States\"; :type \"country\"; :within _:namerica.\n_:namerica a :Location; :name \"North America\"; :type \"continent\"."
  },
  {
    "objectID": "bigdata_lec11.html#graph-model-6",
    "href": "bigdata_lec11.html#graph-model-6",
    "title": "Databases: History, Models and Queries",
    "section": "Graph model",
    "text": "Graph model\n\n\n\nDatalog\n\n\nThis is a generalized version of triple-store model: predicate(subject, object)\nname(namerica, 'North America').\ntype(namerica, continent).\nname(usa, 'United States').\ntype(usa, country).\nwithin(usa, namerica).\nname(idaho, 'Idaho').\ntype(idaho, state).\nwithin(idaho, usa).\nname(lucy, 'Lucy').\nborn_in(lucy, idaho)."
  },
  {
    "objectID": "bigdata.html",
    "href": "bigdata.html",
    "title": "Big Data Analytics",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "bigdata.html#lectures",
    "href": "bigdata.html#lectures",
    "title": "Big Data Analytics",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "bigdata.html#labs",
    "href": "bigdata.html#labs",
    "title": "Big Data Analytics",
    "section": "Labs",
    "text": "Labs\nLab 1: Pandas Optimization\nLab 2: Cython/Numba\nLab 3: Map-reduce, iterators, generators\nLab 4: Intro to Dask\nLab 5: Dask Delayed and Futures\nLab 6: Working with shapes\nLab 7: Spatial relationships\nLab 8: Working with raster data\nLab 9: PostgreSQL for Analytics\nLab 10:"
  },
  {
    "objectID": "bigdata.html#exam-questions",
    "href": "bigdata.html#exam-questions",
    "title": "Big Data Analytics",
    "section": "Exam questions",
    "text": "Exam questions"
  },
  {
    "objectID": "bigdata_lec9.html#spatial-relationships",
    "href": "bigdata_lec9.html#spatial-relationships",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial relationships",
    "text": "Spatial relationships\n\n\n\nOverview\n\n\nQuestion: how the data layers are located in relation to each other?\n\nfinding out if a certain point is located inside an area\nor whether a line intersects with another line or a polygon\n\n\n\n\n\n\n\nNote\n\n\n\nThese kind of queries are commonly called as spatial queries.\nSpatial queries are conducted based on the topological spatial relations\n\ncontains\ntouches\nintersects"
  },
  {
    "objectID": "bigdata_lec9.html#topological-spatial-relations",
    "href": "bigdata_lec9.html#topological-spatial-relations",
    "title": "Big Data: GeoScience part 3",
    "section": "Topological spatial relations",
    "text": "Topological spatial relations\n\n\n\nDE-9IM\n\n\n\nThere are different ways to compute these queries\nMost GIS software rely on Dimensionally Extended 9-Intersection Model (DE-9IM).\nDE-9IM defines the topological relations based on the interior, boundary, and exterior of two geometric shapes and how they intersect with each other.\nDE-9IM also considers the dimensionality of the objects.\n\nThe Point objects are 0-dimensional\nLineString and LinearRing are 1-dimensional\nPolygon objects are 2-dimensional."
  },
  {
    "objectID": "bigdata_lec9.html#topological-spatial-relations-1",
    "href": "bigdata_lec9.html#topological-spatial-relations-1",
    "title": "Big Data: GeoScience part 3",
    "section": "Topological spatial relations",
    "text": "Topological spatial relations\n\nInterior, boundary and exterior for different geometric data types. The data types can be either 0, 1 or 2-dimensional."
  },
  {
    "objectID": "bigdata_lec9.html#topological-spatial-relations-2",
    "href": "bigdata_lec9.html#topological-spatial-relations-2",
    "title": "Big Data: GeoScience part 3",
    "section": "Topological spatial relations",
    "text": "Topological spatial relations\n\n\n\nSpatial predicates\n\n\nWhen testing how two geometries relate to each other, the DE-9IM model gives a result which is called spatial predicate (also called as binary predicate).\n\nintersects,\nwithin,\ncontains,\noverlaps\ntouches\n\nThere are plenty of topological relations: altogether 512 with 2D data."
  },
  {
    "objectID": "bigdata_lec9.html#topological-spatial-relations-3",
    "href": "bigdata_lec9.html#topological-spatial-relations-3",
    "title": "Big Data: GeoScience part 3",
    "section": "Topological spatial relations",
    "text": "Topological spatial relations\n\nEight common spatial predicates formed based on spatial relations between two geometries. Modified after Egenhofer et al. (1992)."
  },
  {
    "objectID": "bigdata_lec9.html#topological-spatial-relations-4",
    "href": "bigdata_lec9.html#topological-spatial-relations-4",
    "title": "Big Data: GeoScience part 3",
    "section": "Topological spatial relations",
    "text": "Topological spatial relations\n\n\n\nTypes\n\n\n\nWhen the geometries have at least one point in common, the geometries are said to be intersecting with each other.\nWhen two geometries touch each other, they have at least one point in common (at the border in this case), but their interiors do not intersect with each other.\nWhen the interiors of the geometries A and B are partially on top of each other and partially outside of each other, the geometries are overlapping with each other.\nThe spatial predicate for covers is when the interior of geometry B is almost totally within A, but they share at least one common coordinate at the border.\nSimilarly, if geometry A is almost totally contained by the geometry B (except at least one common coordinate at the border) the spatial predicate is called covered by."
  },
  {
    "objectID": "bigdata_lec9.html#topological-spatial-relations-5",
    "href": "bigdata_lec9.html#topological-spatial-relations-5",
    "title": "Big Data: GeoScience part 3",
    "section": "Topological spatial relations",
    "text": "Topological spatial relations\n\n\n\nMaking spatial queries in Python\n\n\nIn Python, all the basic spatial predicates are available from shapely library, including:\n\n.intersects()\n.within()\n.contains()\n.overlaps()\n.touches()\n.covers()\n.covered_by()\n.equals()\n.disjoint()\n.crosses()"
  },
  {
    "objectID": "bigdata_lec9.html#topological-spatial-relations-6",
    "href": "bigdata_lec9.html#topological-spatial-relations-6",
    "title": "Big Data: GeoScience part 3",
    "section": "Topological spatial relations",
    "text": "Topological spatial relations\n\n\n\nMaking spatial queries in Python: within()\n\n\nCreate a couple of Point objects and one Polygon object which we can use to test how they relate to each other:\n\nfrom shapely import Point, Polygon\n\n# Create Point objects\npoint1 = Point(24.952242, 60.1696017)\npoint2 = Point(24.976567, 60.1612500)\n\n# Create a Polygon\ncoordinates = [\n    (24.950899, 60.169158),\n    (24.953492, 60.169158),\n    (24.953510, 60.170104),\n    (24.950958, 60.169990),\n]\npolygon = Polygon(coordinates)"
  },
  {
    "objectID": "bigdata_lec9.html#topological-spatial-relations-7",
    "href": "bigdata_lec9.html#topological-spatial-relations-7",
    "title": "Big Data: GeoScience part 3",
    "section": "Topological spatial relations",
    "text": "Topological spatial relations\n\n\n\nMaking spatial queries in Python\n\n\nWe can check the contents of the new variables by printing them to the screen, for example, in which case we would see\n\nprint(point1)\nprint(point2)\nprint(polygon)\n\nPOINT (24.952242 60.1696017)\nPOINT (24.976567 60.16125)\nPOLYGON ((24.950899 60.169158, 24.953492 60.169158, 24.95351 60.170104, 24.950958 60.16999, 24.950899 60.169158))\n\n\nIf you want to test whether these Point geometries stored in point1 and point2 are within the polygon, you can call the .within() method as follows:\n\npoint1.within(polygon)\n\nTrue\n\n\n\npoint2.within(polygon)\n\nFalse"
  },
  {
    "objectID": "bigdata_lec9.html#topological-spatial-relations-8",
    "href": "bigdata_lec9.html#topological-spatial-relations-8",
    "title": "Big Data: GeoScience part 3",
    "section": "Topological spatial relations",
    "text": "Topological spatial relations\n\n\n\nMaking spatial queries in Python\n\n\nOne of the most common spatial queries is to see if a geometry intersects or touches another one. Again, there are binary operations in shapely for checking these spatial relationships:\n\n.intersects() - Two objects intersect if the boundary or interior of one object intersect in any way with the boundary or interior of the other object.\n.touches() - Two objects touch if the objects have at least one point in common and their interiors do not intersect with any part of the other object."
  },
  {
    "objectID": "bigdata_lec9.html#topological-spatial-relations-9",
    "href": "bigdata_lec9.html#topological-spatial-relations-9",
    "title": "Big Data: GeoScience part 3",
    "section": "Topological spatial relations",
    "text": "Topological spatial relations\n\n\n\nMaking spatial queries in Python\n\n\nLet’s try these by creating two LineString geometries and test whether they intersect and touch each other:\n\nfrom shapely import LineString, MultiLineString\n\n# Create two lines\nline_a = LineString([(0, 0), (1, 1)])\nline_b = LineString([(1, 1), (0, 2)])\n\n\nline_a.intersects(line_b)\n\nTrue\n\n\n\nline_a.touches(line_b)\n\nTrue"
  },
  {
    "objectID": "bigdata_lec9.html#topological-spatial-relations-10",
    "href": "bigdata_lec9.html#topological-spatial-relations-10",
    "title": "Big Data: GeoScience part 3",
    "section": "Topological spatial relations",
    "text": "Topological spatial relations\n\n\n\nMaking spatial queries in Python\n\n\nAs we can see, it seems that our two LineString objects are both intersecting and touching each other. We can confirm this by plotting the features together as a MultiLineString:\n\n# Create a MultiLineString from line_a and line_b\nmulti_line = MultiLineString([line_a, line_b])\nmulti_line"
  },
  {
    "objectID": "bigdata_lec9.html#topological-spatial-relations-11",
    "href": "bigdata_lec9.html#topological-spatial-relations-11",
    "title": "Big Data: GeoScience part 3",
    "section": "Topological spatial relations",
    "text": "Topological spatial relations\n\n\n\nMaking spatial queries in Python\n\n\nIf the lines are fully overlapping with each other they don’t touch due to the spatial relationship rule in the DE-9IM. We can confirm this by checking if line_a touches itself:\n\nline_a.touches(line_a)\n\nFalse\n\n\nNo it doesn’t. However, .intersects() and .equals() should produce True for a case when we compare the line_a with itself:\n\nprint(\"Intersects?\", line_a.intersects(line_a))\nprint(\"Equals?\", line_a.equals(line_a))\n\nIntersects? True\nEquals? True"
  },
  {
    "objectID": "bigdata_lec9.html#topological-spatial-relations-12",
    "href": "bigdata_lec9.html#topological-spatial-relations-12",
    "title": "Big Data: GeoScience part 3",
    "section": "Topological spatial relations",
    "text": "Topological spatial relations\n\n\n\nMaking spatial queries in Python\n\n\nThe following prints results for all predicates between the point1 and the polygon which we created earlier:\n\nprint(\"Intersects?\", point1.intersects(polygon))\nprint(\"Within?\", point1.within(polygon))\nprint(\"Contains?\", point1.contains(polygon))\nprint(\"Overlaps?\", point1.overlaps(polygon))\nprint(\"Touches?\", point1.touches(polygon))\nprint(\"Covers?\", point1.covers(polygon))\nprint(\"Covered by?\", point1.covered_by(polygon))\nprint(\"Equals?\", point1.equals(polygon))\nprint(\"Disjoint?\", point1.disjoint(polygon))\nprint(\"Crosses?\", point1.crosses(polygon))\n\nIntersects? True\nWithin? True\nContains? False\nOverlaps? False\nTouches? False\nCovers? False\nCovered by? True\nEquals? False\nDisjoint? False\nCrosses? False"
  },
  {
    "objectID": "bigdata_lec9.html#topological-spatial-relations-13",
    "href": "bigdata_lec9.html#topological-spatial-relations-13",
    "title": "Big Data: GeoScience part 3",
    "section": "Topological spatial relations",
    "text": "Topological spatial relations\n\n\n\n\n\n\nwithin vs contains\n\n\n\nif you have many points and just one polygon and you try to find out which one of them is inside the polygon: You might need to check the separately for each point to see which one is .within() the polygon.\nif you have many polygons and just one point and you want to find out which polygon contains the point: You might need to check separately for each polygon to see which one(s) .contains() the point."
  },
  {
    "objectID": "bigdata_lec9.html#spatial-relations",
    "href": "bigdata_lec9.html#spatial-relations",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial relations",
    "text": "Spatial relations\n\n\n\nSpatial queries using geopandas\n\n\nLet’s check which points are located within specific areas of Helsinki:\n\nimport geopandas as gpd\n\npoints = gpd.read_file(\"_data/Helsinki/addresses.shp\")\ndistricts = gpd.read_file(\"_data/Helsinki/Major_districts.gpkg\")\n\n\nprint(\"Shape:\", points.shape)\nprint(points.head())\n\nShape: (34, 1)\n                    geometry\n0   POINT (24.91556 60.1632)\n1  POINT (24.93166 60.16905)\n2  POINT (24.94168 60.16996)\n3  POINT (24.97865 60.19005)\n4  POINT (24.92151 60.15662)\n\n\n\nprint(\"Shape:\", districts.shape)\nprint(districts.tail(5))\n\nShape: (23, 3)\n           Name Description                                           geometry\n18    Koivukylä              POLYGON Z ((24.99423 60.33296 0, 25.00007 60.3...\n19      Itäinen              POLYGON Z ((25.03517 60.23627 0, 25.03585 60.2...\n20  Östersundom              POLYGON Z ((25.23352 60.25655 0, 25.23744 60.2...\n21     Hakunila              POLYGON Z ((25.08472 60.27248 0, 25.08492 60.2...\n22        Korso              POLYGON Z ((25.1238 60.34191 0, 25.11997 60.34...\n\n\nThe data contains 34 address points and 23 district polygons."
  },
  {
    "objectID": "bigdata_lec9.html#spatial-relations-1",
    "href": "bigdata_lec9.html#spatial-relations-1",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial relations",
    "text": "Spatial relations\n\n\n\nSpatial queries using geopandas\n\n\nLet’s find all points that are within two areas in Helsinki region, namely Itäinen and Eteläinen (‘Eastern’ and ‘Southern’ in English).\n\nselection = districts.loc[districts[\"Name\"].isin([\"Itäinen\", \"Eteläinen\"])]\nprint(selection.head())\n\n         Name Description                                           geometry\n10  Eteläinen              POLYGON Z ((24.78277 60.09997 0, 24.81973 60.1...\n19    Itäinen              POLYGON Z ((25.03517 60.23627 0, 25.03585 60.2..."
  },
  {
    "objectID": "bigdata_lec9.html#spatial-relations-2",
    "href": "bigdata_lec9.html#spatial-relations-2",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial relations",
    "text": "Spatial relations\n\n\n\nSpatial queries using geopandas\n\n\nLet’s now plot the layers on top of each other. The areas with red color represent the districts that we want to use for testing the spatial relationships against the point layer (shown with blue color):\n\nbase = districts.plot(facecolor=\"gray\")\nselection.plot(ax=base, facecolor=\"red\")\npoints.plot(ax=base, color=\"blue\", markersize=5)"
  },
  {
    "objectID": "bigdata_lec9.html#spatial-relations-3",
    "href": "bigdata_lec9.html#spatial-relations-3",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial relations",
    "text": "Spatial relations\n\n\n\nSpatial queries using geopandas\n\n\nWe should check which Points in the points GeoDataFrame are within the selected polygons stored in the selection geodataframe.\nWe will use a spatial join as an efficient way to conduct spatial queries in geopandas.\n\nselected_points = points.sjoin(selection.geometry.to_frame(), predicate=\"within\")\n\n\nax = districts.plot(facecolor=\"gray\")\nax = selection.plot(ax=ax, facecolor=\"red\")\nax = selected_points.plot(ax=ax, color=\"gold\", markersize=2)"
  },
  {
    "objectID": "bigdata_lec9.html#spatial-relations-4",
    "href": "bigdata_lec9.html#spatial-relations-4",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial relations",
    "text": "Spatial relations\n\n\n\nto_frame()\n\n\nUsing to_frame() allows us to avoid attaching any extra attributes from the selection geodataframe to our data, which is what .sjoin() method would normally do (and which it is actually designed for). .geometry.to_frame() will:\n\nfirst select the geometry column from the selection layer\nand then convert it into a GeoDataFrame (which would otherwise be a GeoSeries).\n\nAn alternative approach for doing the same thing is to use selection[[selection.active_geometry_name]], which also returns a GeoDataFrame containing only a column with the geodataframe’s active geometry."
  },
  {
    "objectID": "bigdata_lec9.html#spatial-relations-5",
    "href": "bigdata_lec9.html#spatial-relations-5",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial relations",
    "text": "Spatial relations\n\n\n\nsjoin() examples\n\n\nBy default, the .sjoin() uses \"intersects\" as a spatial predicate, but it is easy to change this. For example, we can investigate which of the districts contain at least one point.\n\ndistricts_with_points = districts.sjoin(\n    points.geometry.to_frame(), predicate=\"contains\"\n)\n\n\nax = districts.plot(facecolor=\"gray\")\nax = districts_with_points.plot(ax=ax, edgecolor=\"gray\")\nax = points.plot(ax=ax, color=\"red\", markersize=2)"
  },
  {
    "objectID": "bigdata_lec9.html#spatial-relations-6",
    "href": "bigdata_lec9.html#spatial-relations-6",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial relations",
    "text": "Spatial relations\n\n\n\nsjoin() examples\n\n\nTo find all possible spatial predicates for a given GeoDataFrame you can call:\n\ndistricts.sindex.valid_query_predicates\n\n{None,\n 'contains',\n 'contains_properly',\n 'covered_by',\n 'covers',\n 'crosses',\n 'dwithin',\n 'intersects',\n 'overlaps',\n 'touches',\n 'within'}\n\n\nWhat is .sindex?\n\ndistricts.sindex\n\n&lt;geopandas.sindex.SpatialIndex at 0x1216f7dd0&gt;"
  },
  {
    "objectID": "bigdata_lec9.html#spatial-relations-7",
    "href": "bigdata_lec9.html#spatial-relations-7",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial relations",
    "text": "Spatial relations\n\n\n\nsindex\n\n\nSpatialIndex object contains the spatial index for our data.\n\nA spatial index is a special data structure that allows for efficient querying of spatial data.\ngeopandas uses a spatial index called R-tree which is a hierarchical, tree-like structure that divides the space into nested, overlapping rectangles and indexes the bounding boxes of each geometry.\nHence, when selecting data based on topological relations, we recommend using .sjoin() instead of directly calling .within(), .contains() that come with the shapely geometries (as shown previously)."
  },
  {
    "objectID": "bigdata_lec9.html#spatial-relations-8",
    "href": "bigdata_lec9.html#spatial-relations-8",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial relations",
    "text": "Spatial relations\n\n\n\nExercise\n\n\nHow many addresses are located in each district? You can find out the answer by grouping the spatial join result based on the district name."
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join",
    "href": "bigdata_lec9.html#spatial-join",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\n\n\nDescription\n\n\nSpatial join retrieves table attributes from one layer and transfers them into another layer based on their spatial relationship. For example, one can join the attributes of a polygon layer into a point layer where each point would get the attributes of a polygon that intersects with the point.\nIt is good to remember that spatial join is always conducted between two layers at a time."
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-1",
    "href": "bigdata_lec9.html#spatial-join-1",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\nSpatial join allows you to combine attribute information from multiple layers based on spatial relationship."
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-2",
    "href": "bigdata_lec9.html#spatial-join-2",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\n\n\nsjoin details\n\n\nIn spatial join, there are two set of options that you can control, which ultimately influence how the data is transferred between the layers:\n\nHow the spatial relationship between geometries should be checked (i.e. spatial predicates), and\nWhat type of table join you want to conduct (inner, left, or right outer join)\n\n\n\n\n\n\n\nSpatial predicates\n\n\n\nThe spatial predicates control how the spatial relationship between the geometries in the two data layers is checked.\nOnly those cases where the spatial predicate returns True will be kept in the result."
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-3",
    "href": "bigdata_lec9.html#spatial-join-3",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join"
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-4",
    "href": "bigdata_lec9.html#spatial-join-4",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\n\n\nsjoin type\n\n\nThe other parameter that you can use to control how the spatial join is conducted is the spatial join type. There are three different join types that influence the outcome of the spatial join:\n\ninner join\nleft outer join\nright outer join"
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-5",
    "href": "bigdata_lec9.html#spatial-join-5",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\n\n\nSpatial join with Python\n\n\nSpatial join can be done easily with geopandas using the .sjoin() method. Next, we will learn how to use this method to perform a spatial join between two layers:\n\naddresses which are the locations that we geocoded previously;\npopulation grid which is a 250m x 250m grid polygon layer that contains population information from the Helsinki Region (source: Helsinki Region Environmental Services Authority)."
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-6",
    "href": "bigdata_lec9.html#spatial-join-6",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\n\n\nSpatial join with Python\n\n\nLet’s start by reading the data:\n\nimport geopandas as gpd\n\naddr_fp = \"_data/Helsinki/addresses.shp\"\naddresses = gpd.read_file(addr_fp)\naddresses.head(2)\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOINT (24.91556 60.1632)\n\n\n1\nPOINT (24.93166 60.16905)\n\n\n\n\n\n\n\nAs we can see, the addresses variable contains address Points which represent a selection of public transport stations in the Helsinki Region."
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-7",
    "href": "bigdata_lec9.html#spatial-join-7",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\n\n\nSpatial join with Python\n\n\n\npop_grid_fp = \"_data/Helsinki/Population_grid_2021_HSY.gpkg\"\npop_grid = gpd.read_file(pop_grid_fp)\npop_grid.head(2)\n\n\n\n\n\n\n\n\nid\ninhabitants\noccupancy_rate\ngeometry\n\n\n\n\n0\nVaestotietoruudukko_2021.1\n5\n50.60\nPOLYGON ((25472499.995 6689749.005, 25472499.9...\n\n\n1\nVaestotietoruudukko_2021.2\n7\n36.71\nPOLYGON ((25472499.995 6685998.998, 25472499.9...\n\n\n\n\n\n\n\nThe pop_grid dataset contains few columns, namely a unique id, the number of inhabitants per grid cell, and the occupancy_rate as percentage."
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-8",
    "href": "bigdata_lec9.html#spatial-join-8",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\n\n\nPreparations for spatial join\n\n\nThe basic requirement for a successful spatial join is that the layers should overlap with each other in space. If the geometries between the layers do not share the same CRS, it is very likely that the spatial join will fail and produces an empty GeoDataFrame.\n\nprint(\"Address points CRS:\", addresses.crs)\nprint(\"Population grid CRS:\", pop_grid.crs.name)\n\nAddress points CRS: None\nPopulation grid CRS: ETRS89 / GK25FIN\n\n\nTo fix this issue, let’s reproject the geometries in the addresses GeoDataFrame to the same CRS as pop_grid using the .to_crs() method.\n\n# Reproject\naddresses = addresses.set_crs(epsg=4326)\naddresses = addresses.to_crs(crs=pop_grid.crs)\n\n# Validate match\naddresses.crs == pop_grid.crs\n\nTrue\n\n\n\naddresses.head(2)\n\n\n\n\n\n\n\n\ngeometry\n\n\n\n\n0\nPOINT (25495311.608 6672258.695)\n\n\n1\nPOINT (25496206.216 6672909.016)"
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-9",
    "href": "bigdata_lec9.html#spatial-join-9",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\n\n\nPreparations for spatial join\n\n\nAs a last preparatory step, let’s visualize both datasets on top of each other to see how the inhabitants are distributed over the region, and how the address points are located in relation to the grid:"
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-10",
    "href": "bigdata_lec9.html#spatial-join-10",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\n\n\nJoin the layers based on spatial relationship\n\n\nThe aim here is to get information about\nHow many people live in a given polygon that contains an individual address-point?\nThus, we want to join the attribute information from the pop_grid layer into the addresses Point layer using the .sjoin() method.\n\njoin = addresses.sjoin(pop_grid, predicate=\"within\", how=\"inner\")\njoin\n\n\n\n\n\n\n\n\ngeometry\nindex_right\nid\ninhabitants\noccupancy_rate\n\n\n\n\n0\nPOINT (25495311.608 6672258.695)\n3262\nVaestotietoruudukko_2021.3263\n505\n14.01\n\n\n1\nPOINT (25496206.216 6672909.016)\n3381\nVaestotietoruudukko_2021.3382\n172\n27.67\n\n\n2\nPOINT (25496762.723 6673010.538)\n3504\nVaestotietoruudukko_2021.3505\n43\n61.44\n\n\n3\nPOINT (25498815.415 6675246.744)\n3845\nVaestotietoruudukko_2021.3846\n757\n33.98\n\n\n4\nPOINT (25495641.151 6671525.076)\n3310\nVaestotietoruudukko_2021.3311\n1402\n29.76\n\n\n5\nPOINT (25504528.607 6680282.118)\n5058\nVaestotietoruudukko_2021.5059\n283\n20.51\n\n\n6\nPOINT (25506082.816 6678702.5)\n5407\nVaestotietoruudukko_2021.5408\n155\n43.42\n\n\n7\nPOINT (25501631.384 6685110.943)\n4302\nVaestotietoruudukko_2021.4303\n236\n37.62\n\n\n8\nPOINT (25501586.789 6683452.706)\n4309\nVaestotietoruudukko_2021.4310\n253\n33.56\n\n\n10\nPOINT (25496896.718 6673162.114)\n3504\nVaestotietoruudukko_2021.3505\n43\n61.44\n\n\n11\nPOINT (25493506.585 6679793.504)\n2985\nVaestotietoruudukko_2021.2986\n629\n36.11\n\n\n12\nPOINT (25493093.193 6680589.107)\n2886\nVaestotietoruudukko_2021.2887\n817\n29.63\n\n\n13\nPOINT (25497118.126 6678784.269)\n3535\nVaestotietoruudukko_2021.3536\n163\n33.08\n\n\n14\nPOINT (25500717.139 6682045.953)\n4114\nVaestotietoruudukko_2021.4115\n99\n33.06\n\n\n15\nPOINT (25494134.034 6678278.391)\n3079\nVaestotietoruudukko_2021.3080\n425\n37.04\n\n\n16\nPOINT (25492575.043 6681972.681)\n2775\nVaestotietoruudukko_2021.2776\n353\n32.35\n\n\n17\nPOINT (25498089.949 6679640.055)\n3728\nVaestotietoruudukko_2021.3729\n255\n38.51\n\n\n19\nPOINT (25492287.623 6679040.234)\n2723\nVaestotietoruudukko_2021.2724\n231\n33.98\n\n\n20\nPOINT (25499646.711 6681218.844)\n3945\nVaestotietoruudukko_2021.3946\n336\n35.39\n\n\n22\nPOINT (25504344.005 6677452.185)\n5008\nVaestotietoruudukko_2021.5009\n453\n21.73\n\n\n23\nPOINT (25507495.008 6677173.504)\n5626\nVaestotietoruudukko_2021.5627\n231\n35.18\n\n\n24\nPOINT (25504169.174 6679129.384)\n4942\nVaestotietoruudukko_2021.4943\n638\n33.41\n\n\n25\nPOINT (25506082.506 6680577.96)\n5399\nVaestotietoruudukko_2021.5400\n361\n24.29\n\n\n26\nPOINT (25497840.365 6675020.358)\n3696\nVaestotietoruudukko_2021.3697\n829\n33.92\n\n\n27\nPOINT (25501570.947 6675734.321)\n4330\nVaestotietoruudukko_2021.4331\n271\n37.29\n\n\n28\nPOINT (25500377.732 6675098.523)\n4079\nVaestotietoruudukko_2021.4080\n321\n38.40\n\n\n29\nPOINT (25497199.344 6674065.972)\n3552\nVaestotietoruudukko_2021.3553\n215\n43.71\n\n\n30\nPOINT (25496286.911 6672913.768)\n3411\nVaestotietoruudukko_2021.3412\n286\n25.86\n\n\n31\nPOINT (25496128.995 6672625.288)\n3382\nVaestotietoruudukko_2021.3383\n1409\n32.37\n\n\n32\nPOINT (25495624.409 6671766.187)\n3309\nVaestotietoruudukko_2021.3310\n995\n37.07\n\n\n33\nPOINT (25497062.747 6673226.621)\n3555\nVaestotietoruudukko_2021.3556\n281\n36.14"
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-11",
    "href": "bigdata_lec9.html#spatial-join-11",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\n\n\nJoin the layers based on spatial relationship\n\n\n\nWe received information about inhabitants and occupancy_rate\nWe got columns index_right and id_right which tell the index and id of the matching polygon in the right-side member of the spatial join\nThe id column in the left-side member of the spatial join was renamed as id_left.\nThe suffices _left and _right are appended to the column names to differentiate the columns in cases where there are identical column names present in both GeoDataFrames."
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-12",
    "href": "bigdata_lec9.html#spatial-join-12",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\n\n\nJoin the layers based on spatial relationship\n\n\nLet’s also visualize the joined output. In the following, we plot the points using the inhabitants column to indicate the color: \n\nax = join.plot(\n    column=\"inhabitants\",\n    cmap=\"Reds\",\n    markersize=15,\n    scheme=\"quantiles\",\n    legend=True,\n    figsize=(10, 6),\n)\nax.set_title(\"Amount of inhabitants living close to the point\");"
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-13",
    "href": "bigdata_lec9.html#spatial-join-13",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\n\n\nJoin the layers based on spatial relationship\n\n\nIt is useful to investigate if we lost any data while doing the spatial join. Let’s check this by comparing the number of rows in our result to how many addresses we had originally:\n\nlen(addresses) - len(join)\n\n3\n\n\nWe can investigate where the points outside of polygons are located:\n\nm = pop_grid.explore(color=\"blue\", style_kwds=dict(color=\"blue\", stroke=False))\naddresses.explore(m=m, color=\"red\")\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-14",
    "href": "bigdata_lec9.html#spatial-join-14",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\n\n\nJoin the layers based on spatial relationship\n\n\nWe might want to keep the information for the points that did not get a match based on the spatial relationship.\n\nleft_join = addresses.sjoin(pop_grid, predicate=\"within\", how=\"left\")\nleft_join\n\n\n\n\n\n\n\n\ngeometry\nindex_right\nid\ninhabitants\noccupancy_rate\n\n\n\n\n0\nPOINT (25495311.608 6672258.695)\n3262.0\nVaestotietoruudukko_2021.3263\n505.0\n14.01\n\n\n1\nPOINT (25496206.216 6672909.016)\n3381.0\nVaestotietoruudukko_2021.3382\n172.0\n27.67\n\n\n2\nPOINT (25496762.723 6673010.538)\n3504.0\nVaestotietoruudukko_2021.3505\n43.0\n61.44\n\n\n3\nPOINT (25498815.415 6675246.744)\n3845.0\nVaestotietoruudukko_2021.3846\n757.0\n33.98\n\n\n4\nPOINT (25495641.151 6671525.076)\n3310.0\nVaestotietoruudukko_2021.3311\n1402.0\n29.76\n\n\n5\nPOINT (25504528.607 6680282.118)\n5058.0\nVaestotietoruudukko_2021.5059\n283.0\n20.51\n\n\n6\nPOINT (25506082.816 6678702.5)\n5407.0\nVaestotietoruudukko_2021.5408\n155.0\n43.42\n\n\n7\nPOINT (25501631.384 6685110.943)\n4302.0\nVaestotietoruudukko_2021.4303\n236.0\n37.62\n\n\n8\nPOINT (25501586.789 6683452.706)\n4309.0\nVaestotietoruudukko_2021.4310\n253.0\n33.56\n\n\n9\nPOINT (25492851.783 6678869.234)\nNaN\nNaN\nNaN\nNaN\n\n\n10\nPOINT (25496896.718 6673162.114)\n3504.0\nVaestotietoruudukko_2021.3505\n43.0\n61.44\n\n\n11\nPOINT (25493506.585 6679793.504)\n2985.0\nVaestotietoruudukko_2021.2986\n629.0\n36.11\n\n\n12\nPOINT (25493093.193 6680589.107)\n2886.0\nVaestotietoruudukko_2021.2887\n817.0\n29.63\n\n\n13\nPOINT (25497118.126 6678784.269)\n3535.0\nVaestotietoruudukko_2021.3536\n163.0\n33.08\n\n\n14\nPOINT (25500717.139 6682045.953)\n4114.0\nVaestotietoruudukko_2021.4115\n99.0\n33.06\n\n\n15\nPOINT (25494134.034 6678278.391)\n3079.0\nVaestotietoruudukko_2021.3080\n425.0\n37.04\n\n\n16\nPOINT (25492575.043 6681972.681)\n2775.0\nVaestotietoruudukko_2021.2776\n353.0\n32.35\n\n\n17\nPOINT (25498089.949 6679640.055)\n3728.0\nVaestotietoruudukko_2021.3729\n255.0\n38.51\n\n\n18\nPOINT (25496358.8 6676198.28)\nNaN\nNaN\nNaN\nNaN\n\n\n19\nPOINT (25492287.623 6679040.234)\n2723.0\nVaestotietoruudukko_2021.2724\n231.0\n33.98\n\n\n20\nPOINT (25499646.711 6681218.844)\n3945.0\nVaestotietoruudukko_2021.3946\n336.0\n35.39\n\n\n21\nPOINT (25501142.787 6681208.443)\nNaN\nNaN\nNaN\nNaN\n\n\n22\nPOINT (25504344.005 6677452.185)\n5008.0\nVaestotietoruudukko_2021.5009\n453.0\n21.73\n\n\n23\nPOINT (25507495.008 6677173.504)\n5626.0\nVaestotietoruudukko_2021.5627\n231.0\n35.18\n\n\n24\nPOINT (25504169.174 6679129.384)\n4942.0\nVaestotietoruudukko_2021.4943\n638.0\n33.41\n\n\n25\nPOINT (25506082.506 6680577.96)\n5399.0\nVaestotietoruudukko_2021.5400\n361.0\n24.29\n\n\n26\nPOINT (25497840.365 6675020.358)\n3696.0\nVaestotietoruudukko_2021.3697\n829.0\n33.92\n\n\n27\nPOINT (25501570.947 6675734.321)\n4330.0\nVaestotietoruudukko_2021.4331\n271.0\n37.29\n\n\n28\nPOINT (25500377.732 6675098.523)\n4079.0\nVaestotietoruudukko_2021.4080\n321.0\n38.40\n\n\n29\nPOINT (25497199.344 6674065.972)\n3552.0\nVaestotietoruudukko_2021.3553\n215.0\n43.71\n\n\n30\nPOINT (25496286.911 6672913.768)\n3411.0\nVaestotietoruudukko_2021.3412\n286.0\n25.86\n\n\n31\nPOINT (25496128.995 6672625.288)\n3382.0\nVaestotietoruudukko_2021.3383\n1409.0\n32.37\n\n\n32\nPOINT (25495624.409 6671766.187)\n3309.0\nVaestotietoruudukko_2021.3310\n995.0\n37.07\n\n\n33\nPOINT (25497062.747 6673226.621)\n3555.0\nVaestotietoruudukko_2021.3556\n281.0\n36.14"
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-15",
    "href": "bigdata_lec9.html#spatial-join-15",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\n\n\nJoin the layers based on spatial relationship\n\n\nLet’s investigate a bit more to see which rows did not have a matching polygon in the population grid.\n\nleft_join.loc[left_join[\"inhabitants\"].isnull()]\n\n\n\n\n\n\n\n\ngeometry\nindex_right\nid\ninhabitants\noccupancy_rate\n\n\n\n\n9\nPOINT (25492851.783 6678869.234)\nNaN\nNaN\nNaN\nNaN\n\n\n18\nPOINT (25496358.8 6676198.28)\nNaN\nNaN\nNaN\nNaN\n\n\n21\nPOINT (25501142.787 6681208.443)\nNaN\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "bigdata_lec9.html#spatial-join-16",
    "href": "bigdata_lec9.html#spatial-join-16",
    "title": "Big Data: GeoScience part 3",
    "section": "Spatial join",
    "text": "Spatial join\n\n\n\nExercise\n\n\nDo the spatial join another way around, i.e. make a spatial join where you join information from the address points into the population grid. How does the result differ from the version where we joined information from the grids to the points? What would be the benefit of doing the join this way around?"
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis",
    "text": "Nearest neighbour analysis\n\n\n\nDescription\n\n\n\na fundamental concept in geographic data analysis and modelling.\nmany of these techniques rely on the idea that proximity in geographic space typically indicates also similarity in attribute space.\n\n\n\n\n\n\n\n\n\n\nFirst Law of Geography\n\n\nEverything is related to everything, but near things are more related than distant things."
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-1",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-1",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis",
    "text": "Nearest neighbour analysis\n\nThe basic idea of finding a nearest neighbour based on geographic distance."
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-2",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-2",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis",
    "text": "Nearest neighbour analysis\n\n\n\nWhy limit?\n\n\n\ncomputational limitations\nwe have specific reasoning that makes it sensible to limit the search area."
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nOverview\n\n\nTypical tasks:\n\nfind the nearest neighbors for all Point geometries in a given GeoDataFrame based on Pointobjects from another GeoDataFrame\nfind nearest neighbor between two Polygon datasets\nuse scipy library to find K-Nearest Neighbors (KNN) with Point data."
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-1",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-1",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nPractical example\n\n\nWhere is the closest public transport stop from my place of residence?\n\nOur aim is to search for each building point in the Helsinki Region the closest public transport stop.\nIn geopandas, we can find nearest neighbors for all geometries in a given GeoDataFrame using the .sjoin_nearest() method.\n\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\nstops = gpd.read_file(\"_data/Helsinki/pt_stops_helsinki.gpkg\")\nbuilding_points = gpd.read_file(\"_data/Helsinki/building_points_helsinki.zip\")\n\nprint(\"Number of stops:\", len(stops))\nstops.head(2)\n\nNumber of stops: 8377\n\n\n\n\n\n\n\n\n\nstop_name\nstop_lat\nstop_lon\nstop_id\ngeometry\n\n\n\n\n0\nRitarihuone\n60.16946\n24.95667\n1010102\nPOINT (24.95667 60.16946)\n\n\n1\nKirkkokatu\n60.17127\n24.95657\n1010103\nPOINT (24.95657 60.17127)\n\n\n\n\n\n\n\n\nprint(\"Number of buildings:\", len(building_points))\nbuilding_points.head(2)\n\nNumber of buildings: 158731\n\n\n\n\n\n\n\n\n\nname\ngeometry\n\n\n\n\n0\nNone\nPOINT (24.85584 60.20727)\n\n\n1\nUimastadion\nPOINT (24.93045 60.18882)"
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-2",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-2",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nPractical example: visualize\n\n\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 10))\n\n# Plot buildings\nbuilding_points.plot(ax=ax1, markersize=0.2, alpha=0.5)\nax1.set_title(\"Buildings\")\n\n# Plot stops\nstops.plot(ax=ax2, markersize=0.2, alpha=0.5, color=\"red\")\nax2.set_title(\"Stops\");"
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-3",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-3",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nsjoin_nearest\n\n\n\nmerges in a similar way to .sjoin() method\nhowever, in this case the method is actually searching for the closest geometries instead of relying on spatial predicates, such as within\nthe method uses a spatial index called STRTree which is an efficient implementation of the R-tree dynamic index structure for spatial searching."
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-4",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-4",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nReproject\n\n\nLet’s start by reprojecting our latitude and longitude values into a metric system using the national EUREF-FIN coordinate reference system (EPSG code 3067) for Finland:\n\nstops = stops.to_crs(epsg=3067)\nbuilding_points = building_points.to_crs(epsg=3067)\n\nstops.head(2)\n\n\n\n\n\n\n\n\nstop_name\nstop_lat\nstop_lon\nstop_id\ngeometry\n\n\n\n\n0\nRitarihuone\n60.16946\n24.95667\n1010102\nPOINT (386623.301 6672037.884)\n\n\n1\nKirkkokatu\n60.17127\n24.95657\n1010103\nPOINT (386623.991 6672239.572)"
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-5",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-5",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nUse sjoin_nearest\n\n\nBecause we are interested to find the closest stop geometry for each building, the buildings GeoDataFrame is on the left hand side of the command.\nNote that we give a name for a column which is used to store information about the distance between a given building and the closest stop (this is optional):\n\n%time\nclosest = building_points.sjoin_nearest(stops, distance_col=\"distance\")\nclosest\n\nCPU times: user 0 ns, sys: 1 μs, total: 1 μs\nWall time: 1.19 μs\n\n\n\n\n\n\n\n\n\nname\ngeometry\nindex_right\nstop_name\nstop_lat\nstop_lon\nstop_id\ndistance\n\n\n\n\n0\nNone\nPOINT (381166.6 6676424.438)\n1131\nMuusantori\n60.20749\n24.857450\n1304138\n92.679893\n\n\n1\nUimastadion\nPOINT (385236.565 6674238.472)\n467\nAuroran sairaala\n60.19145\n24.925540\n1171122\n400.243370\n\n\n2\nNone\nPOINT (386317.478 6672100.648)\n61\nSenaatintori\n60.16901\n24.950460\n1020450\n109.819633\n\n\n3\nHartwall Arena\nPOINT (385225.109 6676120.56)\n532\nVeturitie\n60.20661\n24.929680\n1174112\n104.632434\n\n\n4\nTalli\nPOINT (385079.733 6676989.745)\n496\nPosti 1\n60.21345\n24.917550\n1172143\n472.248282\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n158726\nNone\nPOINT (373896.051 6677472.204)\n4621\nSamis\n60.21369\n24.720970\n3170209\n195.675552\n\n\n158727\nNone\nPOINT (372425.65 6676945.528)\n4654\nYrjö Liipolan tie\n60.20922\n24.695470\n3170244\n137.137640\n\n\n158728\nNone\nPOINT (374696.625 6677972.738)\n4655\nKandidaatintie\n60.21818\n24.736987\n3170245\n135.341745\n\n\n158729\nGranhultsskolan\nPOINT (373287.582 6677731.639)\n4624\nUimahalli\n60.21638\n24.711260\n3170212\n99.408108\n\n\n158730\nKauniaisten kirkko\nPOINT (374112.695 6677330.017)\n4665\nPostitori\n60.21267\n24.728250\n3170257\n67.790422\n\n\n\n\n159818 rows × 8 columns"
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-6",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-6",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nsjoin_nearest\n\n\n\nThe last column in the table shows the distance in meters between a given building and the closest stop.\nThe distance is only returned upon request as we did by specifying distance_col=\"distance\".\nThe column index_right provides information about the index number of the closest stop in the stops GeoDataFrame.\nnumber of rows in our result has actually increased, because for some geometries in the buildings GeoDataFrame, the distance between the building and two (or more) stops have been exactly the same (i.e. they are equidistant).\nIn such cases, the sjoin_nearest() will store both records into the results by duplicating the building information and attaching information from the stops into separate rows accordingly."
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-7",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-7",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nsjoin_nearest\n\n\nWe can make computations faster by specifying a max_distance parameter that specifies the maximum search distance:\n\n%time\nclosest_limited = building_points.sjoin_nearest(\n    stops, max_distance=100, distance_col=\"distance\"\n)\nclosest_limited\n\nCPU times: user 1e+03 ns, sys: 1 μs, total: 2 μs\nWall time: 1.91 μs\n\n\n\n\n\n\n\n\n\nname\ngeometry\nindex_right\nstop_name\nstop_lat\nstop_lon\nstop_id\ndistance\n\n\n\n\n0\nNone\nPOINT (381166.6 6676424.438)\n1131\nMuusantori\n60.207490\n24.857450\n1304138\n92.679893\n\n\n10\nNone\nPOINT (384645.078 6669763.917)\n592\nHernesaaren laituri\n60.148287\n24.923281\n1204101\n57.786201\n\n\n12\nNone\nPOINT (384782.782 6669707.017)\n595\nHernesaaren laituri\n60.148680\n24.924240\n1204108\n79.844881\n\n\n13\nNone\nPOINT (384714.47 6669710.887)\n592\nHernesaaren laituri\n60.148287\n24.923281\n1204101\n32.640335\n\n\n16\nNone\nPOINT (385040.806 6670639.517)\n596\nHernesaarenkatu\n60.156110\n24.930370\n1204109\n87.888087\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n158718\nNone\nPOINT (374219.973 6677006.1)\n4651\nKauniaisten asema\n60.210830\n24.729330\n3170240\n69.803673\n\n\n158719\nNone\nPOINT (374231.494 6676967.402)\n4642\nKauniaistentie\n60.209810\n24.731510\n3170230\n63.384115\n\n\n158720\nNone\nPOINT (374602.815 6677396.18)\n4673\nRaamattuopisto\n60.213524\n24.736685\n3170265\n56.594370\n\n\n158729\nGranhultsskolan\nPOINT (373287.582 6677731.639)\n4624\nUimahalli\n60.216380\n24.711260\n3170212\n99.408108\n\n\n158730\nKauniaisten kirkko\nPOINT (374112.695 6677330.017)\n4665\nPostitori\n60.212670\n24.728250\n3170257\n67.790422\n\n\n\n\n40128 rows × 8 columns"
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-8",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-8",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nConnect with a line\n\n\n\nwe need to merge also the Point geometries from the other layer into our results, which can then be used to create a LineString connecting the points to each other.\nfor merging the closest stop geometries into our results, we can take advantage of the index_right column in our table and conduct a normal table join using the .merge() method\nwe only keep the geometry columns from the stops GeoDataFrame because all the other attributes already exist in our results:\n\n\nclosest = closest.merge(\n    stops[[stops.active_geometry_name]], left_on=\"index_right\", right_index=True\n)\nclosest.head()\n\n\n\n\n\n\n\n\nname\ngeometry_x\nindex_right\nstop_name\nstop_lat\nstop_lon\nstop_id\ndistance\ngeometry_y\n\n\n\n\n0\nNone\nPOINT (381166.6 6676424.438)\n1131\nMuusantori\n60.20749\n24.85745\n1304138\n92.679893\nPOINT (381256.66 6676446.317)\n\n\n1\nUimastadion\nPOINT (385236.565 6674238.472)\n467\nAuroran sairaala\n60.19145\n24.92554\n1171122\n400.243370\nPOINT (384973.331 6674539.973)\n\n\n2\nNone\nPOINT (386317.478 6672100.648)\n61\nSenaatintori\n60.16901\n24.95046\n1020450\n109.819633\nPOINT (386277.25 6671998.462)\n\n\n3\nHartwall Arena\nPOINT (385225.109 6676120.56)\n532\nVeturitie\n60.20661\n24.92968\n1174112\n104.632434\nPOINT (385255.784 6676220.595)\n\n\n4\nTalli\nPOINT (385079.733 6676989.745)\n496\nPosti 1\n60.21345\n24.91755\n1172143\n472.248282\nPOINT (384607.679 6677003.267)"
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-9",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-9",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nConnect with a line\n\n\n\nIn order to create a connecting LineString between the buildings and the closest stops, we can use the linestrings() function of the shapely library\nTo extract the point coordinates from the Point objects stored in the geometry_x and geometry_y columns, we use the .get_coordinates() method of geopandas that returns the x and y coordinates as Series objects/columns.\nThen we convert these into numpy arrays using the to_numpy() method which we pass to the linestrings() function.\nFinally, we store the resulting LineStrings into a column geometry which we set as the active geometry of the GeoDataFrame:\n\n\nfrom shapely import linestrings\n\nclosest[\"geometry\"] = linestrings(\n    closest.geometry_x.get_coordinates().to_numpy(),\n    closest.geometry_y.get_coordinates().to_numpy(),\n)\n\nclosest = closest.set_geometry(\"geometry\")\nclosest.head()\n\n\n\n\n\n\n\n\nname\ngeometry_x\nindex_right\nstop_name\nstop_lat\nstop_lon\nstop_id\ndistance\ngeometry_y\ngeometry\n\n\n\n\n0\nNone\nPOINT (381166.6 6676424.438)\n1131\nMuusantori\n60.20749\n24.85745\n1304138\n92.679893\nPOINT (381256.66 6676446.317)\nLINESTRING (381166.6 381256.66, 6676424.438 66...\n\n\n1\nUimastadion\nPOINT (385236.565 6674238.472)\n467\nAuroran sairaala\n60.19145\n24.92554\n1171122\n400.243370\nPOINT (384973.331 6674539.973)\nLINESTRING (385236.565 384973.331, 6674238.472...\n\n\n2\nNone\nPOINT (386317.478 6672100.648)\n61\nSenaatintori\n60.16901\n24.95046\n1020450\n109.819633\nPOINT (386277.25 6671998.462)\nLINESTRING (386317.478 386277.25, 6672100.648 ...\n\n\n3\nHartwall Arena\nPOINT (385225.109 6676120.56)\n532\nVeturitie\n60.20661\n24.92968\n1174112\n104.632434\nPOINT (385255.784 6676220.595)\nLINESTRING (385225.109 385255.784, 6676120.56 ...\n\n\n4\nTalli\nPOINT (385079.733 6676989.745)\n496\nPosti 1\n60.21345\n24.91755\n1172143\n472.248282\nPOINT (384607.679 6677003.267)\nLINESTRING (385079.733 384607.679, 6676989.745..."
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-10",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-10",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nConnect with a line\n\n\nLet’s create a map that visualizes the buildings, stops and the connecting lines between the buildings and the closest stops in a single figure:\n\nax = closest.plot(lw=0.5, color=\"blue\", figsize=(10, 10))\nax = building_points.plot(ax=ax, color=\"red\", markersize=2)\nax = stops.plot(ax=ax, color=\"black\", markersize=8.5, marker=\"s\")\n# Zoom to specific area\nax.set_xlim(382000, 384100)\nax.set_ylim(6676000, 6678000);"
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-11",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-11",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nConnect with a line\n\n\nIt is easy to do some descriptive analysis based on this data providing information about levels of access to public transport in the region:\n\nclosest[\"distance\"].describe()\n\ncount    159818.000000\nmean        229.029606\nstd         292.348698\nmin           0.743490\n25%          99.771301\n50%         163.805853\n75%         260.461391\nmax        7698.270635\nName: distance, dtype: float64"
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-12",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-12",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nNearest neighbors with Polygon and LineString data\n\n\nLet’s find the closest urban park to building polygons in a neighborhood called Kamppi, which is located in Helsinki, Finland. Then, we’ll find the closest drivable road (LineString) to each building.\n\nimport geopandas as gpd\n\nbuildings = gpd.read_file(\"_data/Helsinki/Kamppi_buildings.gpkg\")\nparks = gpd.read_file(\"_data/Helsinki/Kamppi_parks.gpkg\")\nroads = gpd.read_file(\"_data/Helsinki/Kamppi_roads.gpkg\")\nbuildings\n\n\n\n\n\n\n\n\nosmid\nbuilding\nname\ngeometry\n\n\n\n\n0\n11711721042\nyes\nNice Bike Pyörähuolto\nPOINT (384966.661 6671503.786)\n\n\n1\n8035238\npublic\nLasipalatsi\nPOLYGON ((385459.65 6672184.469, 385456.356 66...\n\n\n2\n8042297\nyes\nRadisson Blu Royal\nPOLYGON ((385104.154 6671916.693, 385101.584 6...\n\n\n3\n14797170\nschool\nNone\nPOLYGON ((384815.326 6671762.71, 384815.792 66...\n\n\n4\n14797171\nyes\nNone\nPOLYGON ((384797.759 6671853.253, 384798.253 6...\n\n\n...\n...\n...\n...\n...\n\n\n450\n8092998\nyes\nNone\nPOLYGON ((384747.465 6671811.996, 384744.27 66...\n\n\n451\n8280536\napartments\nNone\nPOLYGON ((384839.007 6671934.815, 384839.485 6...\n\n\n452\n8525159\ncivic\nNone\nPOLYGON ((385495.275 6672164.009, 385494.928 6...\n\n\n453\n8525161\ncivic\nNone\nPOLYGON ((385486.225 6672173.653, 385486.717 6...\n\n\n454\n8535506\ncivic\nNone\nPOLYGON ((385481.13 6672167.861, 385482.372 66...\n\n\n\n\n455 rows × 4 columns"
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-13",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-13",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nNearest neighbors with Polygon and LineString data\n\n\n\n# Plot buildings, parks and roads\nax = buildings.plot(color=\"gray\", figsize=(10, 10))\nax = parks.plot(ax=ax, color=\"green\")\nax = roads.plot(ax=ax, color=\"red\")"
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-14",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-14",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nNearest neighbors with Polygon and LineString data\n\n\nWe find the nearest park for each building Polygon and store the distance into the column distance:\n\nnearest_parks = buildings.sjoin_nearest(parks, distance_col=\"distance\")\nnearest_parks\n\n\n\n\n\n\n\n\nosmid_left\nbuilding\nname_left\ngeometry\nindex_right\nosmid_right\nleisure\nname_right\ndistance\n\n\n\n\n0\n11711721042\nyes\nNice Bike Pyörähuolto\nPOINT (384966.661 6671503.786)\n12\n1227991181\npark\nKaartin lasaretin puisto\n100.208527\n\n\n1\n8035238\npublic\nLasipalatsi\nPOLYGON ((385459.65 6672184.469, 385456.356 66...\n1\n8042613\npark\nSimonpuistikko\n16.284929\n\n\n2\n8042297\nyes\nRadisson Blu Royal\nPOLYGON ((385104.154 6671916.693, 385101.584 6...\n8\n37390082\npark\nNone\n40.039501\n\n\n3\n14797170\nschool\nNone\nPOLYGON ((384815.326 6671762.71, 384815.792 66...\n5\n26999855\npark\nNone\n0.000000\n\n\n4\n14797171\nyes\nNone\nPOLYGON ((384797.759 6671853.253, 384798.253 6...\n5\n26999855\npark\nNone\n14.873403\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n450\n8092998\nyes\nNone\nPOLYGON ((384747.465 6671811.996, 384744.27 66...\n5\n26999855\npark\nNone\n70.819624\n\n\n451\n8280536\napartments\nNone\nPOLYGON ((384839.007 6671934.815, 384839.485 6...\n8\n37390082\npark\nNone\n38.574646\n\n\n452\n8525159\ncivic\nNone\nPOLYGON ((385495.275 6672164.009, 385494.928 6...\n1\n8042613\npark\nSimonpuistikko\n32.792083\n\n\n453\n8525161\ncivic\nNone\nPOLYGON ((385486.225 6672173.653, 385486.717 6...\n1\n8042613\npark\nSimonpuistikko\n90.919207\n\n\n454\n8535506\ncivic\nNone\nPOLYGON ((385481.13 6672167.861, 385482.372 66...\n1\n8042613\npark\nSimonpuistikko\n87.821936\n\n\n\n\n455 rows × 9 columns\n\n\n\n\nprint(\"Maximum distance:\", nearest_parks[\"distance\"].max().round(0))\nprint(\"Average distance:\", nearest_parks[\"distance\"].mean().round(0))\n\nMaximum distance: 229.0\nAverage distance: 61.0"
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-15",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-15",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nNearest neighbors with Polygon and LineString data\n\n\nIn a similar manner, we can also find the nearest road from each building as follows:\n\nnearest_roads = buildings.sjoin_nearest(roads, distance_col=\"distance\")\nnearest_roads\n\n\n\n\n\n\n\n\nosmid_left\nbuilding\nname_left\ngeometry\nindex_right\nosmid_right\nname_right\nhighway\ndistance\n\n\n\n\n0\n11711721042\nyes\nNice Bike Pyörähuolto\nPOINT (384966.661 6671503.786)\n182\n[126894680, 126894676, 126894678, 126894679]\nEerikinkatu\nresidential\n11.181066\n\n\n0\n11711721042\nyes\nNice Bike Pyörähuolto\nPOINT (384966.661 6671503.786)\n24\n[126894680, 126894676, 126894678, 126894679]\nEerikinkatu\nresidential\n11.181066\n\n\n1\n8035238\npublic\nLasipalatsi\nPOLYGON ((385459.65 6672184.469, 385456.356 66...\n15\n[42574048, 42574049, 28920739, 77891210, 26999...\nArkadiankatu\nsecondary\n52.015824\n\n\n1\n8035238\npublic\nLasipalatsi\nPOLYGON ((385459.65 6672184.469, 385456.356 66...\n33\n[42574048, 42574049, 28920739, 77891210, 26999...\nArkadiankatu\nsecondary\n52.015824\n\n\n2\n8042297\nyes\nRadisson Blu Royal\nPOLYGON ((385104.154 6671916.693, 385101.584 6...\n83\n[37135576, 8035726, 37135575]\nSalomonkatu\nresidential\n6.659959\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n452\n8525159\ncivic\nNone\nPOLYGON ((385495.275 6672164.009, 385494.928 6...\n107\n51707742\nYrjönkatu\nresidential\n88.553223\n\n\n453\n8525161\ncivic\nNone\nPOLYGON ((385486.225 6672173.653, 385486.717 6...\n15\n[42574048, 42574049, 28920739, 77891210, 26999...\nArkadiankatu\nsecondary\n90.569914\n\n\n453\n8525161\ncivic\nNone\nPOLYGON ((385486.225 6672173.653, 385486.717 6...\n33\n[42574048, 42574049, 28920739, 77891210, 26999...\nArkadiankatu\nsecondary\n90.569914\n\n\n454\n8535506\ncivic\nNone\nPOLYGON ((385481.13 6672167.861, 385482.372 66...\n15\n[42574048, 42574049, 28920739, 77891210, 26999...\nArkadiankatu\nsecondary\n96.128437\n\n\n454\n8535506\ncivic\nNone\nPOLYGON ((385481.13 6672167.861, 385482.372 66...\n33\n[42574048, 42574049, 28920739, 77891210, 26999...\nArkadiankatu\nsecondary\n96.128437\n\n\n\n\n703 rows × 9 columns"
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-16",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-16",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nNearest neighbors with Polygon and LineString data\n\n\nTo better understand how the spatial join between the buildings and roads have been conducted, we can again visualize the nearest neighbors with a straight line. To do this, we first bring the geometries from the roads GeoDataFrame into the same table with the buildings:\n\nnearest_roads = nearest_roads.merge(\n    roads[[\"geometry\"]], left_on=\"index_right\", right_index=True\n)\nnearest_roads.head(3)\n\n\n\n\n\n\n\n\nosmid_left\nbuilding\nname_left\ngeometry_x\nindex_right\nosmid_right\nname_right\nhighway\ndistance\ngeometry_y\n\n\n\n\n0\n11711721042\nyes\nNice Bike Pyörähuolto\nPOINT (384966.661 6671503.786)\n182\n[126894680, 126894676, 126894678, 126894679]\nEerikinkatu\nresidential\n11.181066\nLINESTRING (385040.141 6671566.384, 385034.832...\n\n\n0\n11711721042\nyes\nNice Bike Pyörähuolto\nPOINT (384966.661 6671503.786)\n24\n[126894680, 126894676, 126894678, 126894679]\nEerikinkatu\nresidential\n11.181066\nLINESTRING (384942.149 6671500.856, 384950.743...\n\n\n1\n8035238\npublic\nLasipalatsi\nPOLYGON ((385459.65 6672184.469, 385456.356 66...\n15\n[42574048, 42574049, 28920739, 77891210, 26999...\nArkadiankatu\nsecondary\n52.015824\nLINESTRING (385285.226 6672266.801, 385296.799..."
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-17",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-17",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nNearest neighbors with Polygon and LineString data\n\n\nWe can use shortest_line() from the shapely library that returns a LineString object between the input geometries showing the shortest distance between them.\n\nfrom shapely import shortest_line\n\n\n# Generate LineString between nearest points of two geometries\nconnectors = nearest_roads.apply(\n    lambda row: shortest_line(row[\"geometry_x\"], row[\"geometry_y\"]), axis=1\n)\n\n# Create a new GeoDataFrame out of these geometries\nconnectors = gpd.GeoDataFrame({\"geometry\": connectors}, crs=roads.crs)\nconnectors[\"distance\"] = connectors.length\nconnectors.head()\n\n\n\n\n\n\n\n\ngeometry\ndistance\n\n\n\n\n0\nLINESTRING (384966.661 6671503.786, 384960.444...\n11.181066\n\n\n0\nLINESTRING (384966.661 6671503.786, 384960.444...\n11.181066\n\n\n1\nLINESTRING (385487.966 6672217.975, 385460.972...\n52.015824\n\n\n1\nLINESTRING (385487.966 6672217.975, 385460.972...\n52.015824\n\n\n2\nLINESTRING (385050.507 6671936.92, 385046.795 ...\n6.659959"
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-18",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-18",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nNearest neighbors with Polygon and LineString data\n\n\nWe can visualize the buildings, roads and these connectors to better understand the exact points where the distance between a given building and the closest road is shortest:\n\nm = buildings.explore(color=\"gray\", tiles=\"CartoDB Positron\")\nm = roads.explore(m=m, color=\"red\")\nm = connectors.explore(m=m, color=\"green\")\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-19",
    "href": "bigdata_lec9.html#nearest-neighbour-analysis-in-python-19",
    "title": "Big Data: GeoScience part 3",
    "section": "Nearest neighbour analysis in Python",
    "text": "Nearest neighbour analysis in Python\n\n\n\nExercise\n\n\n\nWhat is the closest road to each park?\nUse the parks and roads GeoDataFrames and follow the approaches presented above to find the closest road to each park.\nWhat is the highest (maximum) distance between parks and roads present in our datasets?"
  },
  {
    "objectID": "bigdata_lab8.html",
    "href": "bigdata_lab8.html",
    "title": "Big Data Analytics: Lab 8",
    "section": "",
    "text": "Vector data: pick a country and download its shapefile data from https://gadm.org/download_country.html\nRaster data: download average radiance data for 2 different months from https://eogdata.mines.edu/products/vnl/#monthly\n\n“avg_rade9h” in the filename\npick non-tiled version\nuse stray-light-corrected data (vcmsl)\n\n\n\n\n\n\nOpen vector data with GeoDataFrame and plot it.\nOpen raster data with rioxarray and plot it. See if it works and doesn’t crash.\nTry reducing the size of raster data by factor of 10. How would you do it?\nClip/mask raster data so that it fits the vector geometry of your country’s vector data. Plot it.\nCompute average luminosity for each level 2 region.\nCreate a resulting GeoDataFrame with additional column showing the difference in average luminosity between start_month and end_month, and save it to a file. Ah, and don’t forget to plot it.\n\nNote: use Dask for optimizations: - https://corteva.github.io/rioxarray/stable/examples/dask_read_write.html - https://docs.xarray.dev/en/stable/user-guide/dask.html"
  },
  {
    "objectID": "bigdata_lab8.html#data-preparation",
    "href": "bigdata_lab8.html#data-preparation",
    "title": "Big Data Analytics: Lab 8",
    "section": "",
    "text": "Vector data: pick a country and download its shapefile data from https://gadm.org/download_country.html\nRaster data: download average radiance data for 2 different months from https://eogdata.mines.edu/products/vnl/#monthly\n\n“avg_rade9h” in the filename\npick non-tiled version\nuse stray-light-corrected data (vcmsl)"
  },
  {
    "objectID": "bigdata_lab8.html#data-analysis",
    "href": "bigdata_lab8.html#data-analysis",
    "title": "Big Data Analytics: Lab 8",
    "section": "",
    "text": "Open vector data with GeoDataFrame and plot it.\nOpen raster data with rioxarray and plot it. See if it works and doesn’t crash.\nTry reducing the size of raster data by factor of 10. How would you do it?\nClip/mask raster data so that it fits the vector geometry of your country’s vector data. Plot it.\nCompute average luminosity for each level 2 region.\nCreate a resulting GeoDataFrame with additional column showing the difference in average luminosity between start_month and end_month, and save it to a file. Ah, and don’t forget to plot it.\n\nNote: use Dask for optimizations: - https://corteva.github.io/rioxarray/stable/examples/dask_read_write.html - https://docs.xarray.dev/en/stable/user-guide/dask.html"
  },
  {
    "objectID": "applied_lec5.html#cloud",
    "href": "applied_lec5.html#cloud",
    "title": "Intro to AWS",
    "section": "Cloud",
    "text": "Cloud\n\n\n\nDefinition\n\n\nCloud computing is the on-demand delivery of computing resources through a cloud services platform via the internet with pay-as-you-go pricing."
  },
  {
    "objectID": "applied_lec5.html#cloud-1",
    "href": "applied_lec5.html#cloud-1",
    "title": "Intro to AWS",
    "section": "Cloud",
    "text": "Cloud\n\n\n\nService models\n\n\n\nSoftware as a Service (SaaS) The service vendor provides the user with a completed product that is run and managed by the service provider.\nPlatform as a Service (PaaS) The service vendor provides the user with a set of API which can be used to build, test and deploy applications.\nInfrastructure as a Service (IaaS) The service vendor provides users access to computing resources such as servers, storage and networking."
  },
  {
    "objectID": "applied_lec5.html#cloud-2",
    "href": "applied_lec5.html#cloud-2",
    "title": "Intro to AWS",
    "section": "Cloud",
    "text": "Cloud"
  },
  {
    "objectID": "applied_lec5.html#cloud-3",
    "href": "applied_lec5.html#cloud-3",
    "title": "Intro to AWS",
    "section": "Cloud",
    "text": "Cloud"
  },
  {
    "objectID": "applied_lec5.html#cloud-4",
    "href": "applied_lec5.html#cloud-4",
    "title": "Intro to AWS",
    "section": "Cloud",
    "text": "Cloud"
  },
  {
    "objectID": "applied_lec5.html#cloud-5",
    "href": "applied_lec5.html#cloud-5",
    "title": "Intro to AWS",
    "section": "Cloud",
    "text": "Cloud\n\n\n\nDeployment models for cloud computing\n\n\n\ncloud-based\non-premises\nhybrid"
  },
  {
    "objectID": "applied_lec5.html#cloud-6",
    "href": "applied_lec5.html#cloud-6",
    "title": "Intro to AWS",
    "section": "Cloud",
    "text": "Cloud\n\n\n\nCloud-Based Deployment\n\n\n\nRun all parts of the application in the cloud.\nMigrate existing applications to the cloud.\nDesign and build new applications in the cloud.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nBuild using:\n\nlow-level infrastructure\nhigher-level services"
  },
  {
    "objectID": "applied_lec5.html#cloud-7",
    "href": "applied_lec5.html#cloud-7",
    "title": "Intro to AWS",
    "section": "Cloud",
    "text": "Cloud\n\n\n\nOn-Premises Deployment\n\n\n\nDeploy resources by using virtualization and resource management tools.\nIncrease resource utilization by using application management and virtualization technologies.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nAlso known as a private cloud deployment.\nIn this model, resources are deployed on premises by using virtualization and resource management tools."
  },
  {
    "objectID": "applied_lec5.html#cloud-8",
    "href": "applied_lec5.html#cloud-8",
    "title": "Intro to AWS",
    "section": "Cloud",
    "text": "Cloud\n\n\n\nHybrid Deployment\n\n\n\nConnect cloud-based resources to on-premises infrastructure.\nIntegrate cloud-based resources with legacy IT applications.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIn a hybrid deployment, cloud-based resources are connected to on-premises infrastructure."
  },
  {
    "objectID": "applied_lec5.html#cloud-9",
    "href": "applied_lec5.html#cloud-9",
    "title": "Intro to AWS",
    "section": "Cloud",
    "text": "Cloud\n\n\n\nBenefits\n\n\n\ntrade upfront expense for variable expense\nno data center maintenance\nflexible capacity\neasier to deploy"
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-1",
    "href": "applied_lec5.html#cloud-compute-1",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nAmazon Elastic Compute Cloud (Amazon EC2)\n\n\nAmazon Elastic Compute Cloud (Amazon EC2) provides secure, resizable compute capacity in the cloud as Amazon EC2 instances.\nInstead of setting up physical hardware, one can:\n\nprovision and launch an Amazon EC2 instance within minutes.\nstop using it when you have finished running a workload.\npay only for the compute time you use when an instance is running, not when it is stopped or terminated."
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-2",
    "href": "applied_lec5.html#cloud-compute-2",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nSteps\n\n\n\nLaunch\n\nselect type\nconfigure\n\nConnect\nUse"
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-3",
    "href": "applied_lec5.html#cloud-compute-3",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nMulti-tenancy\n\n\n\nWhen you spin up an EC2 instance, you aren’t necessarily taking an entire host to yourself.\nyou are sharing the host with multiple other instances, otherwise known as virtual machines.\nHypervisor running on the host machine is responsible for sharing the underlying physical resources between the virtual machines.\none EC2 instance is not aware of any other EC2 instances also on that host."
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-4",
    "href": "applied_lec5.html#cloud-compute-4",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nAmazon EC2 instance types\n\n\n\nGeneral purpose instances provide a balance of compute, memory, and networking resources.\nCompute optimized instances are ideal for compute-bound applications that benefit from high-performance processors.\nMemory optimized instances are designed to deliver fast performance for workloads that process large datasets in memory.\nAccelerated computing instances use hardware accelerators, or coprocessors, to perform some functions more efficiently than is possible in software running on CPUs.\nStorage optimized instances are designed for workloads that require high, sequential read and write access to large datasets on local storage.\n\n\n\n\n\n\nhttps://aws.amazon.com/ec2/instance-types/"
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-5",
    "href": "applied_lec5.html#cloud-compute-5",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nPricing\n\n\n\non-demand: short-term, irregular workloads that cannot be interrupted.\nreserved: discounted on-demand, for predictable usage\nspot: for workloads with flexible start and end times, or that can withstand interruptions.\ndedicated: physical servers with Amazon EC2 instance capacity that is fully dedicated to your use."
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-6",
    "href": "applied_lec5.html#cloud-compute-6",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nScalability\n\n\nScalability involves beginning with only the resources you need and designing your architecture to automatically respond to changing demand by scaling out or in.\nAmazon EC2 Auto Scaling enables you to automatically add or remove Amazon EC2 instances in response to changing application demand.\n\nDynamic scaling responds to changing demand.\nPredictive scaling automatically schedules the right number of Amazon EC2 instances based on predicted demand."
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-7",
    "href": "applied_lec5.html#cloud-compute-7",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nElastic Load Balancing\n\n\nElastic Load Balancing (ELB) is the AWS service that automatically distributes incoming application traffic across multiple resources, such as Amazon EC2 instances. A load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group.\n\nrequests route to the load balancer first\nthe requests spread across multiple resources that will handle them."
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-8",
    "href": "applied_lec5.html#cloud-compute-8",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nElastic Load Balancing\n\n\n\n\n\nStage 1"
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-9",
    "href": "applied_lec5.html#cloud-compute-9",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nElastic Load Balancing\n\n\n\n\n\nStage 2"
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-10",
    "href": "applied_lec5.html#cloud-compute-10",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nMonolithic applications"
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-11",
    "href": "applied_lec5.html#cloud-compute-11",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nMicroservices"
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-12",
    "href": "applied_lec5.html#cloud-compute-12",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nMessaging and Queuing\n\n\nBasic idea: messages between components should be placed into a buffer.\n\nTightly coupled: when applications communicate directly\nLoosely coupled: applications communicate indirectly, e.g. using message queue\n\nAWS services:\n\nSimple Queue Service (SQS)\nAmazon Simple Notification Service (SNS)."
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-13",
    "href": "applied_lec5.html#cloud-compute-13",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nAmazon Simple Queue Service (Amazon SQS)\n\n\n\nAmazon Simple Queue Service (Amazon SQS) is a message queuing service.\n\nIn Amazon SQS, an application sends messages into a queue.\nA user or service retrieves a message from the queue, processes it, and then deletes it from the queue.\nAmazon SQS is a message queuing service, and is therefore not the best choice for publishing messages to subscribers."
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-14",
    "href": "applied_lec5.html#cloud-compute-14",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nAmazon Simple Notification Service (Amazon SNS)\n\n\n\nAmazon Simple Notification Service (Amazon SNS) is a publish/subscribe service.\n\nUsing Amazon SNS topics, a publisher publishes messages to subscribers.\nAmazon SNS is similar to SQS in that it is used to send out messages to services, but it can also send out notifications to end users. It does this in a different way called a publish/subscribe or pub/sub model."
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-15",
    "href": "applied_lec5.html#cloud-compute-15",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nEC2 computing\n\n\nWith EC2, in order to run an application, one needs to:\n\nProvision instances (virtual servers).\nUpload your code.\nContinue to manage the instances while your application is running.\n\n\n\n\n\n\n\nServerless computing\n\n\nCode runs on servers, but provisioning/management is not user’s responsibility.\n\nyou cannot actually see or access the underlying infrastructure or instances that are hosting your application.\ninstead, all the management of the underlying environment from a provisioning, scaling, high availability, and maintenance perspective are taken care of."
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-16",
    "href": "applied_lec5.html#cloud-compute-16",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nAWS Lambda\n\n\nAWS Lambda is a service that lets you run code without needing to provision or manage servers.\nIt allows you to upload your code into what’s called a Lambda function. Configure a trigger and from there, the service waits for the trigger. When the trigger is detected, the code is automatically run in a managed environment.\n\n\n\n\n\n\n\n\n\nImportant\n\n\nLambda is designed to run code under 15 minutes so this isn’t for long running processes like deep learning. It’s more suited for quick processing like a web backend, handling requests or a backend expense report processing service where each invocation takes less than 15 minutes to complete."
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-17",
    "href": "applied_lec5.html#cloud-compute-17",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nHow AWS Lambda works\n\n\n\nYou upload your code to Lambda.\nYou set your code to trigger from an event source, such as AWS services, mobile applications, or HTTP endpoints.\nLambda runs your code only when triggered.\nYou pay only for the compute time that you use."
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-18",
    "href": "applied_lec5.html#cloud-compute-18",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nContainers\n\n\nAWS can run Docker containers through:\n\nAmazon Elastic Container Service (ECS)\nAmazon Elastic Kubernetes Service (EKS)."
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-19",
    "href": "applied_lec5.html#cloud-compute-19",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nAmazon Elastic Container Service (Amazon ECS)\n\n\nAmazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container management system that enables you to run and scale containerized applications on AWS.\n\n\n\n\n\n\nAmazon Elastic Kubernetes Service (Amazon EKS)\n\n\nAmazon Elastic Kubernetes Service (Amazon EKS) is a fully managed service that you can use to run Kubernetes on AWS. Kubernetes is open-source software that enables you to deploy and manage containerized applications at scale.\n\n\n\n\n\n\nAWS Fargate\n\n\nBoth Amazon ECS and Amazon EKS can run on top of EC2. If you don’t want to manage EC2, you can use Fargate: a serverless compute engine for containers. It works with both Amazon ECS and Amazon EKS."
  },
  {
    "objectID": "applied_lec5.html#cloud-compute-20",
    "href": "applied_lec5.html#cloud-compute-20",
    "title": "Intro to AWS",
    "section": "Cloud compute",
    "text": "Cloud compute\n\n\n\nChoice of Compute Service\n\n\n\nIf you are trying to host traditional applications and want full access to the underlying operating system like Linux or Windows, you are going to want to use EC2.\nIf you are looking to host short running functions, service-oriented or event driven applications and you don’t want to manage the underlying environment at all, look into the serverless AWS Lambda.\nIf you are looking to run Docker container-based workloads on AWS, you can use ECS/EKS/Fargate"
  },
  {
    "objectID": "applied_lec5.html#storage-1",
    "href": "applied_lec5.html#storage-1",
    "title": "Intro to AWS",
    "section": "Storage",
    "text": "Storage\n\n\n\nInstance stores\n\n\nTerm: block-level storage is a place to store files.\nAn instance store provides temporary block-level storage for an Amazon EC2 instance.\nIf you stop or terminate your EC2 instance, all data written to the instance store volume will be deleted. The reason for this, is that if you start your instance from a stop state, it’s likely that EC2 instance will start up on another host."
  },
  {
    "objectID": "applied_lec5.html#storage-2",
    "href": "applied_lec5.html#storage-2",
    "title": "Intro to AWS",
    "section": "Storage",
    "text": "Storage\n\n\n\nAmazon Elastic Block Store\n\n\nAmazon Elastic Block Store (Amazon EBS) is a service that provides block-level storage volumes that you can use with Amazon EC2 instances. If you stop or terminate an Amazon EC2 instance, all the data on the attached EBS volume remains available. With EBS, you can create virtual hard drives that we call EBS volumes that you can attach to your EC2 instances."
  },
  {
    "objectID": "applied_lec5.html#storage-3",
    "href": "applied_lec5.html#storage-3",
    "title": "Intro to AWS",
    "section": "Storage",
    "text": "Storage\n\n\n\nAmazon EBS snapshots\n\n\nAn EBS snapshot is an incremental backup of EBS volume."
  },
  {
    "objectID": "applied_lec5.html#storage-4",
    "href": "applied_lec5.html#storage-4",
    "title": "Intro to AWS",
    "section": "Storage",
    "text": "Storage\n\n\n\nObject storage\n\n\nAmazon Simple Storage Service (Amazon S3) is a service that provides object-level storage. Amazon S3 is a data store that allows you to store and retrieve an unlimited amount of data at any scale.\n\nData is stored as objects\ninstead of storing them in a file directory, data are stored in buckets\nAmazon S3 offers unlimited storage space.\nno delta updates\nThe maximum file size for an object in Amazon S3 is 5 TB.\neach object consists of data, metadata, and a key."
  },
  {
    "objectID": "applied_lec5.html#storage-5",
    "href": "applied_lec5.html#storage-5",
    "title": "Intro to AWS",
    "section": "Storage",
    "text": "Storage\n\n\n\nS3 objects\n\n\n\nData might be an image, video, text document, or any other type of file.\nMetadata contains information about what the data is, how it is used, the object size, and so on.\nAn object’s key is its unique identifier."
  },
  {
    "objectID": "applied_lec5.html#storage-6",
    "href": "applied_lec5.html#storage-6",
    "title": "Intro to AWS",
    "section": "Storage",
    "text": "Storage\n\n\n\nStorage classes\n\n\n\nS3 Standard\n\nDesigned for frequently accessed data\nStores data in a minimum of three Availability Zones\ncomes with 11 nines of durability. That means an object stored in S3 Standard has a 99.999999999 percentage probability that it will remain intact after a period of one year.\n\nS3 Standard-Infrequent Access (S3 Standard-IA)\n\nIdeal for infrequently accessed data\nSimilar to S3 Standard but has a lower storage price and higher retrieval price\n\nS3 One Zone-Infrequent Access (S3 One Zone-IA)\n\nStores data in a single Availability Zone\nHas a lower storage price than S3 Standard-IA"
  },
  {
    "objectID": "applied_lec5.html#storage-7",
    "href": "applied_lec5.html#storage-7",
    "title": "Intro to AWS",
    "section": "Storage",
    "text": "Storage\n\n\n\nStorage classes\n\n\n\nS3 Intelligent-Tiering\n\nIdeal for data with unknown or changing access patterns\nRequires a small monthly monitoring and automation fee per object\nAmazon S3 monitors objects’ access patterns.\n\nS3 Glacier\n\nLow-cost storage designed for data archiving\nAble to retrieve objects within a few minutes to hours\n\nS3 Glacier Deep Archive\n\nLowest-cost object storage class ideal for archiving\nAble to retrieve objects within 12 hours"
  },
  {
    "objectID": "applied_lec5.html#storage-8",
    "href": "applied_lec5.html#storage-8",
    "title": "Intro to AWS",
    "section": "Storage",
    "text": "Storage\n\n\n\nComparing Amazon EBS and Amazon S3\n\n\n\nbackups: S3\nhuge files continuously updated: EBS"
  },
  {
    "objectID": "applied_lec5.html#storage-9",
    "href": "applied_lec5.html#storage-9",
    "title": "Intro to AWS",
    "section": "Storage",
    "text": "Storage\n\n\n\nRDS\n\n\nAmazon Relational Database Service (Amazon RDS) is a service that enables you to run relational databases in the AWS Cloud.\nAmazon RDS is available on six database engines, which optimize for memory, performance, or input/output (I/O). Supported database engines include:\n\nAmazon Aurora\nPostgreSQL\nMySQL\nMariaDB\nOracle Database\nMicrosoft SQL Server"
  },
  {
    "objectID": "applied_lec5.html#storage-10",
    "href": "applied_lec5.html#storage-10",
    "title": "Intro to AWS",
    "section": "Storage",
    "text": "Storage\n\n\n\n\n\n\n\n\n\nAmazon DynamoDB\n\n\n\nAmazon DynamoDB is a key-value database service.\n\ndelivers single-digit millisecond performance at any scale.\na non-relational, NoSQL database.\nWith DynamoDB, you create tables\nData in tables is organized into items, and items have attributes.\nserverless"
  },
  {
    "objectID": "applied_lec5.html#storage-11",
    "href": "applied_lec5.html#storage-11",
    "title": "Intro to AWS",
    "section": "Storage",
    "text": "Storage\n\n\n\nAmazon Redshift\n\n\n\nAmazon Redshift is a data warehousing service that you can use for big data analytics.\n\nability to collect data from many sources\ndata warehousing as a service.\nmassively scalable (Redshift nodes in multiple petabyte sizes is very common)"
  },
  {
    "objectID": "applied_lab4.html",
    "href": "applied_lab4.html",
    "title": "Applied Analytics: Lab 4",
    "section": "",
    "text": "Overview\nComplete exercises from Dockerfile lecture."
  },
  {
    "objectID": "applied_lec6.html#member-account-creation",
    "href": "applied_lec6.html#member-account-creation",
    "title": "Intro to AWS",
    "section": "Member Account creation",
    "text": "Member Account creation\n\n\n\nImportant\n\n\n\nyou’ll receive several emails\none of these should be Your Amazon Web Services Account is Ready - Get Started Now\nin the email, click on ‘Start building in AWS console’\nSign in with root user email\nClick ‘forgot password’\nreceive reset password email\nupon login, you’ll receive email: Verify your identity. Contains verification code.\nskip MFA"
  },
  {
    "objectID": "applied_lec6.html#aws-regions",
    "href": "applied_lec6.html#aws-regions",
    "title": "Intro to AWS",
    "section": "AWS regions",
    "text": "AWS regions\n\n\n\nTip\n\n\nGo to https://docs.aws.amazon.com/global-infrastructure/latest/regions/aws-regions.html to get an overview of available regions"
  },
  {
    "objectID": "applied_lec6.html#launch-an-instance",
    "href": "applied_lec6.html#launch-an-instance",
    "title": "Intro to AWS",
    "section": "Launch an instance",
    "text": "Launch an instance\n\n\n\nNote\n\n\n\npick name: jupyter\nOS type: ubuntu\ninstance type: t2.micro"
  },
  {
    "objectID": "applied_lec6.html#launch-an-instance-1",
    "href": "applied_lec6.html#launch-an-instance-1",
    "title": "Intro to AWS",
    "section": "Launch an instance",
    "text": "Launch an instance"
  },
  {
    "objectID": "applied_lec6.html#launch-an-instance-2",
    "href": "applied_lec6.html#launch-an-instance-2",
    "title": "Intro to AWS",
    "section": "Launch an instance",
    "text": "Launch an instance\n\n\n\nFree tier note"
  },
  {
    "objectID": "applied_lec6.html#machine-type",
    "href": "applied_lec6.html#machine-type",
    "title": "Intro to AWS",
    "section": "Machine type",
    "text": "Machine type"
  },
  {
    "objectID": "applied_lec6.html#key-pair-creation",
    "href": "applied_lec6.html#key-pair-creation",
    "title": "Intro to AWS",
    "section": "Key pair creation",
    "text": "Key pair creation\n\n\n\nImportant\n\n\n\nclick on ‘Create new key pair’\nname: jupyter\ntype: ed25519\nPrivate key file format: PEM\nsave jupyter.pem somewhere"
  },
  {
    "objectID": "applied_lec6.html#key-pair-creation-1",
    "href": "applied_lec6.html#key-pair-creation-1",
    "title": "Intro to AWS",
    "section": "Key pair creation",
    "text": "Key pair creation"
  },
  {
    "objectID": "applied_lec6.html#network-settings",
    "href": "applied_lec6.html#network-settings",
    "title": "Intro to AWS",
    "section": "Network settings",
    "text": "Network settings\n\nnetwork settings: allow ssh traffic only\nstorage: leave default settings"
  },
  {
    "objectID": "applied_lec6.html#network-settings-1",
    "href": "applied_lec6.html#network-settings-1",
    "title": "Intro to AWS",
    "section": "Network settings",
    "text": "Network settings"
  },
  {
    "objectID": "applied_lec6.html#storage",
    "href": "applied_lec6.html#storage",
    "title": "Intro to AWS",
    "section": "Storage",
    "text": "Storage"
  },
  {
    "objectID": "applied_lec6.html#final-stage",
    "href": "applied_lec6.html#final-stage",
    "title": "Intro to AWS",
    "section": "Final stage",
    "text": "Final stage"
  },
  {
    "objectID": "applied_lec6.html#instances",
    "href": "applied_lec6.html#instances",
    "title": "Intro to AWS",
    "section": "Instances",
    "text": "Instances"
  },
  {
    "objectID": "applied_lec6.html#connect-to-instance",
    "href": "applied_lec6.html#connect-to-instance",
    "title": "Intro to AWS",
    "section": "Connect to instance",
    "text": "Connect to instance\n\n\n\nNote\n\n\n\nIn order to see your instance, go to https://us-east-2.console.aws.amazon.com/ec2/home and click on Instances\nNow click Connect to instance\nclick on SSH client tab"
  },
  {
    "objectID": "applied_lec6.html#connect-to-instance-1",
    "href": "applied_lec6.html#connect-to-instance-1",
    "title": "Intro to AWS",
    "section": "Connect to instance",
    "text": "Connect to instance"
  },
  {
    "objectID": "applied_lec6.html#connect-to-instance-2",
    "href": "applied_lec6.html#connect-to-instance-2",
    "title": "Intro to AWS",
    "section": "Connect to instance",
    "text": "Connect to instance\n\n\n\nSteps:\n\n\nfirst execute\nchmod 0400 jupyter.pem\notherwise during execution you might get:\n\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@         WARNING: UNPROTECTED PRIVATE KEY FILE!          @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nPermissions 0644 for 'jupyter.pem' are too open.\nIt is required that your private key files are NOT accessible by others.\nThis private key will be ignored.\nLoad key \"jupyter.pem\": bad permissions\nexecute:\nssh -i \"jupyter.pem\" ubuntu@ec2-18-191-211-55.us-east-2.compute.amazonaws.com"
  },
  {
    "objectID": "applied_lec6.html#check-ebs-volumes",
    "href": "applied_lec6.html#check-ebs-volumes",
    "title": "Intro to AWS",
    "section": "Check EBS volumes",
    "text": "Check EBS volumes"
  },
  {
    "objectID": "applied_lec6.html#check-ebs-volumes-1",
    "href": "applied_lec6.html#check-ebs-volumes-1",
    "title": "Intro to AWS",
    "section": "Check EBS volumes",
    "text": "Check EBS volumes"
  },
  {
    "objectID": "applied_lec6.html#instance-setup",
    "href": "applied_lec6.html#instance-setup",
    "title": "Intro to AWS",
    "section": "Instance setup",
    "text": "Instance setup\n\n\n\nCheck disks\n\n\nubuntu@ip-172-31-27-235:~$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/root       6.8G  1.8G  5.0G  27% /\ntmpfs           479M     0  479M   0% /dev/shm\ntmpfs           192M  868K  191M   1% /run\ntmpfs           5.0M     0  5.0M   0% /run/lock\n/dev/xvda16     881M   87M  733M  11% /boot\n/dev/xvda15     105M  6.2M   99M   6% /boot/efi\ntmpfs            96M   12K   96M   1% /run/user/1000"
  },
  {
    "objectID": "applied_lec6.html#instance-setup-1",
    "href": "applied_lec6.html#instance-setup-1",
    "title": "Intro to AWS",
    "section": "Instance setup",
    "text": "Instance setup\n\n\n\nMemory\n\n\nubuntu@ip-172-31-27-235:~$ free -h\n               total        used        free      shared  buff/cache   available\nMem:           957Mi       350Mi       323Mi       880Ki       439Mi       606Mi\nSwap:             0B          0B          0B"
  },
  {
    "objectID": "applied_lec6.html#instance-setup-2",
    "href": "applied_lec6.html#instance-setup-2",
    "title": "Intro to AWS",
    "section": "Instance setup",
    "text": "Instance setup\n\n\n\nCPU\n\n\nubuntu@ip-172-31-27-235:~$ lscpu\nArchitecture:                x86_64\n  CPU op-mode(s):            32-bit, 64-bit\n  Address sizes:             46 bits physical, 48 bits virtual\n  Byte Order:                Little Endian\nCPU(s):                      1\n  On-line CPU(s) list:       0\nVendor ID:                   GenuineIntel\n  Model name:                Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz\n    CPU family:              6\n    Model:                   79\n    Thread(s) per core:      1\n    Core(s) per socket:      1\n    Socket(s):               1\n    Stepping:                1\n    BogoMIPS:                4599.99\n    Flags:                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni\n                             pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm pti fsgsbase bmi1 avx2 smep bmi2 erms invpcid xsaveo\n                             pt\nAlternatively, cat /proc/cpuinfo"
  },
  {
    "objectID": "applied_lec6.html#instance-setup-3",
    "href": "applied_lec6.html#instance-setup-3",
    "title": "Intro to AWS",
    "section": "Instance setup",
    "text": "Instance setup\n\n\n\nSystem info\n\n\nubuntu@ip-172-31-27-235:~$ cat /etc/os-release\nPRETTY_NAME=\"Ubuntu 24.04.3 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"24.04\"\nVERSION=\"24.04.3 LTS (Noble Numbat)\"\nVERSION_CODENAME=noble\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=noble\nLOGO=ubuntu-logo"
  },
  {
    "objectID": "applied_lec6.html#instance-setup-4",
    "href": "applied_lec6.html#instance-setup-4",
    "title": "Intro to AWS",
    "section": "Instance setup",
    "text": "Instance setup\n\n\n\nInstall uv\n\n\nubuntu@ip-172-31-27-235:~$ curl -LsSf https://astral.sh/uv/install.sh | sh\ndownloading uv 0.9.5 x86_64-unknown-linux-gnu\nno checksums to verify\ninstalling to /home/ubuntu/.local/bin\n  uv\n  uvx\neverything's installed!\n\nTo add $HOME/.local/bin to your PATH, either restart your shell or run:\n\n    source $HOME/.local/bin/env (sh, bash, zsh)\n    source $HOME/.local/bin/env.fish (fish)\nif you try running it now, you’ll get:\nubuntu@ip-172-31-27-235:~$ uv\nCommand 'uv' not found, but can be installed with:\nsudo snap install astral-uv\nTo fix this, execute:\nsource $HOME/.local/bin/env"
  },
  {
    "objectID": "applied_lec6.html#instance-setup-5",
    "href": "applied_lec6.html#instance-setup-5",
    "title": "Intro to AWS",
    "section": "Instance setup",
    "text": "Instance setup\n\n\n\ninit uv\n\n\nubuntu@ip-172-31-27-235:~$ uv init analytics\nInitialized project `analytics` at `/home/ubuntu/analytics`\nThen:\nuv add jupyter"
  },
  {
    "objectID": "applied_lec6.html#instance-setup-6",
    "href": "applied_lec6.html#instance-setup-6",
    "title": "Intro to AWS",
    "section": "Instance setup",
    "text": "Instance setup\n\n\n\ntmux\n\n\nInstall tmux and run tmux on ubuntu via:\napt install tmux"
  },
  {
    "objectID": "applied_lec6.html#instance-setup-7",
    "href": "applied_lec6.html#instance-setup-7",
    "title": "Intro to AWS",
    "section": "Instance setup",
    "text": "Instance setup\n\n\n\nSecuring server\n\n\ncd ~\nmkdir ssl\ncd ssl\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout mykey.key -out mycert.pem\nDocs here: https://docs.aws.amazon.com/dlami/latest/devguide/setup-jupyter-secure.html."
  },
  {
    "objectID": "applied_lec6.html#instance-setup-8",
    "href": "applied_lec6.html#instance-setup-8",
    "title": "Intro to AWS",
    "section": "Instance setup",
    "text": "Instance setup\n\n\n\nStarting server\n\n\nStart the server:\nuv run jupyter notebook --no-browser --certfile=~/ssl/mycert.pem --keyfile ~/ssl/mykey.key\nDocs here: https://docs.aws.amazon.com/dlami/latest/devguide/setup-jupyter-start-server.html"
  },
  {
    "objectID": "applied_lec6.html#instance-setup-9",
    "href": "applied_lec6.html#instance-setup-9",
    "title": "Intro to AWS",
    "section": "Instance setup",
    "text": "Instance setup\n\n\n\nSSH tunnel\n\n\nOn your host, run ssh tunnel (https://www.digitalocean.com/community/tutorials/ssh-port-forwarding):\nssh -i jupyter.pem -N -f -L 8899:localhost:8888 ubuntu@ec2-18-191-211-55.us-east-2.compute.amazonaws.com\nNow you should be able to access Jupyter in browser on address https://localhost:8899"
  },
  {
    "objectID": "applied_lec6.html#startingstopping-instance",
    "href": "applied_lec6.html#startingstopping-instance",
    "title": "Intro to AWS",
    "section": "Starting/stopping instance",
    "text": "Starting/stopping instance"
  },
  {
    "objectID": "applied_lec6.html#instance-setup-10",
    "href": "applied_lec6.html#instance-setup-10",
    "title": "Intro to AWS",
    "section": "Instance setup",
    "text": "Instance setup\n\n\n\nElastic IP\n\n\nNote that IP address is different after restart. To fix this, use Elastic IP:"
  },
  {
    "objectID": "applied_lec6.html#instance-setup-11",
    "href": "applied_lec6.html#instance-setup-11",
    "title": "Intro to AWS",
    "section": "Instance setup",
    "text": "Instance setup\n\n\n\nElastic IP\n\n\nTwo steps are necessary:\n\nallocate\nassociate"
  },
  {
    "objectID": "applied_lec6.html#instance-setup-12",
    "href": "applied_lec6.html#instance-setup-12",
    "title": "Intro to AWS",
    "section": "Instance setup",
    "text": "Instance setup\n\n\n\nElastic IP"
  },
  {
    "objectID": "applied_lec6.html#instance-setup-13",
    "href": "applied_lec6.html#instance-setup-13",
    "title": "Intro to AWS",
    "section": "Instance setup",
    "text": "Instance setup\n\n\n\nElastic IP\n\n\nAn then you can use\nssh -i jupyter.pem ubuntu@&lt;ELASTIC_IP&gt;\n\n\n\n\n\n\nImportant\n\n\nDon’t forget to re-execute ssh tunnel"
  },
  {
    "objectID": "applied_lab3.html",
    "href": "applied_lab3.html",
    "title": "Applied Analytics: Lab 3",
    "section": "",
    "text": "Overview\nThis lab is about completing Docker installation/first steps from Docker intro lecture."
  },
  {
    "objectID": "applied_lec2.html#dockerfile-1",
    "href": "applied_lec2.html#dockerfile-1",
    "title": "Dockerfiles",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nWhat is it?\n\n\nAn automated way to construct images, as opposed to manually executing commands in a shell.\n\nA Dockerfile is a build recipe for a Docker image.\nIt contains a series of instructions telling Docker how an image is constructed.\nThe docker build command builds an image from a Dockerfile."
  },
  {
    "objectID": "applied_lec2.html#dockerfile-2",
    "href": "applied_lec2.html#dockerfile-2",
    "title": "Dockerfiles",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nCreating a Dockerfile\n\n\nOur Dockerfile must be in a new, empty directory.\nCreate a directory to hold our Dockerfile.\n$ mkdir myimage\nCreate a Dockerfile inside this directory.\n$ cd myimage\n$ nvim Dockerfile\nYou can use whatever editor you like."
  },
  {
    "objectID": "applied_lec2.html#dockerfile-3",
    "href": "applied_lec2.html#dockerfile-3",
    "title": "Dockerfiles",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nContents\n\n\nFROM ubuntu\nRUN apt-get update\nRUN apt-get install figlet\n\nFROM indicates the base image for our build.\nEach RUN line will be executed by Docker during the build.\nOur RUN commands must be non-interactive. (No input can be provided to Docker during the build.)\n\nIn many cases, we will add the -y flag to apt-get."
  },
  {
    "objectID": "applied_lec2.html#dockerfile-4",
    "href": "applied_lec2.html#dockerfile-4",
    "title": "Dockerfiles",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nBuilding\n\n\nSave our file, then execute:\n$ docker build -t figlet .\n\n-t indicates the tag to apply to the image.\n. indicates the location of the build context. We will talk more about the build context later. To keep things simple for now: this is the directory where our Dockerfile is located."
  },
  {
    "objectID": "applied_lec2.html#dockerfile-5",
    "href": "applied_lec2.html#dockerfile-5",
    "title": "Dockerfiles",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nFull log\n\n\n[+] Building 9.3s (8/8) FINISHED                                                                                                                                                                docker:desktop-linux\n =&gt; [internal] load build definition from Dockerfile                                                                                                                                                            0.0s\n =&gt; =&gt; transferring dockerfile: 95B                                                                                                                                                                             0.0s\n =&gt; [internal] load metadata for docker.io/library/ubuntu:latest                                                                                                                                                0.1s\n =&gt; [internal] load .dockerignore                                                                                                                                                                               0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                                                                                 0.0s\n =&gt; [1/3] FROM docker.io/library/ubuntu:latest@sha256:353675e2a41babd526e2b837d7ec780c2a05bca0164f7ea5dbbd433d21d166fc                                                                                          3.5s\n =&gt; =&gt; resolve docker.io/library/ubuntu:latest@sha256:353675e2a41babd526e2b837d7ec780c2a05bca0164f7ea5dbbd433d21d166fc                                                                                          3.4s\n =&gt; [auth] library/ubuntu:pull token for registry-1.docker.io                                                                                                                                                   0.0s\n =&gt; [2/3] RUN apt-get update                                                                                                                                                                                    2.9s\n =&gt; [3/3] RUN apt-get install figlet                                                                                                                                                                            1.3s\n =&gt; exporting to image                                                                                                                                                                                          1.6s\n =&gt; =&gt; exporting layers                                                                                                                                                                                         1.3s\n =&gt; =&gt; exporting manifest sha256:0d82650ef2fb2b1107ee3332b4e9167a3da20e4368b8a74764827a3091819ec9                                                                                                               0.0s\n =&gt; =&gt; exporting config sha256:474cb85cf40523c5ac0e1d337e8d97e2d3f9a3e24407a8680fd033d1820e9644                                                                                                                 0.0s\n =&gt; =&gt; exporting attestation manifest sha256:8c2c8be4e7e3253c54cdc1b68d50dd79dad45c73625a3517c7e79ec4ae2220ea                                                                                                   0.0s\n =&gt; =&gt; exporting manifest list sha256:6447daabcad8e301a8e7920b201c41f4ffc9889445c10d06050c085fd73ab8ea                                                                                                          0.0s\n =&gt; =&gt; naming to docker.io/library/figlet:latest                                                                                                                                                                0.0s\n =&gt; =&gt; unpacking to docker.io/library/figlet:latest"
  },
  {
    "objectID": "applied_lec2.html#dockerfile-6",
    "href": "applied_lec2.html#dockerfile-6",
    "title": "Dockerfiles",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nSteps\n\n\n\nBuildKit transfers the Dockerfile and the build context (these are the first two [internal] stages)\nThen it executes the steps defined in the Dockerfile ([1/3], [2/3], [3/3])\nFinally, it exports the result of the build (image definition + collection of layers)\n\n\n\n\n\n\nIn a CI, the output will be different. Revert to old output with --progress=plain."
  },
  {
    "objectID": "applied_lec2.html#dockerfile-7",
    "href": "applied_lec2.html#dockerfile-7",
    "title": "Dockerfiles",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nCaching system\n\n\n\nAfter each build step, Docker takes a snapshot of the resulting image.\nBefore executing a step, Docker checks if it has already built the same sequence.\nDocker uses the exact strings defined in your Dockerfile, so:\n\nRUN apt-get install figlet cowsay\nis different from\nRUN apt-get install cowsay figlet\n\n\nRUN apt-get update is not re-executed when the mirrors are updated\nYou can force a rebuild with docker build --no-cache ...."
  },
  {
    "objectID": "applied_lec2.html#dockerfile-8",
    "href": "applied_lec2.html#dockerfile-8",
    "title": "Dockerfiles",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nResult\n\n\nIdentical to manual:\ndocker run -it figlet\nroot@25ac5862b142:/# figlet hey\n _\n| |__   ___ _   _\n| '_ \\ / _ \\ | | |\n| | | |  __/ |_| |\n|_| |_|\\___|\\__, |\n            |___/\nroot@25ac5862b142:/#"
  },
  {
    "objectID": "applied_lec2.html#dockerfile-9",
    "href": "applied_lec2.html#dockerfile-9",
    "title": "Dockerfiles",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nImage history\n\n\n\nThe history command lists all the layers composing an image.\nFor each layer, it shows its creation time, size, and creation command.\nWhen an image was built with a Dockerfile, each layer corresponds to a line of the Dockerfile.\n\ndocker history figlet\nIMAGE          CREATED        CREATED BY                                      SIZE      COMMENT\n6447daabcad8   19 hours ago   RUN /bin/sh -c apt-get install figlet # buil…   1.34MB    buildkit.dockerfile.v0\n&lt;missing&gt;      19 hours ago   RUN /bin/sh -c apt-get update # buildkit        56.4MB    buildkit.dockerfile.v0\n&lt;missing&gt;      12 days ago    /bin/sh -c #(nop)  CMD [\"/bin/bash\"]            0B\n&lt;missing&gt;      12 days ago    /bin/sh -c #(nop) ADD file:4e55519deacaaab35…   110MB\n&lt;missing&gt;      12 days ago    /bin/sh -c #(nop)  LABEL org.opencontainers.…   0B\n&lt;missing&gt;      12 days ago    /bin/sh -c #(nop)  LABEL org.opencontainers.…   0B\n&lt;missing&gt;      12 days ago    /bin/sh -c #(nop)  ARG LAUNCHPAD_BUILD_ARCH     0B\n&lt;missing&gt;      12 days ago    /bin/sh -c #(nop)  ARG RELEASE                  0B"
  },
  {
    "objectID": "applied_lec2.html#dockerfile-10",
    "href": "applied_lec2.html#dockerfile-10",
    "title": "Dockerfiles",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\n\n\n\nWhy sh -c?\n\n\n\nOn UNIX, to start a new program, we need two system calls:\n\nfork(), to create a new child process;\nexecve(), to replace the new child process with the program to run.\n\nConceptually, execve() works like this:\n\nexecve(program, [list, of, arguments])\n\nWhen we run a command, e.g. ls -l /tmp, something needs to parse the command. (i.e. split the program and its arguments into a list.)\nThe shell is usually doing that. (It also takes care of expanding environment variables and special things like ~.)"
  },
  {
    "objectID": "applied_lec2.html#dockerfile-11",
    "href": "applied_lec2.html#dockerfile-11",
    "title": "Dockerfiles",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nExec syntax\n\n\nDocker can parse the command by itself.\nInstead of plain string, or shell syntax:\nRUN apt-get install figlet\nwe can use JSON list, or exec syntax:\nRUN [\"apt-get\", \"install\", \"figlet\"]"
  },
  {
    "objectID": "applied_lec2.html#dockerfile-12",
    "href": "applied_lec2.html#dockerfile-12",
    "title": "Dockerfiles",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nCheck exec syntax\n\n\n\nChange Dockerfile to\n\nFROM ubuntu\nRUN apt-get update\nRUN [\"apt-get\", \"install\", \"figlet\"]\n\nBuild it:\n\ndocker build -t figlet .\n\nCheck history:\n\n$ docker history figlet\nIMAGE          CREATED         CREATED BY                                      SIZE      COMMENT\n18c1be63d556   4 seconds ago   RUN apt-get install figlet # buildkit           1.34MB    buildkit.dockerfile.v0\n&lt;missing&gt;      19 hours ago    RUN /bin/sh -c apt-get update # buildkit        56.4MB    buildkit.dockerfile.v0\n...\nExact command!"
  },
  {
    "objectID": "applied_lec2.html#cmd-and-entrypoint-1",
    "href": "applied_lec2.html#cmd-and-entrypoint-1",
    "title": "Dockerfiles",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nDefault commands\n\n\nWhen people run our container, we want to greet them with a nice hello message, and using a custom font.\nFor that, we will execute:\nfiglet -f script hello\n-f script tells figlet to use a fancy font.\nhello is the message that we want it to display."
  },
  {
    "objectID": "applied_lec2.html#cmd-and-entrypoint-2",
    "href": "applied_lec2.html#cmd-and-entrypoint-2",
    "title": "Dockerfiles",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nAdding CMD to Dockerfile\n\n\nOur new Dockerfile will look like this:\nFROM ubuntu\nRUN apt-get update\nRUN [\"apt-get\", \"install\", \"figlet\"]\nCMD figlet -f script hello\nCMD defines a default command to run when none is given.\nIt can appear at any point in the file.\nEach CMD will replace and override the previous one. As a result, while you can have multiple CMD lines, it is useless."
  },
  {
    "objectID": "applied_lec2.html#cmd-and-entrypoint-3",
    "href": "applied_lec2.html#cmd-and-entrypoint-3",
    "title": "Dockerfiles",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nBuild and run\n\n\n$ docker build -t figlet .\n[+] Building 3.4s (8/8) FINISHED                                                                                                                                                                docker:desktop-linux\n...\n$ docker run -it figlet\n _          _   _\n| |        | | | |\n| |     _  | | | |  __\n|/ \\   |/  |/  |/  /  \\_\n|   |_/|__/|__/|__/\\__/"
  },
  {
    "objectID": "applied_lec2.html#cmd-and-entrypoint-4",
    "href": "applied_lec2.html#cmd-and-entrypoint-4",
    "title": "Dockerfiles",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nCMD override\n\n\nIf we want to get a shell into our container (instead of running figlet), we just have to specify a different program to run:\n$ docker run -it figlet bash\nroot@3e95f6bafdd9:/#\nWe specified bash.\nIt replaced the value of CMD."
  },
  {
    "objectID": "applied_lec2.html#cmd-and-entrypoint-5",
    "href": "applied_lec2.html#cmd-and-entrypoint-5",
    "title": "Dockerfiles",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nUsing ENTRYPOINT\n\n\nObjective: we want to be able to specify a different message on the command line, while retaining figlet and some default parameters.\nIn other words, we would like to be able to do this:\n$ docker run figlet salut\n           _            \n          | |           \n ,   __,  | |       _|_ \n/ \\_/  |  |/  |   |  |  \n \\/ \\_/|_/|__/ \\_/|_/|_/\nWe will use the ENTRYPOINT verb in Dockerfile."
  },
  {
    "objectID": "applied_lec2.html#cmd-and-entrypoint-6",
    "href": "applied_lec2.html#cmd-and-entrypoint-6",
    "title": "Dockerfiles",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nDockerfile with ENTRYPOINT\n\n\nOur new Dockerfile will look like this:\nFROM ubuntu\nRUN apt-get update\nRUN [\"apt-get\", \"install\", \"figlet\"]\nENTRYPOINT [\"figlet\", \"-f\", \"script\"]\n\nENTRYPOINT defines a base command (and its parameters) for the container.\nThe command line arguments are appended to those parameters.\nLike CMD, ENTRYPOINT can appear anywhere, and replaces the previous value."
  },
  {
    "objectID": "applied_lec2.html#cmd-and-entrypoint-7",
    "href": "applied_lec2.html#cmd-and-entrypoint-7",
    "title": "Dockerfiles",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nBuild\n\n\ndocker build -t figlet .\n[+] Building 0.1s (7/7) FINISHED                                                                                                                                                                docker:desktop-linux\n...\n$ docker run figlet salve\n           _\n          | |\n ,   __,  | |       _\n/ \\_/  |  |/  |  |_|/\n \\/ \\_/|_/|__/ \\/  |__/"
  },
  {
    "objectID": "applied_lec2.html#cmd-and-entrypoint-8",
    "href": "applied_lec2.html#cmd-and-entrypoint-8",
    "title": "Dockerfiles",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nUsing CMD and ENTRYPOING together\n\n\nIf we use ENTRYPOINT and CMD together:\n\nENTRYPOINT will define the base command for our container.\nCMD will define the default parameter(s) for this command.\nThey both have to use JSON syntax."
  },
  {
    "objectID": "applied_lec2.html#cmd-and-entrypoint-9",
    "href": "applied_lec2.html#cmd-and-entrypoint-9",
    "title": "Dockerfiles",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nDockerfile\n\n\nFROM ubuntu\nRUN apt-get update\nRUN [\"apt-get\", \"install\", \"figlet\"]\nENTRYPOINT [\"figlet\", \"-f\", \"script\"]\nCMD [\"hello world\"]\n\nENTRYPOINT defines a base command (and its parameters) for the container.\nIf we don’t specify extra command-line arguments when starting the container, the value of CMD is appended.\nOtherwise, our extra command-line arguments are used instead of CMD."
  },
  {
    "objectID": "applied_lec2.html#cmd-and-entrypoint-10",
    "href": "applied_lec2.html#cmd-and-entrypoint-10",
    "title": "Dockerfiles",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nBuild and run\n\n\n\nBuild:\n\n$ docker build -t myfiglet .\n[+] Building 0.1s (7/7) FINISHED\n...\n\nRun without parameters:\n\n$ docker run myfiglet\n _          _   _                             _\n| |        | | | |                           | |    |\n| |     _  | | | |  __             __   ,_   | |  __|\n|/ \\   |/  |/  |/  /  \\_  |  |  |_/  \\_/  |  |/  /  |\n|   |_/|__/|__/|__/\\__/    \\/ \\/  \\__/    |_/|__/\\_/|_/"
  },
  {
    "objectID": "applied_lec2.html#cmd-and-entrypoint-11",
    "href": "applied_lec2.html#cmd-and-entrypoint-11",
    "title": "Dockerfiles",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nBuild and run\n\n\n\nRun with parameters:\n\n$ docker run myfiglet hey\n _\n| |\n| |     _\n|/ \\   |/  |   |\n|   |_/|__/ \\_/|/\n              /|\n              \\|"
  },
  {
    "objectID": "applied_lec2.html#cmd-and-entrypoint-12",
    "href": "applied_lec2.html#cmd-and-entrypoint-12",
    "title": "Dockerfiles",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nENTRYPOINT override\n\n\nWhat if we want to run a shell in our container?\nWe cannot just do docker run myfiglet bash because that would just tell figlet to display the word “bash.”\nWe use the --entrypoint parameter:\n$ docker run -it --entrypoint bash myfiglet\nroot@0e2f53d52f7d:/#"
  },
  {
    "objectID": "applied_lec2.html#cmd-and-entrypoint-13",
    "href": "applied_lec2.html#cmd-and-entrypoint-13",
    "title": "Dockerfiles",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nCMD and ENTRYPOINT recap\n\n\n\ndocker run myimage executes ENTRYPOINT + CMD\ndocker run myimage args executes ENTRYPOINT + args (overriding CMD)\ndocker run --entrypoint prog myimage executes prog (overriding both)"
  },
  {
    "objectID": "applied_lec2.html#cmd-and-entrypoint-14",
    "href": "applied_lec2.html#cmd-and-entrypoint-14",
    "title": "Dockerfiles",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\n\n\n\n\n\n\nCommand\nENTRYPOINT\nCMD\nResult\n\n\n\n\ndocker run figlet\nnone\nnone\nUse values from base image (bash)\n\n\ndocker run figlet hola\nnone\nnone\nError (executable hola not found)\n\n\ndocker run figlet\nfiglet -f script\nnone\nfiglet -f script\n\n\ndocker run figlet hola\nfiglet -f script\nnone\nfiglet -f script hola\n\n\ndocker run figlet\nnone\nfiglet -f script\nfiglet -f script\n\n\ndocker run figlet hola\nnone\nfiglet -f script\nError (executable hola not found)\n\n\ndocker run figlet\nfiglet -f script\nhello\nfiglet -f script hello\n\n\ndocker run figlet hola\nfiglet -f script\nhello\nfiglet -f script hola"
  },
  {
    "objectID": "applied_lec2.html#copying-files-during-the-build-1",
    "href": "applied_lec2.html#copying-files-during-the-build-1",
    "title": "Dockerfiles",
    "section": "Copying files during the build",
    "text": "Copying files during the build\n\n\n\nObjectives\n\n\n\nSo far, we have installed things in our container images by downloading packages.\nWe can also copy files from the build context to the container that we are building.\nThe build context is the directory containing the Dockerfile.\nfor that we use a new Dockerfile keyword: COPY."
  },
  {
    "objectID": "applied_lec2.html#copying-files-during-the-build-2",
    "href": "applied_lec2.html#copying-files-during-the-build-2",
    "title": "Dockerfiles",
    "section": "Copying files during the build",
    "text": "Copying files during the build\n\n\n\nBuilding C code\n\n\nWe want to build a container that compiles a basic “Hello world” program in C.\nHere is the program, hello.c:\nint main () {\n  puts(\"Hello, world!\");\n  return 0;\n}\nLet’s create a new directory, and put this file in there.\nThen we will write the Dockerfile."
  },
  {
    "objectID": "applied_lec2.html#copying-files-during-the-build-3",
    "href": "applied_lec2.html#copying-files-during-the-build-3",
    "title": "Dockerfiles",
    "section": "Copying files during the build",
    "text": "Copying files during the build\n\n\n\nDockerfile for building\n\n\nOn Debian and Ubuntu, the package build-essential will get us a compiler.\nWhen installing it, don’t forget to specify the -y flag, otherwise the build will fail (since the build cannot be interactive).\nThen we will use COPY to place the source file into the container.\nFROM ubuntu\nRUN [\"apt-get\", \"update\"]\nRUN [\"apt-get\", \"install\", \"-y\", \"build-essential\"]\nCOPY hello.c /\nRUN make hello\nCMD /hello\nCreate this Dockerfile."
  },
  {
    "objectID": "applied_lec2.html#copying-files-during-the-build-4",
    "href": "applied_lec2.html#copying-files-during-the-build-4",
    "title": "Dockerfiles",
    "section": "Copying files during the build",
    "text": "Copying files during the build\n\n\n\nTesting our C program\n\n\n\nCreate hello.c and Dockerfile in the same directory.\nRun docker build -t hello . in this directory.\nRun docker run hello, you should see Hello, world!."
  },
  {
    "objectID": "applied_lec2.html#copying-files-during-the-build-5",
    "href": "applied_lec2.html#copying-files-during-the-build-5",
    "title": "Dockerfiles",
    "section": "Copying files during the build",
    "text": "Copying files during the build\n\n\n\nCOPY and the build cache\n\n\n\nRun the build again.\nNow, modify hello.c and run the build again.\nDocker can cache steps involving COPY.\nThose steps will not be executed again if the files haven’t been changed."
  },
  {
    "objectID": "applied_lec2.html#copying-files-during-the-build-6",
    "href": "applied_lec2.html#copying-files-during-the-build-6",
    "title": "Dockerfiles",
    "section": "Copying files during the build",
    "text": "Copying files during the build\n\n\n\nDetails\n\n\n\nWe can COPY whole directories recursively It is possible to do e.g. COPY . . (but it might require some extra precautions to avoid copying too much)\nIn older Dockerfiles, you might see the ADD command; consider it deprecated (it is similar to COPY but can automatically extract archives)\nIf we really wanted to compile C code in a container, we would:\n\nplace it in a different directory, with the WORKDIR instruction\neven better, use the gcc official image"
  },
  {
    "objectID": "applied_lec2.html#copying-files-during-the-build-7",
    "href": "applied_lec2.html#copying-files-during-the-build-7",
    "title": "Dockerfiles",
    "section": "Copying files during the build",
    "text": "Copying files during the build\n\n\n\n.dockerignore\n\n\n\nWe can create a file named .dockerignore (at the top-level of the build context)\nIt can contain file names and globs to ignore\nThey won’t be sent to the builder (and won’t end up in the resulting image)\nSee the documentation for the little details (exceptions can be made with !, multiple directory levels with **…)"
  },
  {
    "objectID": "applied_lec2.html#exercise-writing-dockerfiles-1",
    "href": "applied_lec2.html#exercise-writing-dockerfiles-1",
    "title": "Dockerfiles",
    "section": "Exercise — writing Dockerfiles",
    "text": "Exercise — writing Dockerfiles\n\n\n\nExercise — writing Dockerfiles\n\n\n\nLet’s write Dockerfiles for an existing application!\nCheck out the code repository\nRead all the instructions\nWrite Dockerfiles\nBuild and test them individually"
  },
  {
    "objectID": "applied_lec2.html#exercise-writing-dockerfiles-3",
    "href": "applied_lec2.html#exercise-writing-dockerfiles-3",
    "title": "Dockerfiles",
    "section": "Exercise — writing Dockerfiles",
    "text": "Exercise — writing Dockerfiles\n\n\n\nCode repository\n\n\n\nClone the repository available at https://github.com/jpetazzo/wordsmith It should look like this:\n\n├── LICENSE\n├── README\n├── db/\n│   └── words.sql\n├── web/\n│   ├── dispatcher.go\n│   └── static/\n└── words/\n    ├── pom.xml\n    └── src/"
  },
  {
    "objectID": "applied_lec2.html#exercise-writing-dockerfiles-4",
    "href": "applied_lec2.html#exercise-writing-dockerfiles-4",
    "title": "Dockerfiles",
    "section": "Exercise — writing Dockerfiles",
    "text": "Exercise — writing Dockerfiles\n\n\n\nInstructions\n\n\nThe repository contains instructions in English and French.  For now, we only care about the first part (about writing Dockerfiles).  Place each Dockerfile in its own directory, like this:\n├── LICENSE\n├── README\n├── db/\n│   ├── Dockerfile\n│   └── words.sql\n├── web/\n│   ├── Dockerfile\n│   ├── dispatcher.go\n│   └── static/\n└── words/\n    ├── Dockerfile\n    ├── pom.xml\n    └── src/"
  },
  {
    "objectID": "applied_lec2.html#exercise-writing-dockerfiles-5",
    "href": "applied_lec2.html#exercise-writing-dockerfiles-5",
    "title": "Dockerfiles",
    "section": "Exercise — writing Dockerfiles",
    "text": "Exercise — writing Dockerfiles\n\n\n\nBuild and test\n\n\nBuild and run each Dockerfile individually.\nFor db, we should be able to see some messages confirming that the data set was loaded successfully (some INSERT lines in the container output).\nFor web and words, we should be able to see some message looking like “server started successfully”.\nThat’s all we care about for now!\nBonus question: make sure that each container stops correctly when hitting Ctrl-C."
  },
  {
    "objectID": "applied_lec2.html#reducing-image-size-1",
    "href": "applied_lec2.html#reducing-image-size-1",
    "title": "Dockerfiles",
    "section": "Reducing image size",
    "text": "Reducing image size\n\n\n\nSize consideration\n\n\nIn the previous example, our final image contained:\n\nour hello program\nits source code\nthe compiler\n\nOnly the first one is strictly necessary.\nWe are going to see how to obtain an image without the superfluous components."
  },
  {
    "objectID": "applied_lec2.html#reducing-image-size-2",
    "href": "applied_lec2.html#reducing-image-size-2",
    "title": "Dockerfiles",
    "section": "Reducing image size",
    "text": "Reducing image size\n\n\n\nCan’t we remove superfluous files with RUN?\n\n\nWhat happens if we do one of the following commands?\n\nRUN rm -rf ...\nRUN apt-get remove ...\nRUN make clean ...\n\nThis adds a layer which removes a bunch of files. But the previous layers (which added the files) still exist."
  },
  {
    "objectID": "applied_lec2.html#reducing-image-size-3",
    "href": "applied_lec2.html#reducing-image-size-3",
    "title": "Dockerfiles",
    "section": "Reducing image size",
    "text": "Reducing image size\n\n\n\nRemoving files with an extra layer\n\n\nWhen downloading an image, all the layers must be downloaded.\n\n\n\n\n\n\n\n\n\nDockerfile instruction\nLayer size\nImage size\n\n\n\n\nFROM ubuntu\nSize of base image\nSize of base image\n\n\n...\n…\nSum of this layer + all previous ones\n\n\nRUN apt-get install somepackage\nSize of files added (e.g. a few MB)\nSum of this layer + all previous ones\n\n\n...\n…\nSum of this layer + all previous ones\n\n\nRUN apt-get remove somepackage\nAlmost zero (just metadata)\nSame as previous one\n\n\n\n\nTherefore, RUN rm does not reduce the size of the image or free up disk space."
  },
  {
    "objectID": "applied_lec2.html#reducing-image-size-4",
    "href": "applied_lec2.html#reducing-image-size-4",
    "title": "Dockerfiles",
    "section": "Reducing image size",
    "text": "Reducing image size\n\n\n\nRemoving unnecessary files\n\n\nVarious techniques are available to obtain smaller images:\n\ncollapsing layers,\nadding binaries that are built outside of the Dockerfile,\nsquashing the final image,\nmulti-stage builds.\n\nLet’s review them quickly."
  },
  {
    "objectID": "applied_lec2.html#reducing-image-size-5",
    "href": "applied_lec2.html#reducing-image-size-5",
    "title": "Dockerfiles",
    "section": "Reducing image size",
    "text": "Reducing image size\n\n\n\nCollapsing layers\n\n\nYou will frequently see Dockerfiles like this:\nFROM ubuntu\nRUN apt-get update && apt-get install xxx && ... && apt-get remove xxx && ...\nOr the (more readable) variant:\nFROM ubuntu\nRUN apt-get update \\\n && apt-get install xxx \\\n && ... \\\n && apt-get remove xxx \\\n && ...\nThis RUN command gives us a single layer.\nThe files that are added, then removed in the same layer, do not grow the layer size."
  },
  {
    "objectID": "applied_lec2.html#reducing-image-size-6",
    "href": "applied_lec2.html#reducing-image-size-6",
    "title": "Dockerfiles",
    "section": "Reducing image size",
    "text": "Reducing image size\nCollapsing layers: pros and cons\n\n\n\n\n\nPros:\n\n\n\nworks on all versions of Docker\ndoesn’t require extra tools\n\n\n\n\n\n\n\n\nCons:\n\n\n\nnot very readable\nsome unnecessary files might still remain if the cleanup is not thorough\nthat layer is expensive (slow to build)"
  },
  {
    "objectID": "applied_lec2.html#reducing-image-size-7",
    "href": "applied_lec2.html#reducing-image-size-7",
    "title": "Dockerfiles",
    "section": "Reducing image size",
    "text": "Reducing image size\n\n\n\nBuilding binaries outside of the Dockerfile\n\n\nThis results in a Dockerfile looking like this:\nFROM ubuntu\nCOPY xxx /usr/local/bin\nOf course, this implies that the file xxx exists in the build context.\nThat file has to exist before you can run docker build.\nFor instance, it can:\n\nexist in the code repository,\nbe created by another tool (script, Makefile…),\nbe created by another container image and extracted from the image.\n\nSee for instance the busybox official image or this older busybox image."
  },
  {
    "objectID": "applied_lec2.html#reducing-image-size-8",
    "href": "applied_lec2.html#reducing-image-size-8",
    "title": "Dockerfiles",
    "section": "Reducing image size",
    "text": "Reducing image size\nBuilding binaries outside: pros and cons\n\n\n\n\n\nPros:\n\n\n\nfinal image can be very small\n\n\n\n\n\n\n\n\nCons:\n\n\n\nrequires an extra build tool\nwe’re back in dependency hell and “works on my machine”\n\nif binary is added to code repository:\n\nbreaks portability across different platforms\ngrows repository size a lot if the binary is updated frequently"
  },
  {
    "objectID": "applied_lec2.html#reducing-image-size-9",
    "href": "applied_lec2.html#reducing-image-size-9",
    "title": "Dockerfiles",
    "section": "Reducing image size",
    "text": "Reducing image size\n\n\n\nSquashing the final image\n\n\nThe idea is to transform the final image into a single-layer image.\nThis can be done in (at least) two ways.\n\nActivate experimental features and squash the final image:\ndocker image build --squash ...\nExport/import the final image.\ndocker build -t temp-image .\ndocker run --entrypoint true --name temp-container temp-image\ndocker export temp-container | docker import - final-image\ndocker rm temp-container\ndocker rmi temp-image"
  },
  {
    "objectID": "applied_lec2.html#reducing-image-size-10",
    "href": "applied_lec2.html#reducing-image-size-10",
    "title": "Dockerfiles",
    "section": "Reducing image size",
    "text": "Reducing image size\nSquashing the image: pros and cons\n\n\n\n\n\nPros:\n\n\n\nsingle-layer images are smaller and faster to download\nremoved files no longer take up storage and network resources\n\n\n\n\n\n\n\n\nCons:\n\n\n\nwe still need to actively remove unnecessary files\nsquash operation can take a lot of time (on big images)\nsquash operation does not benefit from cache  (even if we change just a tiny file, the whole image needs to be re-squashed)"
  },
  {
    "objectID": "applied_lec2.html#reducing-image-size-11",
    "href": "applied_lec2.html#reducing-image-size-11",
    "title": "Dockerfiles",
    "section": "Reducing image size",
    "text": "Reducing image size\n\n\n\nMulti-stage builds\n\n\nMulti-stage builds allow us to have multiple stages.\nEach stage is a separate image, and can copy files from previous stages.\nWe’re going to see how they work in more detail."
  },
  {
    "objectID": "applied_lec2.html#multi-stage-builds-2",
    "href": "applied_lec2.html#multi-stage-builds-2",
    "title": "Dockerfiles",
    "section": "Multi-stage builds",
    "text": "Multi-stage builds\n\n\n\nDescription\n\n\n\nAt any point in our Dockerfile, we can add a new FROM line.\nThis line starts a new stage of our build.\nEach stage can access the files of the previous stages with COPY --from=....\nWhen a build is tagged (with docker build -t ...), the last stage is tagged.\nPrevious stages are not discarded: they will be used for caching, and can be referenced."
  },
  {
    "objectID": "applied_lec2.html#multi-stage-builds-3",
    "href": "applied_lec2.html#multi-stage-builds-3",
    "title": "Dockerfiles",
    "section": "Multi-stage builds",
    "text": "Multi-stage builds\n\n\n\nMulti-stage builds in practice\n\n\n\nEach stage is numbered, starting at 0\nWe can copy a file from a previous stage by indicating its number, e.g.:\nCOPY --from=0 /file/from/first/stage /location/in/current/stage\nWe can also name stages, and reference these names:\nFROM golang AS builder\nRUN ...\nFROM alpine\nCOPY --from=builder /go/bin/mylittlebinary /usr/local/bin/"
  },
  {
    "objectID": "applied_lec2.html#multi-stage-builds-4",
    "href": "applied_lec2.html#multi-stage-builds-4",
    "title": "Dockerfiles",
    "section": "Multi-stage builds",
    "text": "Multi-stage builds\n\n\n\nMulti-stage builds for our C program\n\n\nWe will change our Dockerfile to:\n\ngive a nickname to the first stage: compiler\nadd a second stage using the same ubuntu base image\nadd the hello binary to the second stage\nmake sure that CMD is in the second stage\n\nThe resulting Dockerfile is on the next slide."
  },
  {
    "objectID": "applied_lec2.html#multi-stage-builds-5",
    "href": "applied_lec2.html#multi-stage-builds-5",
    "title": "Dockerfiles",
    "section": "Multi-stage builds",
    "text": "Multi-stage builds\n\n\n\nMulti-stage build Dockerfile\n\n\nHere is the final Dockerfile:\nFROM ubuntu AS compiler\nRUN [\"apt-get\", \"update\"]\nRUN [\"apt-get\", \"install\", \"-y\", \"build-essential\"]\nCOPY hello.c /\nRUN [\"make\", \"hello\"]\n\nFROM ubuntu\nCOPY --from=compiler /hello /hello\nCMD /hello\nLet’s build it, and check that it works correctly:\ndocker build -t hellomultistage .\ndocker run hellomultistage"
  },
  {
    "objectID": "applied_lec2.html#multi-stage-builds-6",
    "href": "applied_lec2.html#multi-stage-builds-6",
    "title": "Dockerfiles",
    "section": "Multi-stage builds",
    "text": "Multi-stage builds\n\n\n\nComparing single/multi-stage build image sizes\n\n\nList our images with docker images, and check the size of:\n\nthe ubuntu base image,\nthe single-stage hello image,\nthe multi-stage hellomultistage image.\n\nWe can achieve even smaller images if we use smaller base images.\nHowever, if we use common base images (e.g. if we standardize on ubuntu), these common images will be pulled only once per node, so they are virtually “free.”"
  },
  {
    "objectID": "applied_lec2.html#multi-stage-builds-7",
    "href": "applied_lec2.html#multi-stage-builds-7",
    "title": "Dockerfiles",
    "section": "Multi-stage builds",
    "text": "Multi-stage builds\n\n\n\nResults\n\n\n$ docker images | grep ubuntu\nubuntu                                         latest         353675e2a41b   2 weeks ago      139MB\n$ docker images | grep hello\nhellomultistage                                latest         977190f18730   55 seconds ago   139MB\nhello                                          latest         09316393a5fe   30 minutes ago   707MB"
  },
  {
    "objectID": "applied_lec2.html#multi-stage-builds-8",
    "href": "applied_lec2.html#multi-stage-builds-8",
    "title": "Dockerfiles",
    "section": "Multi-stage builds",
    "text": "Multi-stage builds\n\n\n\nBuild targets\n\n\n\nWe can also tag an intermediary stage with the following command:\ndocker build --target STAGE --tag NAME\nThis will create an image (named NAME) corresponding to stage STAGE\nThis can be used to easily access an intermediary stage for inspection\n(instead of parsing the output of docker build to find out the image ID)\nThis can also be used to describe multiple images from a single Dockerfile\n(instead of using multiple Dockerfiles, which could go out of sync)"
  },
  {
    "objectID": "applied_lec2.html#multi-stage-builds-9",
    "href": "applied_lec2.html#multi-stage-builds-9",
    "title": "Dockerfiles",
    "section": "Multi-stage builds",
    "text": "Multi-stage builds\n\n\n\nDealing with download caches\n\n\n\nIn some cases, our images contain temporary downloaded files or caches\n(examples: packages downloaded by pip, Maven, etc.)\nThese can sometimes be disabled\n(e.g. pip install --no-cache-dir ...)\nThe cache can also be cleaned immediately after installing\n(e.g. pip install ... && rm -rf ~/.cache/pip)"
  },
  {
    "objectID": "applied_lec2.html#multi-stage-builds-10",
    "href": "applied_lec2.html#multi-stage-builds-10",
    "title": "Dockerfiles",
    "section": "Multi-stage builds",
    "text": "Multi-stage builds\n\n\n\nDownload caches and multi-stage builds\n\n\n\nDownload+install packages in a build stage\nCopy the installed packages to a run stage\nExample: in the specific case of Python, use a virtual env\n(install in the virtual env; then copy the virtual env directory)"
  },
  {
    "objectID": "applied_lec2.html#multi-stage-builds-11",
    "href": "applied_lec2.html#multi-stage-builds-11",
    "title": "Dockerfiles",
    "section": "Multi-stage builds",
    "text": "Multi-stage builds\n\n\n\nDownload caches and BuildKit\n\n\n\nBuildKit has a caching feature for run stages\nIt can address download caches elegantly\nExample:\nRUN --mount=type=cache,target=/pipcache pip install --cache-dir /pipcache ...\nThe cache won’t be in the final image, but it’ll persist across builds"
  },
  {
    "objectID": "applied_lec2.html#publishing-images-to-the-docker-hub-1",
    "href": "applied_lec2.html#publishing-images-to-the-docker-hub-1",
    "title": "Dockerfiles",
    "section": "Publishing images to the Docker Hub",
    "text": "Publishing images to the Docker Hub\n\n\n\nOverview\n\n\nWe have built our first images.\nWe can now publish it to the Docker Hub!"
  },
  {
    "objectID": "applied_lec2.html#publishing-images-to-the-docker-hub-2",
    "href": "applied_lec2.html#publishing-images-to-the-docker-hub-2",
    "title": "Dockerfiles",
    "section": "Publishing images to the Docker Hub",
    "text": "Publishing images to the Docker Hub\n\n\n\nLogging into our Docker Hub account\n\n\n\nThis can be done from the Docker CLI:\ndocker login"
  },
  {
    "objectID": "applied_lec2.html#publishing-images-to-the-docker-hub-3",
    "href": "applied_lec2.html#publishing-images-to-the-docker-hub-3",
    "title": "Dockerfiles",
    "section": "Publishing images to the Docker Hub",
    "text": "Publishing images to the Docker Hub\n\n\n\nImage tags and registry addresses\n\n\n\nDocker images tags are like Git tags and branches.\nThey are like bookmarks pointing at a specific image ID.\nTagging an image doesn’t rename an image: it adds another tag.\nWhen pushing an image to a registry, the registry address is in the tag.\nExample: registry.example.net:5000/image\nWhat about Docker Hub images?\n\n\n\n\n\n\n\njpetazzo/clock is, in fact, index.docker.io/jpetazzo/clock\nubuntu is, in fact, library/ubuntu, i.e. index.docker.io/library/ubuntu"
  },
  {
    "objectID": "applied_lec2.html#publishing-images-to-the-docker-hub-4",
    "href": "applied_lec2.html#publishing-images-to-the-docker-hub-4",
    "title": "Dockerfiles",
    "section": "Publishing images to the Docker Hub",
    "text": "Publishing images to the Docker Hub\n\n\n\nTagging an image to push it on the Hub\n\n\n\nLet’s tag our figlet image (or any other to our liking):\ndocker tag figlet jpetazzo/figlet\nAnd push it to the Hub:\ndocker push jpetazzo/figlet\nThat’s it!\n\n\n\n\n\n\nAnybody can now docker run jpetazzo/figlet anywhere."
  },
  {
    "objectID": "applied_lec2.html#publishing-images-to-the-docker-hub-5",
    "href": "applied_lec2.html#publishing-images-to-the-docker-hub-5",
    "title": "Dockerfiles",
    "section": "Publishing images to the Docker Hub",
    "text": "Publishing images to the Docker Hub\n\n\n\nThe goodness of automated builds\n\n\n\nYou can link a Docker Hub repository with a GitHub or BitBucket repository\nEach push to GitHub or BitBucket will trigger a build on Docker Hub\nIf the build succeeds, the new image is available on Docker Hub\nYou can map tags and branches between source and container images\nIf you work with public repositories, this is free"
  },
  {
    "objectID": "applied_lec2.html#publishing-images-to-the-docker-hub-6",
    "href": "applied_lec2.html#publishing-images-to-the-docker-hub-6",
    "title": "Dockerfiles",
    "section": "Publishing images to the Docker Hub",
    "text": "Publishing images to the Docker Hub\n\n\n\nSetting up an automated build\n\n\n\nWe need a Dockerized repository!\nLet’s go to https://github.com/jpetazzo/trainingwheels and fork it.\nGo to the Docker Hub (https://hub.docker.com/) and sign-in. Select “Repositories” in the blue navigation menu.\nSelect “Create” in the top-right bar, and select “Create Repository+”.\nConnect your Docker Hub account to your GitHub account.\nClick “Create” button.\nThen go to “Builds” folder.\nClick on Github icon and select your user and the repository that we just forked.\nIn “Build rules” block near page bottom, put /www in “Build Context” column (or whichever directory the Dockerfile is in).\nClick “Save and Build” to build the repository immediately (without waiting for a git push).\nSubsequent builds will happen automatically, thanks to GitHub hooks."
  },
  {
    "objectID": "applied_lec2.html#publishing-images-to-the-docker-hub-7",
    "href": "applied_lec2.html#publishing-images-to-the-docker-hub-7",
    "title": "Dockerfiles",
    "section": "Publishing images to the Docker Hub",
    "text": "Publishing images to the Docker Hub\n\n\n\nBuilding on the fly\n\n\n\nSome services can build images on the fly from a repository\nExample: ctr.run\n\nThere might be a long pause before the first layer is pulled, because the API behind docker pull doesn’t allow to stream build logs, and there is no feedback during the build.\nIt is possible to view the build logs by setting up an account on ctr.run."
  },
  {
    "objectID": "applied_lec2.html#tips-for-efficient-dockerfiles-1",
    "href": "applied_lec2.html#tips-for-efficient-dockerfiles-1",
    "title": "Dockerfiles",
    "section": "Tips for efficient Dockerfiles",
    "text": "Tips for efficient Dockerfiles\n\n\n\nOverview\n\n\nWe will see how to:\n\nReduce the number of layers.\nLeverage the build cache so that builds can be faster.\nEmbed unit testing in the build process."
  },
  {
    "objectID": "applied_lec2.html#tips-for-efficient-dockerfiles-2",
    "href": "applied_lec2.html#tips-for-efficient-dockerfiles-2",
    "title": "Dockerfiles",
    "section": "Tips for efficient Dockerfiles",
    "text": "Tips for efficient Dockerfiles\n\n\n\nReducing the number of layers\n\n\n\nEach line in a Dockerfile creates a new layer.\nBuild your Dockerfile to take advantage of Docker’s caching system.\nCombine commands by using && to continue commands and \\ to wrap lines.\n\nNote: it is frequent to build a Dockerfile line by line:\nRUN apt-get install thisthing\nRUN apt-get install andthatthing andthatotherone\nRUN apt-get install somemorestuff\nAnd then refactor it trivially before shipping:\nRUN apt-get install thisthing andthatthing andthatotherone somemorestuff"
  },
  {
    "objectID": "applied_lec2.html#tips-for-efficient-dockerfiles-3",
    "href": "applied_lec2.html#tips-for-efficient-dockerfiles-3",
    "title": "Dockerfiles",
    "section": "Tips for efficient Dockerfiles",
    "text": "Tips for efficient Dockerfiles\n\n\n\nAvoid re-installing dependencies at each build\n\n\n\nClassic Dockerfile problem:\n\n\n“each time I change a line of code, all my dependencies are re-installed!”\n\n\nSolution: COPY dependency lists (package.json, requirements.txt, etc.) by themselves to avoid reinstalling unchanged dependencies every time."
  },
  {
    "objectID": "applied_lec2.html#tips-for-efficient-dockerfiles-4",
    "href": "applied_lec2.html#tips-for-efficient-dockerfiles-4",
    "title": "Dockerfiles",
    "section": "Tips for efficient Dockerfiles",
    "text": "Tips for efficient Dockerfiles\n\n\n\nExample “bad” Dockerfile\n\n\nThe dependencies are reinstalled every time, because the build system does not know if requirements.txt has been updated.\nFROM python\nWORKDIR /src\nCOPY . .\nRUN pip install -qr requirements.txt\nEXPOSE 5000\nCMD [\"python\", \"app.py\"]"
  },
  {
    "objectID": "applied_lec2.html#tips-for-efficient-dockerfiles-5",
    "href": "applied_lec2.html#tips-for-efficient-dockerfiles-5",
    "title": "Dockerfiles",
    "section": "Tips for efficient Dockerfiles",
    "text": "Tips for efficient Dockerfiles\n\n\n\nFixed Dockerfile\n\n\nAdding the dependencies as a separate step means that Docker can cache more efficiently and only install them when requirements.txt changes.\nFROM python\nWORKDIR /src\nCOPY requirements.txt .\nRUN pip install -qr requirements.txt\nCOPY . .\nEXPOSE 5000\nCMD [\"python\", \"app.py\"]"
  },
  {
    "objectID": "applied_lec2.html#tips-for-efficient-dockerfiles-6",
    "href": "applied_lec2.html#tips-for-efficient-dockerfiles-6",
    "title": "Dockerfiles",
    "section": "Tips for efficient Dockerfiles",
    "text": "Tips for efficient Dockerfiles\n\n\n\nBe careful with chown, chmod, mv\n\n\n\nLayers cannot store efficiently changes in permissions or ownership.\nLayers cannot represent efficiently when a file is moved either.\nAs a result, operations like chown, chmod, mv can be expensive.\nFor instance, in the Dockerfile snippet below, each RUN line creates a layer with an entire copy of some-file.\nCOPY some-file .\nRUN chown www-data:www-data some-file\nRUN chmod 644 some-file\nRUN mv some-file /var/www\nHow can we avoid that?"
  },
  {
    "objectID": "applied_lec2.html#tips-for-efficient-dockerfiles-7",
    "href": "applied_lec2.html#tips-for-efficient-dockerfiles-7",
    "title": "Dockerfiles",
    "section": "Tips for efficient Dockerfiles",
    "text": "Tips for efficient Dockerfiles\n\n\n\nPut files on the right place\n\n\n\nInstead of using mv, directly put files at the right place.\nWhen extracting archives (tar, zip…), merge operations in a single layer.\nExample:\n  ...\n  RUN wget http://.../foo.tar.gz \\\n   && tar -zxf foo.tar.gz \\\n   && mv foo/fooctl /usr/local/bin \\\n   && rm -rf foo foo.tar.gz\n..."
  },
  {
    "objectID": "applied_lec2.html#tips-for-efficient-dockerfiles-8",
    "href": "applied_lec2.html#tips-for-efficient-dockerfiles-8",
    "title": "Dockerfiles",
    "section": "Tips for efficient Dockerfiles",
    "text": "Tips for efficient Dockerfiles\n\n\n\nUse COPY --chown\n\n\n\nThe Dockerfile instruction COPY can take a --chown parameter.\nExamples:\n...\nCOPY --chown=1000 some-file .\nCOPY --chown=1000:1000 some-file .\nCOPY --chown=www-data:www-data some-file .\nThe --chown flag can specify a user, or a user:group pair.\nThe user and group can be specified as names or numbers.\nWhen using names, the names must exist in /etc/passwd or /etc/group.\n(In the container, not on the host!)"
  },
  {
    "objectID": "applied_lec2.html#tips-for-efficient-dockerfiles-9",
    "href": "applied_lec2.html#tips-for-efficient-dockerfiles-9",
    "title": "Dockerfiles",
    "section": "Tips for efficient Dockerfiles",
    "text": "Tips for efficient Dockerfiles\n\n\n\nSet correct permissions locally\n\n\n\nInstead of using chmod, set the right file permissions locally.\nWhen files are copied with COPY, permissions are preserved."
  },
  {
    "objectID": "applied_lec2.html#tips-for-efficient-dockerfiles-10",
    "href": "applied_lec2.html#tips-for-efficient-dockerfiles-10",
    "title": "Dockerfiles",
    "section": "Tips for efficient Dockerfiles",
    "text": "Tips for efficient Dockerfiles\n\n\n\nEmbedding unit tests in the build process\n\n\nFROM &lt;baseimage&gt;\nRUN &lt;install dependencies&gt;\nCOPY &lt;code&gt;\nRUN &lt;build code&gt;\nRUN &lt;install test dependencies&gt;\nCOPY &lt;test data sets and fixtures&gt;\nRUN &lt;unit tests&gt;\nFROM &lt;baseimage&gt;\nRUN &lt;install dependencies&gt;\nCOPY &lt;code&gt;\nRUN &lt;build code&gt;\nCMD, EXPOSE ...\n\nThe build fails as soon as an instruction fails\nIf RUN &lt;unit tests&gt; fails, the build doesn’t produce an image\nIf it succeeds, it produces a clean image (without test libraries and data)"
  },
  {
    "objectID": "applied_lec2.html#dockerfile-examples-1",
    "href": "applied_lec2.html#dockerfile-examples-1",
    "title": "Dockerfiles",
    "section": "Dockerfile examples",
    "text": "Dockerfile examples\n\n\n\nOverview\n\n\nThere are a number of tips, tricks, and techniques that we can use in Dockerfiles.\nBut sometimes, we have to use different (and even opposed) practices depending on:\n\nthe complexity of our project,\nthe programming language or framework that we are using,\nthe stage of our project (early MVP vs. super-stable production),\nwhether we’re building a final image or a base for further images,\netc.\n\nWe are going to show a few examples using very different techniques."
  },
  {
    "objectID": "applied_lec2.html#dockerfile-examples-2",
    "href": "applied_lec2.html#dockerfile-examples-2",
    "title": "Dockerfiles",
    "section": "Dockerfile examples",
    "text": "Dockerfile examples\n\n\n\nWhen to optimize an image\n\n\nWhen authoring official images, it is a good idea to reduce as much as possible:\n\nthe number of layers,\nthe size of the final image.\n\nThis is often done at the expense of build time and convenience for the image maintainer; but when an image is downloaded millions of time, saving even a few seconds of pull time can be worth it.\nRUN apt-get update && apt-get install -y libpng12-dev libjpeg-dev && rm -rf /var/lib/apt/lists/* \\\n    && docker-php-ext-configure gd --with-png-dir=/usr --with-jpeg-dir=/usr \\\n    && docker-php-ext-install gd\n...\nRUN curl -o wordpress.tar.gz -SL https://wordpress.org/wordpress-${WORDPRESS_UPSTREAM_VERSION}.tar.gz \\\n    && echo \"$WORDPRESS_SHA1 *wordpress.tar.gz\" | sha1sum -c - \\\n    && tar -xzf wordpress.tar.gz -C /usr/src/ \\\n    && rm wordpress.tar.gz \\\n    && chown -R www-data:www-data /usr/src/wordpress\n\n\n\n\n\n(Source: Wordpress official image)"
  },
  {
    "objectID": "applied_lec2.html#dockerfile-examples-3",
    "href": "applied_lec2.html#dockerfile-examples-3",
    "title": "Dockerfiles",
    "section": "Dockerfile examples",
    "text": "Dockerfile examples\n\n\n\nWhen to not optimize an image\n\n\nSometimes, it is better to prioritize maintainer convenience.\nIn particular, if:\n\nthe image changes a lot,\nthe image has very few users (e.g. only 1, the maintainer!),\nthe image is built and run on the same machine,\nthe image is built and run on machines with a very fast link …\n\nIn these cases, just keep things simple!\n(Next slide: a Dockerfile that can be used to preview a Jekyll / github pages site.)"
  },
  {
    "objectID": "applied_lec2.html#dockerfile-examples-4",
    "href": "applied_lec2.html#dockerfile-examples-4",
    "title": "Dockerfiles",
    "section": "Dockerfile examples",
    "text": "Dockerfile examples\n\n\n\nDockerfile for Jekyll\n\n\nFROM debian:sid\n\nRUN apt-get update -q\nRUN apt-get install -yq build-essential make\nRUN apt-get install -yq zlib1g-dev\nRUN apt-get install -yq ruby ruby-dev\nRUN apt-get install -yq python-pygments\nRUN apt-get install -yq nodejs\nRUN apt-get install -yq cmake\nRUN gem install --no-rdoc --no-ri github-pages\n\nCOPY . /blog\nWORKDIR /blog\n\nVOLUME /blog/_site\n\nEXPOSE 4000\nCMD [\"jekyll\", \"serve\", \"--host\", \"0.0.0.0\", \"--incremental\"]"
  },
  {
    "objectID": "applied_lec2.html#dockerfile-examples-5",
    "href": "applied_lec2.html#dockerfile-examples-5",
    "title": "Dockerfiles",
    "section": "Dockerfile examples",
    "text": "Dockerfile examples\n\n\n\nMulti-dimensional versioning systems\n\n\nImages can have a tag, indicating the version of the image.\nBut sometimes, there are multiple important components, and we need to indicate the versions for all of them.\nThis can be done with environment variables:\nENV PIP=9.0.3 \\\n    ZC_BUILDOUT=2.11.2 \\\n    SETUPTOOLS=38.7.0 \\\n    PLONE_MAJOR=5.1 \\\n    PLONE_VERSION=5.1.0 \\\n    PLONE_MD5=76dc6cfc1c749d763c32fff3a9870d8d\n\n\n\n\n\n(Source: Plone official image)"
  },
  {
    "objectID": "applied_lec2.html#dockerfile-examples-6",
    "href": "applied_lec2.html#dockerfile-examples-6",
    "title": "Dockerfiles",
    "section": "Dockerfile examples",
    "text": "Dockerfile examples\n\n\n\nEntrypoints and wrappers\n\n\nIt is very common to define a custom entrypoint.\nThat entrypoint will generally be a script, performing any combination of:\n\npre-flights checks (if a required dependency is not available, display a nice error message early instead of an obscure one in a deep log file),\ngeneration or validation of configuration files,\ndropping privileges (with e.g. su or gosu, sometimes combined with chown),\nand more."
  },
  {
    "objectID": "applied_lec2.html#dockerfile-examples-7",
    "href": "applied_lec2.html#dockerfile-examples-7",
    "title": "Dockerfiles",
    "section": "Dockerfile examples",
    "text": "Dockerfile examples\n\n\n\nA typical entrypoint script\n\n\n #!/bin/sh\n set -e\n \n # first arg is '-f' or '--some-option'\n # or first arg is 'something.conf'\n if [ \"${1#-}\" != \"$1\" ] || [ \"${1%.conf}\" != \"$1\" ]; then\n    set -- redis-server \"$@\"\n fi\n \n # allow the container to be started with '--user'\n if [ \"$1\" = 'redis-server' -a \"$(id -u)\" = '0' ]; then\n    chown -R redis .\n    exec su-exec redis \"$0\" \"$@\"\n fi\n \n exec \"$@\"\n\n\n\n\n\n(Source: Redis official image)"
  },
  {
    "objectID": "applied_lec2.html#dockerfile-examples-8",
    "href": "applied_lec2.html#dockerfile-examples-8",
    "title": "Dockerfiles",
    "section": "Dockerfile examples",
    "text": "Dockerfile examples\n\n\n\nFactoring information\n\n\nTo facilitate maintenance (and avoid human errors), avoid to repeat information like:\n\nversion numbers,\nremote asset URLs (e.g. source tarballs) …\n\nInstead, use environment variables.\nENV NODE_VERSION 10.2.1\n...\nRUN ...\n    && curl -fsSLO --compressed \"https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION.tar.xz\" \\\n    && curl -fsSLO --compressed \"https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc\" \\\n    && gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \\\n    && grep \" node-v$NODE_VERSION.tar.xz\\$\" SHASUMS256.txt | sha256sum -c - \\\n    && tar -xf \"node-v$NODE_VERSION.tar.xz\" \\\n    && cd \"node-v$NODE_VERSION\" \\\n...\n\n\n\n\n\n(Source: Nodejs official image)"
  },
  {
    "objectID": "applied_lec2.html#dockerfile-examples-9",
    "href": "applied_lec2.html#dockerfile-examples-9",
    "title": "Dockerfiles",
    "section": "Dockerfile examples",
    "text": "Dockerfile examples\n\n\n\nOverrides\n\n\nIn theory, development and production images should be the same.\nIn practice, we often need to enable specific behaviors in development (e.g. debug statements).\nOne way to reconcile both needs is to use Compose to enable these behaviors.\nLet’s look at the trainingwheels demo app for an example."
  },
  {
    "objectID": "applied_lec2.html#dockerfile-examples-10",
    "href": "applied_lec2.html#dockerfile-examples-10",
    "title": "Dockerfiles",
    "section": "Dockerfile examples",
    "text": "Dockerfile examples\n\n\n\nProduction image\n\n\nThis Dockerfile builds an image leveraging gunicorn:\nFROM python\nRUN pip install flask\nRUN pip install gunicorn\nRUN pip install redis\nCOPY . /src\nWORKDIR /src\nCMD gunicorn --bind 0.0.0.0:5000 --workers 10 counter:app\nEXPOSE 5000\n\n\n\n\n\n(Source: trainingwheels Dockerfile)"
  },
  {
    "objectID": "applied_lec2.html#dockerfile-examples-11",
    "href": "applied_lec2.html#dockerfile-examples-11",
    "title": "Dockerfiles",
    "section": "Dockerfile examples",
    "text": "Dockerfile examples\n\n\n\nDevelopment Compose file\n\n\nThis Compose file uses the same image, but with a few overrides for development:\n\nthe Flask development server is used (overriding CMD),\nthe DEBUG environment variable is set,\na volume is used to provide a faster local development workflow.\n\nservices:\n  www:\n    build: www\n    ports:\n      - 8000:5000\n    user: nobody\n    environment:\n      DEBUG: 1\n    command: python counter.py\n    volumes:\n      - ./www:/src\n\n\n\n\n\n(Source: trainingwheels Compose file)"
  },
  {
    "objectID": "applied_lec2.html#dockerfile-examples-12",
    "href": "applied_lec2.html#dockerfile-examples-12",
    "title": "Dockerfiles",
    "section": "Dockerfile examples",
    "text": "Dockerfile examples\n\n\n\nHow to know which best practices are better?\n\n\n\nThe main goal of containers is to make our lives easier.\nIn this chapter, we showed many ways to write Dockerfiles.\nThese Dockerfiles use sometimes diametrically opposed techniques.\nYet, they were the “right” ones for a specific situation.\nIt’s OK (and even encouraged) to start simple and evolve as needed."
  },
  {
    "objectID": "applied_lec2.html#exercise-multi-stage-builds",
    "href": "applied_lec2.html#exercise-multi-stage-builds",
    "title": "Dockerfiles",
    "section": "Exercise — multi-stage builds",
    "text": "Exercise — multi-stage builds\n\n\n\n\n\n\nExercise\n\n\nLet’s update our Dockerfiles to leverage multi-stage builds!\nThe code is at: https://github.com/jpetazzo/wordsmith.\nUse a different tag for these images, so that we can compare their sizes.\nWhat’s the size difference between single-stage and multi-stage builds?"
  },
  {
    "objectID": "applied_lec2.html#naming-and-inspecting-containers-1",
    "href": "applied_lec2.html#naming-and-inspecting-containers-1",
    "title": "Dockerfiles",
    "section": "Naming and inspecting containers",
    "text": "Naming and inspecting containers\n\n\n\nObjectives\n\n\nIn this lesson, we will learn about an important Docker concept: container naming.\nNaming allows us to:\n\nReference easily a container.\nEnsure unicity of a specific container.\n\nWe will also see the inspect command, which gives a lot of details about a container."
  },
  {
    "objectID": "applied_lec2.html#naming-and-inspecting-containers-2",
    "href": "applied_lec2.html#naming-and-inspecting-containers-2",
    "title": "Dockerfiles",
    "section": "Naming and inspecting containers",
    "text": "Naming and inspecting containers\n\n\n\nNaming our containers\n\n\nSo far, we have referenced containers with their ID.\nWe have copy-pasted the ID, or used a shortened prefix.\nBut each container can also be referenced by its name.\nIf a container is named thumbnail-worker, I can do:\n$ docker logs thumbnail-worker\n$ docker stop thumbnail-worker\netc."
  },
  {
    "objectID": "applied_lec2.html#naming-and-inspecting-containers-3",
    "href": "applied_lec2.html#naming-and-inspecting-containers-3",
    "title": "Dockerfiles",
    "section": "Naming and inspecting containers",
    "text": "Naming and inspecting containers\n\n\n\nDefault names\n\n\nWhen we create a container, if we don’t give a specific name, Docker will pick one for us.\nIt will be the concatenation of:\n\nA mood (furious, goofy, suspicious, boring…)\nThe name of a famous inventor (tesla, darwin, wozniak…)\n\nExamples: happy_curie, clever_hopper, jovial_lovelace …"
  },
  {
    "objectID": "applied_lec2.html#naming-and-inspecting-containers-4",
    "href": "applied_lec2.html#naming-and-inspecting-containers-4",
    "title": "Dockerfiles",
    "section": "Naming and inspecting containers",
    "text": "Naming and inspecting containers\n\n\n\nSpecifying a name\n\n\nYou can set the name of the container when you create it.\n$ docker run --name ticktock jpetazzo/clock\nIf you specify a name that already exists, Docker will refuse to create the container.\nThis lets us enforce unicity of a given resource."
  },
  {
    "objectID": "applied_lec2.html#naming-and-inspecting-containers-5",
    "href": "applied_lec2.html#naming-and-inspecting-containers-5",
    "title": "Dockerfiles",
    "section": "Naming and inspecting containers",
    "text": "Naming and inspecting containers\n\n\n\nRenaming containers\n\n\n\nYou can rename containers with docker rename.\nThis allows you to “free up” a name without destroying the associated container."
  },
  {
    "objectID": "applied_lec2.html#naming-and-inspecting-containers-6",
    "href": "applied_lec2.html#naming-and-inspecting-containers-6",
    "title": "Dockerfiles",
    "section": "Naming and inspecting containers",
    "text": "Naming and inspecting containers\n\n\n\nInspecting a container\n\n\nThe docker inspect command will output a very detailed JSON map.\n$ docker inspect &lt;containerID&gt;\n[{\n...\n(many pages of JSON here)\n...\nThere are multiple ways to consume that information."
  },
  {
    "objectID": "applied_lec2.html#naming-and-inspecting-containers-7",
    "href": "applied_lec2.html#naming-and-inspecting-containers-7",
    "title": "Dockerfiles",
    "section": "Naming and inspecting containers",
    "text": "Naming and inspecting containers\n\n\n\nParsing JSON with the Shell\n\n\n\nYou could grep and cut or awk the output of docker inspect.\nPlease, don’t.\nIt’s painful.\nIf you really must parse JSON from the Shell, use JQ! (It’s great.)\n\n$ docker inspect &lt;containerID&gt; | jq .\n\nWe will see a better solution which doesn’t require extra tools."
  },
  {
    "objectID": "applied_lec2.html#naming-and-inspecting-containers-8",
    "href": "applied_lec2.html#naming-and-inspecting-containers-8",
    "title": "Dockerfiles",
    "section": "Naming and inspecting containers",
    "text": "Naming and inspecting containers\n\n\n\nUsing --format\n\n\nYou can specify a format string, which will be parsed by Go’s text/template package.\n$ docker inspect --format '{{ json .Created }}' &lt;containerID&gt;\n\"2015-02-24T07:21:11.712240394Z\"\n\nThe generic syntax is to wrap the expression with double curly braces.\nThe expression starts with a dot representing the JSON object.\nThen each field or member can be accessed in dotted notation syntax.\nThe optional json keyword asks for valid JSON output. (e.g. here it adds the surrounding double-quotes.)"
  },
  {
    "objectID": "applied_lec2.html#labels-1",
    "href": "applied_lec2.html#labels-1",
    "title": "Dockerfiles",
    "section": "Labels",
    "text": "Labels\n\n\n\nOverview\n\n\n\nLabels allow to attach arbitrary metadata to containers.\nLabels are key/value pairs.\nThey are specified at container creation.\nYou can query them with docker inspect.\nThey can also be used as filters with some commands (e.g. docker ps)."
  },
  {
    "objectID": "applied_lec2.html#labels-2",
    "href": "applied_lec2.html#labels-2",
    "title": "Dockerfiles",
    "section": "Labels",
    "text": "Labels\n\n\n\nUsing labels\n\n\nLet’s create a few containers with a label owner.\ndocker run -d -l owner=alice nginx\ndocker run -d -l owner=bob nginx\ndocker run -d -l owner nginx\nWe didn’t specify a value for the owner label in the last example.\nThis is equivalent to setting the value to be an empty string."
  },
  {
    "objectID": "applied_lec2.html#labels-3",
    "href": "applied_lec2.html#labels-3",
    "title": "Dockerfiles",
    "section": "Labels",
    "text": "Labels\n\n\n\nQuerying labels\n\n\nWe can view the labels with docker inspect.\n$ docker inspect $(docker ps -lq) | grep -A3 Labels\n            \"Labels\": {\n                \"maintainer\": \"NGINX Docker Maintainers &lt;docker-maint@nginx.com&gt;\",\n                \"owner\": \"\"\n            },\nWe can use the --format flag to list the value of a label.\n$ docker inspect $(docker ps -q) --format 'OWNER={{.Config.Labels.owner}}'"
  },
  {
    "objectID": "applied_lec2.html#labels-4",
    "href": "applied_lec2.html#labels-4",
    "title": "Dockerfiles",
    "section": "Labels",
    "text": "Labels\n\n\n\nUsing labels to select containers\n\n\nWe can list containers having a specific label.\n$ docker ps --filter label=owner\nOr we can list containers having a specific label with a specific value.\n$ docker ps --filter label=owner=alice"
  },
  {
    "objectID": "applied_lec2.html#labels-5",
    "href": "applied_lec2.html#labels-5",
    "title": "Dockerfiles",
    "section": "Labels",
    "text": "Labels\n\n\n\nUse-cases for labels\n\n\n\nHTTP vhost of a web app or web service.\n(The label is used to generate the configuration for NGINX, HAProxy, etc.)\nBackup schedule for a stateful service.\n(The label is used by a cron job to determine if/when to backup container data.)\nService ownership.\n(To determine internal cross-billing, or who to page in case of outage.)\netc."
  },
  {
    "objectID": "applied_lec2.html#getting-inside-a-container-1",
    "href": "applied_lec2.html#getting-inside-a-container-1",
    "title": "Dockerfiles",
    "section": "Getting inside a container",
    "text": "Getting inside a container\n\n\n\nObjectives\n\n\nOn a traditional server or VM, we sometimes need to:\n\nlog into the machine (with SSH or on the console),\nanalyze the disks (by removing them or rebooting with a rescue system).\n\nIn this chapter, we will see how to do that with containers."
  },
  {
    "objectID": "applied_lec2.html#getting-inside-a-container-2",
    "href": "applied_lec2.html#getting-inside-a-container-2",
    "title": "Dockerfiles",
    "section": "Getting inside a container",
    "text": "Getting inside a container\n\n\n\nGetting a shell\n\n\nEvery once in a while, we want to log into a machine.\nIn an perfect world, this shouldn’t be necessary.\n\nYou need to install or update packages (and their configuration)?\nUse configuration management. (e.g. Ansible, Chef, Puppet, Salt…)\nYou need to view logs and metrics?\nCollect and access them through a centralized platform.\n\nIn the real world, though … we often need shell access!"
  },
  {
    "objectID": "applied_lec2.html#getting-inside-a-container-3",
    "href": "applied_lec2.html#getting-inside-a-container-3",
    "title": "Dockerfiles",
    "section": "Getting inside a container",
    "text": "Getting inside a container\n\n\n\nNot getting a shell\n\n\nEven without a perfect deployment system, we can do many operations without getting a shell.\n\nInstalling packages can (and should) be done in the container image.\nConfiguration can be done at the image level, or when the container starts.\nDynamic configuration can be stored in a volume (shared with another container).\nLogs written to stdout are automatically collected by the Docker Engine.\nOther logs can be written to a shared volume.\nProcess information and metrics are visible from the host.\n\nLet’s save logging, volumes … for later, but let’s have a look at process information!"
  },
  {
    "objectID": "applied_lec2.html#getting-inside-a-container-4",
    "href": "applied_lec2.html#getting-inside-a-container-4",
    "title": "Dockerfiles",
    "section": "Getting inside a container",
    "text": "Getting inside a container\n\n\n\nViewing container processes from the host\n\n\nIf you run Docker on Linux, container processes are visible on the host.\n$ ps faux | less\n\nScroll around the output of this command.\nYou should see the jpetazzo/clock container.\nA containerized process is just like any other process on the host.\nWe can use tools like lsof, strace, gdb … To analyze them."
  },
  {
    "objectID": "applied_lec2.html#getting-inside-a-container-5",
    "href": "applied_lec2.html#getting-inside-a-container-5",
    "title": "Dockerfiles",
    "section": "Getting inside a container",
    "text": "Getting inside a container\n\n\n\nWhat’s the difference between a container process and a host process?\n\n\n\nEach process (containerized or not) belongs to namespaces and cgroups.\nThe namespaces and cgroups determine what a process can “see” and “do”.\nAnalogy: each process (containerized or not) runs with a specific UID (user ID).\nUID=0 is root, and has elevated privileges. Other UIDs are normal users.\n\nWe will give more details about namespaces and cgroups later."
  },
  {
    "objectID": "applied_lec2.html#getting-inside-a-container-6",
    "href": "applied_lec2.html#getting-inside-a-container-6",
    "title": "Dockerfiles",
    "section": "Getting inside a container",
    "text": "Getting inside a container\n\n\n\nGetting a shell in a running container\n\n\n\nSometimes, we need to get a shell anyway.\nWe could run some SSH server in the container …\nBut it is easier to use docker exec.\n\n$ docker exec -ti ticktock sh\n\nThis creates a new process (running sh) inside the container.\nThis can also be done “manually” with the tool nsenter."
  },
  {
    "objectID": "applied_lec2.html#getting-inside-a-container-7",
    "href": "applied_lec2.html#getting-inside-a-container-7",
    "title": "Dockerfiles",
    "section": "Getting inside a container",
    "text": "Getting inside a container\n\n\n\nCaveats\n\n\n\nThe tool that you want to run needs to exist in the container.\nSome tools (like ip netns exec) let you attach to one namespace at a time.\n(This lets you e.g. setup network interfaces, even if you don’t have ifconfig or ip in the container.)\nMost importantly: the container needs to be running.\nWhat if the container is stopped or crashed?"
  },
  {
    "objectID": "applied_lec2.html#getting-inside-a-container-8",
    "href": "applied_lec2.html#getting-inside-a-container-8",
    "title": "Dockerfiles",
    "section": "Getting inside a container",
    "text": "Getting inside a container\n\n\n\nGetting a shell in a stopped container\n\n\n\nA stopped container is only storage (like a disk drive).\nWe cannot SSH into a disk drive or USB stick!\nWe need to connect the disk to a running machine.\nHow does that translate into the container world?"
  },
  {
    "objectID": "applied_lec2.html#getting-inside-a-container-9",
    "href": "applied_lec2.html#getting-inside-a-container-9",
    "title": "Dockerfiles",
    "section": "Getting inside a container",
    "text": "Getting inside a container\n\n\n\nAnalyzing a stopped container\n\n\nAs an exercise, we are going to try to find out what’s wrong with jpetazzo/crashtest.\ndocker run jpetazzo/crashtest\nThe container starts, but then stops immediately, without any output.\nWhat would MacGyver™ do?\nFirst, let’s check the status of that container.\ndocker ps -l"
  },
  {
    "objectID": "applied_lec2.html#getting-inside-a-container-10",
    "href": "applied_lec2.html#getting-inside-a-container-10",
    "title": "Dockerfiles",
    "section": "Getting inside a container",
    "text": "Getting inside a container\n\n\n\nViewing filesystem changes\n\n\n\nWe can use docker diff to see files that were added / changed / removed.\n\ndocker diff &lt;container_id&gt;\n\nThe container ID was shown by docker ps -l.\nWe can also see it with docker ps -lq.\nThe output of docker diff shows some interesting log files!"
  },
  {
    "objectID": "applied_lec2.html#getting-inside-a-container-11",
    "href": "applied_lec2.html#getting-inside-a-container-11",
    "title": "Dockerfiles",
    "section": "Getting inside a container",
    "text": "Getting inside a container\n\n\n\nAccessing files\n\n\n\nWe can extract files with docker cp.\n\ndocker cp &lt;container_id&gt;:/var/log/nginx/error.log .\n\nThen we can look at that log file.\n\ncat error.log\n(The directory /run/nginx doesn’t exist.)"
  },
  {
    "objectID": "applied_lec2.html#getting-inside-a-container-12",
    "href": "applied_lec2.html#getting-inside-a-container-12",
    "title": "Dockerfiles",
    "section": "Getting inside a container",
    "text": "Getting inside a container\n\n\n\nExploring a crashed container\n\n\n\nWe can restart a container with docker start …\n… But it will probably crash again immediately!\nWe cannot specify a different program to run with docker start\nBut we can create a new image from the crashed container\n\ndocker commit &lt;container_id&gt; debugimage\n\nThen we can run a new container from that image, with a custom entrypoint\n\ndocker run -ti --entrypoint sh debugimage"
  },
  {
    "objectID": "applied_lec2.html#getting-inside-a-container-13",
    "href": "applied_lec2.html#getting-inside-a-container-13",
    "title": "Dockerfiles",
    "section": "Getting inside a container",
    "text": "Getting inside a container\n\n\n\nObtaining a complete dump\n\n\n\nWe can also dump the entire filesystem of a container.\nThis is done with docker export.\nIt generates a tar archive.\n\ndocker export &lt;container_id&gt; | tar tv\nThis will give a detailed listing of the content of the container."
  },
  {
    "objectID": "applied_lec1.html#why-now",
    "href": "applied_lec1.html#why-now",
    "title": "Intro to Docker",
    "section": "Why now?",
    "text": "Why now?\n\n\n\n\n\nBefore\n\n\n\nmonolithic applications\nlong development cycles\nsingle environment\nslowly scaling up\n\n\n\n\n\n\n\n\nNow:\n\n\n\ndecoupled services\nfast, iterative improvements\nmultiple environments\nquickly scaling out"
  },
  {
    "objectID": "applied_lec1.html#vms",
    "href": "applied_lec1.html#vms",
    "title": "Intro to Docker",
    "section": "VMs",
    "text": "VMs\n\n\n\n\n\n\nDescription\n\n\n\nVirtual machines emulate physical computers by running operating systems in isolated instances.\nMultiple VMs are commonly hosted on a single server, with a hypervisor acting as a lightweight software layer positioned between the physical host and the VMs.\nThis hypervisor efficiently manages access to resources, enabling virtual machines to function as distinct servers while offering enhanced flexibility and agility.\nGained popularity in the 2000s due to consolidation and cost saving initiatives"
  },
  {
    "objectID": "applied_lec1.html#containers",
    "href": "applied_lec1.html#containers",
    "title": "Intro to Docker",
    "section": "Containers",
    "text": "Containers\n\n\n\n\n\n\nDescription\n\n\n\nA container is an isolated, lightweight silo for running an application on the host operating system.\nContainers build on top of the host operating system’s kernel\nContainers contain only apps and some lightweight operating system APIs and services"
  },
  {
    "objectID": "applied_lec1.html#vms-vs-containers",
    "href": "applied_lec1.html#vms-vs-containers",
    "title": "Intro to Docker",
    "section": "VMs vs Containers",
    "text": "VMs vs Containers"
  },
  {
    "objectID": "applied_lec1.html#deployment",
    "href": "applied_lec1.html#deployment",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment\n\n\n\n\n\n\nComplexity\n\n\nMany different stacks:\n\nlanguages\nframeworks\ndatabases\n\nMany different targets:\n\nindividual development environments\npre-production, QA, staging…\nproduction: on prem, cloud, hybrid"
  },
  {
    "objectID": "applied_lec1.html#deployment-1",
    "href": "applied_lec1.html#deployment-1",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#deployment-2",
    "href": "applied_lec1.html#deployment-2",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#deployment-3",
    "href": "applied_lec1.html#deployment-3",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#deployment-4",
    "href": "applied_lec1.html#deployment-4",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#deployment-5",
    "href": "applied_lec1.html#deployment-5",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#deployment-6",
    "href": "applied_lec1.html#deployment-6",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#deployment-7",
    "href": "applied_lec1.html#deployment-7",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#deployment-8",
    "href": "applied_lec1.html#deployment-8",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#docker",
    "href": "applied_lec1.html#docker",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nResults\n\n\n\nDev-to-prod reduced from 9 months to 15 minutes (ING)\nContinuous integration job time reduced by more than 60% (BBC)\nDeploy 100 times a day instead of once a week (GILT)\n70% infrastructure consolidation (MetLife)"
  },
  {
    "objectID": "applied_lec1.html#docker-1",
    "href": "applied_lec1.html#docker-1",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\n\n\n\nHow to deploy now?\n\n\nEscape dependency hell:\n\nWrite installation instructions into an INSTALL.txt file\nUsing this file, write an install.sh script that works for you\nTurn this file into a Dockerfile, test it on your machine\nIf the Dockerfile builds on your machine, it will build anywhere\nRejoice as you escape dependency hell and “works on my machine”\nNever again “worked in dev - ops problem now!”"
  },
  {
    "objectID": "applied_lec1.html#docker-2",
    "href": "applied_lec1.html#docker-2",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\n\n\n\nHow to deploy now?\n\n\nQuick onboarding\n\nWrite Dockerfiles for your application components\nUse pre-made images from the Docker Hub (mysql, redis…)\nDescribe your stack with a Compose file\nOn-board somebody with two commands:\n\ngit clone ...\ndocker-compose up"
  },
  {
    "objectID": "applied_lec1.html#docker-3",
    "href": "applied_lec1.html#docker-3",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\n\n\n\nImplement reliable CI easily\n\n\n\nBuild test environment with a Dockerfile or Compose file\nFor each test run, stage up a new container or stack\nEach run is now in a clean environment\nNo pollution from previous tests"
  },
  {
    "objectID": "applied_lec1.html#docker-4",
    "href": "applied_lec1.html#docker-4",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\n\n\n\nUse container images as build artefacts\n\n\n\nBuild your app from Dockerfiles\nStore the resulting images in a registry\nKeep them forever (or as long as necessary)\nTest those images in QA, CI, integration…\nRun the same images in production\nSomething goes wrong? Rollback to previous image\nInvestigating old regression? Old image has your back!\nImages contain all the libraries, dependencies, etc. needed to run the app."
  },
  {
    "objectID": "applied_lec1.html#docker-formats",
    "href": "applied_lec1.html#docker-formats",
    "title": "Intro to Docker",
    "section": "Docker: Formats",
    "text": "Docker: Formats\n\n\n\n\n\nBefore\n\n\n\nNo standardized exchange format. \nContainers are hard to use for developers. \nAs a result, they are hidden from the end users.\nNo re-usable components, APIs, tools. \n\n\n\n\n\n\n\n\n\n\n\nAfter\n\n\n\nStandardize the container format, because containers were not portable.\nMake containers easy to use for developers.\nEmphasis on re-usable components, APIs, ecosystem of standard tools.\nImprovement over ad-hoc, in-house, specific tools."
  },
  {
    "objectID": "applied_lec1.html#docker-deployment",
    "href": "applied_lec1.html#docker-deployment",
    "title": "Intro to Docker",
    "section": "Docker: Deployment",
    "text": "Docker: Deployment\n\n\n\n\n\nBefore\n\n\n\nShip packages: deb, rpm, gem, jar, homebrew…\nDependency hell.\n“Works on my machine.”\nBase deployment often done from scratch and unreliable.\n\n\n\n\n\n\n\n\nAfter\n\n\n\nShip container images with all their dependencies.\nImages are bigger, but they are broken down into layers.\nOnly ship layers that have changed.\nSave disk, network, memory usage."
  },
  {
    "objectID": "applied_lec1.html#docker-5",
    "href": "applied_lec1.html#docker-5",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nExample\n\n\nLayers:\n\nCentOS\nJRE\nTomcat\nDependencies\nApplication JAR\nConfiguration"
  },
  {
    "objectID": "applied_lec1.html#docker-devops",
    "href": "applied_lec1.html#docker-devops",
    "title": "Intro to Docker",
    "section": "Docker: Devops",
    "text": "Docker: Devops\n\n\n\n\n\nBefore\n\n\n\nDrop a tarball (or a commit hash) with instructions.\nDev environment very different from production.\nOps don’t always have a dev environment themselves …\n… and when they do, it can differ from the devs’.\nOps have to sort out differences and make it work …\n… or bounce it back to devs.\nShipping code causes frictions and delays.\n\n\n\n\n\n\n\n\nAfter\n\n\n\nDrop a container image or a Compose file.\nOps can always run that container image.\nOps can always run that Compose file.\nOps still have to adapt to prod environment, but at least they have a reference point.\nOps have tools allowing to use the same image in dev and prod.\nDevs can be empowered to make releases themselves more easily."
  },
  {
    "objectID": "applied_lec1.html#docker-history",
    "href": "applied_lec1.html#docker-history",
    "title": "Intro to Docker",
    "section": "Docker history",
    "text": "Docker history"
  },
  {
    "objectID": "applied_lec1.html#docker-history-1",
    "href": "applied_lec1.html#docker-history-1",
    "title": "Intro to Docker",
    "section": "Docker history",
    "text": "Docker history\n\n\n\ndotCloud\n\n\n\ndotCloud was operating a PaaS, using a custom container engine.\nThis engine was based on OpenVZ (and later, LXC) and AUFS.\nIt started (circa 2008) as a single Python script.\nBy 2012, the engine had multiple (~10) Python components. (and ~100 other micro-services!)\nEnd of 2012, dotCloud refactors this container engine.\nThe codename for this project is “Docker.”"
  },
  {
    "objectID": "applied_lec1.html#docker-history-2",
    "href": "applied_lec1.html#docker-history-2",
    "title": "Intro to Docker",
    "section": "Docker history",
    "text": "Docker history\n\n\n\nFirst public release\n\n\n\nMarch 2013, PyCon, Santa Clara:\n\n“Docker” is shown to a public audience for the first time.\n\nIt is released with an open source license.\nVery positive reactions and feedback!\nThe dotCloud team progressively shifts to Docker development.\nThe same year, dotCloud changes name to Docker."
  },
  {
    "objectID": "applied_lec1.html#docker-history-3",
    "href": "applied_lec1.html#docker-history-3",
    "title": "Intro to Docker",
    "section": "Docker history",
    "text": "Docker history\n\n\n\nAfter release\n\n\n\n2013: fixing bugs around OS support\n2014: Docker Compose v1 (written in Python)\n2015: version 1.0, Open Containers Initiative\n2015: creation of the Cloud Native Computing Foundation\n2020: Docker Compose v2 (re-written in Go)"
  },
  {
    "objectID": "applied_lec1.html#containerd",
    "href": "applied_lec1.html#containerd",
    "title": "Intro to Docker",
    "section": "containerd",
    "text": "containerd"
  },
  {
    "objectID": "applied_lec1.html#docker-installation",
    "href": "applied_lec1.html#docker-installation",
    "title": "Intro to Docker",
    "section": "Docker installation",
    "text": "Docker installation\n\n\n\n\n\n\nWhat is Docker?\n\n\n\n“Installing Docker” really means “Installing the Docker Engine and CLI”.\nThe Docker Engine is a daemon (a service running in the background).\nThis daemon manages containers, the same way that a hypervisor manages VMs.\nWe interact with the Docker Engine by using the Docker CLI.\nThe Docker CLI and the Docker Engine communicate through an API.\nThere are many other programs and client libraries which use that API."
  },
  {
    "objectID": "applied_lec1.html#docker-installation-1",
    "href": "applied_lec1.html#docker-installation-1",
    "title": "Intro to Docker",
    "section": "Docker installation",
    "text": "Docker installation\n\n\n\nDocker Desktop\n\n\n\nLeverages the host OS virtualization subsystem\nUnder the hood, runs a tiny VM\nAccesses network resources like normal applications\nSupports filesystem sharing through volumes"
  },
  {
    "objectID": "applied_lec1.html#docker-installation-2",
    "href": "applied_lec1.html#docker-installation-2",
    "title": "Intro to Docker",
    "section": "Docker installation",
    "text": "Docker installation\n\n\n\nDocker Desktop\n\n\nWhen you execute docker version from the terminal:\n\nthe CLI connects to the Docker Engine over a standard socket,\nthe Docker Engine is, in fact, running in a VM,\n… but the CLI doesn’t know or care about that,\nthe CLI sends a request using the REST API,\nthe Docker Engine in the VM processes the request,\nthe CLI gets the response and displays it to you."
  },
  {
    "objectID": "applied_lec1.html#docker-installation-3",
    "href": "applied_lec1.html#docker-installation-3",
    "title": "Intro to Docker",
    "section": "Docker installation",
    "text": "Docker installation\n\n\n\nCheck that it works\n\n\n$ docker version\n\nClient:\n Version:           28.2.2\n API version:       1.50\n Go version:        go1.24.3\n Git commit:        e6534b4\n Built:             Fri May 30 12:07:35 2025\n OS/Arch:           darwin/arm64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.42.1 (196648)\n Engine:\n  Version:          28.2.2\n  API version:      1.50 (minimum version 1.24)\n  Go version:       go1.24.3\n  Git commit:       45873be\n  Built:            Fri May 30 12:07:27 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0"
  },
  {
    "objectID": "applied_lec1.html#docker-installation-busybox",
    "href": "applied_lec1.html#docker-installation-busybox",
    "title": "Intro to Docker",
    "section": "Docker installation: Busybox",
    "text": "Docker installation: Busybox\n\n\n\n\nWhat is it?\n\n\n\nprovides several Unix utilities in a single executable file.\nvery space-efficient\ncreated for embedded operating systems with very limited resources.\n\n\n\n\n\n\n\nCheck that it works\n\n\n$ docker run busybox echo hello world\nUnable to find image 'busybox:latest' locally\nlatest: Pulling from library/busybox\n499bcf3c8ead: Pull complete\nDigest: sha256:d82f458899c9696cb26a7c02d5568f81c8c8223f8661bb2a7988b269c8b9051e\nStatus: Downloaded newer image for busybox:latest\nhello world"
  },
  {
    "objectID": "applied_lec1.html#docker-installation-ubuntu",
    "href": "applied_lec1.html#docker-installation-ubuntu",
    "title": "Intro to Docker",
    "section": "Docker installation: Ubuntu",
    "text": "Docker installation: Ubuntu\n\n\n\n\nRunning Ubuntu\n\n\n$ docker run -it ubuntu\nUnable to find image 'ubuntu:latest' locally\nlatest: Pulling from library/ubuntu\n59a5d47f84c3: Pull complete\nDigest: sha256:353675e2a41babd526e2b837d7ec780c2a05bca0164f7ea5dbbd433d21d166fc\nStatus: Downloaded newer image for ubuntu:latest\nroot@014ed1f2eac1:/# sudo apt-get moo\nbash: sudo: command not found\nroot@014ed1f2eac1:/# apt-get moo\n                 (__)\n                 (oo)\n           /------\\/\n          / |    ||\n         *  /\\---/\\\n            ~~   ~~\n...\"Have you mooed today?\"...\nroot@014ed1f2eac1:/#"
  },
  {
    "objectID": "applied_lec1.html#docker-installation-ubuntu-1",
    "href": "applied_lec1.html#docker-installation-ubuntu-1",
    "title": "Intro to Docker",
    "section": "Docker installation: Ubuntu",
    "text": "Docker installation: Ubuntu\n\n\n\n\n\n\nWhat does this mean?\n\n\n\nIt runs a bare-bones, no-frills ubuntu system.\n-it is shorthand for -i -t.\n-i tells Docker to connect us to the container’s stdin.: e.g. interactive mode.\n-t tells Docker that we want a pseudo-terminal."
  },
  {
    "objectID": "applied_lec1.html#docker-installation-ubuntu-2",
    "href": "applied_lec1.html#docker-installation-ubuntu-2",
    "title": "Intro to Docker",
    "section": "Docker installation: Ubuntu",
    "text": "Docker installation: Ubuntu\n\n\n\nRun something\n\n\nroot@014ed1f2eac1:/# figlet hello\nbash: figlet: command not found\n\nroot@014ed1f2eac1:/# apt-get update && apt-get install figlet\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  figlet\n...\nUnpacking figlet (2.2.5-3) ...\nroot@014ed1f2eac1:/# figlet hello\n _          _ _\n| |__   ___| | | ___\n| '_ \\ / _ \\ | |/ _ \\\n| | | |  __/ | | (_) |\n|_| |_|\\___|_|_|\\___/\n\nroot@014ed1f2eac1:/#"
  },
  {
    "objectID": "applied_lec1.html#docker-installation-ubuntu-3",
    "href": "applied_lec1.html#docker-installation-ubuntu-3",
    "title": "Intro to Docker",
    "section": "Docker installation: Ubuntu",
    "text": "Docker installation: Ubuntu\n\n\n\n\n\n\nImportant\n\n\nExit the container via exit or Ctrl-D.\nIf we try running figlet again, this won’t work - it’s only installed inside the container."
  },
  {
    "objectID": "applied_lec1.html#docker-installation-ubuntu-4",
    "href": "applied_lec1.html#docker-installation-ubuntu-4",
    "title": "Intro to Docker",
    "section": "Docker installation: Ubuntu",
    "text": "Docker installation: Ubuntu\n\n\n\nHosts vs Containers\n\n\n\nWe ran an ubuntu container on an Linux/Windows/macOS host.\nThey have different, independent packages.\nInstalling something on the host doesn’t expose it to the container.\nAnd vice-versa.\nEven if both the host and the container have the same Linux distro!\nWe can run any container on any host."
  },
  {
    "objectID": "applied_lec1.html#docker-6",
    "href": "applied_lec1.html#docker-6",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nWhere’s our container now\n\n\n\nin a stopped state\nusing disk storage\nNOT using CPU or memory"
  },
  {
    "objectID": "applied_lec1.html#docker-7",
    "href": "applied_lec1.html#docker-7",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nStart a new container\n\n\ndocker run -it ubuntu\nroot@5c6dc90eb867:/# figlet\nbash: figlet: command not found\nroot@5c6dc90eb867:/#\nWhy?\n\nWe started a brand new container.\nThe basic Ubuntu image was used, and figlet is not here."
  },
  {
    "objectID": "applied_lec1.html#docker-8",
    "href": "applied_lec1.html#docker-8",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nCan we restore our container somehow?\n\n\nWe can, but that’s not the default workflow with Docker.\n\n\n\n\n\n\nWhat’s the default workflow, then?\n\n\n\nAlways start with a fresh container.\nIf we need something installed in our container, build a custom image.\n\n\n\n\n\n\n\nWhy so complicated?\n\n\n\nIt’s quite easy actually\nThis puts a strong emphasis on automation and repeatability. Let’s see why …"
  },
  {
    "objectID": "applied_lec1.html#docker-9",
    "href": "applied_lec1.html#docker-9",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker"
  },
  {
    "objectID": "applied_lec1.html#docker-10",
    "href": "applied_lec1.html#docker-10",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\n\n\nPets\n\n\n\nhave distinctive names and unique configurations\nwhen they have an outage, we do everything we can to fix them\n\n\n\n\n\n\n\n\nCattle\n\n\n\nhave generic names (e.g. with numbers) and generic configuration\nconfiguration is enforced by configuration management, golden images …\nwhen they have an outage, we can replace them immediately with a new server"
  },
  {
    "objectID": "applied_lec1.html#docker-11",
    "href": "applied_lec1.html#docker-11",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nPet VM\n\n\nWhen we use local VMs (with e.g. VirtualBox or VMware), our workflow looks like this:\n\ncreate VM from base template (Ubuntu, CentOS…)\ninstall packages, set up environment\nwork on project\nwhen done, shut down VM\nnext time we need to work on project, restart VM as we left it\nif we need to tweak the environment, we do it live\nOver time, the VM configuration evolves, diverges.\nWe don’t have a clean, reliable, deterministic way to provision that environment."
  },
  {
    "objectID": "applied_lec1.html#docker-12",
    "href": "applied_lec1.html#docker-12",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nCattle container\n\n\nWith Docker, the workflow looks like this:\n\ncreate container image with our dev environment\nrun container with that image\nwork on project\nwhen done, shut down container\nnext time we need to work on project, start a new container\nif we need to tweak the environment, we create a new image\nWe have a clear definition of our environment, and can share it reliably with others."
  },
  {
    "objectID": "applied_lec1.html#docker-13",
    "href": "applied_lec1.html#docker-13",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nNon-interactive containers\n\n\nOur first containers were interactive.\nWe will now see how to:\n\nRun a non-interactive container.\nRun a container in the background.\nList running containers.\nCheck the logs of a container.\nStop a container.\nList stopped containers."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-1",
    "href": "applied_lec1.html#non-interactive-containers-1",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nExample\n\n\nWe will run a small custom container. This container just displays the time every second.\n$ docker run jpetazzo/clock\nUnable to find image 'jpetazzo/clock:latest' locally\nlatest: Pulling from jpetazzo/clock\n36fbfd22ebfc: Pull complete\nDigest: sha256:dc06bbc3744f7200404bff0bbb2516925e7adea115e07de9da8b36bf15fe3dd3\nStatus: Downloaded newer image for jpetazzo/clock:latest\nSat Sep 20 11:00:45 UTC 2025\nSat Sep 20 11:00:46 UTC 2025\nSat Sep 20 11:00:47 UTC 2025\n^C%\n\nThis container will run forever.\nTo stop it, press ^C.\nDocker has automatically downloaded the image jpetazzo/clock.\nThis image is a user image, created by jpetazzo."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-2",
    "href": "applied_lec1.html#non-interactive-containers-2",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nCtrl-C might now always work!\n\n\nWhat happens when we hit Ctrl-C:\n\nSIGINT gets sent to the container, which means:\nSIGINT gets sent to PID 1 (default case)\nSIGINT gets sent to foreground processes when running with -ti\n\nBut there is a special case for PID 1: it ignores all signals!\n\nexcept SIGKILL and SIGSTOP\nexcept signals handled explicitly\n\nTL,DR: there are many circumstances when Ctrl-C won’t stop the container."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-3",
    "href": "applied_lec1.html#non-interactive-containers-3",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\n\n\n\nWhy is PID 1 special?\n\n\nPID 1 has some extra responsibilities:\n\nit starts (directly or indirectly) every other process\nwhen a process exits, its processes are “reparented” under PID 1\nWhen PID 1 exits, everything stops:\non a “regular” machine, it causes a kernel panic\nin a container, it kills all the processes\n\nErgo: We don’t want PID 1 to stop accidentally. That’s why it has these extra protections."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-4",
    "href": "applied_lec1.html#non-interactive-containers-4",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nSolution"
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-5",
    "href": "applied_lec1.html#non-interactive-containers-5",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nDaemon mode\n\n\nContainers can be started in the background, with the -d flag (daemon mode):\n$ docker run -d jpetazzo/clock\n896ffc453901fc7d7c417381c8bde9a8911182d07b819dc988aa0b4d1c298d3e\n\nWe don’t see the output of the container.\nBut don’t worry: Docker collects that output and logs it!\nDocker gives us the ID of the container."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-6",
    "href": "applied_lec1.html#non-interactive-containers-6",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nMaxwell demon: MIT’s Project MAC\n\n\n\n\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Maxwell’s_demon"
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-7",
    "href": "applied_lec1.html#non-interactive-containers-7",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nList running containers\n\n\nHow can we check that our container is still running?\nWith docker ps, just like the UNIX ps command, lists running processes.\n$ docker ps\nCONTAINER ID   IMAGE            COMMAND                  CREATED         STATUS                PORTS                                             NAMES\n896ffc453901   jpetazzo/clock   \"/bin/sh -c 'while d…\"   5 minutes ago   Up 5 minutes                                                            quirky_wilson\n\nDocker tells us:\n\nThe (truncated) ID of our container.\nThe image used to start the container.\nThat our container has been running (Up) for a couple of minutes.\nOther information (COMMAND, PORTS, NAMES) that we will explain later."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-8",
    "href": "applied_lec1.html#non-interactive-containers-8",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nMore containers\n\n\nRun 2 more:\n$ docker run -d jpetazzo/clock\n\n42518eae35544162179d3f7086410949256a767244e36e40518f0f9d1dd223ae\n$ docker run -d jpetazzo/clock\n\n31a2d9cc7e40d58280b9e5cdd6135cf824559bc1ab6409f026c3a0c82419273e\nCheck running:\n$ docker ps\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS                PORTS                                             NAMES\n31a2d9cc7e40   jpetazzo/clock   \"/bin/sh -c 'while d…\"   30 seconds ago   Up 29 seconds                                                           optimistic_payne\n42518eae3554   jpetazzo/clock   \"/bin/sh -c 'while d…\"   31 seconds ago   Up 30 seconds                                                           beautiful_kalam\n896ffc453901   jpetazzo/clock   \"/bin/sh -c 'while d…\"   7 minutes ago    Up 7 minutes                                                            quirky_wilson"
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-9",
    "href": "applied_lec1.html#non-interactive-containers-9",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nLast run container\n\n\n$ docker ps -l\nCONTAINER ID   IMAGE            COMMAND                  CREATED              STATUS              PORTS     NAMES\n31a2d9cc7e40   jpetazzo/clock   \"/bin/sh -c 'while d…\"   About a minute ago   Up About a minute             optimistic_payne\n\n\n\n\n\n\nIDs only\n\n\n$ docker ps -q\n31a2d9cc7e40\n42518eae3554\n896ffc453901"
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-10",
    "href": "applied_lec1.html#non-interactive-containers-10",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nContainer logs\n\n\n$ docker logs 31a\n...\n...\n...\nSat Sep 20 11:18:46 UTC 2025\nSat Sep 20 11:18:47 UTC 2025\nSat Sep 20 11:18:48 UTC 2025\nSat Sep 20 11:18:49 UTC 2025\nSat Sep 20 11:18:50 UTC 2025\nSat Sep 20 11:18:51 UTC 2025\nSat Sep 20 11:18:52 UTC 2025\nSat Sep 20 11:18:53 UTC 2025\nSat Sep 20 11:18:54 UTC 2025\nSat Sep 20 11:18:55 UTC 2025\nSat Sep 20 11:18:56 UTC 2025\nSat Sep 20 11:18:57 UTC 2025\nSat Sep 20 11:18:58 UTC 2025\nSat Sep 20 11:18:59 UTC 2025\nSat Sep 20 11:19:00 UTC 2025\n\nAll logs are dumped - a bit too much."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-11",
    "href": "applied_lec1.html#non-interactive-containers-11",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nContainer logs tail\n\n\n$ docker logs --tail 5 31a\nSat Sep 20 11:19:55 UTC 2025\nSat Sep 20 11:19:56 UTC 2025\nSat Sep 20 11:19:57 UTC 2025\nSat Sep 20 11:19:58 UTC 2025\nSat Sep 20 11:19:59 UTC 2025\n\n\n\n\n\n\nContainer logs tail & follow\n\n\n$ docker logs --tail 1 --follow 31a\nSat Sep 20 11:21:45 UTC 2025\nSat Sep 20 11:21:46 UTC 2025\nSat Sep 20 11:21:47 UTC 2025\nSat Sep 20 11:21:48 UTC 2025\nSat Sep 20 11:21:49 UTC 2025"
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-12",
    "href": "applied_lec1.html#non-interactive-containers-12",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\n\n\n\nStopping\n\n\nThere are two ways we can terminate our detached container.\n\nKilling it using the docker kill command.\n\nstops the container immediately, by using the KILL signal.\n\nStopping it using the docker stop command.\n\nsends a TERM signal, and after 10 seconds, if the container has not stopped, it sends KILL.\n\n\nReminder: the KILL signal cannot be intercepted, and will forcibly terminate the container."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-13",
    "href": "applied_lec1.html#non-interactive-containers-13",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\n\n\n\nStopping: Example\n\n\n$ docker stop 31a\n&lt;10 seconds pass&gt;\n31a\n\nDocker sends the TERM signal;\nthe container doesn’t react to this signal (it’s a simple Shell script with no special signal handling);\n10 seconds later, since the container is still running, Docker sends the KILL signal;\nthis terminates the container."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-14",
    "href": "applied_lec1.html#non-interactive-containers-14",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\n\n\n\nKilling: Example\n\n\n$ docker kill 425 896\n425\n896\nThose containers will be terminated immediately (without the 10-second delay)."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-15",
    "href": "applied_lec1.html#non-interactive-containers-15",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\n\n\n\nList stopped containers\n\n\nWe can also see stopped containers, with the -a (–all) option.\n$ docker ps -a\nCONTAINER ID   IMAGE                                          COMMAND                  CREATED             STATUS                            PORTS     NAMES\n31a2d9cc7e40   jpetazzo/clock                                 \"/bin/sh -c 'while d…\"   12 minutes ago      Exited (137) 2 minutes ago                  optimistic_payne\n42518eae3554   jpetazzo/clock                                 \"/bin/sh -c 'while d…\"   12 minutes ago      Exited (137) About a minute ago             beautiful_kalam\n896ffc453901   jpetazzo/clock                                 \"/bin/sh -c 'while d…\"   19 minutes ago      Exited (137) About a minute ago             quirky_wilson\n74b84530ad71   jpetazzo/clock                                 \"/bin/sh -c 'while d…\"   26 minutes ago      Exited (130) 26 minutes ago                 amazing_cohen\n5c6dc90eb867   ubuntu                                         \"/bin/bash\"              40 minutes ago      Exited (130) 26 minutes ago                 thirsty_bardeen\n014ed1f2eac1   ubuntu                                         \"/bin/bash\"              52 minutes ago      Exited (0) 40 minutes ago                   elated_darwin\n42f574fdf1af   busybox                                        \"echo hello world\"       About an hour ago   Exited (0) About an hour ago                suspicious_franklin"
  },
  {
    "objectID": "applied_lec1.html#restarting-and-attaching",
    "href": "applied_lec1.html#restarting-and-attaching",
    "title": "Intro to Docker",
    "section": "Restarting and attaching",
    "text": "Restarting and attaching\n\n\n\n\n\n\nBackground vs foreground\n\n\n\nThe distinction between foreground and background containers is arbitrary.\nFrom Docker’s point of view, all containers are the same.\nAll containers run the same way, whether there is a client attached to them or not.\nIt is always possible to detach from a container, and to reattach to a container.\nAnalogy: attaching to a container is like plugging a keyboard and screen to a physical server."
  },
  {
    "objectID": "applied_lec1.html#restarting-and-attaching-1",
    "href": "applied_lec1.html#restarting-and-attaching-1",
    "title": "Intro to Docker",
    "section": "Restarting and attaching",
    "text": "Restarting and attaching\n\n\n\nDetaching from containers\n\n\nIf you have started an interactive container (with option -it), you can detach from it.\nThe “detach” sequence is Ctrl-P Ctrl-Q or Ctrl-C on Windows.\nOtherwise you can detach by killing the Docker client."
  },
  {
    "objectID": "applied_lec1.html#restarting-and-attaching-2",
    "href": "applied_lec1.html#restarting-and-attaching-2",
    "title": "Intro to Docker",
    "section": "Restarting and attaching",
    "text": "Restarting and attaching\n\n\n\nCustom detach\n\n\nYou can change the sequence with docker run --detach-keys.\nThis can also be passed as a global option to the engine.\nStart a container with a custom detach command:\n$ docker run -ti --detach-keys ctrl-x,x jpetazzo/clock\nDetach by hitting Ctrl-X x.\n$ docker ps -l"
  },
  {
    "objectID": "applied_lec1.html#restarting-and-attaching-3",
    "href": "applied_lec1.html#restarting-and-attaching-3",
    "title": "Intro to Docker",
    "section": "Restarting and attaching",
    "text": "Restarting and attaching\n\n\n\nAttaching\n\n\nYou can attach to a container:\n$ docker attach &lt;containerID&gt;\nThe container must be running.\nThere can be multiple clients attached to the same container.\nIf you don’t specify --detach-keys when attaching, it defaults back to Ctrl-P Ctrl-Q.\nTry it on our previous container:\n$ docker attach $(docker ps -lq)"
  },
  {
    "objectID": "applied_lec1.html#restarting-and-attaching-4",
    "href": "applied_lec1.html#restarting-and-attaching-4",
    "title": "Intro to Docker",
    "section": "Restarting and attaching",
    "text": "Restarting and attaching\n\n\n\nDetaching from non-interactive containers\n\n\nWarning: if the container was started without -it..., you won’t be able to detach with Ctrl-P Ctrl-Q. If you hit Ctrl-C, the signal will be proxied to the container.\nRemember: you can always detach by killing the Docker client."
  },
  {
    "objectID": "applied_lec1.html#restarting-and-attaching-5",
    "href": "applied_lec1.html#restarting-and-attaching-5",
    "title": "Intro to Docker",
    "section": "Restarting and attaching",
    "text": "Restarting and attaching\n\n\n\nRestarting a container\n\n\nWhen a container has exited, it is in stopped state.\nIt can then be restarted with the start command.\n$ docker start &lt;yourContainerID&gt;\nThe container will be restarted using the same options you launched it with.\nYou can re-attach to it if you want to interact with it:\n$ docker attach &lt;yourContainerID&gt;\nUse docker ps -a to identify the container ID of a previous jpetazzo/clock container, and try those commands."
  },
  {
    "objectID": "applied_lec1.html#docker-images-1",
    "href": "applied_lec1.html#docker-images-1",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\n\nOutline\n\n\nWhat we will go through now:\n\nWhat is an image.\nWhat is a layer.\nThe various image namespaces.\nHow to search and download images.\nImage tags and when to use them."
  },
  {
    "objectID": "applied_lec1.html#docker-images-2",
    "href": "applied_lec1.html#docker-images-2",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nWhat is an image?\n\n\n\nImage = files + metadata\nThese files form the root filesystem of our container.\nThe metadata can indicate a number of things, e.g.:\n\nthe author of the image\nthe command to execute in the container when starting it\nenvironment variables to be set\netc.\n\nImages are made of layers, conceptually stacked on top of each other.\nEach layer can add, change, and remove files and/or metadata.\nImages can share layers to optimize disk usage, transfer times, and memory use."
  },
  {
    "objectID": "applied_lec1.html#docker-images-3",
    "href": "applied_lec1.html#docker-images-3",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nDocker Image Example: Website\n\n\nThe images will contain these layers:\n\nCentOS base layer\nPackages and configuration files added by our local IT\nPython installation\nFlask/Django\nOur application’s dependencies\nOur application code and assets\nOur application configuration\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nApp config is generally added by orchestration facilities."
  },
  {
    "objectID": "applied_lec1.html#docker-images-4",
    "href": "applied_lec1.html#docker-images-4",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nRead-write layer\n\n\nExists on top of image layers."
  },
  {
    "objectID": "applied_lec1.html#docker-images-5",
    "href": "applied_lec1.html#docker-images-5",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\n\n\n\nContainers vs Images\n\n\n\nan image is a read-only filesystem.\na container is an encapsulated set of processes,\nrunning in a read-write copy of that filesystem.\nto optimize container boot time, copy-on-write is used instead of regular copy.\ndocker run starts a container from a given image."
  },
  {
    "objectID": "applied_lec1.html#docker-images-6",
    "href": "applied_lec1.html#docker-images-6",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images"
  },
  {
    "objectID": "applied_lec1.html#docker-images-7",
    "href": "applied_lec1.html#docker-images-7",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nCompared to Python objects/classes\n\n\n\nImages are conceptually similar to classes.\nLayers are conceptually similar to inheritance.\nContainers are conceptually similar to instances."
  },
  {
    "objectID": "applied_lec1.html#docker-images-8",
    "href": "applied_lec1.html#docker-images-8",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nHow do change read-only images?\n\n\nIf an image is read-only, how do we change it?\n\nWe don’t.\nWe create a new container from that image.\nThen we make changes to that container.\nWhen we are satisfied with those changes, we transform them into a new layer.\nA new image is created by stacking the new layer on top of the old image."
  },
  {
    "objectID": "applied_lec1.html#docker-images-9",
    "href": "applied_lec1.html#docker-images-9",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nHow do we create empty images?\n\n\n\nThere is a special empty image called scratch.  It allows to build from scratch.\nThe docker import command loads a tarball into Docker.  The imported tarball becomes a standalone image.  That new image has a single layer.\n\nNote: you will probably never have to do this yourself."
  },
  {
    "objectID": "applied_lec1.html#docker-images-10",
    "href": "applied_lec1.html#docker-images-10",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nHow do we create other images?\n\n\ndocker commit\n\nSaves all the changes made to a container into a new layer.\nCreates a new image (effectively a copy of the container).\n\n\n\n\n\n\n\nAnother option\n\n\ndocker build (used 99% of the time)\n\nPerforms a repeatable build sequence.\nThis is the preferred method!"
  },
  {
    "objectID": "applied_lec1.html#docker-images-11",
    "href": "applied_lec1.html#docker-images-11",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nImage namespaces\n\n\nImages have names, and these names can belong to three namespaces:\n\nOfficial images (root namespace)\n\n e.g. ubuntu, busybox …\n\nUser (and organizations) images (user namespace)\n\n e.g. jpetazzo/clock\n\nSelf-hosted images (self-hosted namespace)\n\n e.g. registry.example.com:5000/my-private/image"
  },
  {
    "objectID": "applied_lec1.html#docker-images-12",
    "href": "applied_lec1.html#docker-images-12",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nRoot namespace\n\n\n\nThe root namespace is for official images.\nThey are gated by Docker Inc.\nThey are generally authored and maintained by third parties.\nThose images include:\nSmall, “swiss-army-knife” images like busybox.\nDistro images to be used as bases for your builds, like ubuntu, fedora...\nReady-to-use components and services, like redis, postgresql..."
  },
  {
    "objectID": "applied_lec1.html#docker-images-13",
    "href": "applied_lec1.html#docker-images-13",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nUser namespace\n\n\nThe user namespace holds images for Docker Hub users and organizations. For example:\njpetazzo/clock\nThe Docker Hub user is:\njpetazzo\nThe image name is:\nclock"
  },
  {
    "objectID": "applied_lec1.html#docker-images-14",
    "href": "applied_lec1.html#docker-images-14",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nSelf-hosted namespace\n\n\nThis namespace holds images which are not hosted on Docker Hub, but on third party registries.\nThey contain the hostname (or IP address), and optionally the port, of the registry server.\nFor example:\nlocalhost:5000/wordpress\n\nlocalhost:5000 is the host and port of the registry\nwordpress is the name of the image\n\nOther examples:\nquay.io/coreos/etcd\ngcr.io/google-containers/hugo"
  },
  {
    "objectID": "applied_lec1.html#docker-images-15",
    "href": "applied_lec1.html#docker-images-15",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nImage storage\n\n\nImages can be stored:\n\nOn your Docker host.\nIn a Docker registry.\n\nYou can use the Docker client to download (pull) or upload (push) images."
  },
  {
    "objectID": "applied_lec1.html#docker-images-16",
    "href": "applied_lec1.html#docker-images-16",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nCurrent images\n\n\n$ docker images\nREPOSITORY                                     TAG            IMAGE ID       CREATED         SIZE\nubuntu                                         latest         353675e2a41b   11 days ago     139MB\nmongo                                          latest         a6bda40d00e5   8 weeks ago     1.19GB\nmysql                                          8.0            18dee92bbc23   2 months ago    1.06GB\nbitnami/redis                                  latest         5927ff3702df   2 months ago    253MB\npostgres                                       14             563a4985838f   3 months ago    623MB\nnode                                           18.20.5        8b7f2b36c945   10 months ago   1.56GB\nbusybox                                        latest         d82f458899c9   11 months ago   6.21MB\nmysql                                          8.0.35         c6812f0dcd97   21 months ago   809MB\npostgres                                       11-alpine      ea50b9fd617b   21 months ago   337MB\nnode                                           16             f77a1aef2da8   2 years ago     1.27GB\nmaven                                          3-openjdk-11   805f366910ae   3 years ago     1.03GB\njpetazzo/clock                                 latest         dc06bbc3744f   4 years ago     2.32MB"
  },
  {
    "objectID": "applied_lec1.html#docker-images-17",
    "href": "applied_lec1.html#docker-images-17",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nImage search\n\n\nWe cannot list all images on a remote registry, but we can search for a specific keyword:\n$ docker search jupyter\nNAME                               DESCRIPTION                                     STARS     OFFICIAL\nislasgeci/jupyter                  Jupyter para Ciencia de Datos • GECI            0\nopendatacube/jupyter               An image with OpenDataCube and Jupyter          1\ndatajoint/jupyter                  **Deprecated**: Official DataJoint Jupyter n…   0\njupyter/scipy-notebook             Scientific Jupyter Notebook Python Stack fro…   467\njupyter/all-spark-notebook         Python, Scala, R and Spark Jupyter Notebook …   439\njupyter/pyspark-notebook           Python and Spark Jupyter Notebook Stack from…   316\njupyter/tensorflow-notebook        Scientific Jupyter Notebook Python Stack w/ …   372\nbiocontainers/jupyter                                                              0\njupyter/datascience-notebook       Data Science Jupyter Notebook Python Stack f…   1092\njupyter/minimal-notebook           Minimal Jupyter Notebook Python Stack from h…   199\njupyter/base-notebook              Base image for Jupyter Notebook stacks from …   237\njupyter/nbviewer                   Jupyter Notebook Viewer                         34\njupyter/r-notebook                 R Jupyter Notebook Stack from https://github…   62\njupyter/repo2docker                Turn git repositories into Jupyter enabled D…   22\njupyter/docker-stacks-foundation   Tiny base image on which Jupyter apps can be…   6\njupyter/demo                       (DEPRECATED) Demo of the IPython/Jupyter Not…   16\njupyter/julia-notebook             Julia Jupyter Notebook Stack from https://gi…   4\n\n\nStars indicate the popularity of the image.\nOfficial images are those in the root namespace."
  },
  {
    "objectID": "applied_lec1.html#docker-images-18",
    "href": "applied_lec1.html#docker-images-18",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nDownloading images\n\n\nThere are two ways to download images.\n\nExplicitly, with docker pull.\nImplicitly, when executing docker run and the image is not found locally."
  },
  {
    "objectID": "applied_lec1.html#docker-images-19",
    "href": "applied_lec1.html#docker-images-19",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nPulling an image\n\n\n$ docker pull debian:jessie\njessie: Pulling from library/debian\nf24aff4096a5: Pull complete\nDigest: sha256:32ad5050caffb2c7e969dac873bce2c370015c2256ff984b70c1c08b3a2816a0\nStatus: Downloaded newer image for debian:jessie\ndocker.io/library/debian:jessie\n\nAs seen previously, images are made up of layers.\nDocker has downloaded all the necessary layers.\nIn this example, :jessie indicates which exact version of Debian we would like.\n\nIt is a version tag."
  },
  {
    "objectID": "applied_lec1.html#docker-images-20",
    "href": "applied_lec1.html#docker-images-20",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nImage tags\n\n\nImages can have tags.\n\nTags define image versions or variants.\ndocker pull ubuntu will refer to ubuntu:latest.\nThe :latest tag is generally updated often."
  },
  {
    "objectID": "applied_lec1.html#docker-images-21",
    "href": "applied_lec1.html#docker-images-21",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nDon’t specify tags:\n\n\n\nWhen doing rapid testing and prototyping.\nWhen experimenting.\nWhen you want the latest version.\n\n\n\n\n\n\n\nDo specify tags:\n\n\n\nWhen recording a procedure into a script.\nWhen going to production.\nTo ensure that the same version will be used everywhere.\nTo ensure repeatability later.\nThis is similar to what we would do with pip install, npm install, etc."
  },
  {
    "objectID": "applied_lec1.html#docker-images-22",
    "href": "applied_lec1.html#docker-images-22",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nMulti-arch images\n\n\nAn image can support multiple architectures\nMore precisely, a specific tag in a given repository can have either:\n\na single manifest referencing an image for a single architecture\na manifest list (or fat manifest) referencing multiple images\n\nIn a manifest list, each image is identified by a combination of:\n\nos (linux, windows)\narchitecture (amd64, arm, arm64…)\noptional fields like variant (for arm and arm64), os.version (for windows)"
  },
  {
    "objectID": "applied_lec1.html#docker-images-23",
    "href": "applied_lec1.html#docker-images-23",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nWorking with multi-arch images\n\n\n\nThe Docker Engine will pull “native” images when available (images matching its own os/architecture/variant)\nWe can ask for a specific image platform with --platform\nThe Docker Engine can run non-native images thanks to QEMU+binfmt (automatically on Docker Desktop; with a bit of setup on Linux)"
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-1",
    "href": "applied_lec1.html#building-images-interactively-1",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nWhat we will do\n\n\nWe will create our first container image. It will be a basic distribution image, but we will pre-install the package figlet.\nWe will:\n\nCreate a container from a base image.\nInstall software manually in the container, and turn it into a new image.\nLearn about new commands:\n\ndocker commit\ndocker tag\nand docker diff."
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-2",
    "href": "applied_lec1.html#building-images-interactively-2",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nPlan\n\n\n\nCreate a container (with docker run) using our base distro of choice.\nRun a bunch of commands to install and set up our software in the container.\n(Optionally) review changes in the container with docker diff.\nTurn the container into a new image with docker commit.\n(Optionally) add tags to the image with docker tag."
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-3",
    "href": "applied_lec1.html#building-images-interactively-3",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nSetting up container\n\n\n\nStart an Ubuntu container:\n\n$ docker run -it ubuntu\nroot@65d17729ff6e:/#\n\n\nRun the commands:\n\n\napt-get update to refresh the list of packages available to install.\napt-get install figlet to install the program we are interested in."
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-4",
    "href": "applied_lec1.html#building-images-interactively-4",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nSetting up container\n\n\nroot@65d17729ff6e:/# apt-get update\nGet:1 http://ports.ubuntu.com/ubuntu-ports noble InRelease [256 kB]\n...\nGet:18 http://ports.ubuntu.com/ubuntu-ports noble-security/restricted arm64 Packages [3119 kB]\nFetched 34.7 MB in 2s (14.7 MB/s)\nReading package lists... Done\nroot@65d17729ff6e:/# apt-get install figlet\n...\nAfter this operation, 745 kB of additional disk space will be used.\nGet:1 http://ports.ubuntu.com/ubuntu-ports noble/universe arm64 figlet arm64 2.2.5-3 [130 kB]\nUnpacking figlet (2.2.5-3) ...\nSetting up figlet (2.2.5-3) ...\n...\nroot@65d17729ff6e:/# exit"
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-5",
    "href": "applied_lec1.html#building-images-interactively-5",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nCheck changes\n\n\n\nType exit at the container prompt to leave the interactive session.\nNow let’s run docker diff to see the difference between the base image and our container\n\ndocker diff 65d1\nC /usr\nC /usr/bin\nA /usr/bin/chkfont\nA /usr/bin/figlist\nA /usr/bin/figlet-figlet\nA /usr/bin/figlet\nA /usr/bin/showfigfonts\nC /usr/share\nC /usr/share/doc\nA /usr/share/doc/figlet\nA /usr/share/doc/figlet/examples\nA /usr/share/doc/figlet/changelog.Debian.gz\nA /usr/share/doc/figlet/copyright\nA /usr/share/emacs\nA /usr/share/emacs/site-lisp\nA /usr/share/emacs/site-lisp/figlet\n..."
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-6",
    "href": "applied_lec1.html#building-images-interactively-6",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nDocker tracks filesystem changes\n\n\n\nAn image is read-only.\nWhen we make changes, they happen in a copy of the image.\nDocker can show the difference between the image, and its copy.\nFor performance, Docker uses copy-on-write systems. (i.e. starting a container based on a big image doesn’t incur a huge copy.)\nContainers can also be started in read-only mode (their root filesystem will be read-only, but they can still have read-write data volumes)"
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-7",
    "href": "applied_lec1.html#building-images-interactively-7",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nCommitting changes\n\n\nThe docker commit command will:\n\nCreate a new layer with those changes\nAnd a new image using this new layer.\n\n$ docker commit 65d1\nsha256:289e61ad4776d701b9133249e91c481f6597b4a178f050d1bcc5df171a2a5bec\nThe output of the docker commit command will be the ID for your newly created image. We can use it as an argument to docker run."
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-8",
    "href": "applied_lec1.html#building-images-interactively-8",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nTesting new image\n\n\n$ docker run -it 289e6\nroot@e88f50c79e4c:/# figlet whazzzuuuppp\n          _\n__      _| |__   __ _ _____________   _ _   _ _   _ _ __  _ __  _ __\n\\ \\ /\\ / / '_ \\ / _` |_  /_  /_  / | | | | | | | | | '_ \\| '_ \\| '_ \\\n \\ V  V /| | | | (_| |/ / / / / /| |_| | |_| | |_| | |_) | |_) | |_) |\n  \\_/\\_/ |_| |_|\\__,_/___/___/___|\\__,_|\\__,_|\\__,_| .__/| .__/| .__/\n                                                   |_|   |_|   |_|\nroot@e88f50c79e4c:/#"
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-9",
    "href": "applied_lec1.html#building-images-interactively-9",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nTagging images\n\n\nReferring to an image by its ID is not convenient. Let’s tag it instead.\nWe can use the tag command:\n$ docker tag 289e6 figlet\nBut we can also specify the tag as an extra argument to commit:\n$ docker commit 65d1 figlet\nsha256:b2e5078491301fbba4fdb585e2e7a826998e438dd698d1c96547df72ce22f470\nAnd then run it using its tag:\n$ docker run -it figlet"
  }
]