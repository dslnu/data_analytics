[
  {
    "objectID": "applied_lab1.html",
    "href": "applied_lab1.html",
    "title": "Applied Analytics: Lab 1",
    "section": "",
    "text": "This lab is about environment setup. We will use uv, a package and project manager for Python."
  },
  {
    "objectID": "applied_lab1.html#recommended-reading",
    "href": "applied_lab1.html#recommended-reading",
    "title": "Applied Analytics: Lab 1",
    "section": "Recommended reading:",
    "text": "Recommended reading:\n\nGit Book\nGit Magic"
  },
  {
    "objectID": "applied_lab2.html",
    "href": "applied_lab2.html",
    "title": "Applied Analytics: Lab 1",
    "section": "",
    "text": "Previous lab listed some methods of optimizing Pandas work:\n\ndata storage optimizations\nCython conversion\nNumba decorators\n\nIn this lab we’ll start looking into parallelisation as a way of optimizing data analysis workflows.\nWhy - because today’s systems are multicore (sometimes very much so).\nCheck yours:\n\nimport os\nos.cpu_count()\n\n12\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are two kinds of parallelism, distributed and shared-memory. Here we’ll talk about shared-memory version.\nDistributed parallelism is when we use multiple machines/VMs for computations.\n\n\n\n\nIn functional programming paradigm, functions are first-class citizens of the languages. They can be composed, applied, passed as arguments to another functions, or returned as values.\nFunctional programming is inherently better suited for parallel programs.\nBenefits of functional programming: - minimization of state - no side-effects - pure functions are easier to reason about - and parallelize (!)\nIt is based on lambda calculus. We’ll learn some of its concepts first: - lambda function definition - application and partial application - currying - closures\nFirst off, functions can be values in Python:\n\nadd = lambda x, y: x + y  \nprint (f\"add = {add}\")\n\nadd = &lt;function &lt;lambda&gt; at 0x106368cc0&gt;\n\n\nAll variables support some operations on them: addition for integers, or e.g. read_csv on Pandas DataFrame.\nFunction variables support a single operation: application.\n\ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \n  \nk = i + j  \nprint (f\"k = {k}\")  \n  \nz = add (i, j)  \nprint (f\"z = {z}\")\n\nk = 1\nz = 1\n\n\nPartial function application is also supported. We’ll use functools package (https://docs.python.org/3/library/functools.html).\n\nfrom functools import partial  \ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \ninc = partial (add, 1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\")  \n\nii = 1, jj = 2\n\n\nNote: the code above both passes add as an argument to partial, and stores the return value of partial function into a new variable inc.\nThe lambda in the example above accepts two arguments essentially. This can be simplified by currying:\n\ni = 0  \nj = 1    \nadd = lambda x: lambda y: x + y  \ninc = add(1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\")\n\nii = 1, jj = 2\n\n\n\n\n\n\n\n\nTipDefinition for currying\n\n\n\nCurrying is the technique of translating a function that takes multiple arguments into a sequence of families of functions, each taking a single argument.\nAlternatively, we can say that currying is a method to transform a function of arity n to n functions of arity 1.\n\n\nWith currying, we can express partial application without and extra partial function.\n\n\n\n\n\n\nNote\n\n\n\nNote: currying relies on closures - functions that can refer to variables defined in surrounding context (like x in add definition above).\n\n\nA monstrous example:\n\ndef f_5(a, b, c, d, e):\n    return a + b + c + d + e\n\ndef c_5(a):\n    def c_4(b):\n        def c_3(c):\n            def c_2(d):\n                def c_1(e):\n                    return f_5(a, b, c, d, e)\n                return c_1\n            return c_2\n        return c_3\n    return c_4\n\nHow to use?\n\nf_5(1,2,3,4,5)\n\n15\n\n\n\nc_5(1)(2)(3)(4)(5)\n\n15\n\n\n\n\n\nIn functional programming, we do not iterate - we map. Why? Because map can be parallelised.\nIteration (like a for-loop) is sequential. Mapping is not.\nmap takes its name from a mathematical term for functions - mappings.\n\n\n\n\n\n\nWarningMap\n\n\n\nArguments:\n\na sequence to iterate on\na function to apply to each element of the sequence.\n\nReturn value:\n\na processed sequence of the same size as input\n\n\n\nNow, if we want to convert our sequence to a sequence of a different length, or different object altogether, we use reduce.\n\n\n\n\n\n\nTipReduce\n\n\n\nArguments:\n\na sequence to iterate on\naccumulation seed to start reducing on\na function of two arguments, accumulation result and next element\n\nReturn value::\n\nresult of accumulation\n\n\n\nUsing above examples, we can easily implement map using list comprehensions as as:\n\nmap = lambda f, l: [f(x) for x in l]\n\nLet’s use it:\n\nint_list = [1,2,3,4,5,6]\ninc_list = map(inc, int_list)\nprint(inc_list)\n\n[2, 3, 4, 5, 6, 7]\n\n\nAnd reduce can be implemented as:\n\ndef reduce_v1 (f, id, s):  \n    n = len(s)  \n    if n == 0:  \n        return id  \n    elif n == 1:  \n        return s[0]  \n    else:  \n        return f(reduce_v1(f, id, s[1:]), s[0])\n\nUsage example:\n\nadd = lambda x, y: x + y  \nres = reduce_v1(add, 0, int_list)\nprint(res)\n\n21\n\n\n\n\nSome tasks can be trivially parallelised - these are called embarrassingly parallel. inc is a simple example of this type.\nOther tasks have a degree of interdependence. Consider below example, adapted from the reduce above:\n\n# Sum all elements of int_list\ndef sum(lst):\n    sum = 0\n    \n    for i in range(len(lst)):\n      sum += lst[i]\n\n    return sum\n\nprint(sum(int_list))\n\n21\n\n\nHow can it be parallelised? By using a technique called chunking:\n\nchunk1 = int_list[:3]\nchunk2 = int_list[3:]\n\nsum_1 = sum(chunk1)\nsum_2 = sum(chunk2)\n\nresult = sum_1 + sum_2\n\nprint(result)\n\n21\n\n\nThen there are algorithms that are inherently serial. An example is computation of a Fibonacci sequence.\n\n\n\nSuppose we want to square each number in a list. A naive approach would be:\n\nnumber_list = [1,2,3,4]\n\ndef squarev1(lst):\n    squared = [] # sometimes pre-allocation can also work, like [None]*1000\n    for el in lst:\n        squared.append(el*el)\n    return squared\n\nsquarev1(number_list)\n\n[1, 4, 9, 16]\n\n\nOr we can use a built-in (https://docs.python.org/3/library/functions.html#map):\n\n# Note that we return a map object that has to be converted to a list first\ndef squarev2(lst):\n    return list(map(lambda x: x*x, lst))\n\nsquarev2(number_list)\n\n[1, 4, 9, 16]\n\n\n\n\n\nMap function can be made to execute in parallel on multiple CPU cores. How? We need to use multiprocessing library (https://docs.python.org/3/library/multiprocessing.html)\n\nfrom multiprocessing import Pool\n\nThere are limitations though: in order to arrange communication between proceses, Python uses pickling: a method of serializing/deserializing objects so that they can be sent between processes.\nPython cannot pickle closures, nested functions or classes! More info on pickling: https://docs.python.org/3/library/pickle.html.\nMoreover, we have to move sqr definition into a separate file (https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror)\n\nimport files.defs as defs\n\ndef squarev3(lst):\n    pool = Pool()\n    return pool.map(defs.sqr, number_list)\n\nsquarev3(number_list)\n\n[1, 4, 9, 16]\n\n\n\n\n\n\n\n\nNotePathos map with Dill\n\n\n\nHowever, Dill (https://pypi.org/project/dill/) can overcome this limitation!\nHow to use Dill?\nuv add pathos\n\nuv add toolz\nAnd then\nfrom pathos.multiprocessing import ProcessPool\n\ndef squarev4(lst):\n    pool = ProcessPool()\n    return pool.map(lambda x: x*x, lst)\n\nsquarev4([1,3,5])\n\n\n\n\n\n\nNumpy automatically vectorizes array operations. However, we can explicitly invoke vectorize():\nIn this context vectorization refers to low-level CPU vectorization (single CPU instruction processing items in a vector)\n\nimport numpy as np\n\nnp_array = np.array(number_list)\n\nvectorized_fn = np.vectorize(lambda x: x*x)\n\nresult = vectorized_fn(np_array)\nprint(result)\n\n[ 1  4  9 16]\n\n\n\n\n\n\nWrite a version of curried function c_5 using lambda syntax.\nWrite a program calculating a factorial using parallelisation and map-reduce.\nWrite a parallelised version of reduce (hint: use chunking).\nCompare performance of Numpy vectorization vs parallel map on a really large array of integers.\nUsing computations from previous labs, apply chunking approach to df.apply(...) using parallel map."
  },
  {
    "objectID": "applied_lab2.html#short-intro-to-functional-programming",
    "href": "applied_lab2.html#short-intro-to-functional-programming",
    "title": "Applied Analytics: Lab 1",
    "section": "",
    "text": "In functional programming paradigm, functions are first-class citizens of the languages. They can be composed, applied, passed as arguments to another functions, or returned as values.\nFunctional programming is inherently better suited for parallel programs.\nBenefits of functional programming: - minimization of state - no side-effects - pure functions are easier to reason about - and parallelize (!)\nIt is based on lambda calculus. We’ll learn some of its concepts first: - lambda function definition - application and partial application - currying - closures\nFirst off, functions can be values in Python:\n\nadd = lambda x, y: x + y  \nprint (f\"add = {add}\")\n\nadd = &lt;function &lt;lambda&gt; at 0x106368cc0&gt;\n\n\nAll variables support some operations on them: addition for integers, or e.g. read_csv on Pandas DataFrame.\nFunction variables support a single operation: application.\n\ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \n  \nk = i + j  \nprint (f\"k = {k}\")  \n  \nz = add (i, j)  \nprint (f\"z = {z}\")\n\nk = 1\nz = 1\n\n\nPartial function application is also supported. We’ll use functools package (https://docs.python.org/3/library/functools.html).\n\nfrom functools import partial  \ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \ninc = partial (add, 1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\")  \n\nii = 1, jj = 2\n\n\nNote: the code above both passes add as an argument to partial, and stores the return value of partial function into a new variable inc.\nThe lambda in the example above accepts two arguments essentially. This can be simplified by currying:\n\ni = 0  \nj = 1    \nadd = lambda x: lambda y: x + y  \ninc = add(1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\")\n\nii = 1, jj = 2\n\n\n\n\n\n\n\n\nTipDefinition for currying\n\n\n\nCurrying is the technique of translating a function that takes multiple arguments into a sequence of families of functions, each taking a single argument.\nAlternatively, we can say that currying is a method to transform a function of arity n to n functions of arity 1.\n\n\nWith currying, we can express partial application without and extra partial function.\n\n\n\n\n\n\nNote\n\n\n\nNote: currying relies on closures - functions that can refer to variables defined in surrounding context (like x in add definition above).\n\n\nA monstrous example:\n\ndef f_5(a, b, c, d, e):\n    return a + b + c + d + e\n\ndef c_5(a):\n    def c_4(b):\n        def c_3(c):\n            def c_2(d):\n                def c_1(e):\n                    return f_5(a, b, c, d, e)\n                return c_1\n            return c_2\n        return c_3\n    return c_4\n\nHow to use?\n\nf_5(1,2,3,4,5)\n\n15\n\n\n\nc_5(1)(2)(3)(4)(5)\n\n15"
  },
  {
    "objectID": "applied_lab2.html#map-reduce",
    "href": "applied_lab2.html#map-reduce",
    "title": "Applied Analytics: Lab 1",
    "section": "",
    "text": "In functional programming, we do not iterate - we map. Why? Because map can be parallelised.\nIteration (like a for-loop) is sequential. Mapping is not.\nmap takes its name from a mathematical term for functions - mappings.\n\n\n\n\n\n\nWarningMap\n\n\n\nArguments:\n\na sequence to iterate on\na function to apply to each element of the sequence.\n\nReturn value:\n\na processed sequence of the same size as input\n\n\n\nNow, if we want to convert our sequence to a sequence of a different length, or different object altogether, we use reduce.\n\n\n\n\n\n\nTipReduce\n\n\n\nArguments:\n\na sequence to iterate on\naccumulation seed to start reducing on\na function of two arguments, accumulation result and next element\n\nReturn value::\n\nresult of accumulation\n\n\n\nUsing above examples, we can easily implement map using list comprehensions as as:\n\nmap = lambda f, l: [f(x) for x in l]\n\nLet’s use it:\n\nint_list = [1,2,3,4,5,6]\ninc_list = map(inc, int_list)\nprint(inc_list)\n\n[2, 3, 4, 5, 6, 7]\n\n\nAnd reduce can be implemented as:\n\ndef reduce_v1 (f, id, s):  \n    n = len(s)  \n    if n == 0:  \n        return id  \n    elif n == 1:  \n        return s[0]  \n    else:  \n        return f(reduce_v1(f, id, s[1:]), s[0])\n\nUsage example:\n\nadd = lambda x, y: x + y  \nres = reduce_v1(add, 0, int_list)\nprint(res)\n\n21\n\n\n\n\nSome tasks can be trivially parallelised - these are called embarrassingly parallel. inc is a simple example of this type.\nOther tasks have a degree of interdependence. Consider below example, adapted from the reduce above:\n\n# Sum all elements of int_list\ndef sum(lst):\n    sum = 0\n    \n    for i in range(len(lst)):\n      sum += lst[i]\n\n    return sum\n\nprint(sum(int_list))\n\n21\n\n\nHow can it be parallelised? By using a technique called chunking:\n\nchunk1 = int_list[:3]\nchunk2 = int_list[3:]\n\nsum_1 = sum(chunk1)\nsum_2 = sum(chunk2)\n\nresult = sum_1 + sum_2\n\nprint(result)\n\n21\n\n\nThen there are algorithms that are inherently serial. An example is computation of a Fibonacci sequence.\n\n\n\nSuppose we want to square each number in a list. A naive approach would be:\n\nnumber_list = [1,2,3,4]\n\ndef squarev1(lst):\n    squared = [] # sometimes pre-allocation can also work, like [None]*1000\n    for el in lst:\n        squared.append(el*el)\n    return squared\n\nsquarev1(number_list)\n\n[1, 4, 9, 16]\n\n\nOr we can use a built-in (https://docs.python.org/3/library/functions.html#map):\n\n# Note that we return a map object that has to be converted to a list first\ndef squarev2(lst):\n    return list(map(lambda x: x*x, lst))\n\nsquarev2(number_list)\n\n[1, 4, 9, 16]\n\n\n\n\n\nMap function can be made to execute in parallel on multiple CPU cores. How? We need to use multiprocessing library (https://docs.python.org/3/library/multiprocessing.html)\n\nfrom multiprocessing import Pool\n\nThere are limitations though: in order to arrange communication between proceses, Python uses pickling: a method of serializing/deserializing objects so that they can be sent between processes.\nPython cannot pickle closures, nested functions or classes! More info on pickling: https://docs.python.org/3/library/pickle.html.\nMoreover, we have to move sqr definition into a separate file (https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror)\n\nimport files.defs as defs\n\ndef squarev3(lst):\n    pool = Pool()\n    return pool.map(defs.sqr, number_list)\n\nsquarev3(number_list)\n\n[1, 4, 9, 16]\n\n\n\n\n\n\n\n\nNotePathos map with Dill\n\n\n\nHowever, Dill (https://pypi.org/project/dill/) can overcome this limitation!\nHow to use Dill?\nuv add pathos\n\nuv add toolz\nAnd then\nfrom pathos.multiprocessing import ProcessPool\n\ndef squarev4(lst):\n    pool = ProcessPool()\n    return pool.map(lambda x: x*x, lst)\n\nsquarev4([1,3,5])"
  },
  {
    "objectID": "applied_lab2.html#numpy-vectorization",
    "href": "applied_lab2.html#numpy-vectorization",
    "title": "Applied Analytics: Lab 1",
    "section": "",
    "text": "Numpy automatically vectorizes array operations. However, we can explicitly invoke vectorize():\nIn this context vectorization refers to low-level CPU vectorization (single CPU instruction processing items in a vector)\n\nimport numpy as np\n\nnp_array = np.array(number_list)\n\nvectorized_fn = np.vectorize(lambda x: x*x)\n\nresult = vectorized_fn(np_array)\nprint(result)\n\n[ 1  4  9 16]"
  },
  {
    "objectID": "applied_lab2.html#exercises",
    "href": "applied_lab2.html#exercises",
    "title": "Applied Analytics: Lab 1",
    "section": "",
    "text": "Write a version of curried function c_5 using lambda syntax.\nWrite a program calculating a factorial using parallelisation and map-reduce.\nWrite a parallelised version of reduce (hint: use chunking).\nCompare performance of Numpy vectorization vs parallel map on a really large array of integers.\nUsing computations from previous labs, apply chunking approach to df.apply(...) using parallel map."
  },
  {
    "objectID": "nb/lab13/Lab13.html",
    "href": "nb/lab13/Lab13.html",
    "title": "Docker continued",
    "section": "",
    "text": "Reproduce Python’s venv in Docker.\nRun Postgres via Docker\nConnect Python, Postgres, and MinIO service from Lab12.\n!docker --version\n\nDocker version 27.2.0, build 3ab4256\n!docker container ls\n\nCONTAINER ID   IMAGE      COMMAND                  CREATED       STATUS       PORTS                    NAMES\nbac38f7c5595   postgres   \"docker-entrypoint.s…\"   2 hours ago   Up 2 hours   0.0.0.0:5432-&gt;5432/tcp   some-postgres\n!docker container ls -a\n\nCONTAINER ID   IMAGE          COMMAND                   CREATED       STATUS                   PORTS                    NAMES\nbac38f7c5595   postgres       \"docker-entrypoint.s…\"    2 hours ago   Up 2 hours               0.0.0.0:5432-&gt;5432/tcp   some-postgres\na776ae3b01f9   alpine         \"/bin/sh\"                 2 hours ago   Exited (0) 2 hours ago                            nifty_dubinsky\n2b0d959cfb50   alpine         \"/bin/sh\"                 2 hours ago   Exited (0) 2 hours ago                            suspicious_tu\na290332b35f5   nginx:alpine   \"/docker-entrypoint.…\"    2 hours ago   Exited (0) 2 hours ago                            pedantic_curie\nf39bb803541b   jupyter-test   \"jupyter notebook --…\"    3 hours ago   Exited (0) 3 hours ago                            nervous_shannon\n8cd0784953f0   nginx:alpine   \"/docker-entrypoint.…\"    4 hours ago   Exited (0) 4 hours ago                            my-site\n2c97da6ec87d   alpine         \"/bin/sh -c 'echo \\\"T…\"   5 hours ago   Exited (0) 5 hours ago                            demo\n!docker container run alpine\n!docker container ls\n\nCONTAINER ID   IMAGE      COMMAND                  CREATED       STATUS       PORTS                    NAMES\nbac38f7c5595   postgres   \"docker-entrypoint.s…\"   2 hours ago   Up 2 hours   0.0.0.0:5432-&gt;5432/tcp   some-postgres\n!docker container run alpine echo \"Hello world!\"\n\nHello world!\n!docker container run -it alpine /bin/sh\n\n/ # ^C\n\n/ # \n/ #\n!docker container ls -a\n\nCONTAINER ID   IMAGE          COMMAND                   CREATED              STATUS                          PORTS                    NAMES\n525f5374841b   alpine         \"/bin/sh\"                 15 seconds ago       Up 15 seconds                                            amazing_mendeleev\n427f2656bdea   alpine         \"/bin/sh\"                 About a minute ago   Exited (0) About a minute ago                            pedantic_wilbur\n22cde3220cab   alpine         \"echo 'Hello world!'\"     2 minutes ago        Exited (0) 2 minutes ago                                 sad_jennings\n41c3f54ba8ae   alpine         \"/bin/sh\"                 3 minutes ago        Exited (0) 3 minutes ago                                 focused_babbage\nbac38f7c5595   postgres       \"docker-entrypoint.s…\"    2 hours ago          Up 2 hours                      0.0.0.0:5432-&gt;5432/tcp   some-postgres\na776ae3b01f9   alpine         \"/bin/sh\"                 2 hours ago          Exited (0) 2 hours ago                                   nifty_dubinsky\n2b0d959cfb50   alpine         \"/bin/sh\"                 2 hours ago          Exited (0) 2 hours ago                                   suspicious_tu\na290332b35f5   nginx:alpine   \"/docker-entrypoint.…\"    2 hours ago          Exited (0) 2 hours ago                                   pedantic_curie\nf39bb803541b   jupyter-test   \"jupyter notebook --…\"    3 hours ago          Exited (0) 3 hours ago                                   nervous_shannon\n8cd0784953f0   nginx:alpine   \"/docker-entrypoint.…\"    5 hours ago          Exited (0) 5 hours ago                                   my-site\n2c97da6ec87d   alpine         \"/bin/sh -c 'echo \\\"T…\"   5 hours ago          Exited (0) 5 hours ago                                   demo\n!docker container rm a290332b35f5\n\na290332b35f5\n!docker container rm nervous_shannon\n\nnervous_shannon\n!docker container ls -a\n\nCONTAINER ID   IMAGE          COMMAND                   CREATED         STATUS                     PORTS                    NAMES\n525f5374841b   alpine         \"/bin/sh\"                 2 minutes ago   Up 2 minutes                                        amazing_mendeleev\n427f2656bdea   alpine         \"/bin/sh\"                 3 minutes ago   Exited (0) 3 minutes ago                            pedantic_wilbur\n22cde3220cab   alpine         \"echo 'Hello world!'\"     4 minutes ago   Exited (0) 4 minutes ago                            sad_jennings\n41c3f54ba8ae   alpine         \"/bin/sh\"                 5 minutes ago   Exited (0) 5 minutes ago                            focused_babbage\nbac38f7c5595   postgres       \"docker-entrypoint.s…\"    2 hours ago     Up 2 hours                 0.0.0.0:5432-&gt;5432/tcp   some-postgres\na776ae3b01f9   alpine         \"/bin/sh\"                 2 hours ago     Exited (0) 2 hours ago                              nifty_dubinsky\n2b0d959cfb50   alpine         \"/bin/sh\"                 2 hours ago     Exited (0) 2 hours ago                              suspicious_tu\n8cd0784953f0   nginx:alpine   \"/docker-entrypoint.…\"    5 hours ago     Exited (0) 5 hours ago                              my-site\n2c97da6ec87d   alpine         \"/bin/sh -c 'echo \\\"T…\"   5 hours ago     Exited (0) 5 hours ago                              demo\n!docker container inspect 22cde3220cab\n!docker container ls -al\n\nCONTAINER ID   IMAGE        COMMAND                  CREATED          STATUS                      PORTS     NAMES\n95d880ace529   lab13_test   \"/bin/sh -c /app/bin…\"   24 seconds ago   Exited (0) 24 seconds ago             priceless_dewdney\njjjkkk\n!docker image ls\n\nREPOSITORY                    TAG       IMAGE ID       CREATED       SIZE\njupyter-test                  latest    b177db00a696   5 hours ago   1.98GB\nlab13_test                    latest    4b530e1c13a9   5 hours ago   8.72MB\nhello-world                   latest    3a941dd08651   5 hours ago   8.72MB\nquay.io/minio/minio           latest    9535594ad412   3 weeks ago   217MB\nnginx                         alpine    2140dad235c1   4 weeks ago   76.7MB\npostgres                      latest    8d3be35b184e   5 weeks ago   635MB\npinger                        latest    60cf059f4580   8 weeks ago   11.4MB\nalpine                        latest    beefdbd8a1da   8 weeks ago   13.6MB\nfundamentalsofdocker/trivia   ed2       0bb60b695895   5 years ago   13.1MB\n!docker image rm lab13_test\n\nError response from daemon: conflict: unable to delete lab13_test:latest (must be forced) - container 64b41be797e7 is using its referenced image 4b530e1c13a9\n!docker container rm 64b41be797e7\n\n64b41be797e7\n!docker image rm lab13_test\n\nUntagged: lab13_test:latest\nDeleted: sha256:4b530e1c13a90c633181316e10f8f1e81682de19fa2a103b11dd44bce8038110"
  },
  {
    "objectID": "nb/lab13/Lab13.html#volumes",
    "href": "nb/lab13/Lab13.html#volumes",
    "title": "Docker continued",
    "section": "Volumes",
    "text": "Volumes\n\n!docker volume ls\n\nDRIVER    VOLUME NAME\nlocal     5cf3ff92ba2211406922173a6d82380bb1ea0c28dc44ffeef15fc452f8a38898\nlocal     44923ec30724550c9448f3ac540af04400ade702d877f93cda26d2278d390788\nlocal     sample"
  },
  {
    "objectID": "nb/lab12/Untitled.html",
    "href": "nb/lab12/Untitled.html",
    "title": "Big Data analytics / Applied Data analytics",
    "section": "",
    "text": "import docker\n\nclient = docker.from_env()\n\n\nclient\n\n&lt;docker.client.DockerClient at 0x11103c9e0&gt;\n\n\n\nfrom s3fs import S3FileSystem\n\nkey = \"SPQRbfacnSk7FvG1AYXw\"\nsecret = \"pBhwInewmoZ09ijK3AUeaBlhPjrznGJD45ICpeb1\"\nendpoint_url = \"http://localhost:8080\"\n\ns3 = S3FileSystem(anon=False, endpoint_url=endpoint_url,\n                 key=key,\n                 secret=secret,\n                 use_ssl=False)\n\n\ns3.ls(\"/\")\n\n[]"
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#when-to-use-dask.dataframe",
    "href": "nb/lab4/01_dataframe.html#when-to-use-dask.dataframe",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "When to use dask.dataframe",
    "text": "When to use dask.dataframe\npandas is great for tabular datasets that fit in memory. A general rule of thumb for pandas is:\n\n\n“Have 5 to 10 times as much RAM as the size of your dataset”\n\nWes McKinney (2017) in 10 things I hate about pandas\n\n\n\nHere “size of dataset” means dataset size on the disk.\nDask becomes useful when the datasets exceed the above rule.\nIn this notebook, you will be working with the New York City Airline data. This dataset is only ~200MB, so that you can download it in a reasonable time, but dask.dataframe will scale to datasets much larger than memory."
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#create-datasets",
    "href": "nb/lab4/01_dataframe.html#create-datasets",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Create datasets",
    "text": "Create datasets\nCreate the datasets you will be using in this notebook:\n\n# This step is not necessary - dataset is already downloaded\n#%run prep.py -d flights"
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#suppress-futurewarnings",
    "href": "nb/lab4/01_dataframe.html#suppress-futurewarnings",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Suppress FutureWarnings",
    "text": "Suppress FutureWarnings\n\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)"
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#set-up-your-local-cluster",
    "href": "nb/lab4/01_dataframe.html#set-up-your-local-cluster",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Set up your local cluster",
    "text": "Set up your local cluster\nCreate a local Dask cluster and connect it to the client. Don’t worry about this bit of code for now, you will learn more in the Distributed notebook.\n\nfrom dask.distributed import Client\n\nclient = Client(n_workers=4)\nclient\n\n\nDask Diagnostic Dashboard\nDask Distributed provides a useful Dashboard to visualize the state of your cluster and computations.\nIf you’re on JupyterLab or Binder, you can use the Dask JupyterLab extension (which should be already installed in your environment) to open the dashboard plots: * Click on the Dask logo in the left sidebar * Click on the magnifying glass icon, which will automatically connect to the active dashboard (if that doesn’t work, you can type/paste the dashboard link http://127.0.0.1:8787 in the field) * Click on “Task Stream”, “Progress Bar”, and “Worker Memory”, which will open these plots in new tabs * Re-organize the tabs to suit your workflow!\nAlternatively, click on the dashboard link displayed in the Client details above: http://127.0.0.1:8787/status. It will open a new browser tab with the Dashboard."
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#reading-and-working-with-datasets",
    "href": "nb/lab4/01_dataframe.html#reading-and-working-with-datasets",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Reading and working with datasets",
    "text": "Reading and working with datasets\nLet’s read an extract of flights in the USA across several years. This data is specific to flights out of the three airports in the New York City area.\n\nimport os\nimport dask\n\nBy convention, we import the module dask.dataframe as dd, and call the corresponding DataFrame object ddf.\nNote: The term “Dask DataFrame” is slightly overloaded. Depending on the context, it can refer to the module or the DataFrame object. To avoid confusion, throughout this notebook: - dask.dataframe (note the all lowercase) refers to the API, and - DataFrame (note the CamelCase) refers to the object.\nThe following filename includes a glob pattern *, so all files in the path matching that pattern will be read into the same DataFrame.\n\nimport dask.dataframe as dd\n\nddf = dd.read_csv(\n    os.path.join(\"data\", \"nycflights\", \"*.csv\"), parse_dates={\"Date\": [0, 1, 2]}\n)\nddf\n\nDask has not loaded the data yet, it has: - investigated the input path and found that there are ten matching files - intelligently created a set of jobs for each chunk – one per original CSV file in this case\nNotice that the representation of the DataFrame object contains no data - Dask has just done enough to read the start of the first file, and infer the column names and dtypes.\n\nLazy Evaluation\nMost Dask Collections, including Dask DataFrame are evaluated lazily, which means Dask constructs the logic (called task graph) of your computation immediately but “evaluates” them only when necessary. You can view this task graph using .visualize().\nYou will learn more about this in the Delayed notebook, but for now, note that we need to call .compute() to trigger actual computations.\n\nddf.visualize()\n\nSome functions like len and head also trigger a computation. Specifically, calling len will: - load actual data, (that is, load each file into a pandas DataFrame) - then apply the corresponding functions to each pandas DataFrame (also known as a partition) - combine the subtotals to give you the final grand total\n\n# load and count number of rows\nlen(ddf)\n\nYou can view the start and end of the data as you would in pandas:\n\nddf.head()\n\nddf.tail()\n\n# ValueError: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n# +----------------+---------+----------+\n# | Column         | Found   | Expected |\n# +----------------+---------+----------+\n# | CRSElapsedTime | float64 | int64    |\n# | TailNum        | object  | float64  |\n# +----------------+---------+----------+\n\n# The following columns also raised exceptions on conversion:\n\n# - TailNum\n#   ValueError(\"could not convert string to float: 'N54711'\")\n\n# Usually this is due to dask's dtype inference failing, and\n# *may* be fixed by specifying dtypes manually by adding:\n\n# dtype={'CRSElapsedTime': 'float64',\n#        'TailNum': 'object'}\n\n# to the call to `read_csv`/`read_table`.\nUnlike pandas.read_csv which reads in the entire file before inferring datatypes, dask.dataframe.read_csv only reads in a sample from the beginning of the file (or first file if using a glob). These inferred datatypes are then enforced when reading all partitions.\nIn this case, the datatypes inferred in the sample are incorrect. The first n rows have no value for CRSElapsedTime (which pandas infers as a float), and later on turn out to be strings (object dtype). Note that Dask gives an informative error message about the mismatch. When this happens you have a few options:\n\nSpecify dtypes directly using the dtype keyword. This is the recommended solution, as it’s the least error prone (better to be explicit than implicit) and also the most performant.\nIncrease the size of the sample keyword (in bytes)\nUse assume_missing to make dask assume that columns inferred to be int (which don’t allow missing values) are actually floats (which do allow missing values). In our particular case this doesn’t apply.\n\nIn our case we’ll use the first option and directly specify the dtypes of the offending columns.\n\nddf = dd.read_csv(\n    os.path.join(\"data\", \"nycflights\", \"*.csv\"),\n    #parse_dates={\"Date\": [0, 1, 2]},\n    dtype={\"TailNum\": str, \"CRSElapsedTime\": float, \"Cancelled\": bool},\n)\n\n\nddf.tail()  # now works\n\n\n\nReading from remote storage\nIf you’re thinking about distributed computing, your data is probably stored remotely on services (like Amazon’s S3 or Google’s cloud storage) and is in a friendlier format (like Parquet). Dask can read data in various formats directly from these remote locations lazily and in parallel.\nHere’s how you can read the NYC taxi cab data from Amazon S3:\nddf = dd.read_parquet(\n    \"s3://nyc-tlc/trip data/yellow_tripdata_2012-*.parquet\",\n)\nYou can also leverage Parquet-specific optimizations like column selection and metadata handling, learn more in the Dask documentation on working with Parquet files."
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#computations-with-dask.dataframe",
    "href": "nb/lab4/01_dataframe.html#computations-with-dask.dataframe",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Computations with dask.dataframe",
    "text": "Computations with dask.dataframe\nLet’s compute the maximum of the flight delay.\nWith just pandas, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums.\nimport pandas as pd\n\nfiles = os.listdir(os.path.join('data', 'nycflights'))\n\nmaxes = []\n\nfor file in files:\n    df = pd.read_csv(os.path.join('data', 'nycflights', file))\n    maxes.append(df.DepDelay.max())\n    \nfinal_max = max(maxes)\ndask.dataframe lets us write pandas-like code, that operates on larger-than-memory datasets in parallel.\n\n%%time\nresult = ddf.DepDelay.max()\nresult.compute()\n\nThis creates the lazy computation for us and then runs it.\nNote: Dask will delete intermediate results (like the full pandas DataFrame for each file) as soon as possible. This means you can handle datasets that are larger than memory but, repeated computations will have to load all of the data in each time. (Run the code above again, is it faster or slower than you would expect?)\nYou can view the underlying task graph using .visualize():\n\n# notice the parallelism\nresult.visualize()"
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#exercises",
    "href": "nb/lab4/01_dataframe.html#exercises",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Exercises",
    "text": "Exercises\nIn this section you will do a few dask.dataframe computations. If you are comfortable with pandas then these should be familiar. You will have to think about when to call .compute().\n\n1. How many rows are in our dataset?\nHint: how would you check how many items are in a list?\n\n# Your code here\n\n\n\n2. In total, how many non-canceled flights were taken?\nHint: use boolean indexing.\n\n# Your code here\n\n\n\n3. In total, how many non-canceled flights were taken from each airport?\nHint: use groupby.\n\n# Your code here\n\n\n\n4. What was the average departure delay from each airport?\n\n# Your code here\n\n\n\n5. What day of the week has the worst average departure delay?\n\n# Your code here\n\n\n\n6. Let’s say the distance column is erroneous and you need to add 1 to all values, how would you do this?\n\n# Your code here"
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#sharing-intermediate-results",
    "href": "nb/lab4/01_dataframe.html#sharing-intermediate-results",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Sharing Intermediate Results",
    "text": "Sharing Intermediate Results\nWhen computing all of the above, we sometimes did the same operation more than once. For most operations, dask.dataframe stores the arguments, allowing duplicate computations to be shared and only computed once.\nFor example, let’s compute the mean and standard deviation for departure delay of all non-canceled flights. Since Dask operations are lazy, those values aren’t the final results yet. They’re just the steps required to get the result.\nIf you compute them with two calls to compute, there is no sharing of intermediate computations.\n\nnon_canceled = ddf[~ddf.Cancelled]\nmean_delay = non_canceled.DepDelay.mean()\nstd_delay = non_canceled.DepDelay.std()\n\n\n%%time\n\nmean_delay_res = mean_delay.compute()\nstd_delay_res = std_delay.compute()\n\n\ndask.compute\nBut let’s try by passing both to a single compute call.\n\n%%time\n\nmean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)\n\nUsing dask.compute takes roughly 1/2 the time. This is because the task graphs for both results are merged when calling dask.compute, allowing shared operations to only be done once instead of twice. In particular, using dask.compute only does the following once:\n\nthe calls to read_csv\nthe filter (df[~df.Cancelled])\nsome of the necessary reductions (sum, count)\n\nTo see what the merged task graphs between multiple results look like (and what’s shared), you can use the dask.visualize function (you might want to use filename='graph.pdf' to save the graph to disk so that you can zoom in more easily):\n\ndask.visualize(mean_delay, std_delay)#, engine=\"cytoscape\") # optionally we can use a different visualization engine\n\n\n\n.persist()\nWhile using a distributed scheduler (you will learn more about schedulers in the upcoming notebooks), you can keep some data that you want to use often in the distributed memory.\npersist generates “Futures” (more on this later as well) and stores them in the same structure as your output. You can use persist with any data or computation that fits in memory.\nIf you want to analyze data only for non-canceled flights departing from JFK airport, you can either have two compute calls like in the previous section:\n\nnon_cancelled = ddf[~ddf.Cancelled]\nddf_jfk = non_cancelled[non_cancelled.Origin == \"JFK\"]\n\n\n%%time\nddf_jfk.DepDelay.mean().compute()\nddf_jfk.DepDelay.sum().compute()\n\nOr, consider persisting that subset of data in memory.\nSee the “Graph” dashboard plot, the red squares indicate persisted data stored as Futures in memory. You will also notice an increase in Worker Memory (another dashboard plot) consumption.\n\nddf_jfk = ddf_jfk.persist()  # returns back control immediately\n\n\n%%time\nddf_jfk.DepDelay.mean().compute()\nddf_jfk.DepDelay.std().compute()\n\nAnalyses on this persisted data is faster because we are not repeating the loading and selecting (non-canceled, JFK departure) operations."
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#custom-code-with-dask-dataframe",
    "href": "nb/lab4/01_dataframe.html#custom-code-with-dask-dataframe",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Custom code with Dask DataFrame",
    "text": "Custom code with Dask DataFrame\ndask.dataframe only covers a small but well-used portion of the pandas API.\nThis limitation is for two reasons:\n\nThe Pandas API is huge\nSome operations are genuinely hard to do in parallel, e.g, sorting.\n\nAdditionally, some important operations like set_index work, but are slower than in pandas because they include substantial shuffling of data, and may write out to disk.\nWhat if you want to use some custom functions that aren’t (or can’t be) implemented for Dask DataFrame yet?\nYou can open an issue on the Dask issue tracker to check how feasible the function could be to implement, and you can consider contributing this function to Dask.\nIn case it’s a custom function or tricky to implement, dask.dataframe provides a few methods to make applying custom functions to Dask DataFrames easier:\n\nmap_partitions: to run a function on each partition (each pandas DataFrame) of the Dask DataFrame\nmap_overlap: to run a function on each partition (each pandas DataFrame) of the Dask DataFrame, with some rows shared between neighboring partitions\nreduction: for custom row-wise reduction operations.\n\nLet’s take a quick look at the map_partitions() function:\n\nhelp(ddf.map_partitions)\n\nThe “Distance” column in ddf is currently in miles. Let’s say we want to convert the units to kilometers and we have a general helper function as shown below. In this case, we can use map_partitions to apply this function across each of the internal pandas DataFrames in parallel.\n\nimport pandas as pd\n\ndef my_custom_converter(df, multiplier=1):\n    return df * multiplier\n\n\nmeta = pd.Series(name=\"Distance\", dtype=\"float64\")\n\ndistance_km = ddf.Distance.map_partitions(\n    my_custom_converter, multiplier=0.6, meta=meta\n)\n\n\ndistance_km.visualize()\n\n\ndistance_km.head()\n\n\nWhat is meta?\nSince Dask operates lazily, it doesn’t always have enough information to infer the output structure (which includes datatypes) of certain operations.\nmeta is a suggestion to Dask about the output of your computation. Importantly, meta never infers with the output structure. Dask uses this meta until it can determine the actual output structure.\nEven though there are many ways to define meta, we suggest using a small pandas Series or DataFrame that matches the structure of your final output."
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#close-you-local-dask-cluster",
    "href": "nb/lab4/01_dataframe.html#close-you-local-dask-cluster",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Close you local Dask Cluster",
    "text": "Close you local Dask Cluster\nIt’s good practice to always close any Dask cluster you create:\n\nclient.shutdown()"
  },
  {
    "objectID": "nb/lab3/lab3.html",
    "href": "nb/lab3/lab3.html",
    "title": "map-reduce cont’d",
    "section": "",
    "text": "Suppose we want to apply several transformations. What can we do?\n\nSeveral maps.\nChain functions with compose.\nCreate a function pipeline with pipe.\n\nSuppose we want to apply \\(f(x) = \\exp(\\sin(x))\\) to a list of numbers.\nOption 1:\n\nimport math\n\nfloat_list = [0.234, -1.96, 5.071]\n\n# several maps\nlst1 = map(math.sin, float_list)\nlst2 = map(math.exp, lst1)\n\nprint(list(lst2))\n\nOption 2: we’ll use the toolz package we installed in lab2.\n\nfrom toolz.functoolz import compose\n\ncomposed = compose(math.exp, math.sin) # note reverse order\n\nprint(list(map(composed, float_list)))\n\n** Option 3:**\nAnd there is also pipe:\n\nfrom toolz.functoolz import pipe\n\npiped = lambda x: pipe(x, math.sin, math.exp) # now the order is proper\n\nprint(list(map(piped, float_list)))"
  },
  {
    "objectID": "nb/lab3/lab3.html#function-pipelines",
    "href": "nb/lab3/lab3.html#function-pipelines",
    "title": "map-reduce cont’d",
    "section": "",
    "text": "Suppose we want to apply several transformations. What can we do?\n\nSeveral maps.\nChain functions with compose.\nCreate a function pipeline with pipe.\n\nSuppose we want to apply \\(f(x) = \\exp(\\sin(x))\\) to a list of numbers.\nOption 1:\n\nimport math\n\nfloat_list = [0.234, -1.96, 5.071]\n\n# several maps\nlst1 = map(math.sin, float_list)\nlst2 = map(math.exp, lst1)\n\nprint(list(lst2))\n\nOption 2: we’ll use the toolz package we installed in lab2.\n\nfrom toolz.functoolz import compose\n\ncomposed = compose(math.exp, math.sin) # note reverse order\n\nprint(list(map(composed, float_list)))\n\n** Option 3:**\nAnd there is also pipe:\n\nfrom toolz.functoolz import pipe\n\npiped = lambda x: pipe(x, math.sin, math.exp) # now the order is proper\n\nprint(list(map(piped, float_list)))"
  },
  {
    "objectID": "nb/lab3/lab3.html#laziness",
    "href": "nb/lab3/lab3.html#laziness",
    "title": "map-reduce cont’d",
    "section": "Laziness",
    "text": "Laziness\nmap is lazy! This is the opposite to being eager. That’s why we apply list first to materialize results of map application.\nLaziness provides memory efficiency."
  },
  {
    "objectID": "nb/lab3/lab3.html#filter",
    "href": "nb/lab3/lab3.html#filter",
    "title": "map-reduce cont’d",
    "section": "Filter",
    "text": "Filter\nIn addition to map, there are also filter and zip.\nVariations:\n\nfrom itertools import filterfalse\nfrom toolz.dicttoolz import keyfilter, valfilter, itemfilter\n\ndef is_even(x):\n    if x % 2 == 0: return True\n    else: return False\n        \ndef both_are_even(x):\n    k,v = x\n    if is_even(k) and is_even(v): return True\n    else: return False\n        \nprint(list(filterfalse(is_even, range(10))))\n# [1, 3, 5, 7, 9]\nprint(list(keyfilter(is_even, {1:2, 2:3, 3:4, 4:5, 5:6})))\n# [2, 4]\nprint(list(valfilter(is_even, {1:2, 2:3, 3:4, 4:5, 5:6})))\n# [1, 3, 5]\nprint(list(itemfilter(both_are_even, {1:5, 2:4, 3:3, 4:2, 5:1}))) # [2, 4]\n\n\nzip\n\nprices = [25, 20, 40]\nitems = [\"peas\", \"chicken\", \"beef\"]\n\nzipped = list(zip(prices, items))\n\nimport pandas as pd\n\ndf = pd.DataFrame(list(zipped), columns=['price', 'name'])\n\n\ndf"
  },
  {
    "objectID": "nb/lab3/lab3.html#iterators",
    "href": "nb/lab3/lab3.html#iterators",
    "title": "map-reduce cont’d",
    "section": "Iterators",
    "text": "Iterators\nPython documentation defines iterator as:\nAn iterator is an object representing a stream of data; this object returns the data one element at a time.\n\nAn object is called iterable if you can get an iterator for it.\nIterator classes implement __iter__() and __next__() methods. This means one can create their own iterator, like this:\n\nclass SequenceIterator:\n    def __init__(self, sequence):\n        self._sequence = sequence\n        self._index = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self._index &lt; len(self._sequence):\n            item = self._sequence[self._index]\n            self._index += 1\n            return item\n        else:\n            raise StopIteration\n\n\nfor el in SequenceIterator([1,2,3]):\n    print(el)"
  },
  {
    "objectID": "nb/lab3/lab3.html#generators",
    "href": "nb/lab3/lab3.html#generators",
    "title": "map-reduce cont’d",
    "section": "Generators",
    "text": "Generators\nGenerators are simply an easy way to create iterators.\nA generator example, using yield keyword:\n\ndef even_numbers(n):\n    i=1\n    while i &lt;= n:\n        yield i*2\n        i += 1\n\n\nfor i in even_numbers(20):\n    print(i)\n\nA generator example, using an ad-hoc syntax similar to list comprehensions:\n\nfirst_100_even = (i*2 for i in range(1,20))\nprint(list(first_100_even))\n\nAnd now, what is all the fuss about? We can actually use map, reduce, and friends, on iterables, not just on lists:\n\n# map example\ngen_mapped = map(lambda x: x*x, even_numbers(30))\nprint(list(gen_mapped))\n\n\n# reduce example\nfrom functools import reduce\ngen_reduced = reduce(lambda acc, el: acc+el, even_numbers(5), 0)\nprint(gen_reduced)"
  },
  {
    "objectID": "nb/lab3/lab3.html#exercises",
    "href": "nb/lab3/lab3.html#exercises",
    "title": "map-reduce cont’d",
    "section": "Exercises",
    "text": "Exercises\n\nImplement versions of map, filter, and compose using reduce.\nUsing a csv file you’ve downloaded in lab1, create a generator that will yield first 3 columns of the file. And use this generator to create a Pandas.DataFrame. Hint: use https://docs.python.org/3/library/csv.html\n\nAdditional reading: https://docs.python.org/3/howto/functional.html"
  },
  {
    "objectID": "nb/lab5/02_array.html#create-datasets",
    "href": "nb/lab5/02_array.html#create-datasets",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Create datasets",
    "text": "Create datasets\nCreate the datasets you will be using in this notebook:\n\n%run prep.py -d random"
  },
  {
    "objectID": "nb/lab5/02_array.html#start-the-client",
    "href": "nb/lab5/02_array.html#start-the-client",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Start the Client",
    "text": "Start the Client\n\nfrom dask.distributed import Client\n\nclient = Client(n_workers=4)\nclient"
  },
  {
    "objectID": "nb/lab5/02_array.html#blocked-algorithms-in-a-nutshell",
    "href": "nb/lab5/02_array.html#blocked-algorithms-in-a-nutshell",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Blocked Algorithms in a nutshell",
    "text": "Blocked Algorithms in a nutshell\nLet’s do side by side the sum of the elements of an array using a NumPy array and a Dask array.\n\nimport numpy as np\nimport dask.array as da\n\n\n# NumPy array\na_np = np.ones(10)\na_np\n\nWe know that we can use sum() to compute the sum of the elements of our array, but to show what a blocksized operation would look like, let’s do:\n\na_np_sum = a_np[:5].sum() + a_np[5:].sum()\na_np_sum\n\nNow notice that each sum in the computation above is completely independent so they could be done in parallel. To do this with Dask array, we need to define our “slices”, we do this by defining the amount of elements we want per block using the variable chunks.\n\na_da = da.ones(10, chunks=5)\na_da\n\nImportant!\nNote here that to get two blocks, we specify chunks=5, in other words, we have 5 elements per block.\n\na_da_sum = a_da.sum()\na_da_sum"
  },
  {
    "objectID": "nb/lab5/02_array.html#task-graphs",
    "href": "nb/lab5/02_array.html#task-graphs",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Task Graphs",
    "text": "Task Graphs\nIn general, the code that humans write rely on compilers or interpreters so the computers can understand what we wrote. When we move to parallel execution there is a desire to shift responsibility from the compilers to the human, as they often bring the analysis, optimization, and execution of code into the code itself. In these cases, we often represent the structure of our program explicitly as data within the program itself.\nIn Dask we use task scheduling, where we break our program into into many medium-sized tasks or units of computation.We represent these tasks as nodes in a graph with edges between nodes if one task depends on data produced by another. We call upon a task scheduler to execute this graph in a way that respects these data dependencies and leverages parallelism where possible, so multiple independent tasks can be run simultaneously.\n\n# visualize the low level Dask graph using cytoscape\na_da_sum.visualize(engine=\"cytoscape\")\n\n\na_da_sum.compute()"
  },
  {
    "objectID": "nb/lab5/02_array.html#performance-comparison",
    "href": "nb/lab5/02_array.html#performance-comparison",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Performance comparison",
    "text": "Performance comparison\nLet’s try a more interesting example. We will create a 20_000 x 20_000 array with normally distributed values, and take the mean along one of its axis.\nNote:\nIf you are running on Binder, the Numpy example might need to be a smaller one due to memory issues.\n\nNumpy version\n\n%%time\nxn = np.random.normal(10, 0.1, size=(30_000, 30_000))\nyn = xn.mean(axis=0)\nyn\n\n\n\nDask array version\n\nxd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\nxd\n\n\nxd.nbytes / 1e9  # Gigabytes of the input processed lazily\n\n\nyd = xd.mean(axis=0)\nyd\n\n\n%%time\nxd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\nyd = xd.mean(axis=0)\nyd.compute()\n\nQuestions to think about:\n\nWhat happens if the Dask chunks=(10000,10000)?\nWhat happens if the Dask chunks=(30,30)?\n\nExercise:\nFor Dask arrays, compute the mean along axis=1 of the sum of the x array and its transpose.\n\n# Your code here"
  },
  {
    "objectID": "nb/lab5/02_array.html#choosing-good-chunk-sizes",
    "href": "nb/lab5/02_array.html#choosing-good-chunk-sizes",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Choosing good chunk sizes",
    "text": "Choosing good chunk sizes\nThis section was inspired on a Dask blog by Genevieve Buckley you can read it here\nA common problem when getting started with Dask array is determine what is a good chunk size. But what is a good size, and how do we determine this?\n\nGet to know the chunks\nWe can think of Dask arrays as a big structure composed by chunks of a smaller size, where these chunks are typically an a single numpy array, and they are all arranged to form a larger Dask array.\nIf you have a Dask array and want to know more information about chunks and their size, you can use the chunksize and chunks attributes to access this information. If you are in a jupyter notebook you can also visualize the Dask array via its HTML representation.\n\ndarr = da.random.random((1000, 1000, 1000))\ndarr\n\nNotice that when we created the Dask array, we did not specify the chunks. Dask has set by default chunks='auto' which accommodates ideal chunk sizes. To learn more on how auto-chunking works you can go to this documentation https://docs.dask.org/en/stable/array-chunks.html#automatic-chunking\ndarr.chunksize shows the largest chunk size. If you expect your array to have uniform chunk sizes this is a a good summary of the chunk size information. But if your array have irregular chunks, darr.chunks will show you the explicit sizes of all the chunks along all the dimensions of your dask array.\n\ndarr.chunksize\n\n\ndarr.chunks\n\nLet’s modify our example to see explore chunking a bit more. We can rechunk our array:\n\ndarr = darr.rechunk({0: -1, 1: 100, 2: \"auto\"})\n\n\ndarr\n\n\ndarr.chunksize\n\n\ndarr.chunks\n\nExercise:\n\nWhat does -1 do when specified as the chunk on a certain axis?\n\n\n\nToo small is a problem\nIf your chunks are too small, the amount of actual work done by every task is very tiny, and the overhead of coordinating all these tasks results in a very inefficient process.\nIn general, the dask scheduler takes approximately one millisecond to coordinate a single task. That means we want the computation time to be comparatively large, i.e in the order of seconds.\nIntuitive analogy by Genevieve Buckley:\n\nLets imagine we are building a house. It is a pretty big job, and if there were only one worker it would take much too long to build. So we have a team of workers and a site foreman. The site foreman is equivalent to the Dask scheduler: their job is to tell the workers what tasks they need to do.\nSay we have a big pile of bricks to build a wall, sitting in the corner of the building site. If the foreman (the Dask scheduler) tells workers to go and fetch a single brick at a time, then bring each one to where the wall is being built, you can see how this is going to be very slow and inefficient! The workers are spending most of their time moving between the wall and the pile of bricks. Much less time is going towards doing the actual work of mortaring bricks onto the wall.\nInstead, we can do this in a smarter way. The foreman (Dask scheduler) can tell the workers to go and bring one full wheelbarrow load of bricks back each time. Now workers are spending much less time moving between the wall and the pile of bricks, and the wall will be finished much quicker.\n\n\n\nToo big is a problem\nIf your chunks are too big, this is also a problem because you will likely run out of memory. You will start seeing in the dashboard that data is being spill to disk and this will lead to performance decrements.\nIf we load to much data into memory, Dask workers will start to spill data to disk to avoid crashing. Spilling data to disk will slow things down significantly, because of all the extra read and write operations to disk. This is definitely a situation that we want to avoid, to watch out for this you can look at the worker memory plot on the dashboard. Orange bars are a warning you are close to the limit, and gray means data is being spilled to disk.\nTo watch out for this, look at the worker memory plot on the Dask dashboard. Orange bars are a warning you are close to the limit, and gray means data is being spilled to disk - not good! For more tips, see the section on using the Dask dashboard below. To learn more about the memory plot, check the dashboard documentation.\n\n\nRules of thumb\n\nUsers have reported that chunk sizes smaller than 1MB tend to be bad. In general, a chunk size between 100MB and 1GB is good, while going over 1 or 2GB means you have a really big dataset and/or a lot of memory available per worker.\nUpper bound: Avoid very large task graphs. More than 10,000 or 100,000 chunks may start to perform poorly.\nLower bound: To get the advantage of parallelization, you need the number of chunks to at least equal the number of worker cores available (or better, the number of worker cores times 2). Otherwise, some workers will stay idle.\nThe time taken to compute each task should be much larger than the time needed to schedule the task. The Dask scheduler takes roughly 1 millisecond to coordinate a single task, so a good task computation time would be in the order of seconds (not milliseconds).\nChunks should be aligned with array storage on disk. Modern NDArray storage formats (HDF5, NetCDF, TIFF, Zarr) allow arrays to be stored in chunks so that the blocks of data can be pulled efficiently. However, data stores often chunk more finely than is ideal for Dask array, so it is common to choose a chunking that is a multiple of your storage chunk size, otherwise you might incur high overhead. For example, if you are loading data that is chunked in blocks of (100, 100), the you might might choose a chunking strategy more like (1000, 2000) that is larger but still divisible by (100, 100).\n\nFor more more advice on chunking see https://docs.dask.org/en/stable/array-chunks.html"
  },
  {
    "objectID": "nb/lab5/02_array.html#example-of-chunked-data-with-zarr",
    "href": "nb/lab5/02_array.html#example-of-chunked-data-with-zarr",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Example of chunked data with Zarr",
    "text": "Example of chunked data with Zarr\nZarr is a format for the storage of chunked, compressed, N-dimensional arrays. Zarr provides classes and functions for working with N-dimensional arrays that behave like NumPy arrays (Dask array behave like Numpy arrays) but whose data is divided into chunks and each chunk is compressed. If you are already familiar with HDF5 then Zarr arrays provide similar functionality, but with some additional flexibility.\nFor extra material check the Zarr tutorial\nLet’s read an array from zarr:\n\nimport zarr\n\n\na = da.from_zarr(\"data/random.zarr\")\n\n\na\n\nNotice that the array is already chunked, and we didn’t specify anything when loading it. Now notice that the chunks have a nice chunk size, let’s compute the mean and see how long it takes to run\n\n%%time\na.mean().compute()\n\nLet’s load a separate example where the chunksize is much smaller, and see what happen\n\nb = da.from_zarr(\"data/random_sc.zarr\")\nb\n\n\n%%time\nb.mean().compute()\n\n\nExercise:\nProvide a chunksize when reading b that will improve the time of computation of the mean. Try multiple chunks values and see what happens.\n\n# Your code here\n\n\n%%time\nc.mean().compute()"
  },
  {
    "objectID": "nb/lab5/02_array.html#xarray",
    "href": "nb/lab5/02_array.html#xarray",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Xarray",
    "text": "Xarray\nIn some applications we have multidimensional data, and sometimes working with all this dimensions can be confusing. Xarray is an open source project and Python package that makes working with labeled multi-dimensional arrays easier.\nXarray is inspired by and borrows heavily from pandas, the popular data analysis package focused on labeled tabular data. It is particularly tailored to working with netCDF files, which were the source of xarray’s data model, and integrates tightly with Dask for parallel computing.\nXarray introduces labels in the form of dimensions, coordinates and attributes on top of raw NumPy-like arrays, which allows for a more intuitive, more concise, and less error-prone developer experience.\nLet’s learn how to use xarray and Dask together:\n\nimport xarray as xr\n\n\nds = xr.tutorial.open_dataset(\n    \"air_temperature\",\n    chunks={  # this tells xarray to open the dataset as a dask array\n        \"lat\": 25,\n        \"lon\": 25,\n        \"time\": -1,\n    },\n)\nds\n\n\nds.air\n\n\nds.air.chunks\n\n\nmean = ds.air.mean(\"time\")  # no activity on dashboard\nmean  # contains a dask array\n\n\n# we will see dashboard activity\nmean.load()\n\n\nStandard Xarray Operations\nLet’s grab the air variable and do some operations. Operations using xarray objects are identical, regardless if the underlying data is stored as a Dask array or a NumPy array.\n\ndair = ds.air\n\n\ndair2 = dair.groupby(\"time.month\").mean(\"time\")\ndair_new = dair - dair2\ndair_new\n\nCall .compute() or .load() when you want your result as a xarray.DataArray with data stored as NumPy arrays.\n\n# things happen in the dashboard\ndair_new.load()\n\n\n\nTime Series Operations with xarray\nBecause we have a datetime index time-series operations work efficiently, for example we can do a resample and then plot the result.\n\ndair_resample = dair.resample(time=\"1w\").mean(\"time\").std(\"time\")\n\nFor this one to work, we need to install matplotlib first (if not installed yet):\n\n!pip install matplotlib\n\n\ndair_resample.load().plot(figsize=(12, 8))\n\n\n\nLearn More\nBoth xarray and zarr have their own tutorials that go into greater depth:\n\nZarr tutorial\nXarray tutorial"
  },
  {
    "objectID": "nb/lab5/02_array.html#close-your-cluster",
    "href": "nb/lab5/02_array.html#close-your-cluster",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Close your cluster",
    "text": "Close your cluster\nIt’s good practice to close any Dask cluster you create:\n\nclient.shutdown()"
  },
  {
    "objectID": "nb/lab10/lab10_scratchpad.html",
    "href": "nb/lab10/lab10_scratchpad.html",
    "title": "Postgres for analytics",
    "section": "",
    "text": "Fetch the install from: https://www.postgresql.org/download/windows/\nFor python: pip install \"psycopg[binary,pool]\""
  },
  {
    "objectID": "nb/lab10/lab10_scratchpad.html#installation",
    "href": "nb/lab10/lab10_scratchpad.html#installation",
    "title": "Postgres for analytics",
    "section": "",
    "text": "Fetch the install from: https://www.postgresql.org/download/windows/\nFor python: pip install \"psycopg[binary,pool]\""
  },
  {
    "objectID": "nb/lab10/lab10_scratchpad.html#dataset",
    "href": "nb/lab10/lab10_scratchpad.html#dataset",
    "title": "Postgres for analytics",
    "section": "Dataset",
    "text": "Dataset\nWe’ll use a shrunken version of this dataset: https://ual.sg/project/global-streetscapes/\nhead -n1 gadm.csv &gt; gadm_sorted.csv\ntail -n+2 gadm.csv | sort -k1 -t, &gt;&gt; gadm_sorted.csv\nhead -n10001 gadm_sorted.csv &gt; ../../gadm.csv"
  },
  {
    "objectID": "nb/lab10/lab10_scratchpad.html#perf-tips",
    "href": "nb/lab10/lab10_scratchpad.html#perf-tips",
    "title": "Postgres for analytics",
    "section": "Perf tips",
    "text": "Perf tips\n\nMaterialized views\nIndexes"
  },
  {
    "objectID": "nb/lab10/lab10_scratchpad.html#usage",
    "href": "nb/lab10/lab10_scratchpad.html#usage",
    "title": "Postgres for analytics",
    "section": "Usage",
    "text": "Usage\nFirst, create a database in your cluster:\n create database lab10 owner &lt;your_username&gt;;\n\nimport psycopg\n\n\nconn = psycopg.connect(\"dbname=lab10 user=vitvly\")\n\n\ncur = conn.cursor()\n\n\nconn.rollback()\n\n\nconn.commit()\n\n\ncur.execute(\"\"\"\nCREATE TABLE metadata (\n        uuid VARCHAR NOT NULL,\n        source VARCHAR NOT NULL,\n        orig_id DECIMAL NOT NULL,\n        lat DECIMAL NOT NULL,\n        lon DECIMAL NOT NULL,\n        datetime_local VARCHAR NOT NULL,\n        year DECIMAL NOT NULL,\n        month DECIMAL NOT NULL,\n        day DECIMAL NOT NULL,\n        hour DECIMAL NOT NULL,\n        width DECIMAL NOT NULL,\n        height DECIMAL NOT NULL,\n        heading DECIMAL,\n        projection_type VARCHAR,\n        \"hFoV\" DECIMAL,\n        \"vFoV\" DECIMAL,\n        sequence_index DECIMAL NOT NULL,\n        sequence_id VARCHAR NOT NULL,\n        sequence_img_count DECIMAL NOT NULL\n);\n\"\"\")\n\n&lt;psycopg.Cursor [COMMAND_OK] [INTRANS] (user=vitvly database=lab10) at 0x11b1d0ad0&gt;\n\n\n\nwith open('data/metadata.csv', 'r') as f:\n    # Notice that we don't need the csv module.\n    with cur.copy(\"COPY metadata FROM STDIN WITH (FORMAT CSV, header)\") as copy:\n        for line in f:\n            copy.write(line)\n\n\nconn.commit()\n\n\ncur.execute(\"select count(*) from metadata\")\n\n&lt;psycopg.Cursor [TUPLES_OK] [INTRANS] (user=vitvly database=lab10) at 0x11b1d0ad0&gt;\n\n\n\ncur.fetchone()\n\n(10000,)\n\n\n\ncur.execute(\"\"\"\nCREATE TABLE instances (\n        uuid VARCHAR NOT NULL,\n        source VARCHAR NOT NULL,\n        orig_id DECIMAL NOT NULL,\n        \"Bird\" DECIMAL NOT NULL,\n        \"Ground-Animal\" DECIMAL NOT NULL,\n        \"Curb\" DECIMAL NOT NULL,\n        \"Fence\" DECIMAL NOT NULL,\n        \"Guard-Rail\" DECIMAL NOT NULL,\n        \"Barrier\" DECIMAL NOT NULL,\n        \"Wall\" DECIMAL NOT NULL,\n        \"Bike-Lane\" DECIMAL NOT NULL,\n        \"Crosswalk---Plain\" DECIMAL NOT NULL,\n        \"Curb-Cut\" DECIMAL NOT NULL,\n        \"Parking\" DECIMAL NOT NULL,\n        \"Pedestrian-Area\" DECIMAL NOT NULL,\n        \"Rail-Track\" DECIMAL NOT NULL,\n        \"Road\" DECIMAL NOT NULL,\n        \"Service-Lane\" DECIMAL NOT NULL,\n        \"Sidewalk\" DECIMAL NOT NULL,\n        \"Bridge\" DECIMAL NOT NULL,\n        \"Building\" DECIMAL NOT NULL,\n        \"Tunnel\" DECIMAL NOT NULL,\n        \"Person\" DECIMAL NOT NULL,\n        \"Bicyclist\" DECIMAL NOT NULL,\n        \"Motorcyclist\" DECIMAL NOT NULL,\n        \"Other-Rider\" DECIMAL NOT NULL,\n        \"Lane-Marking---Crosswalk\" DECIMAL NOT NULL,\n        \"Lane-Marking---General\" DECIMAL NOT NULL,\n        \"Mountain\" DECIMAL NOT NULL,\n        \"Sand\" DECIMAL NOT NULL,\n        \"Sky\" DECIMAL NOT NULL,\n        \"Snow\" DECIMAL NOT NULL,\n        \"Terrain\" DECIMAL NOT NULL,\n        \"Vegetation\" DECIMAL NOT NULL,\n        \"Water\" DECIMAL NOT NULL,\n        \"Banner\" DECIMAL NOT NULL,\n        \"Bench\" DECIMAL NOT NULL,\n        \"Bike-Rack\" DECIMAL NOT NULL,\n        \"Billboard\" DECIMAL NOT NULL,\n        \"Catch-Basin\" DECIMAL NOT NULL,\n        \"CCTV-Camera\" DECIMAL NOT NULL,\n        \"Fire-Hydrant\" DECIMAL NOT NULL,\n        \"Junction-Box\" DECIMAL NOT NULL,\n        \"Mailbox\" DECIMAL NOT NULL,\n        \"Manhole\" DECIMAL NOT NULL,\n        \"Phone-Booth\" DECIMAL NOT NULL,\n        \"Pothole\" DECIMAL NOT NULL,\n        \"Street-Light\" DECIMAL NOT NULL,\n        \"Pole\" DECIMAL NOT NULL,\n        \"Traffic-Sign-Frame\" DECIMAL NOT NULL,\n        \"Utility-Pole\" DECIMAL NOT NULL,\n        \"Traffic-Light\" DECIMAL NOT NULL,\n        \"Traffic-Sign-(Back)\" DECIMAL NOT NULL,\n        \"Traffic-Sign-(Front)\" DECIMAL NOT NULL,\n        \"Trash-Can\" DECIMAL NOT NULL,\n        \"Bicycle\" DECIMAL NOT NULL,\n        \"Boat\" DECIMAL NOT NULL,\n        \"Bus\" DECIMAL NOT NULL,\n        \"Car\" DECIMAL NOT NULL,\n        \"Caravan\" DECIMAL NOT NULL,\n        \"Motorcycle\" DECIMAL NOT NULL,\n        \"On-Rails\" DECIMAL NOT NULL,\n        \"Other-Vehicle\" DECIMAL NOT NULL,\n        \"Trailer\" DECIMAL NOT NULL,\n        \"Truck\" DECIMAL NOT NULL,\n        \"Wheeled-Slow\" DECIMAL NOT NULL,\n        \"Car-Mount\" DECIMAL NOT NULL,\n        \"Ego-Vehicle\" DECIMAL NOT NULL\n);\n\"\"\")\n\n&lt;psycopg.Cursor [COMMAND_OK] [INTRANS] (user=vitvly database=lab10) at 0x11b1d0ad0&gt;\n\n\n\nconn.commit()\n\n\nwith open('data/instances.csv', 'r') as f:\n    # Notice that we don't need the csv module.\n    with cur.copy(\"COPY instances FROM STDIN WITH (FORMAT CSV, header)\") as copy:\n        for line in f:\n            copy.write(line)\n\n\nconn.commit()\n\n\ncur.execute(\"\"\"\nCREATE TABLE perception (\n        uuid VARCHAR NOT NULL,\n        source VARCHAR NOT NULL,\n        orig_id DECIMAL NOT NULL,\n        \"Beautiful\" DECIMAL NOT NULL,\n        \"Boring\" DECIMAL NOT NULL,\n        \"Depressing\" DECIMAL NOT NULL,\n        \"Lively\" DECIMAL NOT NULL,\n        \"Safe\" DECIMAL NOT NULL,\n        \"Wealthy\" DECIMAL NOT NULL\n);\n\"\"\")\nconn.commit()\n\n\nwith open('data/perception.csv', 'r') as f:\n    # Notice that we don't need the csv module.\n    with cur.copy(\"COPY perception FROM STDIN WITH (FORMAT CSV, header)\") as copy:\n        for line in f:\n            copy.write(line)\n\n\nconn.commit()\n\n\ncur.execute(\"\"\"\nCREATE TABLE places365 (\n        uuid VARCHAR NOT NULL,\n        source VARCHAR NOT NULL,\n        orig_id DECIMAL NOT NULL,\n        place VARCHAR NOT NULL\n);\n\"\"\")\nconn.commit()\n\n\nwith open('data/places365.csv', 'r') as f:\n    # Notice that we don't need the csv module.\n    with cur.copy(\"COPY places365 FROM STDIN WITH (FORMAT CSV, header)\") as copy:\n        for line in f:\n            copy.write(line)\n\n\nconn.commit()\n\n\nfrom psycopg.rows import dict_row\ncur.row_factory = dict_row\n\n\ncur.execute(\"select * from places365 limit 10\").fetchall()\n\n[{'uuid': '00000043-0dac-4f33-8983-6ad42164f8da',\n  'source': 'Mapillary',\n  'orig_id': Decimal('510636610118964'),\n  'place': 'highway'},\n {'uuid': '000004bd-5497-403a-b4d9-c99e53a73452',\n  'source': 'Mapillary',\n  'orig_id': Decimal('284345186750935'),\n  'place': 'street'},\n {'uuid': '000004d3-480d-4e16-bc5a-cdcdb6ae23b9',\n  'source': 'Mapillary',\n  'orig_id': Decimal('484779149450294'),\n  'place': 'residential_neighborhood'},\n {'uuid': '000006ef-1c9e-411e-98d1-c10b5cfbe535',\n  'source': 'Mapillary',\n  'orig_id': Decimal('228525199034651'),\n  'place': 'residential_neighborhood'},\n {'uuid': '00000802-1691-436f-a5c4-7aaa78a10c7e',\n  'source': 'KartaView',\n  'orig_id': Decimal('1212390509'),\n  'place': 'train_station'},\n {'uuid': '00000abb-222f-4b73-95a9-c0ac2323c52b',\n  'source': 'Mapillary',\n  'orig_id': Decimal('513799200047052'),\n  'place': 'market'},\n {'uuid': '00000d9b-dd31-4652-9ce9-a90b4af45f5e',\n  'source': 'Mapillary',\n  'orig_id': Decimal('4662192483807719'),\n  'place': 'street'},\n {'uuid': '00000df7-94d3-497f-af1d-37ef4f6645db',\n  'source': 'KartaView',\n  'orig_id': Decimal('600250418'),\n  'place': 'residential_neighborhood'},\n {'uuid': '0000116b-c3b8-4bfc-957e-3569b2227bbc',\n  'source': 'Mapillary',\n  'orig_id': Decimal('251543493041513'),\n  'place': 'street'},\n {'uuid': '000011d0-6370-4bcc-8b12-1adda4927875',\n  'source': 'Mapillary',\n  'orig_id': Decimal('466469321100092'),\n  'place': 'highway'}]\n\n\n\nimport pandas as pd\n\n\ndf = pd.read_csv(\"./data/gadm.csv\")\n\n\ndf.head()\n\n\n\n\n\n\n\n\nuuid\nsource\norig_id\nGID_0\nCOUNTRY\nCC_1\nENGTYPE_1\nGID_1\nHASC_1\nISO_1\n...\nENGTYPE_4\nGID_4\nNAME_4\nTYPE_4\nVARNAME_4\nCC_5\nENGTYPE_5\nGID_5\nNAME_5\nTYPE_5\n\n\n\n\n0\n00000043-0dac-4f33-8983-6ad42164f8da\nMapillary\n510636610118964\nSWE\nSweden\nNaN\nCounty\nSWE.15_1\nSE.ST\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n000004bd-5497-403a-b4d9-c99e53a73452\nMapillary\n284345186750935\nJPN\nJapan\nNaN\nPrefecture\nJPN.8_1\nJP.FS\nJP-07\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n000004d3-480d-4e16-bc5a-cdcdb6ae23b9\nMapillary\n484779149450294\nFRA\nFrance\nNaN\nRegion\nFRA.4_1\nFR.CN\nFR-CVL\n...\nCanton\nFRA.4.6.2.6_1\nFleury-les-Aubrais\nCanton\nNaN\nNaN\nCommune\nFRA.4.6.2.6.2_1\nFleury-les-Aubrais\nChef-lieu canton\n\n\n3\n000006ef-1c9e-411e-98d1-c10b5cfbe535\nMapillary\n228525199034651\nDEU\nGermany\n14.0\nState\nDEU.14_1\nDE.SN\nDE-SN\n...\nTown\nDEU.14.3.1.1_1\nDresden\nStadt\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n00000802-1691-436f-a5c4-7aaa78a10c7e\nKartaView\n1212390509\nJPN\nJapan\nNaN\nPrefecture\nJPN.35_1\nJP.ST\nJP-11\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 41 columns\n\n\n\n\n!pip install sqlalchemy\n\n\nCollecting sqlalchemy\n\n  Downloading SQLAlchemy-2.0.35-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n\nRequirement already satisfied: typing-extensions&gt;=4.6.0 in /Users/vitvly/c/lnu/2024-2025.1/bigdata/venv_dask/lib/python3.12/site-packages (from sqlalchemy) (4.12.2)\n\nDownloading SQLAlchemy-2.0.35-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 6.5 MB/s eta 0:00:00a 0:00:01\n\nInstalling collected packages: sqlalchemy\n\nSuccessfully installed sqlalchemy-2.0.35\n\n\n\n\n\nfrom sqlalchemy import create_engine\nengine = create_engine('postgresql+psycopg://vitvly:postgres@localhost:5432/lab10')\n#df.to_sql('table_name', engine)\n\n\ndf.to_sql('gadm', engine)\n\n-1\n\n\n\nconn.rollback()\n\n\ncur.execute(\"select * from instances where uuid='000004bd-5497-403a-b4d9-c99e53a73452'\").fetchall()\n\n[('000004bd-5497-403a-b4d9-c99e53a73452',\n  'Mapillary',\n  Decimal('284345186750935.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('1.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('1.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('1.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('1.0'),\n  Decimal('0.0'),\n  Decimal('1.0'),\n  Decimal('0.0'),\n  Decimal('1.0'),\n  Decimal('0.0'),\n  Decimal('2.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('1.0'),\n  Decimal('1.0'),\n  Decimal('1.0'),\n  Decimal('0.0'),\n  Decimal('1.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('1.0'),\n  Decimal('0.0'),\n  Decimal('2.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('3.0'),\n  Decimal('1.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('1.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('2.0'),\n  Decimal('20.0'),\n  Decimal('0.0'),\n  Decimal('5.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('1.0'),\n  Decimal('0.0'),\n  Decimal('1.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('4.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('0.0'),\n  Decimal('0.0'))]"
  },
  {
    "objectID": "nb/lab16/Lab16.html",
    "href": "nb/lab16/Lab16.html",
    "title": "Lab16",
    "section": "",
    "text": "First, run the included docker-compose.yaml file. It comes with an included Jupyter Notebook, that is already pre-connected to MinIO/Iceberg and other services (standard address: localhost:8888).\nThen, proceed with the below.\nMore info (if needed) here: https://blog.min.io/a-developers-introduction-to-apache-iceberg-using-minio/"
  },
  {
    "objectID": "nb/lab16/Lab16.html#create-tables",
    "href": "nb/lab16/Lab16.html#create-tables",
    "title": "Lab16",
    "section": "Create tables",
    "text": "Create tables\nFirst, we create a database:\n\n%%sql\nCREATE DATABASE IF NOT EXISTS climate;\n\nThen a table:\n\n%%sql\nCREATE TABLE IF NOT EXISTS climate.weather (\n    datetime              timestamp,\n    temp                  double,\n    lat                   double,\n    long                  double,\n    cloud_coverage        string,\n    precip                double,\n    wind_speed            double\n)\nUSING iceberg\nPARTITIONED BY (days(datetime))\n\nUsageError: Cell magic `%%sql` not found."
  },
  {
    "objectID": "nb/lab16/Lab16.html#insert-data",
    "href": "nb/lab16/Lab16.html#insert-data",
    "title": "Lab16",
    "section": "Insert data",
    "text": "Insert data\n\nfrom datetime import datetime\n\nschema = spark.table(\"climate.weather\").schema\n\ndata = [\n    (datetime(2023,8,16), 76.2, 40.951908, -74.075272, \"Partially sunny\", 0.0, 3.5),\n    (datetime(2023,8,17), 82.5, 40.951908, -74.075272, \"Sunny\", 0.0, 1.2),\n    (datetime(2023,8,18), 70.9, 40.951908, -74.075272, \"Cloudy\", .5, 5.2)\n  ]\n\ndf = spark.createDataFrame(data, schema)\ndf.writeTo(\"climate.weather\").append()\n\nAlternatively, using SQL (via JupySQL):\n\n%%sql\ninsert into climate.weather(datetime, temp, lat, long, cloud_coverage, precip, wind_speed)\nvalues(date('2023/8/19'), 83.5, 40.951908, -74.075272, \"Cloudy\", .5, 6.0)\n\nUsageError: Cell magic `%%sql` not found."
  },
  {
    "objectID": "nb/lab16/Lab16.html#fetch-data",
    "href": "nb/lab16/Lab16.html#fetch-data",
    "title": "Lab16",
    "section": "Fetch data",
    "text": "Fetch data\n\nfrom pyiceberg.catalog import load_catalog\nfrom pyiceberg.expressions import GreaterThanOrEqual\n\ncatalog = load_catalog('default')\ntbl = catalog.load_table('climate.weather')\n\nsc = tbl.scan(row_filter=GreaterThanOrEqual(\"datetime\", \"2023-08-01T00:00:00.000000+00:00\"))\ndf = sc.to_arrow().to_pandas()\ndf\n\nAlternatively, using SQL (via JupySQL):\n%sql select * from climate.weather where precip == 0"
  },
  {
    "objectID": "nb/lab9/eog.html",
    "href": "nb/lab9/eog.html",
    "title": "EOG",
    "section": "",
    "text": "import rioxarray\n\n\nrxr_eog = rioxarray.open_rasterio(\"./data/eog/nontiled/SVDNB_npp_20240501-20240528_global_vcmcfg_v10_c202406122300.avg_rade9h.tif\")\n\n\nrxr_eog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 33601, x: 86401)&gt; Size: 12GB\n[2903160001 values with dtype=float32]\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 691kB -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\n  * y            (y) float64 269kB 75.0 75.0 74.99 74.99 ... -64.99 -65.0 -65.0\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 33601x: 86401...[2903160001 values with dtype=float32]Coordinates: (4)band(band)int641array([1])x(x)float64-180.0 -180.0 ... 180.0 180.0array([-180.      , -179.995833, -179.991667, ...,  179.99167 ,  179.995836,\n        180.000003])y(y)float6475.0 75.0 74.99 ... -65.0 -65.0array([ 75.      ,  74.995833,  74.991667, ..., -64.991668, -64.995834,\n       -65.000001])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-180.00208333335 0.0041666667 0.0 75.00208333335 0.0 -0.0041666667array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([            -180.0,    -179.9958333333,    -179.9916666666,\n          -179.9874999999,    -179.9833333332,    -179.9791666665,\n          -179.9749999998,    -179.9708333331,    -179.9666666664,\n          -179.9624999997,\n       ...\n       179.96250287970003,     179.9666695464, 179.97083621310003,\n           179.9750028798, 179.97916954650003,     179.9833362132,\n       179.98750287990003,     179.9916695466, 179.99583621329998,\n             180.00000288],\n      dtype='float64', name='x', length=86401))yPandasIndexPandasIndex(Index([              75.0,      74.9958333333,      74.9916666666,\n            74.9874999999,      74.9833333332,      74.9791666665,\n            74.9749999998,      74.9708333331,      74.9666666664,\n            74.9624999997,\n       ...\n           -64.9625011197,     -64.9666677864,     -64.9708344531,\n           -64.9750011198,     -64.9791677865,     -64.9833344532,\n           -64.9875011199,     -64.9916677866,     -64.9958344533,\n       -65.00000112000001],\n      dtype='float64', name='y', length=33601))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\nrxr_eog2 = rxr_eog.coarsen(x=10, y=10, boundary='pad').max()\n\n\nrxr_eog2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 3361, x: 8641)&gt; Size: 116MB\narray([[[0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        ...,\n        [0.  , 0.  , 0.  , ..., 0.  , 0.82, 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ]]], dtype=float32)\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 69kB -180.0 -179.9 -179.9 ... 179.9 180.0 180.0\n  * y            (y) float64 27kB 74.98 74.94 74.9 74.86 ... -64.94 -64.98 -65.0\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 3361x: 86410.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0array([[[0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        ...,\n        [0.  , 0.  , 0.  , ..., 0.  , 0.82, 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ]]], dtype=float32)Coordinates: (4)band(band)int641array([1])x(x)float64-180.0 -179.9 ... 180.0 180.0array([-179.98125 , -179.939583, -179.897917, ...,  179.93542 ,  179.977086,\n        180.000003])y(y)float6474.98 74.94 74.9 ... -64.98 -65.0array([ 74.98125 ,  74.939583,  74.897917, ..., -64.935418, -64.977084,\n       -65.000001])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-180.00208333335 0.0041666667 0.0 75.00208333335 0.0 -0.0041666667array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([-179.98124999984998,    -179.93958333285,    -179.89791666585,\n          -179.85624999885,    -179.81458333185, -179.77291666485002,\n          -179.73124999785, -179.68958333084998, -179.64791666385003,\n          -179.60624999685,\n       ...\n           179.64375287715,     179.68541954415,     179.72708621115,\n        179.76875287815002,  179.81041954514998,     179.85208621215,\n        179.89375287915001,  179.93541954614997,  179.97708621315002,\n              180.00000288],\n      dtype='float64', name='x', length=8641))yPandasIndexPandasIndex(Index([ 74.98124999985001,  74.93958333284999,     74.89791666585,\n        74.85624999884999,     74.81458333185,  74.77291666484999,\n           74.73124999785,  74.68958333085001,     74.64791666385,\n        74.60624999685001,\n       ...\n          -64.64375111715,    -64.68541778415, -64.72708445114999,\n          -64.76875111815, -64.81041778515001, -64.85208445215001,\n          -64.89375111915,    -64.93541778615,    -64.97708445315,\n       -65.00000112000001],\n      dtype='float64', name='y', length=3361))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\n{\"mean\": rxr_eog2.mean(), \"max\": rxr_eog2.max(), \"min\": rxr_eog2.min(), \"std\": rxr_eog2.std()}\n\n{'mean': &lt;xarray.DataArray ()&gt; Size: 4B\n np.float32(0.92672706)\n Coordinates:\n     spatial_ref  int64 8B 0,\n 'max': &lt;xarray.DataArray ()&gt; Size: 4B\n np.float32(103145.26)\n Coordinates:\n     spatial_ref  int64 8B 0,\n 'min': &lt;xarray.DataArray ()&gt; Size: 4B\n np.float32(0.0)\n Coordinates:\n     spatial_ref  int64 8B 0,\n 'std': &lt;xarray.DataArray ()&gt; Size: 4B\n np.float32(39.215134)\n Coordinates:\n     spatial_ref  int64 8B 0}\n\n\n\nrxr_eog2.plot()\n\n\n\n\n\n\n\n\n\nrxr_eog2.data\n\narray([[[0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        ...,\n        [0.  , 0.  , 0.  , ..., 0.  , 0.82, 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ]]], dtype=float32)\n\n\n\nrxr_eog2.values\n\narray([[[0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        ...,\n        [0.  , 0.  , 0.  , ..., 0.  , 0.82, 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ]]], dtype=float32)\n\n\n\nrxr_eog2.rio.crs\n\nCRS.from_epsg(4326)\n\n\n\nrxr_eog2.plot(robust=True)\n\n\n\n\n\n\n\n\n\nNovember EOG data\n\nrxr_eog_nov = rioxarray.open_rasterio(\"./data/eog/nontiled/SVDNB_npp_20231101-20231130_global_vcmcfg_v10_c202312080900.avg_rade9h.tif\")\n\n\nrxr_eog_nov\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 33601, x: 86401)&gt; Size: 12GB\n[2903160001 values with dtype=float32]\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 691kB -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\n  * y            (y) float64 269kB 75.0 75.0 74.99 74.99 ... -64.99 -65.0 -65.0\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 33601x: 86401...[2903160001 values with dtype=float32]Coordinates: (4)band(band)int641array([1])x(x)float64-180.0 -180.0 ... 180.0 180.0array([-180.      , -179.995833, -179.991667, ...,  179.99167 ,  179.995836,\n        180.000003])y(y)float6475.0 75.0 74.99 ... -65.0 -65.0array([ 75.      ,  74.995833,  74.991667, ..., -64.991668, -64.995834,\n       -65.000001])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-180.00208333335 0.0041666667 0.0 75.00208333335 0.0 -0.0041666667array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([            -180.0,    -179.9958333333,    -179.9916666666,\n          -179.9874999999,    -179.9833333332,    -179.9791666665,\n          -179.9749999998,    -179.9708333331,    -179.9666666664,\n          -179.9624999997,\n       ...\n       179.96250287970003,     179.9666695464, 179.97083621310003,\n           179.9750028798, 179.97916954650003,     179.9833362132,\n       179.98750287990003,     179.9916695466, 179.99583621329998,\n             180.00000288],\n      dtype='float64', name='x', length=86401))yPandasIndexPandasIndex(Index([              75.0,      74.9958333333,      74.9916666666,\n            74.9874999999,      74.9833333332,      74.9791666665,\n            74.9749999998,      74.9708333331,      74.9666666664,\n            74.9624999997,\n       ...\n           -64.9625011197,     -64.9666677864,     -64.9708344531,\n           -64.9750011198,     -64.9791677865,     -64.9833344532,\n           -64.9875011199,     -64.9916677866,     -64.9958344533,\n       -65.00000112000001],\n      dtype='float64', name='y', length=33601))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\nrxr_eog_nov2 = rxr_eog_nov.coarsen(x=20, y=20, boundary='pad').max()\n\n\nrxr_eog_nov2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 1681, x: 4321)&gt; Size: 29MB\narray([[[1.67, 1.97, 2.04, ..., 1.34, 1.7 , 1.51],\n        [2.34, 2.14, 2.26, ..., 2.38, 2.4 , 2.22],\n        [3.18, 2.34, 3.32, ..., 2.07, 2.36, 1.72],\n        ...,\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ]]], dtype=float32)\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 35kB -180.0 -179.9 -179.8 ... 179.9 180.0 180.0\n  * y            (y) float64 13kB 74.96 74.88 74.79 ... -64.87 -64.96 -65.0\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 1681x: 43211.67 1.97 2.04 1.91 2.35 3.36 2.02 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0array([[[1.67, 1.97, 2.04, ..., 1.34, 1.7 , 1.51],\n        [2.34, 2.14, 2.26, ..., 2.38, 2.4 , 2.22],\n        [3.18, 2.34, 3.32, ..., 2.07, 2.36, 1.72],\n        ...,\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ]]], dtype=float32)Coordinates: (4)band(band)int641array([1])x(x)float64-180.0 -179.9 ... 180.0 180.0array([-179.960417, -179.877083, -179.79375 , ...,  179.87292 ,  179.956253,\n        180.000003])y(y)float6474.96 74.88 74.79 ... -64.96 -65.0array([ 74.960417,  74.877083,  74.79375 , ..., -64.872918, -64.956251,\n       -65.000001])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-180.00208333335 0.0041666667 0.0 75.00208333335 0.0 -0.0041666667array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([   -179.96041666635,    -179.87708333235,    -179.79374999835,\n       -179.71041666435002,    -179.62708333035,    -179.54374999635,\n       -179.46041666235004, -179.37708332834998,    -179.29374999435,\n          -179.21041666035,\n       ...\n        179.28958620765002,  179.37291954165002,  179.45625287565002,\n        179.53958620965003,     179.62291954365,     179.70625287765,\n           179.78958621165,     179.87291954565,  179.95625287964998,\n              180.00000288],\n      dtype='float64', name='x', length=4321))yPandasIndexPandasIndex(Index([ 74.96041666634999,     74.87708333235,  74.79374999835001,\n        74.71041666434999,  74.62708333035002,     74.54374999635,\n           74.46041666235,  74.37708332835001,     74.29374999435,\n        74.21041666034999,\n       ...\n       -64.28958444765001, -64.37291778165002,    -64.45625111565,\n       -64.53958444965001,    -64.62291778365, -64.70625111764998,\n       -64.78958445165001,    -64.87291778565, -64.95625111964999,\n       -65.00000112000001],\n      dtype='float64', name='y', length=1681))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\n{\"mean\": rxr_eog_nov2.mean(), \"max\": rxr_eog_nov2.max(), \"min\": rxr_eog_nov2.min(), \"std\": rxr_eog_nov2.std()}\n\n{'mean': &lt;xarray.DataArray ()&gt; Size: 4B\n np.float32(1.3308401)\n Coordinates:\n     spatial_ref  int64 8B 0,\n 'max': &lt;xarray.DataArray ()&gt; Size: 4B\n np.float32(69023.53)\n Coordinates:\n     spatial_ref  int64 8B 0,\n 'min': &lt;xarray.DataArray ()&gt; Size: 4B\n np.float32(0.0)\n Coordinates:\n     spatial_ref  int64 8B 0,\n 'std': &lt;xarray.DataArray ()&gt; Size: 4B\n np.float32(53.162537)\n Coordinates:\n     spatial_ref  int64 8B 0}\n\n\n\nrxr_eog_nov2.data\n\narray([[[1.67, 1.97, 2.04, ..., 1.34, 1.7 , 1.51],\n        [2.34, 2.14, 2.26, ..., 2.38, 2.4 , 2.22],\n        [3.18, 2.34, 3.32, ..., 2.07, 2.36, 1.72],\n        ...,\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n        [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ]]], dtype=float32)\n\n\n\nrxr_eog_nov2.rio.crs\n\nCRS.from_epsg(4326)\n\n\n\nrxr_eog_nov2.plot(robust=True)\n\n\n\n\n\n\n\n\n\n\nShapes (GADM)\n\nimport os\nimport geopandas as gpd\n\ngadm_files = [os.path.join(root, file)\n              for root, dirs, files in os.walk(\"./data/gadm/\")\n              for file in files if file.startswith(\"gadm\")]\n\nprint(gadm_files)\n\n# Select regional level 2 files\ngadm_files_level2 = [file for file in gadm_files if \"2.shp\" in file]\n#print(gadm_files_level2)\n\n# Load the shapefiles\nshps = [gpd.read_file(shp) for shp in gadm_files_level2]\n#print(shps)\n\n['./data/gadm/gadm41_UKR_2.dbf', './data/gadm/gadm41_UKR_1.dbf', './data/gadm/gadm41_UKR_0.dbf', './data/gadm/gadm41_UKR_0.cpg', './data/gadm/gadm41_UKR_0.shp', './data/gadm/gadm41_UKR_1.shp', './data/gadm/gadm41_UKR_2.shx', './data/gadm/gadm41_UKR_1.cpg', './data/gadm/gadm41_UKR_0.shx', './data/gadm/gadm41_UKR_2.shp', './data/gadm/gadm41_UKR_1.shx', './data/gadm/gadm41_UKR_2.cpg', './data/gadm/gadm41_UKR_0.prj', './data/gadm/gadm41_UKR_1.prj', './data/gadm/gadm41_UKR_2.prj']\n\n\n\nimport pandas as pd\n\nukr_shp = gpd.GeoDataFrame(pd.concat(shps, ignore_index=True))\n\n\nukr_shp['NL_NAME_1'].unique()\n\narray(['?', 'Черкаська', 'Чернігівська', 'Чернівецька', 'Крим',\n       'Дніпропетро́вська', 'Доне́цька', 'Івано-Франківська',\n       'Харківська', 'Херсонська', 'Хмельницька', 'Київська', 'Київ',\n       'Кіровоградська', 'Львівська', 'Львівщина', 'Луганська',\n       'Миколаївська', 'Одеська', 'Полтавська', 'Рівненська',\n       'Севастополь', 'Сумська', 'Тернопільська', 'Вінницька',\n       'Волинська', 'Закарпатська', 'Запорізька', 'Житомирська'],\n      dtype=object)\n\n\n\nlen(ukr_shp)\n\n629\n\n\n\nfiltered = ukr_shp[ukr_shp['NL_NAME_1'] == 'Черкаська']\n\n\nlen(filtered)\n\n23\n\n\n\nfiltered['geometry'].values\n\n&lt;GeometryArray&gt;\n[&lt;POLYGON ((32.172 49.439, 32.169 49.417, 32.152 49.382, 32.125 49.382, 32.10...&gt;,\n &lt;POLYGON ((32.039 49.499, 32.026 49.48, 32.026 49.464, 32.004 49.458, 32.009...&gt;,\n &lt;POLYGON ((32.172 49.439, 32.171 49.439, 32.18 49.445, 32.189 49.462, 32.189...&gt;,\n &lt;POLYGON ((32.261 49.209, 32.271 49.209, 32.28 49.209, 32.289 49.209, 32.307...&gt;,\n &lt;POLYGON ((32.419 49.837, 32.409 49.831, 32.4 49.831, 32.382 49.82, 32.391 4...&gt;,\n &lt;POLYGON ((31.56 49.431, 31.57 49.425, 31.579 49.425, 31.588 49.419, 31.624 ...&gt;,\n &lt;POLYGON ((32.198 49.209, 32.225 49.203, 32.243 49.203, 32.261 49.209, 32.26...&gt;,\n &lt;POLYGON ((31.446 49.749, 31.446 49.743, 31.464 49.743, 31.473 49.743, 31.50...&gt;,\n &lt;POLYGON ((31.649 49.654, 31.631 49.648, 31.621 49.642, 31.612 49.642, 31.60...&gt;,\n &lt;POLYGON ((31.197 48.957, 31.197 48.951, 31.197 48.945, 31.197 48.94, 31.215...&gt;,\n &lt;POLYGON ((30.097 48.975, 30.106 48.969, 30.106 48.963, 30.116 48.952, 30.12...&gt;,\n &lt;POLYGON ((31.477 49.537, 31.468 49.526, 31.468 49.52, 31.477 49.514, 31.496...&gt;,\n &lt;POLYGON ((30.926 49.352, 30.926 49.346, 30.935 49.34, 30.944 49.34, 30.953 ...&gt;,\n &lt;POLYGON ((30.099 48.922, 30.098 48.928, 30.116 48.934, 30.116 48.94, 30.125...&gt;,\n &lt;POLYGON ((30.097 48.975, 30.061 48.969, 30.052 48.969, 30.025 48.968, 30.01...&gt;,\n &lt;POLYGON ((31.302 49.109, 31.311 49.115, 31.329 49.127, 31.338 49.133, 31.34...&gt;,\n &lt;POLYGON ((31.671 49.325, 31.68 49.319, 31.698 49.313, 31.716 49.307, 31.726...&gt;,\n &lt;POLYGON ((30.828 48.953, 30.819 48.953, 30.81 48.947, 30.81 48.941, 30.81 4...&gt;,\n &lt;POLYGON ((30.211 48.8, 30.211 48.788, 30.22 48.788, 30.229 48.783, 30.238 4...&gt;,\n &lt;POLYGON ((30.011 48.64, 30.029 48.641, 30.038 48.641, 30.047 48.641, 30.055...&gt;,\n &lt;POLYGON ((30.436 49.311, 30.455 49.311, 30.464 49.311, 30.473 49.305, 30.48...&gt;,\n &lt;POLYGON ((32.171 49.439, 32.162 49.433, 32.143 49.433, 32.125 49.433, 32.11...&gt;,\n &lt;POLYGON ((31.137 49.251, 31.137 49.245, 31.137 49.239, 31.146 49.233, 31.15...&gt;]\nLength: 23, dtype: geometry\n\n\n\nfiltered\n\n\n\n\n\n\n\n\nGID_2\nGID_0\nCOUNTRY\nGID_1\nNAME_1\nNL_NAME_1\nNAME_2\nVARNAME_2\nNL_NAME_2\nTYPE_2\nENGTYPE_2\nCC_2\nHASC_2\ngeometry\n\n\n\n\n1\nUKR.1.1_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nCherkas'ka\nNA\nNA\nMis'ka Rada\nCity of Regional Significance\nNA\nUA.CK.CM\nPOLYGON ((32.1715 49.43881, 32.16858 49.41685,...\n\n\n2\nUKR.1.2_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nCherkas'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.CR\nPOLYGON ((32.0388 49.49877, 32.02555 49.48022,...\n\n\n3\nUKR.1.3_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nChornobaivs'kyi\nChornobayivskyi\nNA\nRaion\nDistrict\nNA\nUA.CK.CB\nPOLYGON ((32.1715 49.43881, 32.1708 49.43904, ...\n\n\n4\nUKR.1.4_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nChyhyryns'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.CY\nPOLYGON ((32.26144 49.20893, 32.2705 49.20885,...\n\n\n5\nUKR.1.5_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nDrabivs'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.DR\nPOLYGON ((32.41852 49.83724, 32.40931 49.83142...\n\n\n6\nUKR.1.6_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nHorodyshchens'kyi\nGorodyschenskyi\nNA\nRaion\nDistrict\nNA\nUA.CK.HO\nPOLYGON ((31.56041 49.43102, 31.56959 49.42509...\n\n\n7\nUKR.1.7_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nKamians'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.KN\nPOLYGON ((32.19797 49.20946, 32.22516 49.20335...\n\n\n8\nUKR.1.8_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nKanivs'ka\nNA\nNA\nMisto\nCity\nNA\nUA.CK.KM\nPOLYGON ((31.4458 49.74903, 31.4459 49.74315, ...\n\n\n9\nUKR.1.9_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nKanivs'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.KR\nPOLYGON ((31.64874 49.65386, 31.63052 49.64808...\n\n\n10\nUKR.1.10_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nKaterynopil's'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.KT\nPOLYGON ((31.19674 48.95712, 31.19684 48.95124...\n\n\n11\nUKR.1.11_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nKhrystynivs'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.KH\nPOLYGON ((30.09668 48.97494, 30.10588 48.96916...\n\n\n12\nUKR.1.12_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nKorsun'-Shevchenkivs'kyi\nKorsunShevchenkiv\nNA\nRaion\nDistrict\nNA\nUA.CK.KO\nPOLYGON ((31.47683 49.53727, 31.4679 49.52557,...\n\n\n13\nUKR.1.13_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nLysians'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.LY\nPOLYGON ((30.92556 49.35152, 30.92572 49.34564...\n\n\n14\nUKR.1.14_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nMan'kivs'kyi\nMankivskyi\nNA\nRaion\nDistrict\nNA\nUA.CK.MA\nPOLYGON ((30.09868 48.9222, 30.09846 48.92806,...\n\n\n15\nUKR.1.15_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nMonastyryshchens'kyi\nMonastyryshchens'\nNA\nRaion\nDistrict\nNA\nUA.CK.MO\nPOLYGON ((30.09668 48.97494, 30.06102 48.96875...\n\n\n16\nUKR.1.16_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nShpolians'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.SH\nPOLYGON ((31.30228 49.10928, 31.31121 49.11511...\n\n\n17\nUKR.1.17_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nSmilians'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.SR\nPOLYGON ((31.67086 49.32463, 31.68001 49.3187,...\n\n\n18\nUKR.1.18_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nTal'nivs'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.TA\nPOLYGON ((30.82753 48.9526, 30.81852 48.95263,...\n\n\n19\nUKR.1.19_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nUmans'ka\nNA\nNA\nMisto\nCity\nNA\nUA.CK.UM\nPOLYGON ((30.21086 48.80001, 30.21128 48.78829...\n\n\n20\nUKR.1.20_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nUmans'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.UR\nPOLYGON ((30.01093 48.64037, 30.02873 48.64053...\n\n\n21\nUKR.1.21_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nZhashkivs'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.ZH\nPOLYGON ((30.43644 49.31108, 30.45458 49.31109...\n\n\n22\nUKR.1.22_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nZolotonis'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.ZR\nPOLYGON ((32.1708 49.43904, 32.1617 49.43323, ...\n\n\n23\nUKR.1.23_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nZvenyhorods'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.ZV\nPOLYGON ((31.13671 49.25097, 31.13684 49.24509...\n\n\n\n\n\n\n\n\nukr_shp.head()\n\n\n\n\n\n\n\n\nGID_2\nGID_0\nCOUNTRY\nGID_1\nNAME_1\nNL_NAME_1\nNAME_2\nVARNAME_2\nNL_NAME_2\nTYPE_2\nENGTYPE_2\nCC_2\nHASC_2\ngeometry\n\n\n\n\n0\n?\nUKR\nUkraine\n?\n?\n?\n?\n?\nNA\n?\nNA\nNA\n?\nPOLYGON ((30.59167 50.41236, 30.60611 50.41604...\n\n\n1\nUKR.1.1_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nCherkas'ka\nNA\nNA\nMis'ka Rada\nCity of Regional Significance\nNA\nUA.CK.CM\nPOLYGON ((32.1715 49.43881, 32.16858 49.41685,...\n\n\n2\nUKR.1.2_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nCherkas'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.CR\nPOLYGON ((32.0388 49.49877, 32.02555 49.48022,...\n\n\n3\nUKR.1.3_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nChornobaivs'kyi\nChornobayivskyi\nNA\nRaion\nDistrict\nNA\nUA.CK.CB\nPOLYGON ((32.1715 49.43881, 32.1708 49.43904, ...\n\n\n4\nUKR.1.4_1\nUKR\nUkraine\nUKR.1_1\nCherkasy\nЧеркаська\nChyhyryns'kyi\nNA\nNA\nRaion\nDistrict\nNA\nUA.CK.CY\nPOLYGON ((32.26144 49.20893, 32.2705 49.20885,...\n\n\n\n\n\n\n\nPlot country map:\n\nimport matplotlib.pyplot as plt\n\n# Plot all shapefiles\nfig, ax = plt.subplots(figsize=(10, 10))\n\nukr_shp.plot(ax=ax, edgecolor='k', facecolor='none', linewidth=1)  # No fill color\n\n\n# Set plot title and labels\nax.set_title('Regional Level 2 Shapefiles')\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nukr_shp.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n\nPlot masked night lights\n\n#from rasterio.warp import reproject, Resampling, calculate_default_transform\nimport rasterio.mask as rio_mask\n#from rasterio.plot import show\n#from rasterio import features\n\n# For converting Shapely geometries to GeoJSON format\nimport shapely.geometry as shp_geom\n\n\nclipped = rxr_eog_nov2.rio.clip(ukr_shp.geometry.values, ukr_shp.crs, drop=True, invert=False)\n\n\nclipped\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 95, x: 217)&gt; Size: 82kB\narray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 2kB 22.21 22.29 22.37 22.46 ... 40.04 40.12 40.21\n  * y            (y) float64 760B 52.29 52.21 52.13 52.04 ... 44.63 44.54 44.46\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 95x: 217nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nanarray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)Coordinates: (4)band(band)int641array([1])x(x)float6422.21 22.29 22.37 ... 40.12 40.21axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([22.206252, 22.289585, 22.372918, ..., 40.039585, 40.122918, 40.206252])y(y)float6452.29 52.21 52.13 ... 44.54 44.46axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([52.29375 , 52.210416, 52.127083, 52.04375 , 51.960416, 51.877083,\n       51.79375 , 51.710416, 51.627083, 51.54375 , 51.460416, 51.377083,\n       51.29375 , 51.210416, 51.127083, 51.04375 , 50.960416, 50.877083,\n       50.79375 , 50.710416, 50.627083, 50.54375 , 50.460416, 50.377083,\n       50.29375 , 50.210416, 50.127083, 50.04375 , 49.960416, 49.877083,\n       49.79375 , 49.710416, 49.627083, 49.54375 , 49.460416, 49.377083,\n       49.29375 , 49.210416, 49.127083, 49.04375 , 48.960416, 48.877083,\n       48.79375 , 48.710416, 48.627083, 48.54375 , 48.460416, 48.377083,\n       48.29375 , 48.210416, 48.127083, 48.04375 , 47.960416, 47.877083,\n       47.79375 , 47.710416, 47.627083, 47.54375 , 47.460416, 47.377083,\n       47.29375 , 47.210416, 47.127083, 47.04375 , 46.960416, 46.877083,\n       46.79375 , 46.710416, 46.627083, 46.54375 , 46.460416, 46.377083,\n       46.29375 , 46.210416, 46.127083, 46.04375 , 45.960416, 45.877083,\n       45.79375 , 45.710416, 45.627083, 45.54375 , 45.460416, 45.377083,\n       45.29375 , 45.210416, 45.127083, 45.04375 , 44.960416, 44.877083,\n       44.79375 , 44.710416, 44.627083, 44.54375 , 44.460416])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :22.164584950650006 0.08333333399999997 0.0 52.33541648534999 0.0 -0.08333333399999986array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([22.206251617650008, 22.289584951650006,  22.37291828565001,\n       22.456251619650004, 22.539584953650007, 22.622918287650005,\n       22.706251621650004, 22.789584955650007,  22.87291828965001,\n           22.95625162365,\n       ...\n        39.45625175565001, 39.539585089650004, 39.622918423650006,\n        39.70625175765001,  39.78958509165001,  39.87291842565001,\n           39.95625175965, 40.039585093650004, 40.122918427650006,\n           40.20625176165],\n      dtype='float64', name='x', length=217))yPandasIndexPandasIndex(Index([ 52.29374981834999,  52.21041648434999,     52.12708315035,\n           52.04374981635,     51.96041648235,     51.87708314835,\n       51.793749814349994,  51.71041648034999,  51.62708314634999,\n        51.54374981235001,     51.46041647835,  51.37708314434999,\n        51.29374981034999,     51.21041647635,     51.12708314235,\n           51.04374980835,     50.96041647435, 50.877083140349995,\n        50.79374980634999,  50.71041647234999,     50.62708313835,\n           50.54374980435,     50.46041647035, 50.377083136349995,\n        50.29374980234999,     50.21041646835,     50.12708313435,\n           50.04374980035,     49.96041646635, 49.877083132349995,\n        49.79374979834999,     49.71041646435,     49.62708313035,\n           49.54374979635,     49.46041646235, 49.377083128349994,\n           49.29374979435, 49.210416460350004,     49.12708312635,\n           49.04374979235,     48.96041645835, 48.877083124349994,\n        48.79374979034999,     48.71041645635,     48.62708312235,\n           48.54374978835, 48.460416454349996,     48.37708312035,\n           48.29374978635,     48.21041645235,     48.12708311835,\n           48.04374978435, 47.960416450349996,  47.87708311634999,\n        47.79374978234999,     47.71041644835,     47.62708311435,\n           47.54374978035, 47.460416446349996,     47.37708311235,\n           47.29374977835,     47.21041644435,     47.12708311035,\n           47.04374977635, 46.960416442349995,  46.87708310834999,\n        46.79374977434999,     46.71041644035,     46.62708310635,\n           46.54374977235,     46.46041643835,  46.37708310434999,\n           46.29374977035,     46.21041643635,     46.12708310235,\n           46.04374976835, 45.960416434349995,  45.87708310034999,\n           45.79374976635,     45.71041643235,     45.62708309835,\n       45.543749764350004, 45.460416430349994,     45.37708309635,\n       45.293749762350004,     45.21041642835,     45.12708309435,\n       45.043749760349996, 44.960416426349994,  44.87708309234999,\n       44.793749758349996,     44.71041642435, 44.627083090350006,\n       44.543749756349996,     44.46041642235],\n      dtype='float64', name='y'))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\nclipped_orig = rxr_eog_nov.rio.clip(ukr_shp.geometry.values, ukr_shp.crs, drop=True, invert=False)\n\n\nclipped_orig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 1918, x: 4339)&gt; Size: 33MB\narray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 35kB 22.14 22.15 22.15 22.15 ... 40.21 40.21 40.22\n  * y            (y) float64 15kB 52.37 52.37 52.37 52.36 ... 44.4 44.39 44.39\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 1918x: 4339nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nanarray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)Coordinates: (4)band(band)int641array([1])x(x)float6422.14 22.15 22.15 ... 40.21 40.22axis :Xlong_name :longitudestandard_name :longitudeunits :degrees_eastarray([22.141668, 22.145835, 22.150002, ..., 40.208335, 40.212502, 40.216668])y(y)float6452.37 52.37 52.37 ... 44.39 44.39axis :Ylong_name :latitudestandard_name :latitudeunits :degrees_northarray([52.375   , 52.370833, 52.366666, ..., 44.395833, 44.391666, 44.3875  ])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :22.139584950450015 0.004166666699999996 0.0 52.37708315235 0.0 -0.0041666667array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([22.141668283800016, 22.145834950500017,  22.15000161720002,\n       22.154168283899992, 22.158334950599993, 22.162501617299995,\n       22.166668283999996, 22.170834950699998,      22.1750016174,\n            22.1791682841,\n       ...\n        40.17916842810001, 40.183335094800015, 40.187501761500016,\n        40.19166842820002,  40.19583509490002,  40.20000176159999,\n       40.204168428299994, 40.208335094999995,      40.2125017617,\n            40.2166684284],\n      dtype='float64', name='x', length=4339))yPandasIndexPandasIndex(Index([52.374999818999996, 52.370833152299994,      52.3666664856,\n            52.3624998189, 52.358333152200004,      52.3541664855,\n            52.3499998188,      52.3458331521,      52.3416664854,\n       52.337499818699996,\n       ...\n       44.424999755399995,      44.4208330887,       44.416666422,\n       44.412499755300004,      44.4083330886,      44.4041664219,\n            44.3999997552,      44.3958330885,      44.3916664218,\n       44.387499755099995],\n      dtype='float64', name='y', length=1918))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\nclipped.plot(robust=True)\n\n\n\n\n\n\n\n\n\nclipped_orig.rio.to_raster(\"./data/clipped_ukr.tif\")\n\n\nclipped_orig2 = rioxarray.open_rasterio(\"./data/clipped_ukr.tif\")\n\n\nclipped_orig2.plot(robust=True)\n\n\n\n\n\n\n\n\n\nclipped.max()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray ()&gt; Size: 4B\nnp.float32(18291.93)\nCoordinates:\n    spatial_ref  int64 8B 0xarray.DataArray1.829e+04np.float32(18291.93)Coordinates: (1)spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :22.164584950650003 0.041666667 0.0 52.377083152350004 0.0 -0.04166666700000002array(0)Indexes: (0)Attributes: (0)\n\n\n\nclipped.plot()\n\n\n\n\n\n\n\n\n\nukr_union = ukr_shp.geometry.union_all()\n\n\n\nAlternative clipping using mask\n\nimport rasterio\nrio_ukr = rasterio.open(\"./data/clipped_ukr.tif\")\n\n\neog_ukr, eog_ukr_transform = rio_mask.mask(rio_ukr, [shp_geom.mapping(ukr_union)], crop=True)\n\n\nrio_ukr\n\n&lt;open DatasetReader name='./data/clipped_ukr.tif' mode='r'&gt;\n\n\n\nclipped.rio\n\n&lt;rioxarray.raster_array.RasterArray at 0x36f698a70&gt;"
  },
  {
    "objectID": "nb/lab9/lab9.html",
    "href": "nb/lab9/lab9.html",
    "title": "Analyzing night lights data",
    "section": "",
    "text": "Vector data: pick a country and download its shapefile data from https://gadm.org/download_country.html\nRaster data: download average radiance data for 2 different months from https://eogdata.mines.edu/products/vnl/#monthly\n\n“avg_rade9h” in the filename\npick non-tiled version\nuse stray-light-corrected data (vcmsl)"
  },
  {
    "objectID": "nb/lab9/lab9.html#data-preparation",
    "href": "nb/lab9/lab9.html#data-preparation",
    "title": "Analyzing night lights data",
    "section": "",
    "text": "Vector data: pick a country and download its shapefile data from https://gadm.org/download_country.html\nRaster data: download average radiance data for 2 different months from https://eogdata.mines.edu/products/vnl/#monthly\n\n“avg_rade9h” in the filename\npick non-tiled version\nuse stray-light-corrected data (vcmsl)"
  },
  {
    "objectID": "nb/lab9/lab9.html#data-analysis",
    "href": "nb/lab9/lab9.html#data-analysis",
    "title": "Analyzing night lights data",
    "section": "Data analysis",
    "text": "Data analysis\n\nOpen vector data with GeoDataFrame and plot it.\nOpen raster data with rioxarray and plot it. See if it works and doesn’t crash.\nTry reducing the size of raster data by factor of 10. How would you do it?\nClip/mask raster data so that it fits the vector geometry of your country’s vector data. Plot it.\nCompute average luminosity for each level 2 region.\nCreate a resulting GeoDataFrame with additional column showing the difference in average luminosity between start_month and end_month, and save it to a file. Ah, and don’t forget to plot it.\n\nNote: use Dask for optimizations: - https://corteva.github.io/rioxarray/stable/examples/dask_read_write.html - https://docs.xarray.dev/en/stable/user-guide/dask.html"
  },
  {
    "objectID": "nb/lab9/lab9.html#example-clipped-file",
    "href": "nb/lab9/lab9.html#example-clipped-file",
    "title": "Analyzing night lights data",
    "section": "Example clipped file",
    "text": "Example clipped file\n\nimport rioxarray\n\nrxr_ukr = rioxarray.open_rasterio(\"./data/clipped_ukr.tif\")\n\n\nrxr_ukr.plot(robust=True)"
  },
  {
    "objectID": "nb/lab6/05_futures.html#a-typical-workflow",
    "href": "nb/lab6/05_futures.html#a-typical-workflow",
    "title": "Futures - non-blocking distributed calculations",
    "section": "A Typical Workflow",
    "text": "A Typical Workflow\nThis is the same workflow that we saw in the delayed notebook. It is for-loopy and the data is not necessarily an array or a dataframe. The following example outlines a read-transform-write:\ndef process_file(filename):\n    data = read_a_file(filename)\n    data = do_a_transformation(data)\n    destination = f\"results/{filename}\"\n    write_out_data(data, destination)\n    return destination\n\nfutures = []\nfor filename in filenames:\n    future = client.submit(process_file, filename)\n    futures.append(future)\n    \nfutures"
  },
  {
    "objectID": "nb/lab6/05_futures.html#basics",
    "href": "nb/lab6/05_futures.html#basics",
    "title": "Futures - non-blocking distributed calculations",
    "section": "Basics",
    "text": "Basics\nJust like we did in the delayed notebook, let’s make some toy functions, inc and add, that sleep for a while to simulate work. We’ll then time running these functions normally.\n\nfrom time import sleep\nimport random\n\nrandom.seed()\n\n\ndef inc(x):\n    sleep(random.randrange(2,4))\n    return x + 1\n\n\ndef double(x):\n    sleep(random.randrange(2,4))\n    return 2 * x\n\n\ndef add(x, y):\n    sleep(random.randrange(2,4))\n    return x + y\n\nWe can run these locally\n\ninc(1)\n\nOr we can submit them to run remotely with Dask. This immediately returns a future that points to the ongoing computation, and eventually to the stored result.\n\nfuture = client.submit(inc, 1)  # returns immediately with pending future\nfuture\n\nIf you wait a second, and then check on the future again, you’ll see that it has finished.\n\nfuture\n\nYou can block on the computation and gather the result with the .result() method.\n\nfuture.result()\n\n\nOther ways to wait for a future\nfrom dask.distributed import wait, progress\nprogress(future)\nshows a progress bar in this notebook, rather than having to go to the dashboard. This progress bar is also asynchronous, and doesn’t block the execution of other code in the meanwhile.\nwait(future)\nblocks and forces the notebook to wait until the computation pointed to by future is done. However, note that if the result of inc() is sitting in the cluster, it would take no time to execute the computation now, because Dask notices that we are asking for the result of a computation it already knows about. More on this later.\n\n\nOther ways to gather results\nclient.gather(futures)\ngathers results from more than one future."
  },
  {
    "objectID": "nb/lab6/05_futures.html#client.compute",
    "href": "nb/lab6/05_futures.html#client.compute",
    "title": "Futures - non-blocking distributed calculations",
    "section": "client.compute",
    "text": "client.compute\nGenerally, any Dask operation that is executed using .compute() or dask.compute() can be submitted for asynchronous execution using client.compute() instead.\n\nimport dask\nimport random\n\n\n@dask.delayed\ndef inc(x):\n    sleep(random.randrange(2,4))\n    return x + 1\n\n\n@dask.delayed\ndef add(x, y):\n    sleep(random.randrange(2,4))\n    return x + y\n\n\nx = inc(1)\ny = inc(2)\nz = add(x, y)\n\nSo far we have a regular dask.delayed output. When we pass z to client.compute we get a future back and Dask starts evaluating the task graph.\n\n# notice the difference from z.compute()\n# notice that this cell completes immediately\nfuture = client.compute(z)\nfuture\n\n\nfuture.result()  # waits until result is ready\n\nWhen using futures, the computation moves to the data rather than the other way around, and the client, in the local Python session, need never see the intermediate values."
  },
  {
    "objectID": "nb/lab6/05_futures.html#client.submit",
    "href": "nb/lab6/05_futures.html#client.submit",
    "title": "Futures - non-blocking distributed calculations",
    "section": "client.submit",
    "text": "client.submit\nclient.submit takes a function and arguments, pushes these to the cluster, returning a Future representing the result to be computed. The function is passed to a worker process for evaluation. This looks a lot like doing client.compute(), above, except now we are passing the function and arguments directly to the cluster.\n\ndef inc(x):\n    sleep(random.randrange(2,4))\n    return x + 1\n\n\ndef double(x):\n    sleep(random.randrange(2,4))\n    return 2 * x\n\n\ndef add(x, y):\n    sleep(random.randrange(2,4))\n    return x + y\n\n\nfuture_x = client.submit(inc, 1)\nfuture_y = client.submit(inc, 2)\nfuture_z = client.submit(sum, [future_x, future_y])\nfuture_z\n\n\nfuture_z.result()  # waits until result is ready\n\nThe arguments toclient.submit can be regular Python functions and objects, Futures from other submit operations or dask.delayed objects. Thus we can create dependencies in a computation graph.\n\nHow does it work?\nEach future represents a result held, or being evaluated by the cluster. Thus we can control caching of intermediate values - when a future is no longer referenced, its value is forgotten. In the solution, above, futures are held for each of the function calls. These results would not need to be re-evaluated if we chose to submit more work that needed them.\nWe can explicitly pass data from our local session into the cluster using client.scatter(), but usually it is better to construct functions that do the loading of data within the workers themselves, so that there is no need to serialize and communicate the data. Most of the loading functions within Dask, such as dd.read_csv, work this way. Similarly, we normally don’t want to gather() results that are too big in memory."
  },
  {
    "objectID": "nb/lab6/05_futures.html#example-sporadically-failing-task",
    "href": "nb/lab6/05_futures.html#example-sporadically-failing-task",
    "title": "Futures - non-blocking distributed calculations",
    "section": "Example: Sporadically failing task",
    "text": "Example: Sporadically failing task\nLet’s imagine a task that sometimes fails. You might encounter this when dealing with input data where sometimes a file is malformed, or maybe a request times out.\n\nfrom random import randrange\n\ndef fetchRand():\n    random.seed()\n    randInt = randrange(0,2)\n    return randInt\n\ndef flaky_inc(i):\n    randInt = fetchRand()\n    print(\"randInt: %d\", randInt)\n    if randInt &lt; 1:\n        raise ValueError(\"You hit the error!\")\n    return i + 1\n\n\nrandrange(0,2)\n\nIf you run this function over and over again, it will sometimes fail.\n&gt;&gt;&gt; flaky_inc(2)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nInput In [65], in &lt;cell line: 1&gt;()\n----&gt; 1 flaky_inc(2)\n\nInput In [61], in flaky_inc(i)\n      3 def flaky_inc(i):\n      4     if random() &lt; 0.5:\n----&gt; 5         raise ValueError(\"You hit the error!\")\n      6     return i + 1\n\nValueError: You hit the error!\nWe can run this function on a range of inputs using client.map.\n\nfutures = client.map(flaky_inc, range(10))\n\nNotice how the cell returned even though some of the computations failed. We can inspect these futures one by one and find the ones that failed:\n\nfor i, future in enumerate(futures):\n    print(i, future.status)\n\nYou can rerun those specific futures to try to get the task to successfully complete:\n\nfutures[5].retry()\n\n\nfor i, future in enumerate(futures):\n    print(i, future.status)\n\nA more concise way of retrying in the case of sporadic failures is by setting the number of retries in the client.compute, client.submit or client.map method.\nNote: In this example we also need to set pure=False to let Dask know that the arguments to the function do not totally determine the output.\n\nfutures = client.map(flaky_inc, range(10), retries=5, pure=False)\nfuture_z = client.submit(sum, futures)\nfuture_z.result()\n\nYou will see a lot of warnings, but the computation should eventually succeed."
  },
  {
    "objectID": "nb/lab6/05_futures.html#example-many-tasks",
    "href": "nb/lab6/05_futures.html#example-many-tasks",
    "title": "Futures - non-blocking distributed calculations",
    "section": "Example: many tasks",
    "text": "Example: many tasks\n\nzs = []\n\nfor i in range(16):\n    x = client.submit(inc, i)     # x = inc(i)\n    y = client.submit(double, x)  # y = inc(x)\n    z = client.submit(add, x, y)  # z = inc(y)\n    zs.append(z)\n\ntotal = client.submit(sum, zs)\n\n\ntotal\n\n\ntotal.result()"
  },
  {
    "objectID": "nb/lab6/05_futures.html#example-tree-summation",
    "href": "nb/lab6/05_futures.html#example-tree-summation",
    "title": "Futures - non-blocking distributed calculations",
    "section": "Example: tree summation",
    "text": "Example: tree summation\nAs an example of a non-trivial algorithm, consider the classic tree reduction. We accomplish this with a nested for loop and a bit of normal Python logic.\nfinish           total             single output\n    ^          /        \\\n    |        c1          c2        neighbors merge\n    |       /  \\        /  \\\n    |     b1    b2    b3    b4     neighbors merge\n    ^    / \\   / \\   / \\   / \\\nstart   a1 a2 a3 a4 a5 a6 a7 a8    many inputs\n\nL = zs\nwhile len(L) &gt; 1:\n    new_L = []\n    for i in range(0, len(L), 2):\n        future = client.submit(add, L[i], L[i + 1])  # add neighbors\n        new_L.append(future)\n    L = new_L                                   # swap old list for new\n\nWe can explicitly wait until this work is done and gather the results to our local process by calling client.gather:\n\n# gather all futures\nallFutures = client.gather(L)\n\n\nallFutures[0]\n\nIf you’re watching the dashboard’s status page then you may want to note two things:\n\nThe red bars are for inter-worker communication. They happen as different workers need to combine their intermediate values\nThere is lots of parallelism at the beginning but less towards the end as we reach the top of the tree where there is less work to do.\n\nAlternatively you may want to navigate to the dashboard’s graph page and then run the cell above again. You will be able to see the task graph evolve during the computation."
  },
  {
    "objectID": "nb/lab6/05_futures.html#example-building-a-computation-dynamically",
    "href": "nb/lab6/05_futures.html#example-building-a-computation-dynamically",
    "title": "Futures - non-blocking distributed calculations",
    "section": "Example: Building a computation dynamically",
    "text": "Example: Building a computation dynamically\nIn the examples above we explicitly specify the task graph ahead of time. We know for example that the first two futures in the list L will be added together.\nSometimes this isn’t always best though, sometimes you want to dynamically define a computation as it is happening. For example we might want to sum up these values based on whichever futures show up first, rather than the order in which they were placed in the list to start with.\nFor this, we can use operations like as_completed. This returns an iterator that yields the input future objects in the order in which they complete.\nWe recommend watching the dashboard’s graph page when running this computation. You should see the graph construct itself during execution.\n\nfrom dask.distributed import as_completed\n\nzs = client.map(inc, zs)\nseq = as_completed(zs)\n\nwhile seq.count() &gt; 1:  # at least two futures left\n    a = next(seq)\n    b = next(seq)\n    new = client.submit(add, a, b, priority=1)  # add them together\n    seq.add(new)                                # add new future back into loop\n\n\nfor future in seq:\n    print(future.result())"
  },
  {
    "objectID": "nb/lab6/05_futures.html#why-use-futures",
    "href": "nb/lab6/05_futures.html#why-use-futures",
    "title": "Futures - non-blocking distributed calculations",
    "section": "Why use Futures?",
    "text": "Why use Futures?\nThe futures API offers a work submission style that can easily emulate the map/reduce paradigm. If that is familiar to you then futures might be the simplest entrypoint into Dask.\nThe other big benefit of futures is that the intermediate results, represented by futures, can be passed to new tasks without having to pull data locally from the cluster. New operations can be setup to work on the output of previous jobs that haven’t even begun yet.\nDocs: https://docs.dask.org/en/latest/futures.html"
  },
  {
    "objectID": "nb/lab6/05_futures.html#exercises",
    "href": "nb/lab6/05_futures.html#exercises",
    "title": "Futures - non-blocking distributed calculations",
    "section": "Exercises",
    "text": "Exercises\nUsing the depression dataset from lab1, calculate the below using dask.futures:\n\nMean income grouped by a) smoking status; b) education level; c) married status\nPerform tree reduction as given above, but using Income column of the depression dataset, filtered by age in [20,30] interval."
  },
  {
    "objectID": "nb/lab8/spatialjoin_geopandas_dask.html",
    "href": "nb/lab8/spatialjoin_geopandas_dask.html",
    "title": "Part 1 : A Gentle Introduction to the Spatial Join",
    "section": "",
    "text": "One problem I came across when analyzing the New York City Taxi Dataset, is that from 2009 to June 2016, both the starting and stopping locations of taxi trips were given as longitude and latitude points. After July 2016, to provide a degree of anonymity when releasing data to the public, the Taxi and Limousine Commission (TLC) only provides the starting and ending “taxi zones” of a trip, and a shapefile that specifies the boundaries, available here. Let’s load this up in Geopandas, and set the coordinate system to ‘epsg:4326’, which is latitude and longitude coordinates.\n# &lt;!-- collapse=True --&gt;\nimport bokeh, bokeh.plotting, bokeh.models\nfrom bokeh.io import output_notebook, show\noutput_notebook()\n\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport urllib\nimport dask.dataframe as dd\nimport dask.distributed\nimport numpy as np\n\nimport sklearn.preprocessing\n\nclient = dask.distributed.Client()\n\ncoord_system = {'init': 'epsg:4326'}\ndf = gpd.read_file('../shapefiles/taxi_zones.shp').to_crs(coord_system)\ndf = df.drop(['Shape_Area', 'Shape_Leng', 'OBJECTID'], axis=1)\ndf.head()\n\n\n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n\n\n\n\n\n\nLocationID\nborough\ngeometry\nzone\n\n\n\n\n0\n1\nEWR\nPOLYGON ((-74.18445299999996 40.6949959999999,...\nNewark Airport\n\n\n1\n2\nQueens\n(POLYGON ((-73.82337597260663 40.6389870471767...\nJamaica Bay\n\n\n2\n3\nBronx\nPOLYGON ((-73.84792614099985 40.87134223399991...\nAllerton/Pelham Gardens\n\n\n3\n4\nManhattan\nPOLYGON ((-73.97177410965318 40.72582128133705...\nAlphabet City\n\n\n4\n5\nStaten Island\nPOLYGON ((-74.17421738099989 40.56256808599987...\nArden Heights\nWe see that the geometry column consists of polygons (from Shapely) that have vertices defined by longitude and latitude points. Let’s plot using bokeh, in order of ascending LocationID.\n# &lt;!-- collapse=True --&gt;\ngjds = bokeh.models.GeoJSONDataSource(geojson=df.to_json())\nTOOLS = \"pan,wheel_zoom,reset,hover,save\"\n\np = bokeh.plotting.figure(title=\"NYC Taxi Districts\", tools=TOOLS,\n    x_axis_location=None, y_axis_location=None, \n    responsive=True)\n\ncolor_mapper = bokeh.models.LinearColorMapper(palette=bokeh.palettes.Viridis256)\n\np.patches('xs', 'ys', \n          fill_color={'field': 'LocationID', 'transform': color_mapper},\n          fill_alpha=1., line_color=\"black\", line_width=0.5,          \n          source=gjds)\n\np.grid.grid_line_color = None\n\nhover = p.select_one(bokeh.models.HoverTool)\nhover.point_policy = \"follow_mouse\"\nhover.tooltips = u\"\"\"\n&lt;div&gt; \n    &lt;div class=\"bokeh_hover_tooltip\"&gt;Name : @zone&lt;/div&gt;\n    &lt;div class=\"bokeh_hover_tooltip\"&gt;Borough : @borough&lt;/div&gt;\n    &lt;div class=\"bokeh_hover_tooltip\"&gt;Zone ID : @LocationID&lt;/div&gt;\n    &lt;div class=\"bokeh_hover_tooltip\"&gt;(Lon, Lat) : ($x ˚E, $y ˚N)&lt;/div&gt;\n&lt;/div&gt;\n\"\"\"\n\np.circle([-73.966,], [40.78,], size=10, fill_color='magenta', line_color='yellow', line_width=1, alpha=1.0)\n\nshow(p)\nThis is a familiar map of New York, with 262 taxi districts shown, colored by the id of the taxi district. I have added a random point (-73.966˚E, 40.78˚N) in magenta, which happens to fall in the middle of Central Park. Assigning a point as within a taxi zone is something humans can do easily, but on a computer it requires solving the point in polygon problem. Luckily the Shapely library provides an easy interface to such geometric operations in Python. But, point in polygon is computationally expensive, and using the Shapely library on 2.4 billion (latitude, longitude) pairs to assign taxi zones as in the NYC Taxi Dataset would take a modern single core cpu about four years. To speed this up, we calculate the bounding boxes for each taxi zone, which looks like:\n# &lt;!-- collapse=True --&gt;\ndf2 = df.copy()\ndf2['geometry'] = df.geometry.envelope\ndf2['borough_categ'] = sklearn.preprocessing.LabelEncoder().fit_transform(df2['borough'])\ngjds2 = bokeh.models.GeoJSONDataSource(geojson=df2.to_json())\n\nTOOLS = \"pan,wheel_zoom,reset,hover,save\"\n\np = bokeh.plotting.figure(title=\"NYC Taxi Districts Bounding Boxes\", tools=TOOLS,\n    x_axis_location=None, y_axis_location=None, \n    responsive=True)\n\ncolor_mapper = bokeh.models.LinearColorMapper(palette=bokeh.palettes.Viridis256)\n\np.patches('xs', 'ys', \n          fill_color={'field': 'borough_categ', 'transform': color_mapper},\n          fill_alpha=0.7, line_color=\"black\", line_width=0.5,          \n          source=gjds2)\n\np.grid.grid_line_color = None\n\nhover = p.select_one(bokeh.models.HoverTool)\nhover.point_policy = \"follow_mouse\"\nhover.tooltips = u\"\"\"\n&lt;div&gt; \n    &lt;div class=\"bokeh_hover_tooltip\"&gt;Name : @zone&lt;/div&gt;\n    &lt;div class=\"bokeh_hover_tooltip\"&gt;Borough : @borough&lt;/div&gt;\n    &lt;div class=\"bokeh_hover_tooltip\"&gt;Zone ID : @LocationID&lt;/div&gt;\n    &lt;div class=\"bokeh_hover_tooltip\"&gt;(Lon, Lat) : ($x ˚E, $y ˚N)&lt;/div&gt;\n&lt;/div&gt;\n\"\"\"\n\np.circle([-73.966,], [40.78,], size=10, fill_color='magenta', line_color='yellow', line_width=1, alpha=1.0)\n\nshow(p)\nNow, given a (longitude, latitude) coordinate pair, bounding boxes that contain that pair can be efficiently calculated with an R-tree. Only the polygons (taxi zones) that have bounding boxes that contain the coordinate pair need to be examined, and then the point in Polygon is solved for those (hopefully) few taxi zones. This reduces computation by a factor of about 100-1000. This process, assigning coordinate pairs to taxi zones is one example of a spatial join. Geopandas provides a nice interface to efficient spatial joins in Python, and it takes care of calculating bounding boxes and R-trees for you, as this snippet shows.\ngpd.sjoin(gpd.GeoDataFrame(crs={'init': 'epsg:4326'}, geometry=[Point(-73.966, 40.78)]), \n          df,\n          how='left', op='within')\n\n\n\n\n\n\n\n\ngeometry\nindex_right\nLocationID\nborough\nzone\n\n\n\n\n0\nPOINT (-73.96599999999999 40.78)\n42\n43\nManhattan\nCentral Park\nThis does the merge for a single point (drawn in magenta) on maps above, and correctly identifies it in Central Park"
  },
  {
    "objectID": "nb/lab8/spatialjoin_geopandas_dask.html#part-2-spatial-joins-at-scale-using-dask",
    "href": "nb/lab8/spatialjoin_geopandas_dask.html#part-2-spatial-joins-at-scale-using-dask",
    "title": "Part 1 : A Gentle Introduction to the Spatial Join",
    "section": "Part 2 : Spatial Joins at scale using Dask",
    "text": "Part 2 : Spatial Joins at scale using Dask\nIn my NYC transit project, I download and process the Taxi dataset. Here I load up a single file from the taxi dataset (May 2016) into Dask, and show the first few rows and a few columns. The file is a bit large at 1.8GB, and Dask chooses to divide up the dataframe into 30 partitions for efficient calculations. Each partition is a pandas DataFrame, and dask takes care of all the logic to view the combination as a single DataFrame. Here are a few columns.\n\n# &lt;!-- collapse=True --&gt;\n!wget --quiet --continue 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-05.csv'\n\ntrips = dd.read_csv('yellow_tripdata_2016-05.csv')\nprint(\"There are {} partitions. \".format(trips.npartitions))\ntrips[('tpep_pickup_datetime pickup_longitude'\n        ' pickup_latitude').split()].head()\n\nThere are 30 partitions. \n\n\n\n\n\n\n\n\n\ntpep_pickup_datetime\npickup_longitude\npickup_latitude\n\n\n\n\n0\n2016-05-01 00:00:00\n-73.985901\n40.768040\n\n\n1\n2016-05-01 00:00:00\n-73.991577\n40.744751\n\n\n2\n2016-05-01 00:00:00\n-73.993073\n40.741573\n\n\n3\n2016-05-01 00:00:00\n-73.991943\n40.684601\n\n\n4\n2016-05-01 00:00:00\n-74.005280\n40.740192\n\n\n\n\n\n\n\n\n# &lt;!-- collapse=True --&gt;\ntrips[('tpep_dropoff_datetime dropoff_longitude'\n        ' dropoff_latitude').split()].head()\n\n\n\n\n\n\n\n\ntpep_dropoff_datetime\ndropoff_longitude\ndropoff_latitude\n\n\n\n\n0\n2016-05-01 00:17:31\n-73.983986\n40.730099\n\n\n1\n2016-05-01 00:07:31\n-73.975700\n40.765469\n\n\n2\n2016-05-01 00:07:01\n-73.980995\n40.744633\n\n\n3\n2016-05-01 00:19:47\n-74.002258\n40.733002\n\n\n4\n2016-05-01 00:06:39\n-73.997498\n40.737564\n\n\n\n\n\n\n\nSo each trip has pickup and dropoff (longitude, latitude) coordinate pairs. Just to give you a feel for the data, I plot the start and end locations of the first trip, ending in the East Village. Driving directions come with a great deal of additional complexity, so here I just plot an arrow, as the crow flies. A spatial join identifies the taxi zones as Clinton East and East Village.\n\n# &lt;!-- collapse=True --&gt;\n\nx, y = (trips[('tpep_pickup_datetime pickup_longitude'\n        ' pickup_latitude').split()].head()).iloc[0, 1:3]\nx2, y2 = (trips[('tpep_dropoff_datetime dropoff_longitude'\n        ' dropoff_latitude').split()].head()).iloc[0, 1:3]\n\ndf['borough_categ'] = sklearn.preprocessing.LabelEncoder().fit_transform(df['borough'])\ngjds = bokeh.models.GeoJSONDataSource(geojson=df.to_json())\nTOOLS = \"pan,wheel_zoom,reset,hover,save\"\n\np = bokeh.plotting.figure(title=\"NYC Taxi Districts\", tools=TOOLS,\n    x_axis_location=None, y_axis_location=None, \n    responsive=True)\n\ncolor_mapper = bokeh.models.LinearColorMapper(palette=bokeh.palettes.Viridis256)\n\np.patches('xs', 'ys', \n          fill_color={'field': 'borough_categ', 'transform': color_mapper},\n          fill_alpha=1., line_color=\"black\", line_width=0.5,          \n          source=gjds)\n\np.grid.grid_line_color = None\n\nhover = p.select_one(bokeh.models.HoverTool)\nhover.point_policy = \"follow_mouse\"\nhover.tooltips = u\"\"\"\n&lt;div&gt; \n    &lt;div class=\"bokeh_hover_tooltip\"&gt;Name : @zone&lt;/div&gt;\n    &lt;div class=\"bokeh_hover_tooltip\"&gt;Borough : @borough&lt;/div&gt;\n    &lt;div class=\"bokeh_hover_tooltip\"&gt;Zone ID : @LocationID&lt;/div&gt;\n    &lt;div class=\"bokeh_hover_tooltip\"&gt;(Lon, Lat) : ($x ˚E, $y ˚N)&lt;/div&gt;\n&lt;/div&gt;\n\"\"\"\n\np.circle([x,], [y,], size=13, \n         fill_color='aqua', line_color='black', \n         line_width=1, alpha=1.0)\np.circle([x2,], [y2,], size=13, \n         fill_color='aqua', line_color='black', \n         line_width=1, alpha=1.0)\n\np.add_layout(bokeh.models.Arrow(line_color='orange', line_width=4,\n        end=bokeh.models.OpenHead(line_color=\"orange\", line_width=4),\n                   x_start=x, y_start=y, x_end=x2, y_end=y2))\n\np.x_range=bokeh.models.Range1d(-74.02, -73.92)\np.y_range=bokeh.models.Range1d(40.70, 40.78)\n\nshow(p)\n\n\n\n    \n        \n    \n\n\n\n\ngpd.sjoin(gpd.GeoDataFrame(crs={'init': 'epsg:4326'}, \n                           geometry=[Point(x, y), Point(x2, y2)]), \n                           df,\n                           how='left', op='within')\n\n\n\n\n\n\n\n\ngeometry\nindex_right\nLocationID\nborough\nzone\nborough_categ\n\n\n\n\n0\nPOINT (-73.98590087890625 40.76803970336913)\n47\n48\nManhattan\nClinton East\n3\n\n\n1\nPOINT (-73.98398590087891 40.73009872436523)\n78\n79\nManhattan\nEast Village\n3\n\n\n\n\n\n\n\nSo, Dask DataFrames are just collections of Pandas DataFrames, and I know how to perform a spatial join on a Pandas DataFrame. Let’s take advantage of Dask’s map_partitions function to do a spatial join with the taxi zones on every partition. Here is the function to do the spatial join, given a Pandas DataFrame, and the names of the longitude, latitude, and taxizone id columns.\n\n# &lt;!-- collapse=False --&gt;\ndef assign_taxi_zones(df, lon_var, lat_var, locid_var):\n    \"\"\"Joins DataFrame with Taxi Zones shapefile.\n    This function takes longitude values provided by `lon_var`, and latitude\n    values provided by `lat_var` in DataFrame `df`, and performs a spatial join\n    with the NYC taxi_zones shapefile. \n    The shapefile is hard coded in, as this function makes a hard assumption of\n    latitude and longitude coordinates. It also assumes latitude=0 and \n    longitude=0 is not a datapoint that can exist in your dataset. Which is \n    reasonable for a dataset of New York, but bad for a global dataset.\n    Only rows where `df.lon_var`, `df.lat_var` are reasonably near New York,\n    and `df.locid_var` is set to np.nan are updated. \n    Parameters\n    ----------\n    df : pandas.DataFrame or dask.DataFrame\n        DataFrame containing latitudes, longitudes, and location_id columns.\n    lon_var : string\n        Name of column in `df` containing longitude values. Invalid values \n        should be np.nan.\n    lat_var : string\n        Name of column in `df` containing latitude values. Invalid values \n        should be np.nan\n    locid_var : string\n        Name of series to return. \n    \"\"\"\n\n    import geopandas\n    from shapely.geometry import Point\n\n\n    # make a copy since we will modify lats and lons\n    localdf = df[[lon_var, lat_var]].copy()\n    \n    # missing lat lon info is indicated by nan. Fill with zero\n    # which is outside New York shapefile. \n    localdf[lon_var] = localdf[lon_var].fillna(value=0.)\n    localdf[lat_var] = localdf[lat_var].fillna(value=0.)\n    \n\n    shape_df = geopandas.read_file('../shapefiles/taxi_zones.shp')\n    shape_df.drop(['OBJECTID', \"Shape_Area\", \"Shape_Leng\", \"borough\", \"zone\"],\n                  axis=1, inplace=True)\n    shape_df = shape_df.to_crs({'init': 'epsg:4326'})\n\n    try:\n        local_gdf = geopandas.GeoDataFrame(\n            localdf, crs={'init': 'epsg:4326'},\n            geometry=[Point(xy) for xy in\n                      zip(localdf[lon_var], localdf[lat_var])])\n\n        local_gdf = geopandas.sjoin(\n            local_gdf, shape_df, how='left', op='within')\n\n        return local_gdf.LocationID.rename(locid_var)\n    except ValueError as ve:\n        print(ve)\n        print(ve.stacktrace())\n        series = localdf[lon_var]\n        series = np.nan\n        return series\n\n\nAt Scale\nUsing the map_partitions function, I apply the spatial join to each of the Pandas DataFrames that make up the Dask DataFrame. For simplicity, I just call the function twice, once for pickup locations, and once for dropoff locations. To assist dask in determining schema of returned data, we specify it as a column of floats (allowing for NaN values).\n\n# &lt;!-- collapse=False --&gt;\ntrips['pickup_taxizone_id'] = trips.map_partitions(\n    assign_taxi_zones, \"pickup_longitude\", \"pickup_latitude\",\n    \"pickup_taxizone_id\", meta=('pickup_taxizone_id', np.float64))\ntrips['dropoff_taxizone_id'] = trips.map_partitions(\n    assign_taxi_zones, \"dropoff_longitude\", \"dropoff_latitude\",\n    \"dropoff_taxizone_id\", meta=('dropoff_taxizone_id', np.float64))\ntrips[['pickup_taxizone_id', 'dropoff_taxizone_id']].head()\n\n\n\n\n\n\n\n\npickup_taxizone_id\ndropoff_taxizone_id\n\n\n\n\n0\n48.0\n79.0\n\n\n1\n90.0\n43.0\n\n\n2\n234.0\n170.0\n\n\n3\n25.0\n249.0\n\n\n4\n158.0\n249.0\n\n\n\n\n\n\n\nAt this point, the trips Dask DataFrame will have valid taxizone_id information. Lets save this data to Parquet, which is a columnar format that is well supported in Dask and Apache Spark. This prevents Dask from recalculating the spatial join (very expensive) every time an operation on the trips DataFrame is required.\n\ntrips.to_parquet('trips_2016-05.parquet', has_nulls=True, object_encoding='json', compression=\"SNAPPY\")\ntrips = dd.read_parquet('trips_2016-05.parquet', columns=['pickup_taxizone_id', 'dropoff_taxizone_id'])\n\nTo close this post out, I’ll produce a heatmap of taxi dropoff locations, aggregated by taxi zone using Dask. Unsurprisingly (for New Yorkers at least) the vast majority of taxi dropoffs tend to be in Downtown and Midtown Manhattan. I will analyze this dataset further in future posts.\n\n# &lt;!-- collapse=True --&gt;\ncounts = trips.groupby('dropoff_taxizone_id').count().compute()\ncounts.columns = ['N']\ncounts2 = df.merge(counts, left_on='LocationID', right_index=True, how='left')\n\ngjds = bokeh.models.GeoJSONDataSource(geojson=counts2.to_json())\nTOOLS = \"pan,wheel_zoom,reset,hover,save\"\n\np = bokeh.plotting.figure(title=\"NYC Taxi Dropoffs Heatmap\", tools=TOOLS,\n    x_axis_location=None, y_axis_location=None, \n    responsive=True)\n\ncolor_mapper = bokeh.models.LogColorMapper(palette=bokeh.palettes.Viridis256, low=1, high=500000)\n\np.patches('xs', 'ys', \n          fill_color={'field': 'N', 'transform': color_mapper},\n          fill_alpha=1., line_color=\"black\", line_width=0.5,          \n          source=gjds)\n\np.grid.grid_line_color = None\n\nhover = p.select_one(bokeh.models.HoverTool)\nhover.point_policy = \"follow_mouse\"\nhover.tooltips = u\"\"\"\n&lt;div&gt; \n    &lt;div class=\"bokeh_hover_tooltip\"&gt;Name : @zone&lt;/div&gt;\n    &lt;div class=\"bokeh_hover_tooltip\"&gt;Borough : @borough&lt;/div&gt;\n    &lt;div class=\"bokeh_hover_tooltip\"&gt;Trips Start : @N&lt;/div&gt;\n&lt;/div&gt;\n\"\"\"\n\ncolor_bar = bokeh.models.ColorBar(\n    color_mapper=color_mapper, orientation='horizontal',\n    ticker=bokeh.models.FixedTicker(ticks=[3, 10, 30, 100, 300, 1000, 3000, 10000, 30000, 100000, 300000]),\n    formatter=bokeh.models.PrintfTickFormatter(format='%d'),\n    label_standoff=12, border_line_color=None, location=(0,0))\np.add_layout(color_bar, 'below')\n\nshow(p)"
  },
  {
    "objectID": "nb/lab8/spatialjoin_geopandas_dask.html#summary",
    "href": "nb/lab8/spatialjoin_geopandas_dask.html#summary",
    "title": "Part 1 : A Gentle Introduction to the Spatial Join",
    "section": "Summary",
    "text": "Summary\nIn this post I described the process of a Spatial Join, and doing the Spatial Join at scale on a cluster using Dask and Pandas. I glossed over some details that are important for the full New York Taxi Dataset, but my full code is available here on Github. In future posts, I will analyze this data more thoroughly, and possibly look into releasing the processed data as a parquet file for others to analyze.\n\nProject on Github\n\nA side note on spatial join performance\nThe spatial join as written above with GeoPandas, using the New York Taxi Dataset, can assign taxi zones to approxmately 40 million taxi trips per hour on a 4 GHz 4-core i5 system. A lot of the code that supports this join is some amalgamation of Python and wrapped C code.\nThis is approxmately a factor of two slower than performing the same spatial join in highly optimized PostGIS C/C++ code. However, PostGIS does not efficiently use multiple cores (at least without multiple spatial joins running simultaneously), and more importantly, the network and serialization overhead of a round trip to the PostgreSQL database puts PostgreSQL/PostGIS at roughly the same speed as the GeoPandas implementation I describe in this post, with vastly more moving parts to break.\nBasically, Python is actually pretty fast for these kinds of data structure operations."
  },
  {
    "objectID": "bigdata.html",
    "href": "bigdata.html",
    "title": "Big Data Analytics",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "bigdata.html#lectures",
    "href": "bigdata.html#lectures",
    "title": "Big Data Analytics",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "bigdata.html#labs",
    "href": "bigdata.html#labs",
    "title": "Big Data Analytics",
    "section": "Labs",
    "text": "Labs\nLab 1\nLab 2"
  },
  {
    "objectID": "bigdata.html#exam-questions",
    "href": "bigdata.html#exam-questions",
    "title": "Big Data Analytics",
    "section": "Exam questions",
    "text": "Exam questions"
  },
  {
    "objectID": "bigdata_lab1.html",
    "href": "bigdata_lab1.html",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "Primary docs:\n\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html\n\nPerformance deps for Pandas:\n\nhttps://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#install-recommended-dependencies\n\n\n\n\n\n\n\nchunksize parameter in Pandas functions.\n\n\n\nUse to_numeric (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html) for downcasting\n\n\n\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n\n\n\n\nuse DataFrame.memory_usage(deep=True) func.\nuse DataFrame.info() func\n\n\n\n\n\nWe’ll get the sample dataset from https://www.kaggle.com/datasets/anthonytherrien/depression-dataset.\n\nimport pandas as pd\n\ndd = pd.read_csv('files/depression_data.csv')\ndd.head()\n\n\n\n\n\n\n\n\nName\nAge\nMarital Status\nEducation Level\nNumber of Children\nSmoking Status\nPhysical Activity Level\nEmployment Status\nIncome\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\n0\nChristine Barker\n31\nMarried\nBachelor's Degree\n2\nNon-smoker\nActive\nUnemployed\n26265.67\nModerate\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n1\nJacqueline Lewis\n55\nMarried\nHigh School\n1\nNon-smoker\nSedentary\nEmployed\n42710.36\nHigh\nUnhealthy\nFair\nYes\nNo\nNo\nYes\n\n\n2\nShannon Church\n78\nWidowed\nMaster's Degree\n1\nNon-smoker\nSedentary\nEmployed\n125332.79\nLow\nUnhealthy\nGood\nNo\nNo\nYes\nNo\n\n\n3\nCharles Jordan\n58\nDivorced\nMaster's Degree\n3\nNon-smoker\nModerate\nUnemployed\n9992.78\nModerate\nModerate\nPoor\nNo\nNo\nNo\nNo\n\n\n4\nMichael Rich\n18\nSingle\nHigh School\n0\nNon-smoker\nSedentary\nUnemployed\n8595.08\nLow\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n\n\n\n\n\nLet’s see some memory usage stats:\n\ndd.info(memory_usage='deep')\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 413768 entries, 0 to 413767\nData columns (total 16 columns):\n #   Column                        Non-Null Count   Dtype  \n---  ------                        --------------   -----  \n 0   Name                          413768 non-null  object \n 1   Age                           413768 non-null  int64  \n 2   Marital Status                413768 non-null  object \n 3   Education Level               413768 non-null  object \n 4   Number of Children            413768 non-null  int64  \n 5   Smoking Status                413768 non-null  object \n 6   Physical Activity Level       413768 non-null  object \n 7   Employment Status             413768 non-null  object \n 8   Income                        413768 non-null  float64\n 9   Alcohol Consumption           413768 non-null  object \n 10  Dietary Habits                413768 non-null  object \n 11  Sleep Patterns                413768 non-null  object \n 12  History of Mental Illness     413768 non-null  object \n 13  History of Substance Abuse    413768 non-null  object \n 14  Family History of Depression  413768 non-null  object \n 15  Chronic Medical Conditions    413768 non-null  object \ndtypes: float64(1), int64(2), object(13)\nmemory usage: 294.9 MB\n\n\nNote: Pandas stores memory in blocks, managed by BlockManager class. There is a separate block class for each type, like ObjectBlock or FloatBlock.\nNice write-up here: https://uwekorn.com/2020/05/24/the-one-pandas-internal.html.\nLet’s examine memory usage for each type:\n\nfor dtype in ['float','int','object']:\n    selected_dtype = dd.select_dtypes(include=[dtype])\n    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n    mean_usage_mb = mean_usage_b / 1024 ** 2\n    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))\n\nAverage memory usage for float columns: 1.58 MB\nAverage memory usage for int columns: 2.10 MB\nAverage memory usage for object columns: 20.39 MB\n\n\n\n\nLet’s first use iinfo to check ranges for different subtypes:\n\nimport numpy as np\nint_types = [\"uint8\", \"int8\", \"int16\", \"int32\", \"int64\"]\nfor it in int_types:\n    print(np.iinfo(it))\n\nMachine parameters for uint8\n---------------------------------------------------------------\nmin = 0\nmax = 255\n---------------------------------------------------------------\n\nMachine parameters for int8\n---------------------------------------------------------------\nmin = -128\nmax = 127\n---------------------------------------------------------------\n\nMachine parameters for int16\n---------------------------------------------------------------\nmin = -32768\nmax = 32767\n---------------------------------------------------------------\n\nMachine parameters for int32\n---------------------------------------------------------------\nmin = -2147483648\nmax = 2147483647\n---------------------------------------------------------------\n\nMachine parameters for int64\n---------------------------------------------------------------\nmin = -9223372036854775808\nmax = 9223372036854775807\n---------------------------------------------------------------\n\n\n\nWe can use pd.to_numeric() to downcast numeric types.\nFirst, let’s write a helper function for memory usage display:\n\ndef mem_usage(pandas_obj):\n    if isinstance(pandas_obj,pd.DataFrame):\n        usage_b = pandas_obj.memory_usage(deep=True).sum()\n    else: # we assume if not a df it's a series\n        usage_b = pandas_obj.memory_usage(deep=True)\n    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n    return \"{:03.2f} MB\".format(usage_mb)\n\nNote that “Age” and “Number of Children” columns can be presented as unsigned ints. Let’s convert:\n\ndd_int = dd.select_dtypes(include=['int'])\nconverted_int = dd_int.apply(pd.to_numeric,downcast='unsigned')\nprint(\"Before: \", mem_usage(dd_int))\nprint(\"After: \", mem_usage(converted_int))\ncompare_ints = pd.concat([dd_int.dtypes,converted_int.dtypes],axis=1)\ncompare_ints.columns = ['before','after']\ncompare_ints.apply(pd.Series.value_counts)\n\nBefore:  6.31 MB\nAfter:  0.79 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nuint8\nNaN\n2.0\n\n\nint64\n2.0\nNaN\n\n\n\n\n\n\n\nNice. Now let’s process float columns (Income):\n\ndd_float = dd.select_dtypes(include=['float'])\nconverted_float = dd_float.apply(pd.to_numeric,downcast='float')\nprint(\"Before: \", mem_usage(dd_float))\nprint(\"After: \", mem_usage(converted_float))\ncompare_floats = pd.concat([dd_float.dtypes,converted_float.dtypes],axis=1)\ncompare_floats.columns = ['before','after']\ncompare_floats.apply(pd.Series.value_counts)\n\nBefore:  3.16 MB\nAfter:  3.16 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nfloat64\n1\n1\n\n\n\n\n\n\n\nNothing spectacular.\nNow let’s create a new optimized DataFrame:\n\noptimized_dd = dd.copy()\noptimized_dd[converted_int.columns] = converted_int\noptimized_dd[converted_float.columns] = converted_float\nprint(mem_usage(dd))\nprint(mem_usage(optimized_dd))\n\n294.92 MB\n289.40 MB\n\n\nJust a bit. Let’s proceed with object types.\nFirst, read https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/.\nNow, back to the dataset.\nWe can use categoricals (https://pandas.pydata.org/pandas-docs/stable/categorical.html) to optimize object columns.\n\nLet’s look at the number of unique values for each object type:\n\ndd_obj = dd.select_dtypes(include=['object']).copy()\ndd_obj.describe()\n\n\n\n\n\n\n\n\nName\nMarital Status\nEducation Level\nSmoking Status\nPhysical Activity Level\nEmployment Status\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\ncount\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n\n\nunique\n196851\n4\n5\n3\n3\n2\n3\n3\n3\n2\n2\n2\n2\n\n\ntop\nMichael Smith\nMarried\nBachelor's Degree\nNon-smoker\nSedentary\nEmployed\nModerate\nUnhealthy\nFair\nNo\nNo\nNo\nNo\n\n\nfreq\n198\n240444\n124329\n247416\n176850\n265659\n173440\n170817\n196789\n287943\n284880\n302515\n277561\n\n\n\n\n\n\n\nLet’s start with one column first: “Education Level”\n\nedu_level = dd_obj[\"Education Level\"]\nprint(edu_level.head())\nedu_level_cat = edu_level.astype('category')\nprint(edu_level_cat.head())\n\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: object\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: category\nCategories (5, object): ['Associate Degree', 'Bachelor's Degree', 'High School', 'Master's Degree', 'PhD']\n\n\nWe can look at the category codes:\n\nedu_level_cat.head().cat.codes\n\n0    1\n1    2\n2    3\n3    3\n4    2\ndtype: int8\n\n\nCompare memory usage:\n\nprint(mem_usage(edu_level))\nprint(mem_usage(edu_level_cat))\n\n24.92 MB\n0.40 MB\n\n\nWe should only convert objects to categoricals if most values are repeated. We have to pick a threshold, say, 25% of unique values.\nNote: when reading a csv, we can also provide a dtype dictionary param with preferred types.\nConverting all object columns and creating a new optimized DataFrame is left as part of Exercise 1.\n\n\n\n\n\nPick your own dataset from\n\n\nKaggle\nHuggingFace\nor https://archive.ics.uci.edu.\n\n\nUse Dora library installed in Applied lab1 for data cleaning.\n\n\n\n\n\n\n\nPerform numerical and object types conversions aimed at minimizing storage space, as outlined above.\nMeasure performance impact and plot it via e.g. matplotlib."
  },
  {
    "objectID": "bigdata_lab1.html#notes",
    "href": "bigdata_lab1.html#notes",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "chunksize parameter in Pandas functions.\n\n\n\nUse to_numeric (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html) for downcasting\n\n\n\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n\n\n\n\nuse DataFrame.memory_usage(deep=True) func.\nuse DataFrame.info() func"
  },
  {
    "objectID": "bigdata_lab1.html#storage-optimization",
    "href": "bigdata_lab1.html#storage-optimization",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "We’ll get the sample dataset from https://www.kaggle.com/datasets/anthonytherrien/depression-dataset.\n\nimport pandas as pd\n\ndd = pd.read_csv('files/depression_data.csv')\ndd.head()\n\n\n\n\n\n\n\n\nName\nAge\nMarital Status\nEducation Level\nNumber of Children\nSmoking Status\nPhysical Activity Level\nEmployment Status\nIncome\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\n0\nChristine Barker\n31\nMarried\nBachelor's Degree\n2\nNon-smoker\nActive\nUnemployed\n26265.67\nModerate\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n1\nJacqueline Lewis\n55\nMarried\nHigh School\n1\nNon-smoker\nSedentary\nEmployed\n42710.36\nHigh\nUnhealthy\nFair\nYes\nNo\nNo\nYes\n\n\n2\nShannon Church\n78\nWidowed\nMaster's Degree\n1\nNon-smoker\nSedentary\nEmployed\n125332.79\nLow\nUnhealthy\nGood\nNo\nNo\nYes\nNo\n\n\n3\nCharles Jordan\n58\nDivorced\nMaster's Degree\n3\nNon-smoker\nModerate\nUnemployed\n9992.78\nModerate\nModerate\nPoor\nNo\nNo\nNo\nNo\n\n\n4\nMichael Rich\n18\nSingle\nHigh School\n0\nNon-smoker\nSedentary\nUnemployed\n8595.08\nLow\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n\n\n\n\n\nLet’s see some memory usage stats:\n\ndd.info(memory_usage='deep')\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 413768 entries, 0 to 413767\nData columns (total 16 columns):\n #   Column                        Non-Null Count   Dtype  \n---  ------                        --------------   -----  \n 0   Name                          413768 non-null  object \n 1   Age                           413768 non-null  int64  \n 2   Marital Status                413768 non-null  object \n 3   Education Level               413768 non-null  object \n 4   Number of Children            413768 non-null  int64  \n 5   Smoking Status                413768 non-null  object \n 6   Physical Activity Level       413768 non-null  object \n 7   Employment Status             413768 non-null  object \n 8   Income                        413768 non-null  float64\n 9   Alcohol Consumption           413768 non-null  object \n 10  Dietary Habits                413768 non-null  object \n 11  Sleep Patterns                413768 non-null  object \n 12  History of Mental Illness     413768 non-null  object \n 13  History of Substance Abuse    413768 non-null  object \n 14  Family History of Depression  413768 non-null  object \n 15  Chronic Medical Conditions    413768 non-null  object \ndtypes: float64(1), int64(2), object(13)\nmemory usage: 294.9 MB\n\n\nNote: Pandas stores memory in blocks, managed by BlockManager class. There is a separate block class for each type, like ObjectBlock or FloatBlock.\nNice write-up here: https://uwekorn.com/2020/05/24/the-one-pandas-internal.html.\nLet’s examine memory usage for each type:\n\nfor dtype in ['float','int','object']:\n    selected_dtype = dd.select_dtypes(include=[dtype])\n    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n    mean_usage_mb = mean_usage_b / 1024 ** 2\n    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))\n\nAverage memory usage for float columns: 1.58 MB\nAverage memory usage for int columns: 2.10 MB\nAverage memory usage for object columns: 20.39 MB\n\n\n\n\nLet’s first use iinfo to check ranges for different subtypes:\n\nimport numpy as np\nint_types = [\"uint8\", \"int8\", \"int16\", \"int32\", \"int64\"]\nfor it in int_types:\n    print(np.iinfo(it))\n\nMachine parameters for uint8\n---------------------------------------------------------------\nmin = 0\nmax = 255\n---------------------------------------------------------------\n\nMachine parameters for int8\n---------------------------------------------------------------\nmin = -128\nmax = 127\n---------------------------------------------------------------\n\nMachine parameters for int16\n---------------------------------------------------------------\nmin = -32768\nmax = 32767\n---------------------------------------------------------------\n\nMachine parameters for int32\n---------------------------------------------------------------\nmin = -2147483648\nmax = 2147483647\n---------------------------------------------------------------\n\nMachine parameters for int64\n---------------------------------------------------------------\nmin = -9223372036854775808\nmax = 9223372036854775807\n---------------------------------------------------------------\n\n\n\nWe can use pd.to_numeric() to downcast numeric types.\nFirst, let’s write a helper function for memory usage display:\n\ndef mem_usage(pandas_obj):\n    if isinstance(pandas_obj,pd.DataFrame):\n        usage_b = pandas_obj.memory_usage(deep=True).sum()\n    else: # we assume if not a df it's a series\n        usage_b = pandas_obj.memory_usage(deep=True)\n    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n    return \"{:03.2f} MB\".format(usage_mb)\n\nNote that “Age” and “Number of Children” columns can be presented as unsigned ints. Let’s convert:\n\ndd_int = dd.select_dtypes(include=['int'])\nconverted_int = dd_int.apply(pd.to_numeric,downcast='unsigned')\nprint(\"Before: \", mem_usage(dd_int))\nprint(\"After: \", mem_usage(converted_int))\ncompare_ints = pd.concat([dd_int.dtypes,converted_int.dtypes],axis=1)\ncompare_ints.columns = ['before','after']\ncompare_ints.apply(pd.Series.value_counts)\n\nBefore:  6.31 MB\nAfter:  0.79 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nuint8\nNaN\n2.0\n\n\nint64\n2.0\nNaN\n\n\n\n\n\n\n\nNice. Now let’s process float columns (Income):\n\ndd_float = dd.select_dtypes(include=['float'])\nconverted_float = dd_float.apply(pd.to_numeric,downcast='float')\nprint(\"Before: \", mem_usage(dd_float))\nprint(\"After: \", mem_usage(converted_float))\ncompare_floats = pd.concat([dd_float.dtypes,converted_float.dtypes],axis=1)\ncompare_floats.columns = ['before','after']\ncompare_floats.apply(pd.Series.value_counts)\n\nBefore:  3.16 MB\nAfter:  3.16 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nfloat64\n1\n1\n\n\n\n\n\n\n\nNothing spectacular.\nNow let’s create a new optimized DataFrame:\n\noptimized_dd = dd.copy()\noptimized_dd[converted_int.columns] = converted_int\noptimized_dd[converted_float.columns] = converted_float\nprint(mem_usage(dd))\nprint(mem_usage(optimized_dd))\n\n294.92 MB\n289.40 MB\n\n\nJust a bit. Let’s proceed with object types.\nFirst, read https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/.\nNow, back to the dataset.\nWe can use categoricals (https://pandas.pydata.org/pandas-docs/stable/categorical.html) to optimize object columns.\n\nLet’s look at the number of unique values for each object type:\n\ndd_obj = dd.select_dtypes(include=['object']).copy()\ndd_obj.describe()\n\n\n\n\n\n\n\n\nName\nMarital Status\nEducation Level\nSmoking Status\nPhysical Activity Level\nEmployment Status\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\ncount\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n\n\nunique\n196851\n4\n5\n3\n3\n2\n3\n3\n3\n2\n2\n2\n2\n\n\ntop\nMichael Smith\nMarried\nBachelor's Degree\nNon-smoker\nSedentary\nEmployed\nModerate\nUnhealthy\nFair\nNo\nNo\nNo\nNo\n\n\nfreq\n198\n240444\n124329\n247416\n176850\n265659\n173440\n170817\n196789\n287943\n284880\n302515\n277561\n\n\n\n\n\n\n\nLet’s start with one column first: “Education Level”\n\nedu_level = dd_obj[\"Education Level\"]\nprint(edu_level.head())\nedu_level_cat = edu_level.astype('category')\nprint(edu_level_cat.head())\n\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: object\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: category\nCategories (5, object): ['Associate Degree', 'Bachelor's Degree', 'High School', 'Master's Degree', 'PhD']\n\n\nWe can look at the category codes:\n\nedu_level_cat.head().cat.codes\n\n0    1\n1    2\n2    3\n3    3\n4    2\ndtype: int8\n\n\nCompare memory usage:\n\nprint(mem_usage(edu_level))\nprint(mem_usage(edu_level_cat))\n\n24.92 MB\n0.40 MB\n\n\nWe should only convert objects to categoricals if most values are repeated. We have to pick a threshold, say, 25% of unique values.\nNote: when reading a csv, we can also provide a dtype dictionary param with preferred types.\nConverting all object columns and creating a new optimized DataFrame is left as part of Exercise 1."
  },
  {
    "objectID": "bigdata_lab1.html#exercises",
    "href": "bigdata_lab1.html#exercises",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "Pick your own dataset from\n\n\nKaggle\nHuggingFace\nor https://archive.ics.uci.edu.\n\n\nUse Dora library installed in Applied lab1 for data cleaning.\n\n\n\n\n\n\n\nPerform numerical and object types conversions aimed at minimizing storage space, as outlined above.\nMeasure performance impact and plot it via e.g. matplotlib."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analytics",
    "section": "",
    "text": "Big data analytics / Applied data analytics courses."
  },
  {
    "objectID": "bigdata_lec2.html#moores-law",
    "href": "bigdata_lec2.html#moores-law",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\n\n\nDefinition (1965)\n\n\nNumber of transistors in an integrated circuit (IC) doubles about every two years."
  },
  {
    "objectID": "bigdata_lec2.html#moores-law-1",
    "href": "bigdata_lec2.html#moores-law-1",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law"
  },
  {
    "objectID": "bigdata_lec2.html#moores-law-2",
    "href": "bigdata_lec2.html#moores-law-2",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law"
  },
  {
    "objectID": "bigdata_lec2.html#moores-law-3",
    "href": "bigdata_lec2.html#moores-law-3",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\n\n\nTrends\n\n\n\nGordon Moore: Moore’s law will end by around 2025.\nNvidia CEO Jensen Huang: declared Moore’s law dead in 2022.\n\n\n\n\n\n\n\nPat Gelsinger, Intel CEO, end of 2023\n\n\n\nWe’re no longer in the golden era of Moore’s Law, it’s much, much harder now, so we’re probably doubling effectively closer to every three years now, so we’ve definitely seen a slowing."
  },
  {
    "objectID": "bigdata_lec2.html#edholms-law",
    "href": "bigdata_lec2.html#edholms-law",
    "title": "Big Data: Speeding up computation",
    "section": "Edholm’s law",
    "text": "Edholm’s law\n\n\n\nDefinition (2004)\n\n\n\nthree categories of telecommunication, namely\n\nwireless (mobile),\nnomadic (wireless without mobility)\nand wired networks (fixed),\n\nare in lockstep and gradually converging\nthe bandwidth and data rates double every 18 months, which has proven to be true since the 1970s"
  },
  {
    "objectID": "bigdata_lec2.html#edholms-law-1",
    "href": "bigdata_lec2.html#edholms-law-1",
    "title": "Big Data: Speeding up computation",
    "section": "Edholm’s law",
    "text": "Edholm’s law"
  },
  {
    "objectID": "bigdata_lec2.html#edholms-law-2",
    "href": "bigdata_lec2.html#edholms-law-2",
    "title": "Big Data: Speeding up computation",
    "section": "Edholm’s law",
    "text": "Edholm’s law\n\n\n\n\nData deluge\n\n\n\n90% of data humankind has produced happened in the last two years.\n80% of data could be unstructured\n99% of data produced is never analyzed"
  },
  {
    "objectID": "bigdata_lec2.html#core-counts",
    "href": "bigdata_lec2.html#core-counts",
    "title": "Big Data: Speeding up computation",
    "section": "Core counts",
    "text": "Core counts"
  },
  {
    "objectID": "bigdata_lec2.html#concurrency-vs-parallelism",
    "href": "bigdata_lec2.html#concurrency-vs-parallelism",
    "title": "Big Data: Speeding up computation",
    "section": "Concurrency vs Parallelism",
    "text": "Concurrency vs Parallelism\n\n\n\n\n\nConcurrency Parallelism\n\n\nIndividual steps of both tasks are executed in an interleaved fashion\n\n\n\n\n\n\n\nParallelism\n\n\nTask statements are executed at the same time."
  },
  {
    "objectID": "bigdata_lec2.html#process",
    "href": "bigdata_lec2.html#process",
    "title": "Big Data: Speeding up computation",
    "section": "Process",
    "text": "Process\n\n\n\nDefinition\n\n\nA process can be defined as an instance of a running program with its own memory.\nAlternatively: a context maintained for an executing program.\nProcesses have:\n\nlifetimes\nparents\nchildren.\nmemory/resources allocated."
  },
  {
    "objectID": "bigdata_lec2.html#process-1",
    "href": "bigdata_lec2.html#process-1",
    "title": "Big Data: Speeding up computation",
    "section": "Process",
    "text": "Process"
  },
  {
    "objectID": "bigdata_lec2.html#threads",
    "href": "bigdata_lec2.html#threads",
    "title": "Big Data: Speeding up computation",
    "section": "Threads",
    "text": "Threads\n\n\n\nDefinition\n\n\nA lightweight unit of execution within a process that can operate independently.\nThread is a basic unit to which the operating system allocates processor time.\nManaged with help of thread context, which consists of processor context and information required for thread management."
  },
  {
    "objectID": "bigdata_lec2.html#threads-1",
    "href": "bigdata_lec2.html#threads-1",
    "title": "Big Data: Speeding up computation",
    "section": "Threads",
    "text": "Threads"
  },
  {
    "objectID": "bigdata_lec2.html#green-threadscoroutines",
    "href": "bigdata_lec2.html#green-threadscoroutines",
    "title": "Big Data: Speeding up computation",
    "section": "Green threads/Coroutines",
    "text": "Green threads/Coroutines\n\n\n\nDefinition\n\n\nThese are threads managed by the process runtime, multiplexed onto OS threads."
  },
  {
    "objectID": "bigdata_lec2.html#comparison-table",
    "href": "bigdata_lec2.html#comparison-table",
    "title": "Big Data: Speeding up computation",
    "section": "Comparison table",
    "text": "Comparison table"
  },
  {
    "objectID": "bigdata_lec2.html#imperative",
    "href": "bigdata_lec2.html#imperative",
    "title": "Big Data: Speeding up computation",
    "section": "Imperative",
    "text": "Imperative\n\n\n\nWikipedia definition\n\n\nA programming paradigm of software that uses statements that change a program’s state.\nImperative program is a step-by-step description of program’s algorithm.\n\n\n\n\n\n\nExamples\n\n\n\nFortran\nCOBOL\nC\nPython\nGo"
  },
  {
    "objectID": "bigdata_lec2.html#imperative-1",
    "href": "bigdata_lec2.html#imperative-1",
    "title": "Big Data: Speeding up computation",
    "section": "Imperative",
    "text": "Imperative\n\n\n\nCons\n\n\n\nDifficult to parallelize\nLots of state management\n\n\n\n\n\n\n\nPros\n\n\n\nIntuitive concept, maps well to how people think, program as a recipe.\nEasy to optimize by the compiler"
  },
  {
    "objectID": "bigdata_lec2.html#functional",
    "href": "bigdata_lec2.html#functional",
    "title": "Big Data: Speeding up computation",
    "section": "Functional",
    "text": "Functional\n\n\n\nWikipedia definition\n\n\nA programming paradigm where programs are constructed by applying and composing functions.\n\n\n\n\n\n\nExamples\n\n\n\nML (Ocaml)\nLisp\nHaskell\nJulia"
  },
  {
    "objectID": "bigdata_lec2.html#functional-1",
    "href": "bigdata_lec2.html#functional-1",
    "title": "Big Data: Speeding up computation",
    "section": "Functional",
    "text": "Functional\n\n\n\nCons\n\n\n\nOften non-intuitive to reason about\nDepending on a specific algorithm, might be slower\n\n\n\n\n\n\n\nPros\n\n\n\nEasier to parallelize\nLend themselves beautifully to certain types of problems"
  },
  {
    "objectID": "bigdata_lec2.html#object-oriented",
    "href": "bigdata_lec2.html#object-oriented",
    "title": "Big Data: Speeding up computation",
    "section": "Object-oriented",
    "text": "Object-oriented\n\n\n\nWikipedia definition\n\n\nA programming paradigm based on the concept of objects, which can contain data and code: data in the form of fields (often known as attributes or properties), and code in the form of procedures (often known as methods).\n\n\n\n\n\n\nExamples\n\n\n\nSmalltalk\nJava\nC++\nPython\nC#"
  },
  {
    "objectID": "bigdata_lec2.html#object-oriented-1",
    "href": "bigdata_lec2.html#object-oriented-1",
    "title": "Big Data: Speeding up computation",
    "section": "Object-oriented",
    "text": "Object-oriented"
  },
  {
    "objectID": "bigdata_lec2.html#object-oriented-2",
    "href": "bigdata_lec2.html#object-oriented-2",
    "title": "Big Data: Speeding up computation",
    "section": "Object-oriented",
    "text": "Object-oriented\n\n\n\nCons\n\n\n\nDoes not map well to many problems\nLots of state management\n\n\n\n\n\n\n\nPros\n\n\n\nIntuitively easy to grasp, as human thinking is largely noun-oriented\nUseful for UIs"
  },
  {
    "objectID": "bigdata_lec2.html#symbolic",
    "href": "bigdata_lec2.html#symbolic",
    "title": "Big Data: Speeding up computation",
    "section": "Symbolic",
    "text": "Symbolic\n\n\n\nWikipedia definition\n\n\nA programming paradigm in which the program can manipulate its own formulas and program components as if they were plain data.\n\n\n\n\n\n\nExamples\n\n\n\nLisp\nProlog\nJulia"
  },
  {
    "objectID": "bigdata_lec2.html#lisp",
    "href": "bigdata_lec2.html#lisp",
    "title": "Big Data: Speeding up computation",
    "section": "Lisp",
    "text": "Lisp"
  },
  {
    "objectID": "bigdata_lec2.html#prolog",
    "href": "bigdata_lec2.html#prolog",
    "title": "Big Data: Speeding up computation",
    "section": "Prolog",
    "text": "Prolog\n :::"
  },
  {
    "objectID": "bigdata_lec2.html#typing-1",
    "href": "bigdata_lec2.html#typing-1",
    "title": "Big Data: Speeding up computation",
    "section": "Typing",
    "text": "Typing\n\n\n\nStatic vs dynamic\n\n\n\nstatic: types are known and checked before running the program\ndynamic: types become known when the program is running\n\n\n\n\n\n\n\nStrong vs weak\n\n\n\nstrong: variable types are not changed easily\nweak: types can be changed by the compiler if necessary"
  },
  {
    "objectID": "bigdata_lec2.html#typing-2",
    "href": "bigdata_lec2.html#typing-2",
    "title": "Big Data: Speeding up computation",
    "section": "Typing",
    "text": "Typing"
  },
  {
    "objectID": "applied.html",
    "href": "applied.html",
    "title": "Applied Data Analytics",
    "section": "",
    "text": "Course will cover the following:\n\ndata fetching/preprocessing\nspecialized data types:\n\ngeospatial\nfinancial\nsupply chain management\nmore\n\nvisualization/presentation (e.g. Gradio)\nLLM toolkits (LangChain/LangFlow)\nML Pipelines (MLFlow/KubeFlow/AirFlow/Prefect/Metaflow)\nMLOps\nAmazon Web Services\nDocker"
  },
  {
    "objectID": "applied.html#overview",
    "href": "applied.html#overview",
    "title": "Applied Data Analytics",
    "section": "",
    "text": "Course will cover the following:\n\ndata fetching/preprocessing\nspecialized data types:\n\ngeospatial\nfinancial\nsupply chain management\nmore\n\nvisualization/presentation (e.g. Gradio)\nLLM toolkits (LangChain/LangFlow)\nML Pipelines (MLFlow/KubeFlow/AirFlow/Prefect/Metaflow)\nMLOps\nAmazon Web Services\nDocker"
  },
  {
    "objectID": "applied.html#lectures",
    "href": "applied.html#lectures",
    "title": "Applied Data Analytics",
    "section": "Lectures",
    "text": "Lectures\nSlides"
  },
  {
    "objectID": "applied.html#labs",
    "href": "applied.html#labs",
    "title": "Applied Data Analytics",
    "section": "Labs",
    "text": "Labs\nLab 1\nLab 2"
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag",
    "href": "bigdata_lec4.html#program-as-dag",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nRepresentation\n\n\nA parallel program can be represented by a node- and edge-weighted directed acyclic graph (DAG), in which:\n\nnode weights represent task processing times\nedge weights represent data dependencies as well as the communication times between tasks."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-1",
    "href": "bigdata_lec4.html#program-as-dag-1",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG"
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-2",
    "href": "bigdata_lec4.html#program-as-dag-2",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nGeneralization\n\n\nMultithreaded computing can be viewed as a natural generalization of sequential computing in the following sense:\n\nin sequential computing, a computation can be defined as a totally ordered set of instructions, where the ordering corresponds to the sequential execution order;\nin multithreaded computing, a computation can be viewed as a partially ordered set of instructions (as specified by the DAG), where the instructions may be executed in any order compatible with the specified partial order."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-3",
    "href": "bigdata_lec4.html#program-as-dag-3",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nOrdering re-cap\n\n\nA binary relation \\(\\preccurlyeq\\) on some set \\(X\\) is called a partial order if \\(\\forall a,b,c \\in X\\) the following is true:\n\nReflexivity: \\(a \\preccurlyeq a\\)\nTransitiity: \\(a \\preccurlyeq b, b \\preccurlyeq c \\Rightarrow a \\preccurlyeq c\\)\nAntisymmetricity: \\(a \\preccurlyeq b, b \\preccurlyeq a \\Rightarrow a = b\\)\n\nIf, additionally, \\(\\forall a,b \\ in X\\) either \\(a \\preccurlyeq b\\) or \\(b \\preccurlyeq a\\), then the order is total."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-4",
    "href": "bigdata_lec4.html#program-as-dag-4",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nDefinitions\n\n\nA parallel program can be represented by a directed acyclic graph (DAG) \\[\nG=(V,E),\n\\]\nwhere \\(V\\) is a set of \\(v\\) nodes and \\(E\\) is a set of \\(e\\) directed edges.\nA node in the DAG represents a task which in turn is a set of instructions which must be executed sequentially without preemption in the same processor.\nThe weight of a node \\(n_i\\) is called the computation cost and is denoted by \\(w(n_i)\\)."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-5",
    "href": "bigdata_lec4.html#program-as-dag-5",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nDefinitions\n\n\nThe edges in the DAG, each of which is denoted by \\((n_i,n_j)\\), correspond to the communication messages and precedence constraints among the nodes.\nThe weight of an edge is called the communication cost of the edge and is denoted by \\(c(n_i, n_j)\\).\nThe source node of an edge is called the parent node while the sink node is called the child node.\nA node with no parent is called an entry node and a node with no child is called an exit node.\nThe communication-to-computation-ratio (CCR) of a parallel program is defined as its average edge weight divided by its average node weight."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-6",
    "href": "bigdata_lec4.html#program-as-dag-6",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nDefinition\n\n\nScheduling involves executing a parallel program by mapping the computation over the processors so that:\n\ncompletion time is minimized\nuse of other resources such as storage as energy is optimal."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-7",
    "href": "bigdata_lec4.html#program-as-dag-7",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nDefinitions\n\n\n\\(ST(n_i)\\) and \\(FT(n_i)\\) denote start time and finish time at some processor.\nAfter all the nodes have been scheduled, the schedule length is defined as \\(\\max_i\\left\\{FT(n_i)\\right\\}\\) across all processors.\nThe goal of scheduling is to minimize \\(\\max_i\\left\\{FT(n_i)\\right\\}\\).\nScheduling is done in such a manner that the precedence constraints among the program tasks are preserved."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-8",
    "href": "bigdata_lec4.html#program-as-dag-8",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG"
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-9",
    "href": "bigdata_lec4.html#program-as-dag-9",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG"
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-10",
    "href": "bigdata_lec4.html#program-as-dag-10",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG"
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-11",
    "href": "bigdata_lec4.html#program-as-dag-11",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nWork\n\n\nWork is defined as the number of vertices in the DAG.\nWork of a computation corresponds to the total number of operations it performs.\n\n\n\n\n\n\nSpan\n\n\nSpan is the length of the longest path in the DAG.\nSpan corresponds to the longest chain of dependencies in the computation.\n\n\n\n\n\n\nWork[make]span\n\n\nThe overall finish-time of a parallel program is commonly called the schedule length or makespan."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-1",
    "href": "bigdata_lec4.html#scheduling-1",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nTaxonomy"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-2",
    "href": "bigdata_lec4.html#scheduling-2",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nTypes\n\n\n\nStatic: the characteristics of a parallel program (such as task processing times, communication, data dependencies, and synchronization requirements) are known before program execution\nDynamic: a few assumptions about the parallel program can be made before execution, and thus, scheduling decisions have to be made on-the-fly."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-3",
    "href": "bigdata_lec4.html#scheduling-3",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nCategories\n\n\n\nJob scheduling: independent jobs are to be scheduled among the processors of a distributed computing system to optimize overall system performance\nScheduling and mapping: allocation of multiple interacting tasks of a single parallel program in order to minimize the completion time on the parallel computer system."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-model-variations",
    "href": "bigdata_lec4.html#scheduling-model-variations",
    "title": "Big Data: Dask intro",
    "section": "Scheduling model variations",
    "text": "Scheduling model variations\n\n\n\nPreemptive vs non-preemptive\n\n\n\npreemptive: execution of the task might be interrupted so that it’s allocated to a different processor\nnon-preemptive: execution must complete on a single processor\n\n\n\n\n\n\n\nParallel vs non-parallel\n\n\nParallel task requires more than one processor for its execution.\n\n\n\n\n\n\nWith vs without conditional branches\n\n\nIn conditional model, each edge in the DAG is associated with a non-zero probability that the child will be executed immediately after the parent."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-4",
    "href": "bigdata_lec4.html#scheduling-4",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nList scheduling\n\n\nThe basic idea of list scheduling is to make a scheduling list (a sequence of nodes for scheduling) by assigning them some priorities, and then repeatedly execute the following two steps until all the nodes in the graph are scheduled:\n\nRemove the first node from the scheduling list;\nAllocate the node to a processor which allows the earliest start-time."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-5",
    "href": "bigdata_lec4.html#scheduling-5",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nGreedy Scheduler\n\n\nWe say that a scheduler is greedy if whenever there is a processor available and a task ready to execute, then it assigns the task to the processor and starts running it immediately. Greedy schedulers have an important property that is summarized by the greedy scheduling principle.\n\n\n\n\n\n\nGreedy Scheduling Principle\n\n\nThe greedy scheduling principle postulates that if a computation is run on \\(P\\) processors using a greedy scheduler, then the total time (clock cycles) for running the computation is bounded by \\[\nT_P &lt; \\frac{W}{P} + S\n\\] where \\(W\\) is the work of the computation, and \\(S\\) is the span of the computation (both measured in units of clock cycles)."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-6",
    "href": "bigdata_lec4.html#scheduling-6",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nOptimality of Greedy Schedulers\n\n\nFirstly, the time to execute the computation cannot be less than \\(\\frac{W}{P}\\) clock cycles since we have a total of \\(W\\) clock cycles of work to do and the best we can possibly do is divide it evenly among the processors. Secondly, the time to execute the computation cannot be any less than \\(S\\) clock cycles, because \\(S\\) represents the longest chain of sequential dependencies. Therefore we have \\[\nT_P \\geq \\max\\left(\\frac{W}{P},S\\right).\n\\] We therefore see that a greedy scheduler does reasonably close to the best possible. In particular, \\(\\frac{W}{P} +S\\) is never more than twice \\(\\max\\left(\\frac{W}{P} ,S\\right)\\)."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-1",
    "href": "bigdata_lec4.html#scheduling-algorithms-1",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-2",
    "href": "bigdata_lec4.html#scheduling-algorithms-2",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms\n\n\n\nWhat’s a heuristic algorithm?\n\n\nAlgorithm used when classic methods are too slow for finding an exact or approximate solution, or when classic methods fail to find any exact solution in a search space.\nThis is achieved by trading\n\noptimality,\ncompleteness,\naccuracy,\nor precision\n\nfor speed.\nIn a way, it can be considered a shortcut."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-3",
    "href": "bigdata_lec4.html#scheduling-algorithms-3",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms\n\n\n\nList scheduling\n\n\nA list-scheduling heuristic maintains a list of all tasks of a given graph according to their priorities. It has two phases:\n\nthe task prioritizing or task selection phase for selecting the highest-priority ready task\nand the processor selection phase for selecting a suitable processor that minimizes a predefined cost function which can be the execution start time.\n\n\n\n\n\n\n\nFeatures\n\n\n\nfor a bounded number of fully connected homogeneous processors\nprovide better performance results at a lower scheduling time than the other groups"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-4",
    "href": "bigdata_lec4.html#scheduling-algorithms-4",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms\n\n\n\nClustering\n\n\n\nMaps the tasks to unlimited number of clusters. The selected tasks for clustering can be any task, not necessarily a ready task.\nEach iteration refines the previous clustering by merging some clusters.\nIf two tasks are assigned to the same cluster, they will be executed on the same processor."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-5",
    "href": "bigdata_lec4.html#scheduling-algorithms-5",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms\n\n\n\nClustering: Extra final steps\n\n\n\na cluster merging step for merging the clusters so that the remaining number of clusters equal the number of processors\na cluster mapping step for mapping the clusters on the available processors\na task ordering step for ordering the mapped tasks within each processor"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-6",
    "href": "bigdata_lec4.html#scheduling-algorithms-6",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms\n\n\n\nGuided random search\n\n\nGuided random search techniques (or randomized search techniques) use random choice to guide themselves through the problem space, which is not the same as performing merely random walks as in the random search methods.\nThese techniques combine the knowledge gained from previous search results with some randomizing features to generate new results."
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop",
    "href": "bigdata_lec4.html#heftcpop",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nDefinition\n\n\nHeterogeneous earliest finish time (HEFT) is a heuristic algorithm to schedule a set of dependent tasks onto a network of heterogeneous workers taking communication time into account.\nCritical-Path-On-a-Processor (CPOP) algorithm uses the summation of upward and downward rank values for prioritizing tasks.\n\n\n\n\n\nH. Topcuoglu, S. Hariri and Min-You Wu, “Performance-effective and low-complexity task scheduling for heterogeneous computing,” in IEEE Transactions on Parallel and Distributed Systems, vol. 13, no. 3, pp. 260-274, March 2002, doi: 10.1109/71.993206."
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-1",
    "href": "bigdata_lec4.html#heftcpop-1",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nDefinitions\n\n\n\n\\(V\\) – set of \\(v\\) nodes\n\\(E\\) – set of \\(e\\) edges\n\\(data\\) – a \\(v \\times v\\) matrix of communication data\n\\(data_{i,k}\\) – amount of data to be transmitted from \\(n_i\\) to \\(n_k\\)\n\\(Q\\) – set of \\(q\\) processors\n\\(W\\) – a \\(v \\times q\\) computation cost matrix, in which each \\(w_{i,j}\\) gives the estimated execution cost to complete task \\(n_i\\) on processor \\(p_j\\)\n\\(B\\) – \\(q \\times q\\) data transfer rates matrix\n\\(L\\) – \\(q\\)-dimensional vector of communication startup costs"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-2",
    "href": "bigdata_lec4.html#heftcpop-2",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nCosts\n\n\nAverage execution cost: \\[\n  \\overline{w_i} = \\sum\\limits_{j=1}^q \\frac{w_{i,j}}{q}.\n\\] Communication cost of the edge \\((i,k)\\) for transfering data from task \\(n_i\\) scheduled on processor \\(p_m\\) to task \\(n_k\\) scheduled on processor \\(p_n\\): \\[\nc_{i,k} = L_m + \\frac{data_{i,k}}{B_{m,n}}\n\\] Average communication cost: \\[\n\\overline{c_{i,k}} = \\overline{L} + \\frac{data_{i,k}}{\\overline{B}}\n\\]"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-3",
    "href": "bigdata_lec4.html#heftcpop-3",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nTimes\n\n\nEarliest execution start time of task \\(n_i\\) on processor \\(p_j\\): \\[\n\\begin{align*}\n& EST(n_{entry}, p_j) = 0, \\\\\n& EST(n_i, p_j) = \\max\\left\\{avail[j], \\max\\limits_{n_m \\in pred(n_i)} (AFT(n_m)+c_{m,i})\\right\\},\n\\end{align*}\n\\] where \\(avail[j]\\) is the earliest time at which processor \\(p_j\\) is ready for task execution.\nEarliest execution finish time of task \\(n_i\\) on processor \\(p_j\\): \\[\nEFT(n_i, p_j) = w_{i,j} + EST(n_i,p_j)\n\\]\nAfter a task \\(n_m\\) is scheduled on processor \\(p_j\\), the earliest start time and the earliest finish time of \\(n_m\\) on processor \\(p_j\\) is equal to the actual start time \\(AST(n_m)\\) and the actual finish time \\(AFT(n_m)\\), respectively."
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-4",
    "href": "bigdata_lec4.html#heftcpop-4",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nWorkspan\n\n\nAfter all tasks in the graph have been scheduled, the schedule length (makespan) will be equal to the actual finish time of the exit task \\(n_{exit}\\). In case of several exits: \\[\nmakespan = \\max \\left\\{AFT(n_{exit})\\right\\}\n\\]\n\n\n\n\n\n\nDefinition\n\n\nThe objective function of the task scheduling problem is to determine the assignment of tasks to processors such that its makespan is minimized."
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-5",
    "href": "bigdata_lec4.html#heftcpop-5",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nUpward rank\n\n\n\\[\n\\begin{align*}\n  rank_u(n_i) = \\overline{w_i} + \\max\\limits_{n_j \\in succ(n_i)} \\left(\\overline{c_{i,j}} + rank_u(n_j)\\right)\n\\end{align*}\n\\]\n\n\\(succ(n_i)\\) – set of immediate successors of \\(n_i\\)\n\\(\\overline{c_{i,j}}\\) – average communication cost of edge \\((i,j)\\)\n\\(\\overline{w_i}\\) – average computation cost of task \\(n_i\\)\n\n\\[\nrank_u(n_{exit}) = \\overline{w_{exit}}\n\\]"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-6",
    "href": "bigdata_lec4.html#heftcpop-6",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nDownward rank\n\n\n\\[\n\\begin{align*}\n  rank_d(n_i) = \\max\\limits_{n_j \\in pred(n_i)} \\left(rank_d(n_j)+ \\overline{w_j} + \\overline{c_{j,i}}\\right)\n  \\end{align*}\n\\]\n\n\\(pred(n_i)\\) – set of immediate predecessors of \\(n_i\\)\n\\(rank_d(n_{entry}) = 0\\)\ncan be thought of as the longest distance from the entry task to \\(n_i\\) without computation costs"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-7",
    "href": "bigdata_lec4.html#heftcpop-7",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nHEFT\n\n\n\nSet the computation costs of tasks and communication costs of edges with mean values\nCompute \\(rank_u\\) for all tasks by traversing graph upward, starting from the exit task\nSort the tasks in the scheduling list by non-increasing order of \\(rank_u\\) values\nwhile there are unscheduled tasks in the list do:\n   select the task \\(n_i\\) from the scheduling list\n   for each processor \\(p_k\\) in the processor set \\(Q\\) do:\n     Compute \\(EFT(n_i,p_k)\\) using the insertion-based scheduling policy\n   Assign task \\(n_i\\) to the processor \\(p_j\\) minimizing \\(EFT(n_i, p_j)\\)"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-8",
    "href": "bigdata_lec4.html#heftcpop-8",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nCPOP\n\n\n\nSet the computation costs of tasks and communication costs of edges with mean values\nCompute \\(rank_u\\) for all tasks by traversing graph upward, starting from the exit task\nCompute \\(rank_d\\) for all tasks by traversing graph downward, starting from the entry task\nCompute \\(priority(n_i) = rank_d(n_i) + rank_u(n_i) \\; \\forall n_i\\)\n\\(|CP| = priority(n_{entry})\\)\n\\(SET_{CP} = \\{n_{entry}\\}\\), where \\(SET_{CP}\\) is a set of tasks on a critical path\n\\(n_k \\leftarrow n_{entry}\\)"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-9",
    "href": "bigdata_lec4.html#heftcpop-9",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nCPOP\n\n\n\nwhile \\(n_k \\neq n_{exit}\\) do\n   select \\(n_j: n_j \\in succ(n_k) \\textbf{ and } priority(n_j) = |CP|\\)\n   \\(SET_{CP} = SET_{CP} \\cup {n_j}\\)\n   \\(n_k \\leftarrow n_j\\)\nselect \\(p_{CP}\\) minimizing \\(\\sum\\limits_{n_i \\in SET_{CP}} w_{i,j} \\; \\forall p_j \\in Q\\)\ninitialize priority queue \\(PQ\\) with the entry task"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-10",
    "href": "bigdata_lec4.html#heftcpop-10",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nCPOP\n\n\n\nwhile there are unscheduled tasks in the \\(PQ\\)do\n   select the highest priority task \\(n_i\\) from \\(PQ\\)\n   if \\(n_i \\ in SET_{CP}\\) then\n     assign \\(n_i\\) to \\(p_{CP}\\)\n   else\n     assign \\(n_i\\) to \\(p_j\\) minimizing \\(EFT(n_i,p_j)\\)\n   update \\(PQ\\) with the successors of \\(n_i\\) if ready"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-11",
    "href": "bigdata_lec4.html#heftcpop-11",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-12",
    "href": "bigdata_lec4.html#heftcpop-12",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\nHEFT - (a), CPOP - (b)"
  },
  {
    "objectID": "bigdata_lec4.html#dask",
    "href": "bigdata_lec4.html#dask",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\nDask is a library to perform parallel computation for analytics."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-7",
    "href": "bigdata_lec4.html#scheduling-7",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nWhat does Dask scheduler do?\n\n\n\nexecute DAGs on parallel hardware\nmanage resource allocation across DAG nodes"
  },
  {
    "objectID": "bigdata_lec4.html#dask-scheduling-1",
    "href": "bigdata_lec4.html#dask-scheduling-1",
    "title": "Big Data: Dask intro",
    "section": "Dask Scheduling",
    "text": "Dask Scheduling\n\n\n\nDask Scheduler types\n\n\n\nSingle-machine scheduler: This scheduler provides basic features on a local process or thread pool. This scheduler was made first and is the default. It is simple and cheap to use, although it can only be used on a single machine and does not scale\nDistributed scheduler: This scheduler is more sophisticated, offers more features, but also requires a bit more effort to set up. It can run locally or distributed across a cluster"
  },
  {
    "objectID": "bigdata_lec4.html#dag",
    "href": "bigdata_lec4.html#dag",
    "title": "Big Data: Dask intro",
    "section": "DAG",
    "text": "DAG"
  },
  {
    "objectID": "bigdata_lec4.html#dask-scheduling-2",
    "href": "bigdata_lec4.html#dask-scheduling-2",
    "title": "Big Data: Dask intro",
    "section": "Dask Scheduling",
    "text": "Dask Scheduling"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-8",
    "href": "bigdata_lec4.html#scheduling-8",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nSingle-thread scheduler\n\n\nimport dask\ndask.config.set(scheduler='synchronous')  # overwrite default with single-threaded scheduler\n\n\n\n\n\n\nNotes\n\n\n\nUseful for debugging or profiling\nNo parallelism at all"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-9",
    "href": "bigdata_lec4.html#scheduling-9",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nThread scheduler\n\n\nimport dask\ndask.config.set(scheduler='threads')  # overwrite default with threaded scheduler \n\n\n\n\n\n\nNotes\n\n\n\nSmall overhead of 50 microseconds per task\nOnly provides parallelism when executing non-Python code (because of GIL)"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-10",
    "href": "bigdata_lec4.html#scheduling-10",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nProcess scheduler\n\n\nimport dask\ndask.config.set(scheduler='processes')  # overwrite default with multiprocessing scheduler\n\n\n\n\n\n\nNotes\n\n\n\nPerformance penalties when inter-process communication is heavy\nCan provide parallelism when executing Python code"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-11",
    "href": "bigdata_lec4.html#scheduling-11",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nDistributed (local) scheduler\n\n\nfrom dask.distributed import Client\nclient = Client()\n# or\nclient = Client(processes=False)\n\n\n\n\n\n\nNotes\n\n\n\nCan be more efficient than the multiprocessing scheduler on workloads that require multiple processes\nDiagnostic dashboard and async API"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-12",
    "href": "bigdata_lec4.html#scheduling-12",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nDistributed (cluster) scheduler\n\n\n# You can swap out LocalCluster for other cluster types\n\nfrom dask.distributed import LocalCluster\nfrom dask_kubernetes import KubeCluster\n\n# cluster = LocalCluster()\ncluster = KubeCluster()  # example, you can swap out for Kubernetes\n\nclient = cluster.get_client()\n\n\n\n\n\n\nNotes\n\n\n\nCan be setup either locally, or e.g. on a pre-existing Kubernetes cluster\nDifferent cluster backends easy to swap"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-13",
    "href": "bigdata_lec4.html#scheduling-13",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling"
  },
  {
    "objectID": "bigdata_lec4.html#issues",
    "href": "bigdata_lec4.html#issues",
    "title": "Big Data: Dask intro",
    "section": "Issues",
    "text": "Issues\n\n\n\nIssues\n\n\n\nResource starvation\nWorker failures\nData loss"
  },
  {
    "objectID": "bigdata_lec4.html#dask-1",
    "href": "bigdata_lec4.html#dask-1",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nParallel\n\n\nWe refer to algorithms that use multiple cores simultaneously as parallel.\n\n\n\n\n\n\nOut-of-core\n\n\nWe refer to systems that efficiently use disk as extensions of memory as out-of-core."
  },
  {
    "objectID": "bigdata_lec4.html#dask-2",
    "href": "bigdata_lec4.html#dask-2",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nHow to execute parallel code?\n\n\n\nrepresent the structure of our program explicitly as data within the program itself\nencode task schedules programmatically within a framework"
  },
  {
    "objectID": "bigdata_lec4.html#dask-3",
    "href": "bigdata_lec4.html#dask-3",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask Graph definition\n\n\n\nA Python dictionary mapping keys to tasks or values.\nA key is any Python hashable\na value is any Python object that is not a task\na task is a Python tuple with a callable first element."
  },
  {
    "objectID": "bigdata_lec4.html#dask-4",
    "href": "bigdata_lec4.html#dask-4",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\ndef inc(i):\n  return i + 1\n\ndef add(a, b):\n  return a + b\n\nx = 1\ny = inc(x)\nz = add(y, 10)"
  },
  {
    "objectID": "bigdata_lec4.html#dask-5",
    "href": "bigdata_lec4.html#dask-5",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask"
  },
  {
    "objectID": "bigdata_lec4.html#dask-6",
    "href": "bigdata_lec4.html#dask-6",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nDictionary representation\n\n\nd = {'x': 1,\n     'y': (inc, 'x'),\n     'z': (add, 'y', 10)}"
  },
  {
    "objectID": "bigdata_lec4.html#dask-7",
    "href": "bigdata_lec4.html#dask-7",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask computation\n\n\nDask represents a computation as a directed acyclic graph of tasks with data dependencies.\nIt can be said that Dask is a specification to encode such a graph using ordinary Python data structures, namely dicts, tuples, functions, and arbitrary Python values."
  },
  {
    "objectID": "bigdata_lec4.html#dask-8",
    "href": "bigdata_lec4.html#dask-8",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n{'x': 1,\n 'y': 2,\n 'z': (add, 'x', 'y'),\n 'w': (sum, ['x', 'y', 'z'])}\n\n\n\nExamples\n\n\n\nkey: 'x', ('x', 2, 3)\ntask: (add, 'x', 'y')\ntask argument: 'x', 1, (inc, 'x'), [1, 'x', (inc, 'x')]"
  },
  {
    "objectID": "bigdata_lec4.html#dask-9",
    "href": "bigdata_lec4.html#dask-9",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nValid tasks in a Dask graph\n\n\n(add, 1, 2)\n(add, 'x' , 2)\n(add, (inc, 'x'), 2)\n(sum, [1, 2])\n(sum, ['x', (inc, 'x')])\n(np.dot, np.array([...]), np.array([...]))"
  },
  {
    "objectID": "bigdata_lec4.html#dask-10",
    "href": "bigdata_lec4.html#dask-10",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask Array\n\n\nThe dask.array submodule uses dask graphs to create a NumPy-like library that uses all of your cores and operates on datasets that do not fit in memory.\nIt does this by building up a dask graph of blocked array algorithms.\nDask array functions produce Array objects that hold on to Dask graphs. These Dask graphs use several NumPy functions to achieve the full result."
  },
  {
    "objectID": "bigdata_lec4.html#dask-11",
    "href": "bigdata_lec4.html#dask-11",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nBlocked Array Algorithms\n\n\nBlocked algorithms compute a large result like\n\n“take the sum of these trillion numbers”\n\nwith many small computations like\n\n“break up the trillion numbers into one million chunks of size one million”,\n“sum each chunk”,\n“then sum all of the intermediate sums.”\n\nThrough tricks like this we can evaluate one large problem by solving very many small problems.\nBlocked algorithm organizes a computation so that it works on contiguous chunks of data."
  },
  {
    "objectID": "bigdata_lec4.html#dask-12",
    "href": "bigdata_lec4.html#dask-12",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nBlocked Array Algorithms"
  },
  {
    "objectID": "bigdata_lec4.html#dask-13",
    "href": "bigdata_lec4.html#dask-13",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nUnblocked\n\n\nfor i in range(N):\n for k in range(N):\n   r = X[i,k]\n   for j in range(N):\n     Z[i,j] += r*Y[k,j]\n\n\n\n\n\n\nBlocked\n\n\nfor kk in range(N/B):\n for jj in range(N/B): \n   for i in range(N):\n     for k in range(kk, min(kk+B-1, N)):\n       r = X[i,k]\n       for j in range(jj, min(jj+B-1,N)):\n         Z[i,j] += r*Y[k,j]"
  },
  {
    "objectID": "bigdata_lec4.html#dask-14",
    "href": "bigdata_lec4.html#dask-14",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\n\n\n\nNote\n\n\nIt is well known that the memory hierarchy can be better utilized if scientific algorithms are blocked.\nBlocking is also known as tiling.\nIn matrix multiplication example: instead of operating on individual matrix entries, the calculation is performed on submatrices.\nB is a blocking factor."
  },
  {
    "objectID": "bigdata_lec4.html#dask-15",
    "href": "bigdata_lec4.html#dask-15",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nBlocking features\n\n\n\nBlocking is a general optimization technique for increasing the effectiveness of a memory hierarchy.\nBy reusing data in the faster level of the hierarchy, it cuts down the average access latency.\nIt also reduces the number of references made to slower levels of the hierarchy.\nBlocking is superior to optimization such as prefetching, which hides the latency but does not reduce the memory bandwidth requirement.\nThis reduction is especially im portant for multiprocessors since memory bandwidth is often the bottleneck of the system."
  },
  {
    "objectID": "bigdata_lec4.html#dask-16",
    "href": "bigdata_lec4.html#dask-16",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\nimport dask.array as da\nx = da.arange(15, chunks=(5,))"
  },
  {
    "objectID": "bigdata_lec4.html#dask-17",
    "href": "bigdata_lec4.html#dask-17",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nMetadata\n\n\nx # Array object metadata\ndask.array&lt;x-1, shape=(15,), chunks=((5, 5, 5)), dtype=int64&gt;\n\n\n\n\n\n\nDask Graph\n\n\nx.dask # Every dask array holds a dask graph\n{('x' , 0): (np.arange, 0, 5),\n ('x', 1): (np.arange, 5, 10),\n ('x' , 2): (np.arange, 10, 15)}"
  },
  {
    "objectID": "bigdata_lec4.html#dask-18",
    "href": "bigdata_lec4.html#dask-18",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nMore complex graph\n\n\nz = (x + 100).sum()\nz.dask\n{('x', 0): (np.arange, 0, 5),\n('x', 1): (np.arange, 5, 10),\n('x', 2): (np.arange, 10, 15),\n('y', 0): (add, ('x', 0), 100),\n('y', 1): (add, ('x', 1), 100),\n('y', 2): (add, ('x', 2), 100),\n('z', 0): (np.sum, ('y', 0)),\n('z', 1): (np.sum, ('y', 1)),\n('z', 2): (np.sum, ('y', 2)),\n('z',): (sum, [('z', 0), ('z', 1), ('z', 2)])}\n\n\n\n\n\n\nExecute the Graph\n\n\nz.compute()\n1605"
  },
  {
    "objectID": "bigdata_lec4.html#dask-19",
    "href": "bigdata_lec4.html#dask-19",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\ndask.array.Array objects\n\n\nx and z are both dask.array.Array objects containing:\n\nDask graph .dask\narray shape and chunk shape .chunks\na name identifying which keys in the graph correspond to the result, .name\na dtype"
  },
  {
    "objectID": "bigdata_lec4.html#dask-20",
    "href": "bigdata_lec4.html#dask-20",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nChunks\n\n\nA normal NumPy array knows its shape, a dask array must know its shape and the shape of all of the internal NumPy blocks that make up the larger array.\nThese shapes can be concisely described by a tuple of tuples of integers, where each internal tuple corresponds to the lengths along a single dimension.\nIn the example above we have a 20 by 24 array cut into uniform blocks of size 5 by 8. The chunks attribute describing this array is the following:\nchunks = ((5, 5, 5, 5), (8, 8, 8))"
  },
  {
    "objectID": "bigdata_lec4.html#dask-21",
    "href": "bigdata_lec4.html#dask-21",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nChunks need not be uniform!\n\n\nx[::2].chunks\n((3, 2, 3, 2), (8, 8, 8))\nx[::2].T.chunks\n((8, 8, 8), (3, 2, 3, 2))"
  },
  {
    "objectID": "bigdata_lec4.html#dask-22",
    "href": "bigdata_lec4.html#dask-22",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask Array operations\n\n\n\narithmetic and scalar math: +, *, exp, log\nreductions along axes: sum(), mean(), std(), sum(axis=0)\ntensor contractions / dot products / matrix multiplication: tensordot\naxis reordering / transposition: transpose\nslicing: x[:100, 500:100:-2]\nutility functions: bincount, where"
  },
  {
    "objectID": "bigdata_lec4.html#dask-23",
    "href": "bigdata_lec4.html#dask-23",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nAhead-of-time shape limitations\n\n\nx[x &gt; 0]"
  },
  {
    "objectID": "bigdata_lec4.html#dask-24",
    "href": "bigdata_lec4.html#dask-24",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\nGraph creation and graph scheduling are separate problems!\nCurrent Dask scheduler is dynamic.\n\n\n\nCurrent Dask scheduler logic\n\n\n\nA worker reports that it has completed a task and that it is ready for another.\nWe update runtime state to record the finished task,\nmark which new tasks can be run, which data can be released, etc.\nWe then choose a task to give to this worker from among the set of ready-to-run tasks. This small choice governs the macroscale performance of the scheduler."
  },
  {
    "objectID": "bigdata_lec4.html#dask-25",
    "href": "bigdata_lec4.html#dask-25",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nOut-of-core computation - which task to choose?\n\n\n\nlast in, first out\nselect tasks whose data dependencies were most recently made available.\nthis causes a behavior where long chains of related tasks trigger each other\nit forces the scheduler to finish related tasks before starting new ones.\nimplementation: a simple stack, which can operate in constant time."
  },
  {
    "objectID": "bigdata_lec4.html#dask-26",
    "href": "bigdata_lec4.html#dask-26",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask"
  },
  {
    "objectID": "bigdata_lec4.html#dask-27",
    "href": "bigdata_lec4.html#dask-27",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask"
  },
  {
    "objectID": "bigdata_lec4.html#dask-28",
    "href": "bigdata_lec4.html#dask-28",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nCollections\n\n\n\ndask.array = numpy+ threading\ndask.bag = toolz+ multiprocessing\ndask.dataframe = pandas+ threading"
  },
  {
    "objectID": "bigdata_lec4.html#dask-29",
    "href": "bigdata_lec4.html#dask-29",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask Bag - Definition\n\n\nA bag is an unordered collection with repeats.\nIt is like a Python list but does not guarantee the order of elements.\nThe dask.bag API contains functions like map and filter and generally follows the PyToolz API."
  },
  {
    "objectID": "bigdata_lec4.html#dask-30",
    "href": "bigdata_lec4.html#dask-30",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n&gt;&gt;&gt; import dask.bag as db\n&gt;&gt;&gt; import json\n&gt;&gt;&gt; b = db.from_filenames('2014-*.json.gz').map(json.loads)\n&gt;&gt;&gt; alices = b.filter(lambda d: d['name'] == 'Alice')\n&gt;&gt;&gt; alices.take(3)\n({'name': 'Alice', 'city': 'LA', 'balance': 100},\n{'name': 'Alice', 'city': 'LA', 'balance': 200},\n{'name': 'Alice', 'city': 'NYC', 'balance': 300},)\n\n&gt;&gt;&gt; dict(alices.pluck('city').frequencies())\n{'LA': 10000, 'NYC': 20000, ...}"
  },
  {
    "objectID": "bigdata_lec4.html#dask-31",
    "href": "bigdata_lec4.html#dask-31",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nS3 example\n\n\n&gt;&gt;&gt; import dask.bag as db\n&gt;&gt;&gt; b = db.from_s3('githubarchive-data', '2015-01-01-*.json.gz')\n          .map(json.loads)\n          .map(lambda d: d['type'] == 'PushEvent')\n          .count()"
  },
  {
    "objectID": "bigdata_lec4.html#dask-32",
    "href": "bigdata_lec4.html#dask-32",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask"
  },
  {
    "objectID": "bigdata_lec4.html#dask-33",
    "href": "bigdata_lec4.html#dask-33",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask DataFrame - Definition\n\n\nThe dask.dataframe module implements a large dataframe out of many Pandas DataFrames.\nIt uses a threaded scheduler."
  },
  {
    "objectID": "bigdata_lec4.html#dask-34",
    "href": "bigdata_lec4.html#dask-34",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nPartitioned datasets\n\n\nThe dask dataframe can compute efficiently on partitioned datasets where the different blocks are well separated along an index.\nFor example in time series data we may know that all of January is in one block while all of February is in another.\nJoin, groupby, and range queries along this index are significantly faster when working on partitioned datasets."
  },
  {
    "objectID": "bigdata_lec4.html#dask-35",
    "href": "bigdata_lec4.html#dask-35",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask DataFrame join"
  },
  {
    "objectID": "bigdata_lec4.html#dask-36",
    "href": "bigdata_lec4.html#dask-36",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nOut-of-core parallel SVD example\n\n\n&gt;&gt;&gt; import dask.array as da\n&gt;&gt;&gt; x = da.ones((5000, 1000), chunks=(1000, 1000))\n&gt;&gt;&gt; u, s, v = da.svd(x)\nOut-of-core parallel non-negative matrix factorizations on top of dask.array."
  },
  {
    "objectID": "bigdata_lec4.html#dask-37",
    "href": "bigdata_lec4.html#dask-37",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\n\n\n\nOut-of-core parallel SVD"
  },
  {
    "objectID": "bigdata_lec4.html#usage-1",
    "href": "bigdata_lec4.html#usage-1",
    "title": "Big Data: Dask intro",
    "section": "Usage",
    "text": "Usage\n\nscida.io - astrophysical simulations"
  },
  {
    "objectID": "bigdata_lec4.html#usage-2",
    "href": "bigdata_lec4.html#usage-2",
    "title": "Big Data: Dask intro",
    "section": "Usage",
    "text": "Usage\n\nPangeo - open, reproducible, scalable geoscience. A global slice of Sea Water Temperature"
  },
  {
    "objectID": "bigdata_lec4.html#usage-3",
    "href": "bigdata_lec4.html#usage-3",
    "title": "Big Data: Dask intro",
    "section": "Usage",
    "text": "Usage"
  },
  {
    "objectID": "bigdata_lec4.html#comparison-with-spark",
    "href": "bigdata_lec4.html#comparison-with-spark",
    "title": "Big Data: Dask intro",
    "section": "Comparison with Spark",
    "text": "Comparison with Spark\n\n\n\nSetup\n\n\n\nBigBrain20, a 3-D image of the human brain, total data size of 606 GiB.\ndataset provided by the Consortium for Reliability and Reproducibility, entire dataset is 379.83 GiB, used all 3,491 anatomical images, representing 26.67 GiB overall."
  },
  {
    "objectID": "bigdata_lec4.html#comparison-with-spark-1",
    "href": "bigdata_lec4.html#comparison-with-spark-1",
    "title": "Big Data: Dask intro",
    "section": "Comparison with Spark",
    "text": "Comparison with Spark"
  },
  {
    "objectID": "bigdata_lec4.html#comparison-with-spark-2",
    "href": "bigdata_lec4.html#comparison-with-spark-2",
    "title": "Big Data: Dask intro",
    "section": "Comparison with Spark",
    "text": "Comparison with Spark"
  },
  {
    "objectID": "bigdata_lec4.html#comparison-with-spark-3",
    "href": "bigdata_lec4.html#comparison-with-spark-3",
    "title": "Big Data: Dask intro",
    "section": "Comparison with Spark",
    "text": "Comparison with Spark"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management",
    "href": "bigdata_lec3.html#memory-management",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\n\n\nManual\n\n\n\nC/C++\nPascal\nForth\nFortran\nZig\n\n\n\n\n\n\n\n\nAutomatic\n\n\n\nLisp\nJava\nPython\nGo\nJulia"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-1",
    "href": "bigdata_lec3.html#memory-management-1",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\nCode\n\n\nimport os\nimport gc\nimport psutil\n\nproc = psutil.Process(os.getpid())\ngc.collect()\ninitial_memory = proc.memory_info().rss\n\n## Allocate memory by creating large lists\nfoo = ['abc' for _ in range(10**7)]\nallocated_memory = proc.memory_info().rss\n\n## Deallocate memory\ndel foo\ngc.collect()\nfinal_memory = proc.memory_info().rss"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-2",
    "href": "bigdata_lec3.html#memory-management-2",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\nPrint memory statistics\n\n\nincrease = lambda x2, x1: 100.0 * (x2 - x1) / initial_memory\nprint(\"Allocated Memory Increase: %0.2f%%\" % increase(allocated_memory, initial_memory))\nprint(\"Memory After Deletion: %0.2f%%\" % increase(final_memory, allocated_memory))\n\n&gt;&gt;&gt; Allocated Memory Increase: 23.35%\n&gt;&gt;&gt; Memory After Deletion: -10.78%"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-3",
    "href": "bigdata_lec3.html#memory-management-3",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\n\n\n\n\nPython internals\n\n\n\npools\nblocks\narenas\n\n\n\n\n\n\n\nhttps://docs.python.org/3/c-api/memory.html\nhttps://realpython.com/python-memory-management"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-4",
    "href": "bigdata_lec3.html#memory-management-4",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management"
  },
  {
    "objectID": "bigdata_lec3.html#python-options",
    "href": "bigdata_lec3.html#python-options",
    "title": "Big Data: Speeding up computation",
    "section": "Python Options",
    "text": "Python Options\n\n\n\n\n\n\n\n\n\nLibraries\nLow-level langs\nAlt Python Impls\nJIT\n\n\n\n\nNumPy,  SciPy\nC, Rust, Cython, PyO3\nPyPy, Jython\nNumba, PyPy\n\n\n\n\n\nOptions above are not mutually exclusive!"
  },
  {
    "objectID": "bigdata_lec3.html#interpreters",
    "href": "bigdata_lec3.html#interpreters",
    "title": "Big Data: Speeding up computation",
    "section": "Interpreters",
    "text": "Interpreters\n\n\n\nWikipedia definition\n\n\nAn interpreter is a computer program that directly executes instructions written in a programming or scripting language, without requiring them previously to have been compiled into a machine language program.\n\n\n\n\n\n\nExamples\n\n\n\nPython\nRuby\nLua\nJavascript"
  },
  {
    "objectID": "bigdata_lec3.html#cpython",
    "href": "bigdata_lec3.html#cpython",
    "title": "Big Data: Speeding up computation",
    "section": "CPython",
    "text": "CPython\n\n\n\n\nFlow\n\n\n\nRead Python code\nConvert Python into bytecode\nExecute bytecode inside a VM\nVM converts bytecode to machine code"
  },
  {
    "objectID": "bigdata_lec3.html#cpython-1",
    "href": "bigdata_lec3.html#cpython-1",
    "title": "Big Data: Speeding up computation",
    "section": "CPython",
    "text": "CPython"
  },
  {
    "objectID": "bigdata_lec3.html#compilers",
    "href": "bigdata_lec3.html#compilers",
    "title": "Big Data: Speeding up computation",
    "section": "Compilers",
    "text": "Compilers\n\n\n\nWikipedia definition\n\n\nSource code is compiled - in this context, translated into machine code for better performance.\n\n\n\n\n\n\nExamples\n\n\n\nC/C++\nGo\nPython (to intermediate VM code)\nJava\nCython"
  },
  {
    "objectID": "bigdata_lec3.html#compilers-1",
    "href": "bigdata_lec3.html#compilers-1",
    "title": "Big Data: Speeding up computation",
    "section": "Compilers",
    "text": "Compilers"
  },
  {
    "objectID": "bigdata_lec3.html#cython",
    "href": "bigdata_lec3.html#cython",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nDefinition\n\n\nCython is an optimising static compiler for the Python programming language.\n\nconverts Python code to C\nsupports static type declarations"
  },
  {
    "objectID": "bigdata_lec3.html#cython-1",
    "href": "bigdata_lec3.html#cython-1",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython"
  },
  {
    "objectID": "bigdata_lec3.html#cython-2",
    "href": "bigdata_lec3.html#cython-2",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython"
  },
  {
    "objectID": "bigdata_lec3.html#cython-3",
    "href": "bigdata_lec3.html#cython-3",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nPython code"
  },
  {
    "objectID": "bigdata_lec3.html#cython-4",
    "href": "bigdata_lec3.html#cython-4",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nAnnotated Python code"
  },
  {
    "objectID": "bigdata_lec3.html#cython-5",
    "href": "bigdata_lec3.html#cython-5",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nCython code"
  },
  {
    "objectID": "bigdata_lec3.html#parallel-cython",
    "href": "bigdata_lec3.html#parallel-cython",
    "title": "Big Data: Speeding up computation",
    "section": "Parallel Cython",
    "text": "Parallel Cython"
  },
  {
    "objectID": "bigdata_lec3.html#parallel-cython-1",
    "href": "bigdata_lec3.html#parallel-cython-1",
    "title": "Big Data: Speeding up computation",
    "section": "Parallel Cython",
    "text": "Parallel Cython"
  },
  {
    "objectID": "bigdata_lec3.html#jit",
    "href": "bigdata_lec3.html#jit",
    "title": "Big Data: Speeding up computation",
    "section": "JIT",
    "text": "JIT\n\n\n\nWikipedia definition\n\n\nA compilation (of computer code) during execution of a program (at run time) rather than before execution.\n\n\n\n\n\n\nFeatures\n\n\n\nwarm-up time: JIT causes a slight to noticeable delay in the initial execution of an application, due to the time taken to load and compile the input code.\nstatistics collection: performed by the system during runtime, shows how the program is actually running in the environment it is in; helps JIT to rearrange and recompile for optimum performance.\nparticularly suited for dynamic programming languages"
  },
  {
    "objectID": "bigdata_lec3.html#jit-1",
    "href": "bigdata_lec3.html#jit-1",
    "title": "Big Data: Speeding up computation",
    "section": "JIT",
    "text": "JIT\n\n\n\nExamples\n\n\n\nHotSpot Java Virtual Machine\nLuaJIT\nNumba\nPyPy"
  },
  {
    "objectID": "bigdata_lec3.html#numba",
    "href": "bigdata_lec3.html#numba",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba"
  },
  {
    "objectID": "bigdata_lec3.html#numba-1",
    "href": "bigdata_lec3.html#numba-1",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nDescription\n\n\n\nNumba translates Python byte-code to machine code immediately before execution to improve the execution speed.\nFor that we add a @jit decorator\nWorks well for numeric operations, NumPy, and loops"
  },
  {
    "objectID": "bigdata_lec3.html#numba-2",
    "href": "bigdata_lec3.html#numba-2",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nSteps\n\n\n\nread the Python bytecode for a decorated function\ncombine it with information about the types of the input arguments to the function\nanalyze and optimize the code\nuse the LLVM compiler library to generate a machine code version of the function, tailored to specific CPU capabilities."
  },
  {
    "objectID": "bigdata_lec3.html#numba-3",
    "href": "bigdata_lec3.html#numba-3",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nWorks great"
  },
  {
    "objectID": "bigdata_lec3.html#numba-4",
    "href": "bigdata_lec3.html#numba-4",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nNope"
  },
  {
    "objectID": "bigdata_lec3.html#numba-5",
    "href": "bigdata_lec3.html#numba-5",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba"
  },
  {
    "objectID": "bigdata_lec3.html#numpy",
    "href": "bigdata_lec3.html#numpy",
    "title": "Big Data: Speeding up computation",
    "section": "Numpy",
    "text": "Numpy\n\n\n\nWhy so fast?\n\n\n\nOptimized C code\nDensely packed arrays\nUses BLAS - Basic Linear Algebra Subroutines."
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-1",
    "href": "bigdata_lec3.html#rustpyo3-example-1",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nDescription\n\n\nWe show an example of a simple algebraic cipher that utilizes PyO3 bindings to speed up encoding/decoding.\n\n\n\n\n\n\nCipher definition\n\n\nThe basic mechanism for encrypting a message using a shared secret key is called a cipher (or encryption scheme)"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-2",
    "href": "bigdata_lec3.html#rustpyo3-example-2",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\n\n\n\nDefinition\n\n\nEncryption and decryption use the same secret key.\n\n\n\n\n\n\nExamples\n\n\n\nAES\nSalsa20\nTwofish\nDES"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-3",
    "href": "bigdata_lec3.html#rustpyo3-example-3",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nTypes\n\n\n\nblock ciphers\nstream ciphers\nhash functions"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-4",
    "href": "bigdata_lec3.html#rustpyo3-example-4",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nOverview\n\n\n\\[\nf: \\mathcal{K}\\times\\mathcal{D} \\rightarrow C\n\\] where\n\n\\(\\mathcal{K}\\) is key space\n\\(\\mathcal{D}\\) is domain (or message space)\n\\(\\mathcal{C}\\) is co-domain (or ciphertext space)"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-5",
    "href": "bigdata_lec3.html#rustpyo3-example-5",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nShannon cipher\n\n\nA Shannon cipher is a pair \\(\\mathcal{E} = (E,D)\\) of functions:\n\nThe function \\(E\\) (the encryption function) takes as input a key \\(k\\) and message \\(m\\) (also called plaintext) and produces as output a ciphertext \\(c):\\)$ c = E(k,m) $$\n\n\\(c\\) is the encryption of \\(m\\) under \\(k\\).\n\nThe function \\(D\\) (the decryption function) takes as input a key \\(k\\) and ciphertext \\(c\\), and produces a message \\(m\\): \\[\nm = D(k,c)\n\\]\n\n\\(m\\) is the decryption of \\(c\\) under \\(k\\)."
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-6",
    "href": "bigdata_lec3.html#rustpyo3-example-6",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nCorrectness property\n\n\nFor all keys \\(k\\) and messages \\(m\\), we have \\[\nD(k, E(k,m)) = m\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-7",
    "href": "bigdata_lec3.html#rustpyo3-example-7",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nParameters\n\n\nNow we describe the cipher.\nFirst, we define cipher parameters:\n\nAn alphabet \\(A\\) with size \\(L \\equiv |A|\\).\nA matrix \\(M\\) with size \\(N \\gg L\\)\n\\(\\sigma_1, \\sigma_2\\) are some permutations \\(N \\rightarrow N\\)\n\\(\\phi\\) is some bit sequence of length \\(P\\): \\(\\phi \\in \\{0,1\\}^P\\)\n\nA triple \\((\\sigma_1, \\sigma_2, \\phi)\\) will be our secret key.\nWe define each symbol \\(z\\) by a corresponding set of diagonals \\(D\\) in the matrix \\(M\\), so that \\(\\forall (x,y) \\in D: x - y = z (\\mod L)\\) (see Figure 1)."
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-8",
    "href": "bigdata_lec3.html#rustpyo3-example-8",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-9",
    "href": "bigdata_lec3.html#rustpyo3-example-9",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nEncoder flow\n\n\nSuppose we receive some text \\(T\\) containing symbols to be encoded.\nFor each \\(t_i \\in T\\), obtain its numeric representation \\(z_i\\). \\[\nz_i \\in [0,L)\n\\] Then we map each \\(z_i\\) to a pair of matrix coordinates \\((x_i, y_i)\\) such that:\n\nFirst, we pick a random \\(x_i \\in [0,N)\\) (e.g., horizontal coordinate in a matrix)\nThen, we randomly pick some \\(y_i \\in [0,N)\\) such that: \\[\nx_i - y_i = z_i (\\mod L)\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-10",
    "href": "bigdata_lec3.html#rustpyo3-example-10",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nEncoder flow\n\n\nHaving thus obtained a sequence \\(\\{(x_i, y_i), \\, i \\in [0, |T|) \\}\\), we now apply permutations \\(\\sigma_k: [0,N) \\rightarrow [0,N), \\, k=1,2\\): \\[\n\\text{ciphertext } (\\xi,\\eta) := (\\sigma_j(x),\\sigma_{j+1}(y))\n\\] where \\[\n\\sigma_j = \\begin{cases}\n\\sigma_1, \\; \\text{if}\\; \\phi_j=0,\\\\\n\\sigma_2\\; \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-11",
    "href": "bigdata_lec3.html#rustpyo3-example-11",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nDecoder flow\n\n\nBelow are steps executing during decoding phase:\n\nReceive encoded ciphertext \\(\\{(\\xi_i, \\eta_i)\\}\\).\nApply inverse permutations \\(\\sigma_j^{-1}, \\sigma_{j+1}^{-1}\\): \\[\n(x_i,y_i) = (\\sigma_j^{-1}(\\xi_i), \\sigma_{j+1}^{-1}(\\eta_i))\n\\]\nFind \\(z_i\\): \\[\nz_i = x_i - y_i \\; (\\mod L)\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-12",
    "href": "bigdata_lec3.html#rustpyo3-example-12",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nImplementation: PyO3\n\n\n\nRust bindings for Python extension modules\n\n\n\n\n\n\n\nUsers\n\n\n\nQiskit https://www.ibm.com/quantum/qiskit\nPython Cryptography package https://github.com/pyca/cryptography\nScallop https://www.scallop-lang.org\nHuggingFace Tokenizers https://github.com/huggingface/tokenizers"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-13",
    "href": "bigdata_lec3.html#rustpyo3-example-13",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nRust wrapper\n\n\n/// A Python module implemented in Rust.\n#[pymodule]\nfn cipher(m: &Bound&lt;'_, PyModule&gt;) -&gt; PyResult&lt;()&gt; {\n    m.add_function(wrap_pyfunction!(encode, m)?)?;\n    m.add_function(wrap_pyfunction!(decode, m)?)?;\n    Ok(())\n}"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-14",
    "href": "bigdata_lec3.html#rustpyo3-example-14",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nEncoding\n\n\n// Generate random permutation of N integers\nlet mut numbers: Vec&lt;usize&gt; = (0..N).collect();\nlet mut rng = thread_rng();\nnumbers.shuffle(&mut rng);\nlet permutation = numbers;\n\n// Generate pairs for each z\nlet mut pairs = Vec::with_capacity(zs.len());\nfor &z in &zs {\n    // Generate random x between 0 and N-1\n    let x = rng.gen_range(0..N);\n\n    // Compute y such that x - y = z (mod L)\n    let y = if x &gt;= z {\n        (x - z) % L\n    } else {\n        ((x + L) - z) % L\n    };\n\n    // Apply permutation to x and y\n    let px = permutation[x];\n    let py = permutation[y];\n    pairs.push((px, py));"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-15",
    "href": "bigdata_lec3.html#rustpyo3-example-15",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nDecoding\n\n\n// Create inverse permutation\nlet mut inverse = vec![0; N];\nfor (i, &p) in permutation.iter().enumerate() {\n    inverse[p] = i;\n}\n\n// Recover z for each pair\nlet mut zs = Vec::with_capacity(pairs.len());\nfor &(px, py) in pairs {\n    // Apply inverse permutation to get x and y\n    let x = inverse[px];\n    let y = inverse[py];\n\n    // Compute z = x - y (mod L)\n    let z = if x &gt;= y {\n        (x - y) % L\n    } else {\n        ((x + L) - y) % L\n    };\n    zs.push(z);\n}"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-16",
    "href": "bigdata_lec3.html#rustpyo3-example-16",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nPython\n\n\nmkdir cipher; cd cipher\npython -m venv venv\n. venv/bin/activate\n!pip install maturin\nmaturin init\nmaturin develop\n\n\n\n\n\n\nUsage\n\n\nimport cipher\nencoded = cipher.encode(\"Whazzuuupppp??!!\")\ndecoded = cipher.decode(encoded)"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-1",
    "href": "bigdata_lec3.html#distributed-computing-1",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nTypes\n\n\n\nCluster computing: collection of similar workstations\nGrid computing: federation of different computer systems\nCloud computing: provide the facilities to dynamically construct an infrastructure and compose what is needed from available services. Not only providing lots of resources."
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-2",
    "href": "bigdata_lec3.html#distributed-computing-2",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nOriginal Beowulf cluster at NASA (1994)"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-3",
    "href": "bigdata_lec3.html#distributed-computing-3",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nBeowulf cluster diagram"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-4",
    "href": "bigdata_lec3.html#distributed-computing-4",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\n\n\nGrid architecture diagram (Foster et al. 2001)\n\n\n\n\n\n\n\n\n\n\nNotation\n\n\n\n\nfabric: interfaces to local resources at a specific site\nconnectivity: communication protocols for supporting grid transactions that span the usage of multiple resources\nresource: responsible for managing a single resource\ncollective: handling access to multiple resources and typically consists of services for resource discovery, allocation and scheduling of tasks onto multiple resources, data replication, and so on\napplication: applications that operate within a virtual organization"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-5",
    "href": "bigdata_lec3.html#distributed-computing-5",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nCloud architecture"
  },
  {
    "objectID": "bigdata_lab2.html",
    "href": "bigdata_lab2.html",
    "title": "Big Data Analytics: Lab 2",
    "section": "",
    "text": "First, install Cython and Numba:\nuv add cython\nuv add numba\nThen we can load Cython extension in Jupyter:\n\n%load_ext Cython\n\nDefine a function that generates a list of first \\(n\\) primes:\n\n\n\n\n\n\nImportantPure Python\n\n\n\n\ndef primes1(nb_primes):\n    p = [0] * 1000\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nImportantPure Python results\n\n\n\nWe can measure its execution time:\n\n%timeit primes1(100)\n\n193 μs ± 3.49 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nNow, define a function using Cython type annotations. Note that we use %%cython magic before function definition:\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\n\n%%cython\ndef primes2(nb_primes: cython.int):\n    i: cython.int\n    p: cython.int[1000] = [0] * 1000\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p: cython.int = 0  # The current number of elements in p.\n    n: cython.int = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nNotePython with Cython type hints: results\n\n\n\nWe measure its execution time:\n\n%timeit primes2(100)\n\n82.2 μs ± 748 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nDifferent syntax:\n\n\n\n\n\n\nTipCython syntax\n\n\n\n\n%%cython\ndef primes3(int nb_primes):\n    cdef int n, i, len_p\n    cdef int[1000] p\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nTipCython syntax results\n\n\n\nWe measure its execution time:\n\n%timeit primes3(100)\n\n4.32 μs ± 131 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPassing --annotate flag will produce more relevant info."
  },
  {
    "objectID": "bigdata_lab2.html#cython",
    "href": "bigdata_lab2.html#cython",
    "title": "Big Data Analytics: Lab 2",
    "section": "",
    "text": "First, install Cython and Numba:\nuv add cython\nuv add numba\nThen we can load Cython extension in Jupyter:\n\n%load_ext Cython\n\nDefine a function that generates a list of first \\(n\\) primes:\n\n\n\n\n\n\nImportantPure Python\n\n\n\n\ndef primes1(nb_primes):\n    p = [0] * 1000\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nImportantPure Python results\n\n\n\nWe can measure its execution time:\n\n%timeit primes1(100)\n\n193 μs ± 3.49 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nNow, define a function using Cython type annotations. Note that we use %%cython magic before function definition:\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\n\n%%cython\ndef primes2(nb_primes: cython.int):\n    i: cython.int\n    p: cython.int[1000] = [0] * 1000\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p: cython.int = 0  # The current number of elements in p.\n    n: cython.int = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nNotePython with Cython type hints: results\n\n\n\nWe measure its execution time:\n\n%timeit primes2(100)\n\n82.2 μs ± 748 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nDifferent syntax:\n\n\n\n\n\n\nTipCython syntax\n\n\n\n\n%%cython\ndef primes3(int nb_primes):\n    cdef int n, i, len_p\n    cdef int[1000] p\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nTipCython syntax results\n\n\n\nWe measure its execution time:\n\n%timeit primes3(100)\n\n4.32 μs ± 131 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPassing --annotate flag will produce more relevant info."
  },
  {
    "objectID": "bigdata_lab2.html#using-cython-in-pandas",
    "href": "bigdata_lab2.html#using-cython-in-pandas",
    "title": "Big Data Analytics: Lab 2",
    "section": "Using Cython in Pandas",
    "text": "Using Cython in Pandas\nDefine a random DataFrame:\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {\n        \"a\": np.random.randn(1000),\n        \"b\": np.random.randn(1000),\n        \"N\": np.random.randint(100, 1000, (1000)),\n        \"x\": \"x\",\n    }\n)\n\nDefine functions that will be applied to the DataFrame:\n\n\n\n\n\n\nImportantPure Python\n\n\n\n\ndef f(x):\n    return x * (x - 1)\n\n\ndef integrate_f(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f(a + i * dx)\n    return s * dx\n\n\n\n\n\n\n\n\n\nImportantPure Python results\n\n\n\nApply functions to DataFrame:\n\n%timeit df.apply(lambda x: integrate_f(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n\n24.8 ms ± 85.3 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nNow let’s use Cython-annotated functions:\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\n\n%%cython\ndef f_typed2(x: cython.double) -&gt; cython.double:\n    return x * (x - 1)\n\ndef integrate_f_typed2(a: cython.double, b: cython.double, N: cython.int) -&gt; cython.double:\n    i: cython.int\n    s: cython.double\n    dx: cython.double\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f_typed2(a + i * dx)\n    return s * dx\n\n\n\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\nApply annotated functions to DataFrame:\n\n%timeit df.apply(lambda x: integrate_f_typed2(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n\n26 ms ± 118 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nWith a different syntax:\n\n\n\n\n\n\nTipCython syntax\n\n\n\n\n%%cython\ncdef double f_typed(double x) except? -2:\n   return x * (x - 1)\ncpdef double integrate_f_typed(double a, double b, int N):\n   cdef int i\n   cdef double s, dx\n   s = 0\n   dx = (b - a) / N\n   for i in range(N):\n       s += f_typed(a + i * dx)\n   return s * dx\n\n\n\n\n\n\n\n\n\nTipCython syntax results\n\n\n\n\n%timeit df.apply(lambda x: integrate_f_typed(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n\n2.95 ms ± 24.5 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n\n\n\n\n\nNotePandas\n\n\n\nRead more about type annotations for Pandas: https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html."
  },
  {
    "objectID": "bigdata_lab2.html#numba",
    "href": "bigdata_lab2.html#numba",
    "title": "Big Data Analytics: Lab 2",
    "section": "Numba",
    "text": "Numba\nEither add engine=\"numba\" parameter to Pandas functions, or add a @jit annotation to Python funcs.\n\n\n\n\n\n\nWarningNumba-annotated code\n\n\n\n\nimport numba\n\n\n@numba.jit\ndef f_numba(x):\n    return x * (x - 1)\n\n\n@numba.jit\ndef integrate_f_numba(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f_numba(a + i * dx)\n    return s * dx\n\n\n@numba.jit\ndef apply_integrate_f_numba(col_a, col_b, col_N):\n    n = len(col_N)\n    result = np.empty(n, dtype=\"float64\")\n    assert len(col_a) == len(col_b) == n\n    for i in range(n):\n        result[i] = integrate_f_numba(col_a[i], col_b[i], col_N[i])\n    return result\n\n\ndef compute_numba(df):\n    result = apply_integrate_f_numba(\n        df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy()\n    )\n    return pd.Series(result, index=df.index, name=\"result\")\n\n\n\n\n\n\n\n\n\nWarningNumba Results\n\n\n\n\n%timeit compute_numba(df)\n\n380 μs ± 27.6 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "bigdata_lab2.html#exercises",
    "href": "bigdata_lab2.html#exercises",
    "title": "Big Data Analytics: Lab 2",
    "section": "Exercises",
    "text": "Exercises\n\nApply Cython/Numba optimizations to some computations on your dataframe from Lab 1.\nGo through the tutorial on Black-Scholes option pricing (https://louis-finegan.github.io/2024/10/10/Black-Scholes.html). Modify the code so that:\n\n\nit works for some different company\nhas 2 versions: using Cython and Numba\nmeasure results"
  },
  {
    "objectID": "bigdata_lab2.html#references",
    "href": "bigdata_lab2.html#references",
    "title": "Big Data Analytics: Lab 2",
    "section": "References",
    "text": "References\n\nCython tutorial\nCython build instructions\nCython language\nPandas optimization"
  },
  {
    "objectID": "bigdata_lec1.html#what-is-big-data",
    "href": "bigdata_lec1.html#what-is-big-data",
    "title": "Big Data: Intro",
    "section": "What is Big Data?",
    "text": "What is Big Data?\n\n\n\nDefinition\n\n\nBig Data is a set of technologies designed to store, manage and analyze data that is:\n\ntoo large to fit on a single machine\nwhile accommodating for the issue of growing discrepancy between capacity, throughput and latency."
  },
  {
    "objectID": "bigdata_lec1.html#prefixes",
    "href": "bigdata_lec1.html#prefixes",
    "title": "Big Data: Intro",
    "section": "Prefixes",
    "text": "Prefixes\n\n\n\nPrefixes\n\n\n\nkilo (k) 1,000 (3 zeros)\nMega (M) 1,000,000 (6 zeros)\nGiga (G) 1,000,000,000 (9 zeros)\nTera (T) 1,000,000,000,000 (12 zeros)\nPeta (P) 1,000,000,000,000,000 (15 zeros)\nExa (E) 1,000,000,000,000,000,000 (18 zeros)\nZetta (Z) 1,000,000,000,000,000,000,000 (21 zeros)\nYotta (Y) 1,000,000,000,000,000,000,000,000 (24 zeros)\nRonna (R) 1,000,000,000,000,000,000,000,000,000 (27 zeros)\nQuetta (Q) 1,000,000,000,000,000,000,000,000,000,000 (30 zeros)"
  },
  {
    "objectID": "bigdata_lec1.html#total-estimate",
    "href": "bigdata_lec1.html#total-estimate",
    "title": "Big Data: Intro",
    "section": "Total estimate",
    "text": "Total estimate\n\n\n\nEstimate\n\n\nThe total amount of data stored digitally worldwide is estimated to be getting close to 100 ZB as of 2021 (zettabytes)"
  },
  {
    "objectID": "bigdata_lec1.html#three-vs",
    "href": "bigdata_lec1.html#three-vs",
    "title": "Big Data: Intro",
    "section": "Three Vs",
    "text": "Three Vs\n\n\n\nVolume\nVariety\nVelocity"
  },
  {
    "objectID": "bigdata_lec1.html#volume",
    "href": "bigdata_lec1.html#volume",
    "title": "Big Data: Intro",
    "section": "Volume",
    "text": "Volume\n\n\n\nIssue\n\n\nData volume has exponentially increased in recent decades.\n\n\n\n\n\n\nWait but why?\n\n\n\nInternet-of-Things sensor data\nSocial networks\nStorage device progress"
  },
  {
    "objectID": "bigdata_lec1.html#variety",
    "href": "bigdata_lec1.html#variety",
    "title": "Big Data: Intro",
    "section": "Variety",
    "text": "Variety\n\n\n\nTypes\n\n\n\ntrees - XML, JSON, Parquet, Avro, etc\nunstructured - text, pictures, audio, video\ndata cubes\ngraphs"
  },
  {
    "objectID": "bigdata_lec1.html#velocity",
    "href": "bigdata_lec1.html#velocity",
    "title": "Big Data: Intro",
    "section": "Velocity",
    "text": "Velocity\n\n\n\nDefinion\n\n\nSpeed at which data is being generated, collected, and processed.\n\n\n\n\n\n\nAttributes\n\n\n\nCapacity: how much data can we store per unit of volume?\nThroughput: how many bytes can we read per unit of time?\nLatency: how much time do we need to wait until the bytes start arriving?"
  },
  {
    "objectID": "bigdata_lec1.html#velocity-1",
    "href": "bigdata_lec1.html#velocity-1",
    "title": "Big Data: Intro",
    "section": "Velocity",
    "text": "Velocity\nEvolution since 1950s"
  },
  {
    "objectID": "bigdata_lec1.html#features",
    "href": "bigdata_lec1.html#features",
    "title": "Big Data: Intro",
    "section": "Features",
    "text": "Features\n\n\n\nFeatures\n\n\n\nReliability\nScalability\nMaintainability"
  },
  {
    "objectID": "bigdata_lec1.html#reliability",
    "href": "bigdata_lec1.html#reliability",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nKleppmann’s definition\n\n\nThe system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity\n\nhardware faults\nsoftware faults\nand even human error"
  },
  {
    "objectID": "bigdata_lec1.html#reliability-1",
    "href": "bigdata_lec1.html#reliability-1",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nFaults\n\n\nBasically, theses are things that could go wrong.\nSystems that can anticipate faults are called fault-tolerant or resilient.\nFault can be defined as one component of the system deviating from the spec.\n\n\n\n\n\n\nFailures\n\n\nFailures occur when system stops providing services to the user.\n\n\n\nFaults might degenerate into failures."
  },
  {
    "objectID": "bigdata_lec1.html#reliability-2",
    "href": "bigdata_lec1.html#reliability-2",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nTypes of errors\n\n\n\nHardware\nSoftware\nHuman"
  },
  {
    "objectID": "bigdata_lec1.html#reliability-3",
    "href": "bigdata_lec1.html#reliability-3",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability"
  },
  {
    "objectID": "bigdata_lec1.html#scalability",
    "href": "bigdata_lec1.html#scalability",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nKleppmann\n\n\nAs the system grows (in data volume, traffic volume, or complexity), there should be reasonable ways of dealing with that growth.\nIn other words, scalability is a system’s ability to cope with increased load.\n\n\n\n\n\nNote that scalability is a multi-dimensional term. When saying “system scales well”, it’s important to state exactly along which axis."
  },
  {
    "objectID": "bigdata_lec1.html#scalability-1",
    "href": "bigdata_lec1.html#scalability-1",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nWhat is load?\n\n\nLoad is described by load parameters. These might include:\n\ndata set size\ndata write speed\ndata read speed\ncomputational complexity\netc…"
  },
  {
    "objectID": "bigdata_lec1.html#scalability-2",
    "href": "bigdata_lec1.html#scalability-2",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nPerformance\n\n\nIncreasing load affects performance. There are several meanings to this term:\n\nthroughput – time required to process a dataset of certain size\nresponse time – time between sending a request and receiving a response\nlatency – duration of waiting for a request to be processed. Included in response time.\n\n\n\n\n\n\nPerformance might be more strictly defined by service level objectives (SLOs) and service level agreements (SLAs)."
  },
  {
    "objectID": "bigdata_lec1.html#scalability-3",
    "href": "bigdata_lec1.html#scalability-3",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nHow to deal with load\n\n\n\nvertical scaling - scaling up\nhorizontal scaling - scaling out\narchitectural changes\n\n\n\n\n\n\n\nElasticity\n\n\nAn approach to load handling whereby a system automatically adds resources in case of load increase, and can decrease resources if load decreases."
  },
  {
    "objectID": "bigdata_lec1.html#scalability-4",
    "href": "bigdata_lec1.html#scalability-4",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nCommon wisdom\n\n\n\nKeep your database on a single node (scale up) until scaling cost or high-availability requirements forces you to make it distributed.\nOptimize code so that it can run on a single node."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability",
    "href": "bigdata_lec1.html#maintainability",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nKleppmann\n\n\nOver time, many different people will work on the system\n\nengineering\noperations\nboth maintaining current behavior and adapting the system to new use cases),\n\nand they should all be able to work on it productively."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-1",
    "href": "bigdata_lec1.html#maintainability-1",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nPrinciples\n\n\n\nOperability – make it easy for operations teams to keep the system running smoothly.\nSimplicity – make it easy for new engineers to understand the system, by removing as much complexity as possible from the system.\nEvolvability – Make it easy for engineers to make changes to the system in the future, adapting it for unanticipated use cases as requirements change. Also known as extensibility, modifiability, or plasticity."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-2",
    "href": "bigdata_lec1.html#maintainability-2",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nOperability\n\n\n\nHealth monitoring\nGood deployment practices\nConfiguration management\nVisibility into the internals of the system\nKnowledge preservation – documentation (!).\netc…"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-3",
    "href": "bigdata_lec1.html#maintainability-3",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nComplexity symptoms\n\n\n\nLots of hidden state\nLoose cohesion, tight coupling\nBad naming (!)\nUnnecessary hacks\netc…"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity",
    "href": "bigdata_lec1.html#maintainability-complexity",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity-1",
    "href": "bigdata_lec1.html#maintainability-complexity-1",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity\n\n\n\nTypes\n\n\n\nincidental\naccidental"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity-2",
    "href": "bigdata_lec1.html#maintainability-complexity-2",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity\n\n\n\nIncidental\n\n\n\nEasy things can be complex. There can be complex constructs that are succinctly described, familiar, available and easy to use. That is incidental complexity.\n\nRich Hickey talk “Simple made easy”: https://www.youtube.com/watch?v=SxdOUGdseq4"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity-3",
    "href": "bigdata_lec1.html#maintainability-complexity-3",
    "title": "Big Data: Intro",
    "section": "Maintainability: complexity",
    "text": "Maintainability: complexity\nHowever: Complexity is often caused by\n\n\n\nAccidental complexity\n\n\nMoseley and Marks define complexity as accidental if it is not inherent in the problem that the software solves (as seen by the users) but arises only from the implementation.\n\n\n\n\n\n\nHow to remove?\n\n\nBy providing proper abstractions."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-abstractions",
    "href": "bigdata_lec1.html#maintainability-abstractions",
    "title": "Big Data: Intro",
    "section": "Maintainability: abstractions",
    "text": "Maintainability: abstractions\n\n\n\n\n\n\nDefinition (Ousterhout)\n\n\nAn abstraction is a simplified view of an entity, which omits unimportant details.\nIn modular programming, each module provides an abstraction in the form of its interface."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-abstractions-1",
    "href": "bigdata_lec1.html#maintainability-abstractions-1",
    "title": "Big Data: Intro",
    "section": "Maintainability: abstractions",
    "text": "Maintainability: abstractions\n\n\n\n\n\n\nWhat can abstractions do?\n\n\n\nHide implementation details\nProvide reusable building blocks"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-4",
    "href": "bigdata_lec1.html#maintainability-4",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nEvolvability\n\n\nOne needs to adapt their big data system to possible future requirements changes.\nHowever, keep in mind the following:\n\nInability to foresee exact nature of changes\nNeed to strike the balance of flexibility and fitness for a particular task"
  },
  {
    "objectID": "bigdata_lec1.html#types-of-big-data-analytics",
    "href": "bigdata_lec1.html#types-of-big-data-analytics",
    "title": "Big Data: Intro",
    "section": "Types of big data analytics",
    "text": "Types of big data analytics\n\n\n\nTypes\n\n\n\nPrescriptive\nDiagnostic\nDescriptive\nPredictive"
  },
  {
    "objectID": "bigdata_lec1.html#types-prescriptive",
    "href": "bigdata_lec1.html#types-prescriptive",
    "title": "Big Data: Intro",
    "section": "Types: Prescriptive",
    "text": "Types: Prescriptive\n\n\n\nPrescriptive\n\n\n\nForward looking\nOptimal decisions for future situations"
  },
  {
    "objectID": "bigdata_lec1.html#types-diagnostic",
    "href": "bigdata_lec1.html#types-diagnostic",
    "title": "Big Data: Intro",
    "section": "Types: Diagnostic",
    "text": "Types: Diagnostic\n\n\n\nDiagnostic\n\n\n\nBackward looking\nFocused on causal relationships"
  },
  {
    "objectID": "bigdata_lec1.html#types-descriptive",
    "href": "bigdata_lec1.html#types-descriptive",
    "title": "Big Data: Intro",
    "section": "Types: Descriptive",
    "text": "Types: Descriptive\n\n\n\nDescriptive\n\n\n\nBackward looking\nFocused on descriptions and comparisons"
  },
  {
    "objectID": "bigdata_lec1.html#types-descriptive-1",
    "href": "bigdata_lec1.html#types-descriptive-1",
    "title": "Big Data: Intro",
    "section": "Types: Descriptive",
    "text": "Types: Descriptive\n\n\n\nPredictive\n\n\n\nForward looking\nFocused on the prediction of future states, relationship, and patterns"
  },
  {
    "objectID": "bigdata_lec1.html#challenges",
    "href": "bigdata_lec1.html#challenges",
    "title": "Big Data: Intro",
    "section": "Challenges",
    "text": "Challenges\nThere are 2 main challenges associated with Big Data.\n\n\n\nChallenges\n\n\n\nhow do we store and manage such a huge volume of data efficiently?\nhow do we process and extract valuable information from the data within the given time frame?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "nb/lab8/Lab8.html",
    "href": "nb/lab8/Lab8.html",
    "title": "Lab8",
    "section": "",
    "text": "Fetch taxi zones and yellow taxi trip records from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page. Pick an arbitrary month from 2023.\nOpen taxi zones as a GeoPandas shapefile. Visualize taxi zones in different colors (heatmap), depending on the value of Shape_Area.\nCalculate centroids of each taxi zone.\nOpen trip records using Pandas’s read_parquet(). Then:\n\nConvert it into GeoPandas DataFrame by calculating each trip’s geometry as a LineString between PULocationID (pick-up location) and DOLocationID (drop-off location)\nUse centroids calculated in the previous step as Points on the LineString.\n\nDraw heatmap of drop-off zones depending on:\n\nthe average tip amount\ntotal amount for all trips\ngeneralize the above 2 calculations for boroughs\n\nCompare GeoPandas and Dask-GeoPandas performance for these calculations."
  },
  {
    "objectID": "nb/lab1/Lab1.html",
    "href": "nb/lab1/Lab1.html",
    "title": "Pandas optimization",
    "section": "",
    "text": "Primary docs: - https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html#numba-jit-compilation - https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html\nPerformance deps for Pandas: - https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#install-recommended-dependencies\nUse Numba for JIT optimizations.\nhttps://github.com/modin-project/modin"
  },
  {
    "objectID": "nb/lab1/Lab1.html#notes",
    "href": "nb/lab1/Lab1.html#notes",
    "title": "Pandas optimization",
    "section": "Notes",
    "text": "Notes\n\nChunking\nchunksize parameter in Pandas functions.\n\n\nData types\nUse to_numeric (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html) for downcasting\n\n\nCategoricals\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n\n\nMemory usage\nUse DataFrame.memory_usage(deep=True) func. Use DataFrame.info() func"
  },
  {
    "objectID": "nb/lab1/Lab1.html#storage-optimization",
    "href": "nb/lab1/Lab1.html#storage-optimization",
    "title": "Pandas optimization",
    "section": "Storage optimization",
    "text": "Storage optimization\nWe’ll get the sample dataset from https://www.kaggle.com/datasets/anthonytherrien/depression-dataset.\n\nimport pandas as pd\n\ndd = pd.read_csv('depression_data.csv')\ndd.head()\n\n\n\n\n\n\n\n\nName\nAge\nMarital Status\nEducation Level\nNumber of Children\nSmoking Status\nPhysical Activity Level\nEmployment Status\nIncome\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\n0\nChristine Barker\n31\nMarried\nBachelor's Degree\n2\nNon-smoker\nActive\nUnemployed\n26265.67\nModerate\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n1\nJacqueline Lewis\n55\nMarried\nHigh School\n1\nNon-smoker\nSedentary\nEmployed\n42710.36\nHigh\nUnhealthy\nFair\nYes\nNo\nNo\nYes\n\n\n2\nShannon Church\n78\nWidowed\nMaster's Degree\n1\nNon-smoker\nSedentary\nEmployed\n125332.79\nLow\nUnhealthy\nGood\nNo\nNo\nYes\nNo\n\n\n3\nCharles Jordan\n58\nDivorced\nMaster's Degree\n3\nNon-smoker\nModerate\nUnemployed\n9992.78\nModerate\nModerate\nPoor\nNo\nNo\nNo\nNo\n\n\n4\nMichael Rich\n18\nSingle\nHigh School\n0\nNon-smoker\nSedentary\nUnemployed\n8595.08\nLow\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n\n\n\n\n\nLet’s see some memory usage stats:\n\ndd.info(memory_usage='deep')\n\nNote: Pandas stores memory in blocks, managed by BlockManager class. There is a separate block class for each type, like ObjectBlock or FloatBlock.\nNice write-up here: https://uwekorn.com/2020/05/24/the-one-pandas-internal.html\nLet’s examine memory usage for each type:\n\nfor dtype in ['float','int','object']:\n    selected_dtype = dd.select_dtypes(include=[dtype])\n    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n    mean_usage_mb = mean_usage_b / 1024 ** 2\n    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))\n\n\nNumeric types optimization\nLet’s first use iinfo to check ranges for different subtypes:\n\nimport numpy as np\nint_types = [\"uint8\", \"int8\", \"int16\", \"int32\", \"int64\"]\nfor it in int_types:\n    print(np.iinfo(it))\n\nWe can use pd.to_numeric() to downcast numeric types.\nFirst, let’s write a helper function for memory usage display:\n\ndef mem_usage(pandas_obj):\n    if isinstance(pandas_obj,pd.DataFrame):\n        usage_b = pandas_obj.memory_usage(deep=True).sum()\n    else: # we assume if not a df it's a series\n        usage_b = pandas_obj.memory_usage(deep=True)\n    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n    return \"{:03.2f} MB\".format(usage_mb)\n\nNote that “Age” and “Number of Children” columns can be presented as unsigned ints. Let’s convert:\n\ndd_int = dd.select_dtypes(include=['int'])\nconverted_int = dd_int.apply(pd.to_numeric,downcast='unsigned')\nprint(mem_usage(dd_int))\nprint(mem_usage(converted_int))\ncompare_ints = pd.concat([dd_int.dtypes,converted_int.dtypes],axis=1)\ncompare_ints.columns = ['before','after']\ncompare_ints.apply(pd.Series.value_counts)\n\nNice. Now let’s process float columns (Income):\n\ndd_float = dd.select_dtypes(include=['float'])\nconverted_float = dd_float.apply(pd.to_numeric,downcast='float')\nprint(mem_usage(dd_float))\nprint(mem_usage(converted_float))\ncompare_floats = pd.concat([dd_float.dtypes,converted_float.dtypes],axis=1)\ncompare_floats.columns = ['before','after']\ncompare_floats.apply(pd.Series.value_counts)\n\nNow let’s create a new optimized DataFrame:\n\noptimized_dd = dd.copy()\noptimized_dd[converted_int.columns] = converted_int\noptimized_dd[converted_float.columns] = converted_float\nprint(mem_usage(dd))\nprint(mem_usage(optimized_dd))\n\nJust a bit. Let’s proceed with object types.\nFirst, read https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/.\nNow, back to the dataset.\nWe can use categoricals (https://pandas.pydata.org/pandas-docs/stable/categorical.html) to optimize object columns.\n\n\n\ncategoricals\n\n\nLet’s look at the number of unique values for each object type:\n\ndd_obj = dd.select_dtypes(include=['object']).copy()\ndd_obj.describe()\n\nLet’s start with one column first: “Education Level”\n\nedu_level = dd_obj[\"Education Level\"]\nprint(edu_level.head())\nedu_level_cat = edu_level.astype('category')\nprint(edu_level_cat.head())\n\nWe can look at the category codes:\n\nedu_level_cat.head().cat.codes\n\nCompare memory usage:\n\nprint(mem_usage(edu_level))\nprint(mem_usage(edu_level_cat))\n\nWe should only convert objects to categoricals if most values are repeated. We have to pick a threshold, say, 25% of unique values.\nNote: when reading a csv, we can also provide a dtype dictionary param with preferred types.\nConverting all object columns and creating a new optimized DataFrame is left as part of Exercise 1."
  },
  {
    "objectID": "nb/lab1/Lab1.html#computation-optimization",
    "href": "nb/lab1/Lab1.html#computation-optimization",
    "title": "Pandas optimization",
    "section": "Computation optimization",
    "text": "Computation optimization\n\nCython\n\nimport math\ndef process(x):\n    if x &lt;= 9:\n        return x\n    lst = [int(i) for i in str(x).replace(\".\", \"\")]\n    result = 0\n    for el in lst:\n        result = result + el\n    return process(result)\n\n\nprocess(123688)\n\nObtain timing:\n\n%timeit dd.apply(lambda x: process(x[\"Income\"]), axis=1)\n\nSee more detailed breakdown:\n\n%prun -l 4 dd.apply(lambda x: process(x[\"Income\"]), axis=1)\n\n\n%load_ext Cython\n\n\n%%cython\ndef process_cython(x):\n    if x &lt;= 9:\n        return x\n    lst = [int(i) for i in str(x).replace(\".\", \"\")]\n    result = 0\n    for el in lst:\n        result = result + el\n    return process_cython(result)\n\n\n%timeit dd.apply(lambda x: process_cython(x[\"Income\"]), axis=1)\n\n\n%prun -l 8 dd.apply(lambda x: process_cython(x[\"Income\"]), axis=1)\n\nRead more about type annotations: https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html#declaring-c-types\n\n\nNumba\nEither add engine=\"numba\" parameter to Pandas functions, or add a @jit annotation to Python funcs.\n\nimport numba\n\n@numba.jit\ndef process_jit(x):\n    if x &lt;= 9:\n        return x\n    char_lst = [i for i in str(x).replace(\".\", \"\")]\n    result = 0\n    for el in char_lst:\n        i = int(el)\n        result = result + i\n    return process_jit(result)\n\n\n%prun -l 4 dd.apply(lambda x: process_jit(x[\"Income\"]), axis=1)"
  },
  {
    "objectID": "nb/lab1/Lab1.html#problems",
    "href": "nb/lab1/Lab1.html#problems",
    "title": "Pandas optimization",
    "section": "Problems",
    "text": "Problems\n\nPick your own dataset from Kaggle or HuggingFace or https://archive.ics.uci.edu. Perform numerical and object types conversions aimed at minimizing storage space, as outlined above.\nFix the Numba issue related to missing int() (more context here: https://github.com/numba/numba/issues/5650)"
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#a-typical-workflow",
    "href": "nb/lab6/03_dask.delayed.html#a-typical-workflow",
    "title": "dask.delayed - parallelize any code",
    "section": "A Typical Workflow",
    "text": "A Typical Workflow\nTypically if a workflow contains a for-loop it can benefit from delayed. The following example outlines a read-transform-write:\nimport dask\n    \n@dask.delayed\ndef process_file(filename):\n    data = read_a_file(filename)\n    data = do_a_transformation(data)\n    destination = f\"results/{filename}\"\n    write_out_data(data, destination)\n    return destination\n\nresults = []\nfor filename in filenames:\n    results.append(process_file(filename))\n    \ndask.compute(results)"
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#basics",
    "href": "nb/lab6/03_dask.delayed.html#basics",
    "title": "dask.delayed - parallelize any code",
    "section": "Basics",
    "text": "Basics\nFirst let’s make some toy functions, inc and add, that sleep for a while to simulate work. We’ll then time running these functions normally.\nIn the next section we’ll parallelize this code.\n\nfrom time import sleep\n\n\ndef inc(x):\n    sleep(1)\n    return x + 1\n\n\ndef add(x, y):\n    sleep(1)\n    return x + y\n\nWe time the execution of this normal code using the %%time magic, which is a special function of the Jupyter Notebook.\n\n%%time\n# This takes three seconds to run because we call each\n# function sequentially, one after the other\n\nx = inc(1)\ny = inc(2)\nz = add(x, y)\n\n\nParallelize with the dask.delayed decorator\nThose two increment calls could be called in parallel, because they are totally independent of one-another.\nWe’ll make the inc and add functions lazy using the dask.delayed decorator. When we call the delayed version by passing the arguments, exactly as before, the original function isn’t actually called yet - which is why the cell execution finishes very quickly. Instead, a delayed object is made, which keeps track of the function to call and the arguments to pass to it.\n\nimport dask\n\n\n@dask.delayed\ndef inc(x):\n    sleep(1)\n    return x + 1\n\n\n@dask.delayed\ndef add(x, y):\n    sleep(1)\n    return x + y\n\n\n%%time\n# This runs immediately, all it does is build a graph\n\nx = inc(1)\ny = inc(2)\nz = add(x, y)\n\nThis ran immediately, since nothing has really happened yet.\nTo get the result, call compute. Notice that this runs faster than the original code.\n\n%%time\n# This actually runs our computation using a local thread pool\n\nz.compute()\n\n\n\nAlternate syntax\nInstead of a decorator, one can use functions instead:\n\nimport dask\n\nx1 = dask.delayed(inc)(1)\n\ny1 = dask.delayed(inc)(2)\n\nz1 = dask.delayed(add)(x1, y1)\n\nz1.compute()\n5\n\nz1.visualize()"
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#what-just-happened",
    "href": "nb/lab6/03_dask.delayed.html#what-just-happened",
    "title": "dask.delayed - parallelize any code",
    "section": "What just happened?",
    "text": "What just happened?\nThe z object is a lazy Delayed object. This object holds everything we need to compute the final result, including references to all of the functions that are required and their inputs and relationship to one-another. We can evaluate the result with .compute() as above or we can visualize the task graph for this value with .visualize().\n\nz\n\n\n# Look at the task graph for `z`\nz.visualize()\n\nNotice that this includes the names of the functions from before, and the logical flow of the outputs of the inc functions to the inputs of add.\n\nSome questions to consider:\n\nWhy did we go from 3s to 2s? Why weren’t we able to parallelize down to 1s?\nWhat would have happened if the inc and add functions didn’t include the sleep(1)? Would Dask still be able to speed up this code?\nWhat if we have multiple outputs or also want to get access to x or y?"
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#exercise-parallelize-a-for-loop",
    "href": "nb/lab6/03_dask.delayed.html#exercise-parallelize-a-for-loop",
    "title": "dask.delayed - parallelize any code",
    "section": "Exercise: Parallelize a for loop",
    "text": "Exercise: Parallelize a for loop\nfor loops are one of the most common things that we want to parallelize. Use dask.delayed on inc and sum to parallelize the computation below:\n\ndata = [1, 2, 3, 4, 5, 6, 7, 8]\n\n\n%%time\n# Sequential code\n\n\ndef inc(x):\n    sleep(1)\n    return x + 1\n\n\nresults = []\nfor x in data:\n    y = inc(x)\n    results.append(y)\n\ntotal = sum(results)\n\n\ntotal\n\n\n%%time\n# Your parallel code here...\n\nHow do the graph visualizations compare with the given solution, compared to a version with the sum function used directly rather than wrapped with delayed? Can you explain the latter version? You might find the result of the following expression illuminating\ninc(1) + inc(2)"
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#exercise-parallelize-a-for-loop-code-with-control-flow",
    "href": "nb/lab6/03_dask.delayed.html#exercise-parallelize-a-for-loop-code-with-control-flow",
    "title": "dask.delayed - parallelize any code",
    "section": "Exercise: Parallelize a for-loop code with control flow",
    "text": "Exercise: Parallelize a for-loop code with control flow\nOften we want to delay only some functions, running a few of them immediately. This is especially helpful when those functions are fast and help us to determine what other slower functions we should call. This decision, to delay or not to delay, is usually where we need to be thoughtful when using dask.delayed.\nIn the example below we iterate through a list of inputs. If that input is even then we want to call inc. If the input is odd then we want to call double. This is_even decision to call inc or double has to be made immediately (not lazily) in order for our graph-building Python code to proceed.\n\ndef double(x):\n    sleep(1)\n    return 2 * x\n\n\ndef is_even(x):\n    return not x % 2\n\n\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n%%time\n# Sequential code\n\nresults = []\nfor x in data:\n    if is_even(x):\n        y = double(x)\n    else:\n        y = inc(x)\n    results.append(y)\n\ntotal = sum(results)\nprint(total)\n\n\n%%time\n# Your parallel code here...\n# TODO: parallelize the sequential code above using dask.delayed\n# You will need to delay some functions, but not all\n\n\n%time total.compute()\n\n\ntotal.visualize()\n\n\nSome questions to consider:\n\nWhat are other examples of control flow where we can’t use delayed?\nWhat would have happened if we had delayed the evaluation of is_even(x) in the example above?\nWhat are your thoughts on delaying sum? This function is both computational but also fast to run."
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#exercise-parallelize-a-pandas-groupby-reduction",
    "href": "nb/lab6/03_dask.delayed.html#exercise-parallelize-a-pandas-groupby-reduction",
    "title": "dask.delayed - parallelize any code",
    "section": "Exercise: Parallelize a Pandas Groupby Reduction",
    "text": "Exercise: Parallelize a Pandas Groupby Reduction\nIn this exercise we read several CSV files and perform a groupby operation in parallel. We are given sequential code to do this and parallelize it with dask.delayed.\nThe computation we will parallelize is to compute the mean departure delay per airport from some historical flight data. We will do this by using dask.delayed together with pandas. In a future section we will do this same exercise with dask.dataframe."
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#create-data",
    "href": "nb/lab6/03_dask.delayed.html#create-data",
    "title": "dask.delayed - parallelize any code",
    "section": "Create data",
    "text": "Create data\nRun this code to prep some data.\nThis downloads and extracts some historical flight data for flights out of NYC between 1990 and 2000. The data is originally from here.\n\n#%run prep.py -d flights # not necessary to manually invoke this\n\n\nInspect data\n\nimport os\n\nsorted(os.listdir(os.path.join(\"data\", \"nycflights\")))\n\n\n\nRead one file with pandas.read_csv and compute mean departure delay\n\nimport pandas as pd\n\ndf = pd.read_csv(os.path.join(\"data\", \"nycflights\", \"1990.csv\"))\ndf.head()\n\n\n# What is the schema?\ndf.dtypes\n\n\n# What originating airports are in the data?\ndf.Origin.unique()\n\n\n# Mean departure delay per-airport for one year\ndf.groupby(\"Origin\").DepDelay.mean()\n\n\n\nSequential code: Mean Departure Delay Per Airport\nThe above cell computes the mean departure delay per-airport for one year. Here we expand that to all years using a sequential for loop.\n\nfrom glob import glob\n\nfilenames = sorted(glob(os.path.join(\"data\", \"nycflights\", \"*.csv\")))\n\n\n%%time\n\nsums = []\ncounts = []\nfor fn in filenames:\n    # Read in file\n    df = pd.read_csv(fn)\n\n    # Groupby origin airport\n    by_origin = df.groupby(\"Origin\")\n\n    # Sum of all departure delays by origin\n    total = by_origin.DepDelay.sum()\n\n    # Number of flights by origin\n    count = by_origin.DepDelay.count()\n\n    # Save the intermediates\n    sums.append(total)\n    counts.append(count)\n\n# Combine intermediates to get total mean-delay-per-origin\ntotal_delays = sum(sums)\nn_flights = sum(counts)\nmean = total_delays / n_flights\n\n\nmean\n\n\n\nParallelize the code above\nUse dask.delayed to parallelize the code above. Some extra things you will need to know.\n\nMethods and attribute access on delayed objects work automatically, so if you have a delayed object you can perform normal arithmetic, slicing, and method calls on it and it will produce the correct delayed calls.\nCalling the .compute() method works well when you have a single output. When you have multiple outputs you might want to use the dask.compute function. This way Dask can share the intermediate values.\n\nSo your goal is to parallelize the code above (which has been copied below) using dask.delayed. You may also want to visualize a bit of the computation to see if you’re doing it correctly.\n\n%%time\n# your code here\n\nIf you load the solution, add %%time to the top of the cell to measure the running time.\n\n(sum(sums)).visualize()\n\n\n# ensure the results still match\nmean\n\n\n\nSome questions to consider:\n\nHow much speedup did you get? Is this how much speedup you’d expect?\nExperiment with where to call compute. What happens when you call it on sums and counts? What happens if you wait and call it on mean?\nExperiment with delaying the call to sum. What does the graph look like if sum is delayed? What does the graph look like if it isn’t?\nCan you think of any reason why you’d want to do the reduction one way over the other?\n\n\n\nLearn More\nVisit the Delayed documentation. In particular, this delayed screencast will reinforce the concepts you learned here and the delayed best practices document collects advice on using dask.delayed well."
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#close-the-client",
    "href": "nb/lab6/03_dask.delayed.html#close-the-client",
    "title": "dask.delayed - parallelize any code",
    "section": "Close the Client",
    "text": "Close the Client\nBefore moving on to the next exercise, make sure to close your client or stop this kernel.\n\nclient.close()"
  },
  {
    "objectID": "nb/lab9/test_coarsen.html",
    "href": "nb/lab9/test_coarsen.html",
    "title": "Big Data analytics / Applied Data analytics",
    "section": "",
    "text": "import rioxarray\n\n\ndata2014 = rioxarray.open_rasterio(\"data_2014.tif\", chunks=True)\n\n\ndata2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 33601, x: 86401)&gt; Size: 12GB\ndask.array&lt;open_rasterio-9da101afd2d6ee47e505ba0f85ffc579&lt;this-array&gt;, shape=(1, 33601, 86401), dtype=float32, chunksize=(1, 5632, 5632), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 691kB -180.0 -180.0 -180.0 ... 180.0 180.0 180.0\n  * y            (y) float64 269kB 75.0 75.0 74.99 74.99 ... -64.99 -65.0 -65.0\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Area\n    _FillValue:     3.4028235e+38\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 33601x: 86401dask.array&lt;chunksize=(1, 5632, 5632), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.82 GiB\n121.00 MiB\n\n\nShape\n(1, 33601, 86401)\n(1, 5632, 5632)\n\n\nDask graph\n96 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                 86401 33601 1\n\n\n\n\nCoordinates: (4)band(band)int641array([1])x(x)float64-180.0 -180.0 ... 180.0 180.0array([-180.      , -179.995833, -179.991667, ...,  179.99167 ,  179.995836,\n        180.000003])y(y)float6475.0 75.0 74.99 ... -65.0 -65.0array([ 75.      ,  74.995833,  74.991667, ..., -64.991668, -64.995834,\n       -65.000001])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-180.00208333335 0.0041666667 0.0 75.00208333335 0.0 -0.0041666667array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([            -180.0,    -179.9958333333,    -179.9916666666,\n          -179.9874999999,    -179.9833333332,    -179.9791666665,\n          -179.9749999998,    -179.9708333331,    -179.9666666664,\n          -179.9624999997,\n       ...\n       179.96250287970003,     179.9666695464, 179.97083621310003,\n           179.9750028798, 179.97916954650003,     179.9833362132,\n       179.98750287990003,     179.9916695466, 179.99583621329998,\n             180.00000288],\n      dtype='float64', name='x', length=86401))yPandasIndexPandasIndex(Index([              75.0,      74.9958333333,      74.9916666666,\n            74.9874999999,      74.9833333332,      74.9791666665,\n            74.9749999998,      74.9708333331,      74.9666666664,\n            74.9624999997,\n       ...\n           -64.9625011197,     -64.9666677864,     -64.9708344531,\n           -64.9750011198,     -64.9791677865,     -64.9833344532,\n           -64.9875011199,     -64.9916677866,     -64.9958344533,\n       -65.00000112000001],\n      dtype='float64', name='y', length=33601))Attributes: (4)AREA_OR_POINT :Area_FillValue :3.4028235e+38scale_factor :1.0add_offset :0.0\n\n\n\ncoarsened = data2014.coarsen(x=10, y=10, boundary='pad').max()\n\n\ncoarsened\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 3361, x: 8641)&gt; Size: 116MB\ndask.array&lt;_nanmax_skip-aggregate, shape=(1, 3361, 8641), dtype=float32, chunksize=(1, 563, 563), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 69kB -180.0 -179.9 -179.9 ... 179.9 180.0 180.0\n  * y            (y) float64 27kB 74.98 74.94 74.9 74.86 ... -64.94 -64.98 -65.0\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Area\n    _FillValue:     3.4028235e+38\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 3361x: 8641dask.array&lt;chunksize=(1, 563, 563), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n110.79 MiB\n1.21 MiB\n\n\nShape\n(1, 3361, 8641)\n(1, 563, 563)\n\n\nDask graph\n160 chunks in 11 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                           8641 3361 1\n\n\n\n\nCoordinates: (4)band(band)int641array([1])x(x)float64-180.0 -179.9 ... 180.0 180.0array([-179.98125 , -179.939583, -179.897917, ...,  179.93542 ,  179.977086,\n        180.000003])y(y)float6474.98 74.94 74.9 ... -64.98 -65.0array([ 74.98125 ,  74.939583,  74.897917, ..., -64.935418, -64.977084,\n       -65.000001])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-180.00208333335 0.0041666667 0.0 75.00208333335 0.0 -0.0041666667array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([-179.98124999984998,    -179.93958333285,    -179.89791666585,\n          -179.85624999885,    -179.81458333185, -179.77291666485002,\n          -179.73124999785, -179.68958333084998, -179.64791666385003,\n          -179.60624999685,\n       ...\n           179.64375287715,     179.68541954415,     179.72708621115,\n        179.76875287815002,  179.81041954514998,     179.85208621215,\n        179.89375287915001,  179.93541954614997,  179.97708621315002,\n              180.00000288],\n      dtype='float64', name='x', length=8641))yPandasIndexPandasIndex(Index([ 74.98124999985001,  74.93958333284999,     74.89791666585,\n        74.85624999884999,     74.81458333185,  74.77291666484999,\n           74.73124999785,  74.68958333085001,     74.64791666385,\n        74.60624999685001,\n       ...\n          -64.64375111715,    -64.68541778415, -64.72708445114999,\n          -64.76875111815, -64.81041778515001, -64.85208445215001,\n          -64.89375111915,    -64.93541778615,    -64.97708445315,\n       -65.00000112000001],\n      dtype='float64', name='y', length=3361))Attributes: (4)AREA_OR_POINT :Area_FillValue :3.4028235e+38scale_factor :1.0add_offset :0.0\n\n\n\ncoarsened.plot(robust=True)\n\n\n\n\n\n\n\n\n\nname = r\"U$_\\omega$\"\n\n\nname\n\n'U$_\\\\omega$'"
  },
  {
    "objectID": "nb/lab11/lab11.html",
    "href": "nb/lab11/lab11.html",
    "title": "Lab 11",
    "section": "",
    "text": "Performance affects depth and breadth of data analysis. May cause subsampling.\nIn order to improve performance (aka do profiling), one has to: - know where to improve. Here EXPLAIN helps - then apply relevant improvement methods. First and foremost, for analytic queries, this means indexing."
  },
  {
    "objectID": "nb/lab11/lab11.html#interpret-explain",
    "href": "nb/lab11/lab11.html#interpret-explain",
    "title": "Lab 11",
    "section": "Interpret EXPLAIN",
    "text": "Interpret EXPLAIN\nFirst off, we start with EXPLAIN.\nFine quote: “plan reading is an art that deserves significant attention.”\nWe can run several queries on lab10 DB: 1. explain select * from instances; 2. explain select * from instances limit 10; 3. explain select * from instances limit 10 where \"Bird\" between 1 and 5;\nExample output:\n Seq Scan on places365  (cost=0.00..225.00 rows=10000 width=67)\n\nFirst cost number is startup cost.\nSecond is total cost if all records are fetched. Units are cost units.\nWidth param is size of each row in bytes.\n\nBear in mind that the output of EXPLAIN is a tree with 2 node types: - scan nodes. WHERE conditions will appear as attributes to scan nodes. - additional nodes for joining, aggregation, sorting, etc.\nSometimes scans might have two levels: - The lower level will identify the row locations to fetch - And the upper level (bitmap) will sort the location prior to fetching\n\nEXPLAIN types and settings\n\nOne can have more elaborate output with ANALYZE (adds execution time, among other things). Note that this will actually execute the query.\nThere is also EXPLAIN (ANALYZE, BUFFERS) that shows cache and disk data sizes.\n\n\n\nEXPLAIN JOINs\nexplain select i.\"Car\" from instances i, metadata m where i.orig_id=m.orig_id and m.year=2023 limit 20;\nOutput:\n                                   QUERY PLAN\n---------------------------------------------------------------------------------\n Limit  (cost=438.19..499.36 rows=20 width=4)\n   -&gt;  Hash Join  (cost=438.19..973.44 rows=175 width=4)\n         Hash Cond: (i.orig_id = m.orig_id)\n         -&gt;  Seq Scan on instances i  (cost=0.00..496.00 rows=10000 width=14)\n         -&gt;  Hash  (cost=436.00..436.00 rows=175 width=10)\n               -&gt;  Seq Scan on metadata m  (cost=0.00..436.00 rows=175 width=10)\n                     Filter: (year = '2023'::numeric)\n(7 rows)\nHash join happens when rows of one table are entered into an in-memory hash table, in which rows of another table are looked up."
  },
  {
    "objectID": "nb/lab11/lab11.html#indexing",
    "href": "nb/lab11/lab11.html#indexing",
    "title": "Lab 11",
    "section": "Indexing",
    "text": "Indexing\nDifferent index types:\n\nB-trees\nhash indexes\ngeneralized inverted indexes (GINs)\nBRIN indexes\ngeneralized search trees (GiSTs).\n\nIndexes can be checked either via pgAdmin UI, or in psql via \\d command.\nMore detail: https://www.postgresql.org/docs/current/indexes-types.html\n\nB-trees\nUseful when &lt;   &lt;=   =   &gt;=   &gt; operations are used. Also LIKE, BETWEEN, ~.\nThis one is used by default in Postgres, when no index type is explicitly specified.\n\nNumeric column examples\ncreate index bird_ind on instances(\"Bird\") where \"Bird\" &lt; 10;\ncreate index bird_ind2 on instances using btree(\"Bird\");\nNow we can invoke EXPLAIN. Let’s first select all column values:\nexplain select \"Bird\" from instances where \"Bird\" &lt; 10;\nQUERY PLAN\n------------------------------------------------------------------------------------\n  Index Only Scan using bird_ind on instances  (cost=0.29..270.27 rows=9999 width=3)\n\nFind unique entries:\nexplain select distinct \"Bird\" from instances;\n                                         QUERY PLAN\n--------------------------------------------------------------------------------------------\n Unique  (cost=0.29..295.29 rows=5 width=3)\n   -&gt;  Index Only Scan using bird_ind2 on instances  (cost=0.29..270.29 rows=10000 width=3)\n(2 rows)\n\n\nString column examples\nNow let’s apply index to a string column. First a query plan without index:\nlab10=# explain select distinct source from instances;\nQUERY PLAN\n----------------------------------------------------------------------\nHashAggregate  (cost=521.00..521.02 rows=2 width=10)\n  Group Key: source\n  -&gt;  Seq Scan on instances  (cost=0.00..496.00 rows=10000 width=10)\n\nCreate index and check query plan:\nlab10=# create index source_ind on instances using btree(\"source\");\nCREATE INDEX\nlab10=# explain select distinct source from instances;\n                                          QUERY PLAN\n----------------------------------------------------------------------------------------------\n Unique  (cost=0.29..219.28 rows=2 width=10)\n   -&gt;  Index Only Scan using source_ind on instances  (cost=0.29..194.28 rows=10000 width=10)\n(2 rows)\n\n\nBitmap heap scans\nWhat is we create another index on numeric column:\nlab10=# explain select * from instances where \"Car\" &gt; 1 and \"Car\" &lt; 3;\n                          QUERY PLAN\n---------------------------------------------------------------\n Seq Scan on instances  (cost=0.00..546.00 rows=981 width=264)\n   Filter: ((\"Car\" &gt; '1'::numeric) AND (\"Car\" &lt; '3'::numeric))\n(2 rows)\nCreate index:\nlab10=# create index car_ind on instances(\"Car\");\nCREATE INDEX\nlab10=# explain select * from instances where \"Car\" &gt; 1 and \"Car\" &lt; 3;\n                               QUERY PLAN\n-------------------------------------------------------------------------\n Bitmap Heap Scan on instances  (cost=22.34..433.06 rows=981 width=264)\n   Recheck Cond: ((\"Car\" &gt; '1'::numeric) AND (\"Car\" &lt; '3'::numeric))\n   -&gt;  Bitmap Index Scan on car_ind  (cost=0.00..22.09 rows=981 width=0)\n         Index Cond: ((\"Car\" &gt; '1'::numeric) AND (\"Car\" &lt; '3'::numeric))\n(4 rows)\nNote the Bitmap Heap Scan type.\nBitmaps are created ad-hoc for each query. Their contents depend on the query predicates.\n\n\nEXPLAIN ANALYZE example\nlab10=# explain analyze select * from instances where \"Car\" &gt; 1 and \"Car\" &lt; 3;\n                                                     QUERY PLAN                                            \n---------------------------------------------------------------------------------------------------------------------\n Bitmap Heap Scan on instances  (cost=22.34..433.06 rows=981 width=264) (actual time=0.144..0.449 rows=981 loops=1)\n   Recheck Cond: ((\"Car\" &gt; '1'::numeric) AND (\"Car\" &lt; '3'::numeric))\n   Heap Blocks: exact=362\n   -&gt;  Bitmap Index Scan on car_ind  (cost=0.00..22.09 rows=981 width=0) (actual time=0.123..0.123 rows=981 loops=1)\n         Index Cond: ((\"Car\" &gt; '1'::numeric) AND (\"Car\" &lt; '3'::numeric))\n Planning Time: 0.112 ms\n Execution Time: 0.484 ms\n(7 rows)\nBuffers: shared hit=246 means 246 pages were read from cache, read means reading from disk.\n\n\n\nHash indexes\n\nHash index uses equality operator only\nHowever, doesn’t take much space\nUses a 32-bit hash code.\n\nlab10=# explain select count(*) from instances where \"Road\" = 2;\n  QUERY PLAN\n-------------------------------------------------------------------\nAggregate  (cost=521.92..521.93 rows=1 width=8)\n  -&gt;  Seq Scan on instances  (cost=0.00..521.00 rows=368 width=0)\n      Filter: (\"Road\" = '2'::numeric)\ncreate index road_hash_ind on instances using hash(\"Road\");\n\nlab10=# explain select count(*) from instances where \"Road\" = 2;\n                                     QUERY PLAN\n-------------------------------------------------------------------------------------\n Aggregate  (cost=429.29..429.30 rows=1 width=8)\n   -&gt;  Bitmap Heap Scan on instances  (cost=18.85..428.37 rows=368 width=0)\n         Recheck Cond: (\"Road\" = '2'::numeric)\n         -&gt;  Bitmap Index Scan on road_hash_ind  (cost=0.00..18.76 rows=368 width=0)\n               Index Cond: (\"Road\" = '2'::numeric)\n(5 rows)\n\n\n\nGIN indexes\nUseful for composite types like text data, arrays or JSONB.\nAn example of an GIN index for full-text search:\ncreate index on gadm using gin(to_tsvector('english',\"COUNTRY\"));\nAnd query:\n select count(*) from gadm where to_tsvector('english', \"COUNTRY\") @@ to_tsquery('english', 'United');\nSee more: https://www.postgresql.org/docs/current/functions-textsearch.html.\nThese indexes need AUTOVACUUM from time to time.\n\n\nBRIN indexes\nBRIN stands for Block Range Index. BRIN is designed for handling very large tables in which certain columns have some natural correlation with their physical location within the table.\nUsage considerations: - Good for tables with some natural order - Which contain huge amount of rows\nDocs here: https://www.postgresql.org/docs/current/indexes-types.html"
  },
  {
    "objectID": "nb/lab11/lab11.html#parallelization-settings",
    "href": "nb/lab11/lab11.html#parallelization-settings",
    "title": "Lab 11",
    "section": "Parallelization settings",
    "text": "Parallelization settings\nQuery parallelization is controlled by various settings, some of these are listed below:\n\nParallel workers count/settings. Can be set by max_parallel_workers_per_gather like this:[\n\nSHOW max_parallel_workers_per_gather;\nSET max_parallel_workers_per_gather = 4;\nSome extra parameters can be inspected via:\nSHOW max_worker_processes;\nSHOW max_parallel_workers;\nMore info is here.\n\nparallel_setup_cost: sets the planner’s estimate of the cost of launching parallel worker processes. The default is 1000.\nparallel_tuple_cost: sets the planner’s estimate of the cost of transferring one tuple from a parallel worker process to another process. The default is 0.1."
  },
  {
    "objectID": "nb/lab11/lab11.html#exercises",
    "href": "nb/lab11/lab11.html#exercises",
    "title": "Lab 11",
    "section": "Exercises",
    "text": "Exercises\n\nImprove execution of queries from lab10 via indexing. Try different index types. Use EXPLAIN for decision making."
  },
  {
    "objectID": "nb/lab16/notebooks/Untitled.html",
    "href": "nb/lab16/notebooks/Untitled.html",
    "title": "Big Data analytics / Applied Data analytics",
    "section": "",
    "text": "%%sql\nCREATE DATABASE IF NOT EXISTS climate;\n\n24/11/26 16:53:17 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n\n\n\n\n\n\n\n\n\n%%sql\nCREATE TABLE IF NOT EXISTS climate.weather (\n    datetime              timestamp,\n    temp                  double,\n    lat                   double,\n    long                  double,\n    cloud_coverage        string,\n    precip                double,\n    wind_speed            double\n)\nUSING iceberg\nPARTITIONED BY (days(datetime))"
  },
  {
    "objectID": "nb/lab10/lab10.html",
    "href": "nb/lab10/lab10.html",
    "title": "Postgres for analytics",
    "section": "",
    "text": "Fetch the PostgreSQL installation package from: https://www.postgresql.org/download/windows/\nFor python: pip install \"psycopg[binary,pool]\"\n\nUse the docs from https://www.psycopg.org/psycopg3/docs/basic/usage.html"
  },
  {
    "objectID": "nb/lab10/lab10.html#installation",
    "href": "nb/lab10/lab10.html#installation",
    "title": "Postgres for analytics",
    "section": "",
    "text": "Fetch the PostgreSQL installation package from: https://www.postgresql.org/download/windows/\nFor python: pip install \"psycopg[binary,pool]\"\n\nUse the docs from https://www.psycopg.org/psycopg3/docs/basic/usage.html"
  },
  {
    "objectID": "nb/lab10/lab10.html#dataset",
    "href": "nb/lab10/lab10.html#dataset",
    "title": "Postgres for analytics",
    "section": "Dataset",
    "text": "Dataset\nWe’ll use a shrunken version of this dataset: https://ual.sg/project/global-streetscapes/\nA subset of the dataset is available in the data folder.\n\ninfo.csv: file/column descriptions\ngadm.csv: administrative areas for each image\nghsl.csv: degree of urbanisation associated with the location of the image, calculated using the Global Human Settlement Layer (GHSL)\ninstances.csv: counts of object instances detected on each image\nmetadata.csv: common metadata attributes, e.g. when and where the image was taken\nperception.csv: contains the 0-10 scores for each of six perceptual dimensions (Safe, Lively, Beautiful, Wealthy, Boring, Depressing)\nplaces365.csv: contains the place/scene classification for each image."
  },
  {
    "objectID": "nb/lab10/lab10.html#exercises",
    "href": "nb/lab10/lab10.html#exercises",
    "title": "Postgres for analytics",
    "section": "Exercises",
    "text": "Exercises\n\n1. Load the data into Postgres.\nThere are at least 3 ways how to do this:\n\n1.1. Use SQLAlchemy package:\n!pip install sqlalchemy\n\nfrom sqlalchemy import create_engine\n\nsqlAlchemyUri = 'postgresql+psycopg://&lt;username&gt;:&lt;password&gt;@localhost:5432/lab10'\nengine = create_engine(sqlAlchemyUri)\n\ndf = pd.read_csv(\"somedata.csv\")\ndf.to_sql('somedata', engine)\n\n\n1.2. Dask DataFrame built-in\nUse Dask’s DataFrame to_sql method (https://docs.dask.org/en/latest/generated/dask.dataframe.to_sql.html). It also relies on SQLAlchemy.\n\n\n1.3. Use PostgreSQL COPY FROM and csvkit Python package.\nFirst, install csvkit\n!pip install csvkit\nUse csvsql to generate a CREATE TABLE statement:\ncsvsql -i postgresql somedata.csv\nExecute the statement generated by the above line. And then execute the COPY FROM from Python:\nwith open('data/places365.csv', 'r') as f:\n    # Notice that we don't need the csv module.\n    with cur.copy(\"COPY places365 FROM STDIN WITH (FORMAT CSV, header)\") as copy:\n        for line in f:\n            copy.write(line)\nAlso: please compare performance of the above 3 methods on a larger (~2GB) file from the original dataset.\n\n\n\n2. Table setup\n\nCreate primary keys on all tables.\nMake metadata the master table, link other tables to it via foreign keys.\n\n\n\n3. Some queries\n\nCalculate average perception scores for:\n\nurban_term column from ghsl table\nplace column from places365 table\ncountry column from gadm table\n\nCreate a MATERIALIZED VIEW (https://www.postgresql.org/docs/15/sql-creatematerializedview.html) that will contain total object instance counts (instance.csv) for each gadm level2 area (gadm.csv)."
  },
  {
    "objectID": "nb/lab2/lab2.html",
    "href": "nb/lab2/lab2.html",
    "title": "Lab2: Parallelisation",
    "section": "",
    "text": "Previous lab listed some methods of optimizing Pandas work - data storage optimizations, Cython conversion, Numba annotations.\nIn this lab we’ll start looking into parallelisation as a way of optimizing data analysis workflows.\nWhy - because today’s systems are multicore (sometimes very much so).\nCheck yours:\nimport os\nos.cpu_count()\nNote: there are two kinds of parallelism, distributed and shared-memory. Here we’ll talk about shared-memory version.\nDistributed parallelism is when we use multiple machines/VMs for computations."
  },
  {
    "objectID": "nb/lab2/lab2.html#short-intro-to-functional-programming",
    "href": "nb/lab2/lab2.html#short-intro-to-functional-programming",
    "title": "Lab2: Parallelisation",
    "section": "Short intro to functional programming",
    "text": "Short intro to functional programming\nIn functional programming paradigm, functions are first-class citizens of the languages. They can be composed, applied, passed as arguments to another functions, or returned as values.\nFunctional programming is inherently better suited for parallel programs.\nBenefits of functional programming: - minimization of state - no side-effects - pure functions are easier to reason about - and parallelize (!)\nIt is based on lambda calculus. We’ll learn some of its concepts first: - lambda function definition - application and partial application - currying - closures\nFirst off, functions can be values in Python:\n\nadd = lambda x, y: x + y  \nprint (f\"add = {add}\")  \n\nAll variables support some operations on them: addition for integers, or e.g. read_csv on Pandas DataFrame.\nFunction variables support a single operation: application.\n\ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \n  \nk = i + j  \nprint (f\"k = {k}\")  \n  \nz = add (i, j)  \nprint (f\"z = {z}\") \n\nPartial function application is also supported. We’ll use functools package (https://docs.python.org/3/library/functools.html).\n\nfrom functools import partial  \ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \ninc = partial (add, 1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\")  \n\nNote: the code above both passes add as an argument to partial, and stores the return value of partial function into a new variable inc.\nThe lambda in the example above accepts two arguments essentially. This can be simplified by currying:\n\ni = 0  \nj = 1    \nadd = lambda x: lambda y: x + y  \ninc = add(1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\") \n\nDefinition for currying: currying is the technique of translating a function that takes multiple arguments into a sequence of families of functions, each taking a single argument.\nAlternatively, we can say that currying is the mean to transform a function of arity n to n functions of arity 1.\nWith currying, we can express partial application without and extra partial function.\nNote: currying relies on closures - functions that can refer to variables defined in surrounding context (like x in add definition above).\nA monstrous example:\n\ndef f_5(a, b, c, d, e):\n    return a + b + c + d + e\n\ndef c_5(a):\n    def c_4(b):\n        def c_3(c):\n            def c_2(d):\n                def c_1(e):\n                    return f_5(a, b, c, d, e)\n                return c_1\n            return c_2\n        return c_3\n    return c_4\n\nHow to use?\n\nf_5(1,2,3,4,5)\n\n\nc_5(1)(2)(3)(4)(5)"
  },
  {
    "objectID": "nb/lab2/lab2.html#map-reduce",
    "href": "nb/lab2/lab2.html#map-reduce",
    "title": "Lab2: Parallelisation",
    "section": "map-reduce",
    "text": "map-reduce\nIn functional programming, we do not iterate - we map. Why? Because map can be parallelised.\nIteration (like a for-loop) is sequential. Mapping is not.\nmap takes its name from a mathematical term for functions - mappings.\nmap’s arguments: - a sequence to iterate on - a function to apply to each element of the sequence.\nmap’s return value: - a processed sequence of the same size as input\nNow, if we want to convert our sequence to a sequence of a different length, or different object altogether, we use reduce.\nreduce’s arguments: - a sequence to iterate on - accumulation seed to start reducing on - a function of two arguments, accumulation result and next element\nUsing above examples, we can easily implement map using list comprehensions as as:\n\nmap = lambda f, l: [f(x) for x in l]\n\nLet’s use it:\n\nint_list = [1,2,3,4,5,6]\ninc_list = map(inc, int_list)\nprint(inc_list)\n\nAnd reduce can be implemented as:\n\ndef reduce_v1 (f, id, s):  \n    n = len(s)  \n    if n == 0:  \n        return id  \n    elif n == 1:  \n        return s[0]  \n    else:  \n        return f(reduce_v1(f, id, s[1:]), s[0]) \n\nUsage example:\n\nadd = lambda x, y: x + y  \nres = reduce_v1(add, 0, int_list)\nprint(res)\n\n\nLimitations and parallelisation types\nSome tasks can be trivially parallelised - these are called embarrassingly parallel. inc is a simple example of this type.\nOther tasks have a degree of interdependence. Consider below example, adapted from the reduce above:\n\n# Sum all elements of int_list\ndef sum(lst):\n    sum = 0\n    \n    for i in range(len(lst)):\n      sum += lst[i]\n\n    return sum\n\nprint(sum(int_list))\n\nHow can it be parallelised? By using a technique called chunking:\n\nchunk1 = int_list[:3]\nchunk2 = int_list[3:]\n\nsum_1 = sum(chunk1)\nsum_2 = sum(chunk2)\n\nresult = sum_1 + sum_2\n\nprint(result)\n\nThen there algorithms that are inherently serial. An example is computation of a Fibonacci sequence.\n\n\nPython’s builtin map\nSuppose we want to square each number in a list. A naive approach would be:\n\nnumber_list = [1,2,3,4]\n\ndef squarev1(lst):\n    squared = [] # sometimes pre-allocation can also work, like [None]*1000\n    for el in lst:\n        squared.append(el*el)\n    return squared\n\nsquarev1(number_list)\n\n\n# Built in map: https://docs.python.org/3/library/functions.html#map\n# Note that we return a map object that has to be converted to a list first\ndef squarev2(lst):\n    return list(map(lambda x: x*x, lst))\n\n\nsquarev2(number_list)\n\n\n\nPython parallel map\nWe need to use multiprocessing library (https://docs.python.org/3/library/multiprocessing.html)\n\nfrom multiprocessing import Pool\n\nThere are limitations though: in order to arrange communication between proceses, Python uses pickling: a method of serializing/deserializing objects so that they can be sent between processes.\nPython cannot pickle closures, nested functions or classes! More info on pickling: https://docs.python.org/3/library/pickle.html\nMoreover, we have to move sqr definition into a separate file (https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror)\n\nimport defs\n\n\ndef squarev3(lst):\n    pool = Pool()\n    return pool.map(defs.sqr, number_list)\n\n\nsquarev3(number_list)\n\n\n\nPathos map\nNote:: however, Dill (https://pypi.org/project/dill/) can overcome this limitation!\nHow to use Dill?\npip3 install pathos\n\npip3 install toolz\n\nfrom pathos.multiprocessing import ProcessPool\n\ndef squarev4(lst):\n    pool = ProcessPool()\n    return pool.map(lambda x: x*x, lst)\n\n\nsquarev4([1,3,5])\n\n\n\nNumpy vectorization\nNumpy automatically vectorizes array operations. However, we can explicitly invoke vectorize():\n\nimport numpy as np\n\nnp_array = np.array(number_list)\n\nvectorized_fn = np.vectorize(lambda x: x*x)\n\nresult = vectorized_fn(np_array)\nprint(result)\n\n## Exercises\n\n1. Write a version of curried function `c_5` using lambda syntax.\n2. Write a program calculating a factorial using parallelisation and `map-reduce`.\n3. Write a parallelised version of `reduce` (hint: use chunking).\n4. Compare performance of Numpy vectorization vs parallel map on a really large array of integers."
  },
  {
    "objectID": "nb/lab4/00_overview.html",
    "href": "nb/lab4/00_overview.html",
    "title": "Welcome to the Dask Tutorial",
    "section": "",
    "text": "Dask is a parallel and distributed computing library that scales the existing Python and PyData ecosystem.\nDask can scale up to your full laptop capacity and out to a cloud cluster."
  },
  {
    "objectID": "nb/lab4/00_overview.html#an-example-dask-computation",
    "href": "nb/lab4/00_overview.html#an-example-dask-computation",
    "title": "Welcome to the Dask Tutorial",
    "section": "An example Dask computation",
    "text": "An example Dask computation\nIn the following lines of code, we’re reading the NYC taxi cab data from 2015 and finding the mean tip amount. Don’t worry about the code, this is just for a quick demonstration. We’ll go over all of this in the next notebook. :)\n\nimport dask.dataframe as dd\nfrom dask.distributed import Client\n\n\nclient = Client()\nclient\n\n\nddf = dd.read_parquet(\n    \"./data/taxi.parquet\",\n    #\"s3://dask-data/nyc-taxi/nyc-2015.parquet/part.*.parquet\",\n    columns=[\"passenger_count\", \"tip_amount\"],\n    storage_options={\"anon\": True},\n)\n\n\nresult = ddf.groupby(\"passenger_count\").tip_amount.mean().compute()\nresult"
  },
  {
    "objectID": "nb/lab4/00_overview.html#what-is-dask",
    "href": "nb/lab4/00_overview.html#what-is-dask",
    "title": "Welcome to the Dask Tutorial",
    "section": "What is Dask?",
    "text": "What is Dask?\nThere are many parts to the “Dask” the project: * Collections/API also known as “core-library”. * Distributed – to create clusters * Intergrations and broader ecosystem\n\nDask Collections\nDask provides multi-core and distributed+parallel execution on larger-than-memory datasets\nWe can think of Dask’s APIs (also called collections) at a high and a low level:\n\n\n\n\nHigh-level collections: Dask provides high-level Array, Bag, and DataFrame collections that mimic NumPy, lists, and pandas but can operate in parallel on datasets that don’t fit into memory.\nLow-level collections: Dask also provides low-level Delayed and Futures collections that give you finer control to build custom parallel and distributed computations.\n\n\n\nDask Cluster\nMost of the times when you are using Dask, you will be using a distributed scheduler, which exists in the context of a Dask cluster. The Dask cluster is structured as:\n\n\n\n\n\nDask Ecosystem\nIn addition to the core Dask library and its distributed scheduler, the Dask ecosystem connects several additional initiatives, including:\n\nDask-ML (parallel scikit-learn-style API)\nDask-image\nDask-cuDF\nDask-sql\nDask-snowflake\nDask-mongo\nDask-bigquery\n\nCommunity libraries that have built-in dask integrations like:\n\nXarray\nXGBoost\nPrefect\nAirflow\n\nDask deployment libraries - Dask-kubernetes - Dask-YARN - Dask-gateway - Dask-cloudprovider - jobqueue\n… When we talk about the Dask project we include all these efforts as part of the community."
  },
  {
    "objectID": "nb/lab4/00_overview.html#dask-use-cases",
    "href": "nb/lab4/00_overview.html#dask-use-cases",
    "title": "Welcome to the Dask Tutorial",
    "section": "Dask Use Cases",
    "text": "Dask Use Cases\nDask is used in multiple fields such as:\n\nGeospatial\nFinance\nAstrophysics\nMicrobiology\nEnvironmental science\n\nCheck out the Dask use cases page that provides a number of sample workflows."
  },
  {
    "objectID": "nb/lab4/00_overview.html#prepare",
    "href": "nb/lab4/00_overview.html#prepare",
    "title": "Welcome to the Dask Tutorial",
    "section": "Prepare",
    "text": "Prepare\n\n1. You should clone this repository\ngit clone http://github.com/dask/dask-tutorial\nand then install necessary packages. There are three different ways to achieve this, pick the one that best suits you, and only pick one option. They are, in order of preference:\n\n\n2a) Create a conda environment (preferred)\nIn the main repo directory\nconda env create -f binder/environment.yml\nconda activate dask-tutorial\n\n\n2b) Install into an existing environment\nYou will need the following core libraries\nconda install -c conda-forge ipycytoscape jupyterlab python-graphviz matplotlib zarr xarray pooch pyarrow s3fs scipy dask distributed dask-labextension\nNote that these options will alter your existing environment, potentially changing the versions of packages you already have installed.\n\n\n2c) Use Python venv\n  - python -m venv venv_dask\n  - pip install \"dask[complete]\"\n  - pip install jupyter\n  - pip install s3fs\n  - pip install graphviz\n  - pip install zarr\n  - pip install ipycytoscape\n  - pip install xarray\n  - pip install pooch\n  - pip install scipy\nGraphviz installation notes: https://pypi.org/project/graphviz/\nPython virtual environments: https://python.land/virtual-environments/virtualenv"
  },
  {
    "objectID": "nb/lab4/00_overview.html#tutorial-structure",
    "href": "nb/lab4/00_overview.html#tutorial-structure",
    "title": "Welcome to the Dask Tutorial",
    "section": "Tutorial Structure",
    "text": "Tutorial Structure\nEach section is a Jupyter notebook. There’s a mixture of text, code, and exercises.\n\nOverview - dask’s place in the universe.\nDataframe - parallelized operations on many pandas dataframes spread across your cluster.\nArray - blocked numpy-like functionality with a collection of numpy arrays spread across your cluster.\nDelayed - the single-function way to parallelize general python code.\nDeployment/Distributed - Dask’s scheduler for clusters, with details of how to view the UI.\nDistributed Futures - non-blocking results that compute asynchronously.\nConclusion\n\nIf you haven’t used Jupyterlab, it’s similar to the Jupyter Notebook. If you haven’t used the Notebook, the quick intro is\n\nThere are two modes: command and edit\nFrom command mode, press Enter to edit a cell (like this markdown cell)\nFrom edit mode, press Esc to change to command mode\nPress shift+enter to execute a cell and move to the next cell.\n\nThe toolbar has commands for executing, converting, and creating cells.\n\nExercise: Print Hello, world!\nEach notebook will have exercises for you to solve. You’ll be given a blank or partially completed cell, followed by a hidden cell with a solution. For example.\nPrint the text “Hello, world!”.\n\n# Your code here\n\nThe next cell has the solution. Click the ellipses to expand the solution, and always make sure to run the solution cell, in case later sections of the notebook depend on the output from the solution.\n\nprint(\"Hello, world!\")\n\n\n\nDon’t forget to shutdown the client\n\nclient.shutdown()"
  },
  {
    "objectID": "nb/lab4/00_overview.html#useful-links",
    "href": "nb/lab4/00_overview.html#useful-links",
    "title": "Welcome to the Dask Tutorial",
    "section": "Useful Links",
    "text": "Useful Links\n\nReference\n\nDocs\nExamples\nCode\nBlog\n\nAsk for help\n\ndask tag on Stack Overflow, for usage questions\ngithub issues for bug reports and feature requests\ndiscourse forum for general, non-bug, questions and discussion\nAttend a live tutorial"
  },
  {
    "objectID": "nb/bigdata_lab2.html",
    "href": "nb/bigdata_lab2.html",
    "title": "Big Data analytics / Applied Data analytics",
    "section": "",
    "text": "import pandas as pd\n\ndd = pd.read_csv('../files/depression_data.csv')\ndd.head()\n\n\n\n\n\n\n\n\nName\nAge\nMarital Status\nEducation Level\nNumber of Children\nSmoking Status\nPhysical Activity Level\nEmployment Status\nIncome\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\n0\nChristine Barker\n31\nMarried\nBachelor's Degree\n2\nNon-smoker\nActive\nUnemployed\n26265.67\nModerate\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n1\nJacqueline Lewis\n55\nMarried\nHigh School\n1\nNon-smoker\nSedentary\nEmployed\n42710.36\nHigh\nUnhealthy\nFair\nYes\nNo\nNo\nYes\n\n\n2\nShannon Church\n78\nWidowed\nMaster's Degree\n1\nNon-smoker\nSedentary\nEmployed\n125332.79\nLow\nUnhealthy\nGood\nNo\nNo\nYes\nNo\n\n\n3\nCharles Jordan\n58\nDivorced\nMaster's Degree\n3\nNon-smoker\nModerate\nUnemployed\n9992.78\nModerate\nModerate\nPoor\nNo\nNo\nNo\nNo\n\n\n4\nMichael Rich\n18\nSingle\nHigh School\n0\nNon-smoker\nSedentary\nUnemployed\n8595.08\nLow\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n\n\n\n\n\n\nimport math\ndef process(x):\n    if x &lt;= 9:\n        return x\n    lst = [int(i) for i in str(x).replace(\".\", \"\")]\n    result = 0\n    for el in lst:\n        result = result + el\n    return process(result)\n\n\nprocess(123688)\n\n1\n\n\n\n%timeit dd.apply(lambda x: process(x[\"Income\"]), axis=1)\n\n931 ms ± 7.41 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n%prun -l 4 dd.apply(lambda x: process(x[\"Income\"]), axis=1)\n\n \n\n\n         12191139 function calls (11268225 primitive calls) in 2.512 seconds\n\n   Ordered by: internal time\n   List reduced from 289 to 4 due to restriction &lt;4&gt;\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n1336657/413768    0.571    0.000    0.632    0.000 3049257351.py:2(process)\n   413769    0.245    0.000    0.544    0.000 apply.py:1242(series_generator)\n   413768    0.212    0.000    0.827    0.000 series.py:1104(__getitem__)\n2482837/2482833    0.159    0.000    0.282    0.000 {built-in method builtins.isinstance}\n\n\n\nimport os\nimport gc\nimport psutil\n\nproc = psutil.Process(os.getpid())\ngc.collect()\ninitial_memory = proc.memory_info().rss\n\n## Allocate memory by creating large lists\nfoo = ['abc' for _ in range(10**7)]\nallocated_memory = proc.memory_info().rss\n\n## Deallocate memory\ndel foo\ngc.collect()\nfinal_memory = proc.memory_info().rss\n\n## Print memory statistics\nincrease = lambda x2, x1: 100.0 * (x2 - x1) / initial_memory\nprint(\"Allocated Memory Increase: %0.2f%%\" % increase(allocated_memory, initial_memory))\nprint(\"Memory After Deletion: %0.2f%%\" % increase(final_memory, allocated_memory))\n\nAllocated Memory Increase: 23.35%\nMemory After Deletion: -10.78%\n\n\n\n%load_ext cython\n\n\n%%cython --annotate\ndef process_cython(x: cython.int):\n    if x &lt;= 9:\n        return x\n    lst = [int(i) for i in str(x).replace(\".\", \"\")]\n    result: cython.int = 0\n    for el in lst:\n        result = result + el\n    return process_cython(result)\n\n\n\nGenerated by Cython 3.1.3\n\n    Yellow lines hint at Python interaction.\n    Click on a line that starts with a \"+\" to see the C code that Cython generated for it.\n\n+1: def process_cython(x: cython.int):\n/* Python wrapper */\nstatic PyObject *__pyx_pw_78_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6_1process_cython(PyObject *__pyx_self, \n#if CYTHON_METH_FASTCALL\nPyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds\n#else\nPyObject *__pyx_args, PyObject *__pyx_kwds\n#endif\n); /*proto*/\nstatic PyMethodDef __pyx_mdef_78_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6_1process_cython = {\"process_cython\", (PyCFunction)(void(*)(void))(__Pyx_PyCFunction_FastCallWithKeywords)__pyx_pw_78_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6_1process_cython, __Pyx_METH_FASTCALL|METH_KEYWORDS, 0};\nstatic PyObject *__pyx_pw_78_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6_1process_cython(PyObject *__pyx_self, \n#if CYTHON_METH_FASTCALL\nPyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds\n#else\nPyObject *__pyx_args, PyObject *__pyx_kwds\n#endif\n) {\n  PyObject *__pyx_v_x = 0;\n  #if !CYTHON_METH_FASTCALL\n  CYTHON_UNUSED Py_ssize_t __pyx_nargs;\n  #endif\n  CYTHON_UNUSED PyObject *const *__pyx_kwvalues;\n  PyObject *__pyx_r = 0;\n  __Pyx_RefNannyDeclarations\n  __Pyx_RefNannySetupContext(\"process_cython (wrapper)\", 0);\n  #if !CYTHON_METH_FASTCALL\n  #if CYTHON_ASSUME_SAFE_SIZE\n  __pyx_nargs = PyTuple_GET_SIZE(__pyx_args);\n  #else\n  __pyx_nargs = PyTuple_Size(__pyx_args); if (unlikely(__pyx_nargs &lt; 0)) return NULL;\n  #endif\n  #endif\n  __pyx_kwvalues = __Pyx_KwValues_FASTCALL(__pyx_args, __pyx_nargs);\n  {\n    PyObject ** const __pyx_pyargnames[] = {&__pyx_mstate_global-&gt;__pyx_n_u_x,0};\n  PyObject* values[1] = {0};\n    const Py_ssize_t __pyx_kwds_len = (__pyx_kwds) ? __Pyx_NumKwargs_FASTCALL(__pyx_kwds) : 0;\n    if (unlikely(__pyx_kwds_len) &lt; 0) __PYX_ERR(0, 1, __pyx_L3_error)\n    if (__pyx_kwds_len &gt; 0) {\n      switch (__pyx_nargs) {\n        case  1:\n        values[0] = __Pyx_ArgRef_FASTCALL(__pyx_args, 0);\n        if (!CYTHON_ASSUME_SAFE_MACROS && unlikely(!values[0])) __PYX_ERR(0, 1, __pyx_L3_error)\n        CYTHON_FALLTHROUGH;\n        case  0: break;\n        default: goto __pyx_L5_argtuple_error;\n      }\n      const Py_ssize_t kwd_pos_args = __pyx_nargs;\n      if (__Pyx_ParseKeywords(__pyx_kwds, __pyx_kwvalues, __pyx_pyargnames, 0, values, kwd_pos_args, __pyx_kwds_len, \"process_cython\", 0) &lt; 0) __PYX_ERR(0, 1, __pyx_L3_error)\n      for (Py_ssize_t i = __pyx_nargs; i &lt; 1; i++) {\n        if (unlikely(!values[i])) { __Pyx_RaiseArgtupleInvalid(\"process_cython\", 1, 1, 1, i); __PYX_ERR(0, 1, __pyx_L3_error) }\n      }\n    } else if (unlikely(__pyx_nargs != 1)) {\n      goto __pyx_L5_argtuple_error;\n    } else {\n      values[0] = __Pyx_ArgRef_FASTCALL(__pyx_args, 0);\n      if (!CYTHON_ASSUME_SAFE_MACROS && unlikely(!values[0])) __PYX_ERR(0, 1, __pyx_L3_error)\n    }\n    __pyx_v_x = values[0];\n  }\n  goto __pyx_L6_skip;\n  __pyx_L5_argtuple_error:;\n  __Pyx_RaiseArgtupleInvalid(\"process_cython\", 1, 1, 1, __pyx_nargs); __PYX_ERR(0, 1, __pyx_L3_error)\n  __pyx_L6_skip:;\n  goto __pyx_L4_argument_unpacking_done;\n  __pyx_L3_error:;\n  for (Py_ssize_t __pyx_temp=0; __pyx_temp &lt; (Py_ssize_t)(sizeof(values)/sizeof(values[0])); ++__pyx_temp) {\n    Py_XDECREF(values[__pyx_temp]);\n  }\n  __Pyx_AddTraceback(\"_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6.process_cython\", __pyx_clineno, __pyx_lineno, __pyx_filename);\n  __Pyx_RefNannyFinishContext();\n  return NULL;\n  __pyx_L4_argument_unpacking_done:;\n  __pyx_r = __pyx_pf_78_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6_process_cython(__pyx_self, __pyx_v_x);\n  int __pyx_lineno = 0;\n  const char *__pyx_filename = NULL;\n  int __pyx_clineno = 0;\n\n  /* function exit code */\n  for (Py_ssize_t __pyx_temp=0; __pyx_temp &lt; (Py_ssize_t)(sizeof(values)/sizeof(values[0])); ++__pyx_temp) {\n    Py_XDECREF(values[__pyx_temp]);\n  }\n  __Pyx_RefNannyFinishContext();\n  return __pyx_r;\n}\n\nstatic PyObject *__pyx_pf_78_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6_process_cython(CYTHON_UNUSED PyObject *__pyx_self, PyObject *__pyx_v_x) {\n  PyObject *__pyx_v_lst = NULL;\n  PyObject *__pyx_v_result = NULL;\n  PyObject *__pyx_v_el = NULL;\n  Py_UCS4 __pyx_7genexpr__pyx_v_i;\n  PyObject *__pyx_r = NULL;\n/* … */\n  __pyx_t_2 = __Pyx_PyDict_NewPresized(1); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_2);\n  if (PyDict_SetItem(__pyx_t_2, __pyx_mstate_global-&gt;__pyx_n_u_x, __pyx_mstate_global-&gt;__pyx_kp_u_cython_int) &lt; 0) __PYX_ERR(0, 1, __pyx_L1_error)\n  __pyx_t_3 = __Pyx_CyFunction_New(&__pyx_mdef_78_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6_1process_cython, 0, __pyx_mstate_global-&gt;__pyx_n_u_process_cython, NULL, __pyx_mstate_global-&gt;__pyx_n_u_cython_magic_bd7b64be6d8b40f63b, __pyx_mstate_global-&gt;__pyx_d, ((PyObject *)__pyx_mstate_global-&gt;__pyx_codeobj_tab[0])); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_3);\n  __Pyx_CyFunction_SetAnnotationsDict(__pyx_t_3, __pyx_t_2);\n  __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;\n  if (PyDict_SetItem(__pyx_mstate_global-&gt;__pyx_d, __pyx_mstate_global-&gt;__pyx_n_u_process_cython, __pyx_t_3) &lt; 0) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;\n  __pyx_t_3 = __Pyx_PyDict_NewPresized(0); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_3);\n  if (PyDict_SetItem(__pyx_mstate_global-&gt;__pyx_d, __pyx_mstate_global-&gt;__pyx_n_u_test, __pyx_t_3) &lt; 0) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;\n+2:     if x &lt;= 9:\n  __pyx_t_1 = PyObject_RichCompare(__pyx_v_x, __pyx_mstate_global-&gt;__pyx_int_9, Py_LE); __Pyx_XGOTREF(__pyx_t_1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 2, __pyx_L1_error)\n  __pyx_t_2 = __Pyx_PyObject_IsTrue(__pyx_t_1); if (unlikely((__pyx_t_2 &lt; 0))) __PYX_ERR(0, 2, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n  if (__pyx_t_2) {\n/* … */\n  }\n+3:         return x\n    __Pyx_XDECREF(__pyx_r);\n    __Pyx_INCREF(__pyx_v_x);\n    __pyx_r = __pyx_v_x;\n    goto __pyx_L0;\n+4:     lst = [int(i) for i in str(x).replace(\".\", \"\")]\n  { /* enter inner scope */\n    __pyx_t_1 = PyList_New(0); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 4, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_1);\n    __pyx_t_3 = __Pyx_PyObject_Unicode(__pyx_v_x); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 4, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_3);\n    __pyx_t_4 = PyUnicode_Replace(((PyObject*)__pyx_t_3), __pyx_mstate_global-&gt;__pyx_kp_u_, __pyx_mstate_global-&gt;__pyx_kp_u__2, -1L); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 4, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_4);\n    __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;\n    __pyx_t_9 = __Pyx_init_unicode_iteration(__pyx_t_4, (&__pyx_t_6), (&__pyx_t_7), (&__pyx_t_8)); if (unlikely(__pyx_t_9 == ((int)-1))) __PYX_ERR(0, 4, __pyx_L1_error)\n    for (__pyx_t_10 = 0; __pyx_t_10 &lt; __pyx_t_6; __pyx_t_10++) {\n      __pyx_t_5 = __pyx_t_10;\n      __pyx_7genexpr__pyx_v_i = __Pyx_PyUnicode_READ(__pyx_t_8, __pyx_t_7, __pyx_t_5);\n      __pyx_t_11 = NULL;\n      __Pyx_INCREF((PyObject *)(&PyLong_Type));\n      __pyx_t_12 = ((PyObject *)(&PyLong_Type)); \n      __pyx_t_13 = __Pyx_PyUnicode_FromOrdinal(__pyx_7genexpr__pyx_v_i); if (unlikely(!__pyx_t_13)) __PYX_ERR(0, 4, __pyx_L1_error)\n      __Pyx_GOTREF(__pyx_t_13);\n      __pyx_t_14 = 1;\n      {\n        PyObject *__pyx_callargs[2] = {__pyx_t_11, __pyx_t_13};\n        __pyx_t_3 = __Pyx_PyObject_FastCall(__pyx_t_12, __pyx_callargs+__pyx_t_14, (2-__pyx_t_14) | (__pyx_t_14*__Pyx_PY_VECTORCALL_ARGUMENTS_OFFSET));\n        __Pyx_XDECREF(__pyx_t_11); __pyx_t_11 = 0;\n        __Pyx_DECREF(__pyx_t_13); __pyx_t_13 = 0;\n        __Pyx_DECREF(__pyx_t_12); __pyx_t_12 = 0;\n        if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 4, __pyx_L1_error)\n        __Pyx_GOTREF(__pyx_t_3);\n      }\n      if (unlikely(__Pyx_ListComp_Append(__pyx_t_1, (PyObject*)__pyx_t_3))) __PYX_ERR(0, 4, __pyx_L1_error)\n      __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;\n    }\n    __Pyx_DECREF(__pyx_t_4); __pyx_t_4 = 0;\n  } /* exit inner scope */\n  __pyx_v_lst = ((PyObject*)__pyx_t_1);\n  __pyx_t_1 = 0;\n+5:     result: cython.int = 0\n  __Pyx_INCREF(__pyx_mstate_global-&gt;__pyx_int_0);\n  __pyx_v_result = __pyx_mstate_global-&gt;__pyx_int_0;\n+6:     for el in lst:\n  __pyx_t_1 = __pyx_v_lst; __Pyx_INCREF(__pyx_t_1);\n  __pyx_t_6 = 0;\n  for (;;) {\n    {\n      Py_ssize_t __pyx_temp = __Pyx_PyList_GET_SIZE(__pyx_t_1);\n      #if !CYTHON_ASSUME_SAFE_SIZE\n      if (unlikely((__pyx_temp &lt; 0))) __PYX_ERR(0, 6, __pyx_L1_error)\n      #endif\n      if (__pyx_t_6 &gt;= __pyx_temp) break;\n    }\n    __pyx_t_4 = __Pyx_PyList_GetItemRef(__pyx_t_1, __pyx_t_6);\n    ++__pyx_t_6;\n    if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 6, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_4);\n    __Pyx_XDECREF_SET(__pyx_v_el, __pyx_t_4);\n    __pyx_t_4 = 0;\n/* … */\n  }\n  __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n+7:         result = result + el\n    __pyx_t_4 = PyNumber_Add(__pyx_v_result, __pyx_v_el); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 7, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_4);\n    __Pyx_DECREF_SET(__pyx_v_result, __pyx_t_4);\n    __pyx_t_4 = 0;\n+8:     return process_cython(result)\n  __Pyx_XDECREF(__pyx_r);\n  __pyx_t_4 = NULL;\n  __Pyx_GetModuleGlobalName(__pyx_t_3, __pyx_mstate_global-&gt;__pyx_n_u_process_cython); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 8, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_3);\n  __pyx_t_14 = 1;\n  #if CYTHON_UNPACK_METHODS\n  if (unlikely(PyMethod_Check(__pyx_t_3))) {\n    __pyx_t_4 = PyMethod_GET_SELF(__pyx_t_3);\n    assert(__pyx_t_4);\n    PyObject* __pyx__function = PyMethod_GET_FUNCTION(__pyx_t_3);\n    __Pyx_INCREF(__pyx_t_4);\n    __Pyx_INCREF(__pyx__function);\n    __Pyx_DECREF_SET(__pyx_t_3, __pyx__function);\n    __pyx_t_14 = 0;\n  }\n  #endif\n  {\n    PyObject *__pyx_callargs[2] = {__pyx_t_4, __pyx_v_result};\n    __pyx_t_1 = __Pyx_PyObject_FastCall(__pyx_t_3, __pyx_callargs+__pyx_t_14, (2-__pyx_t_14) | (__pyx_t_14*__Pyx_PY_VECTORCALL_ARGUMENTS_OFFSET));\n    __Pyx_XDECREF(__pyx_t_4); __pyx_t_4 = 0;\n    __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;\n    if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 8, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_1);\n  }\n  __pyx_r = __pyx_t_1;\n  __pyx_t_1 = 0;\n  goto __pyx_L0;\n\n\n\n\n%%cython --annotate\ndef primes(nb_primes: cython.int):\n    i: cython.int\n    p: cython.int[1000] = [0] * 1000\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p: cython.int = 0  # The current number of elements in p.\n    n: cython.int = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.pyx:1:28: Unknown type declaration 'cython.int' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.pyx:2:13: Unknown type declaration 'cython.int' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.pyx:8:17: Unknown type declaration 'cython.int' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.pyx:9:13: Unknown type declaration 'cython.int' in annotation, ignoring\n\n\nContent of stderr:\n/Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.c:4550:17: warning: fallthrough annotation in unreachable code [-Wunreachable-code-fallthrough]\n 4550 |                 CYTHON_FALLTHROUGH;\n      |                 ^\n/Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.c:512:34: note: expanded from macro 'CYTHON_FALLTHROUGH'\n  512 |       #define CYTHON_FALLTHROUGH __attribute__((fallthrough))\n      |                                  ^\n/Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.c:4561:17: warning: fallthrough annotation in unreachable code [-Wunreachable-code-fallthrough]\n 4561 |                 CYTHON_FALLTHROUGH;\n      |                 ^\n/Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.c:512:34: note: expanded from macro 'CYTHON_FALLTHROUGH'\n  512 |       #define CYTHON_FALLTHROUGH __attribute__((fallthrough))\n      |                                  ^\n2 warnings generated.\nld: warning: search path 'Modules/_hacl' not found\n\n\n\n\nGenerated by Cython 3.1.3\n\n    Yellow lines hint at Python interaction.\n    Click on a line that starts with a \"+\" to see the C code that Cython generated for it.\n\n+01: def primes(nb_primes: cython.int):\n/* Python wrapper */\nstatic PyObject *__pyx_pw_78_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1_1primes(PyObject *__pyx_self, \n#if CYTHON_METH_FASTCALL\nPyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds\n#else\nPyObject *__pyx_args, PyObject *__pyx_kwds\n#endif\n); /*proto*/\nstatic PyMethodDef __pyx_mdef_78_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1_1primes = {\"primes\", (PyCFunction)(void(*)(void))(__Pyx_PyCFunction_FastCallWithKeywords)__pyx_pw_78_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1_1primes, __Pyx_METH_FASTCALL|METH_KEYWORDS, 0};\nstatic PyObject *__pyx_pw_78_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1_1primes(PyObject *__pyx_self, \n#if CYTHON_METH_FASTCALL\nPyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds\n#else\nPyObject *__pyx_args, PyObject *__pyx_kwds\n#endif\n) {\n  PyObject *__pyx_v_nb_primes = 0;\n  #if !CYTHON_METH_FASTCALL\n  CYTHON_UNUSED Py_ssize_t __pyx_nargs;\n  #endif\n  CYTHON_UNUSED PyObject *const *__pyx_kwvalues;\n  PyObject *__pyx_r = 0;\n  __Pyx_RefNannyDeclarations\n  __Pyx_RefNannySetupContext(\"primes (wrapper)\", 0);\n  #if !CYTHON_METH_FASTCALL\n  #if CYTHON_ASSUME_SAFE_SIZE\n  __pyx_nargs = PyTuple_GET_SIZE(__pyx_args);\n  #else\n  __pyx_nargs = PyTuple_Size(__pyx_args); if (unlikely(__pyx_nargs &lt; 0)) return NULL;\n  #endif\n  #endif\n  __pyx_kwvalues = __Pyx_KwValues_FASTCALL(__pyx_args, __pyx_nargs);\n  {\n    PyObject ** const __pyx_pyargnames[] = {&__pyx_mstate_global-&gt;__pyx_n_u_nb_primes,0};\n  PyObject* values[1] = {0};\n    const Py_ssize_t __pyx_kwds_len = (__pyx_kwds) ? __Pyx_NumKwargs_FASTCALL(__pyx_kwds) : 0;\n    if (unlikely(__pyx_kwds_len) &lt; 0) __PYX_ERR(0, 1, __pyx_L3_error)\n    if (__pyx_kwds_len &gt; 0) {\n      switch (__pyx_nargs) {\n        case  1:\n        values[0] = __Pyx_ArgRef_FASTCALL(__pyx_args, 0);\n        if (!CYTHON_ASSUME_SAFE_MACROS && unlikely(!values[0])) __PYX_ERR(0, 1, __pyx_L3_error)\n        CYTHON_FALLTHROUGH;\n        case  0: break;\n        default: goto __pyx_L5_argtuple_error;\n      }\n      const Py_ssize_t kwd_pos_args = __pyx_nargs;\n      if (__Pyx_ParseKeywords(__pyx_kwds, __pyx_kwvalues, __pyx_pyargnames, 0, values, kwd_pos_args, __pyx_kwds_len, \"primes\", 0) &lt; 0) __PYX_ERR(0, 1, __pyx_L3_error)\n      for (Py_ssize_t i = __pyx_nargs; i &lt; 1; i++) {\n        if (unlikely(!values[i])) { __Pyx_RaiseArgtupleInvalid(\"primes\", 1, 1, 1, i); __PYX_ERR(0, 1, __pyx_L3_error) }\n      }\n    } else if (unlikely(__pyx_nargs != 1)) {\n      goto __pyx_L5_argtuple_error;\n    } else {\n      values[0] = __Pyx_ArgRef_FASTCALL(__pyx_args, 0);\n      if (!CYTHON_ASSUME_SAFE_MACROS && unlikely(!values[0])) __PYX_ERR(0, 1, __pyx_L3_error)\n    }\n    __pyx_v_nb_primes = values[0];\n  }\n  goto __pyx_L6_skip;\n  __pyx_L5_argtuple_error:;\n  __Pyx_RaiseArgtupleInvalid(\"primes\", 1, 1, 1, __pyx_nargs); __PYX_ERR(0, 1, __pyx_L3_error)\n  __pyx_L6_skip:;\n  goto __pyx_L4_argument_unpacking_done;\n  __pyx_L3_error:;\n  for (Py_ssize_t __pyx_temp=0; __pyx_temp &lt; (Py_ssize_t)(sizeof(values)/sizeof(values[0])); ++__pyx_temp) {\n    Py_XDECREF(values[__pyx_temp]);\n  }\n  __Pyx_AddTraceback(\"_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.primes\", __pyx_clineno, __pyx_lineno, __pyx_filename);\n  __Pyx_RefNannyFinishContext();\n  return NULL;\n  __pyx_L4_argument_unpacking_done:;\n  __pyx_r = __pyx_pf_78_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1_primes(__pyx_self, __pyx_v_nb_primes);\n  int __pyx_lineno = 0;\n  const char *__pyx_filename = NULL;\n  int __pyx_clineno = 0;\n\n  /* function exit code */\n  for (Py_ssize_t __pyx_temp=0; __pyx_temp &lt; (Py_ssize_t)(sizeof(values)/sizeof(values[0])); ++__pyx_temp) {\n    Py_XDECREF(values[__pyx_temp]);\n  }\n  __Pyx_RefNannyFinishContext();\n  return __pyx_r;\n}\n\nstatic PyObject *__pyx_pf_78_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1_primes(CYTHON_UNUSED PyObject *__pyx_self, PyObject *__pyx_v_nb_primes) {\n  PyObject *__pyx_v_i = 0;\n  PyObject *__pyx_v_p = NULL;\n  PyObject *__pyx_v_len_p = NULL;\n  PyObject *__pyx_v_n = NULL;\n  PyObject *__pyx_v_result_as_list = NULL;\n  PyObject *__pyx_7genexpr__pyx_v_prime = NULL;\n  PyObject *__pyx_r = NULL;\n  __Pyx_INCREF(__pyx_v_nb_primes);\n/* … */\n  __pyx_t_2 = __Pyx_PyDict_NewPresized(1); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_2);\n  if (PyDict_SetItem(__pyx_t_2, __pyx_mstate_global-&gt;__pyx_n_u_nb_primes, __pyx_mstate_global-&gt;__pyx_kp_u_cython_int) &lt; 0) __PYX_ERR(0, 1, __pyx_L1_error)\n  __pyx_t_3 = __Pyx_CyFunction_New(&__pyx_mdef_78_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1_1primes, 0, __pyx_mstate_global-&gt;__pyx_n_u_primes, NULL, __pyx_mstate_global-&gt;__pyx_n_u_cython_magic_3354ed55144f0f9820, __pyx_mstate_global-&gt;__pyx_d, ((PyObject *)__pyx_mstate_global-&gt;__pyx_codeobj_tab[0])); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_3);\n  __Pyx_CyFunction_SetAnnotationsDict(__pyx_t_3, __pyx_t_2);\n  __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;\n  if (PyDict_SetItem(__pyx_mstate_global-&gt;__pyx_d, __pyx_mstate_global-&gt;__pyx_n_u_primes, __pyx_t_3) &lt; 0) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;\n  __pyx_t_3 = __Pyx_PyDict_NewPresized(0); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_3);\n  if (PyDict_SetItem(__pyx_mstate_global-&gt;__pyx_d, __pyx_mstate_global-&gt;__pyx_n_u_test, __pyx_t_3) &lt; 0) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;\n 02:     i: cython.int\n+03:     p: cython.int[1000] = [0] * 1000\n  __pyx_t_1 = PyList_New(1 * 1000); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 3, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_1);\n  { Py_ssize_t __pyx_temp;\n    for (__pyx_temp=0; __pyx_temp &lt; 0x3E8; __pyx_temp++) {\n      __Pyx_INCREF(__pyx_mstate_global-&gt;__pyx_int_0);\n      __Pyx_GIVEREF(__pyx_mstate_global-&gt;__pyx_int_0);\n      if (__Pyx_PyList_SET_ITEM(__pyx_t_1, __pyx_temp, __pyx_mstate_global-&gt;__pyx_int_0) != (0)) __PYX_ERR(0, 3, __pyx_L1_error);\n    }\n  }\n  __pyx_v_p = ((PyObject*)__pyx_t_1);\n  __pyx_t_1 = 0;\n 04: \n+05:     if nb_primes &gt; 1000:\n  __pyx_t_1 = PyObject_RichCompare(__pyx_v_nb_primes, __pyx_mstate_global-&gt;__pyx_int_1000, Py_GT); __Pyx_XGOTREF(__pyx_t_1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 5, __pyx_L1_error)\n  __pyx_t_2 = __Pyx_PyObject_IsTrue(__pyx_t_1); if (unlikely((__pyx_t_2 &lt; 0))) __PYX_ERR(0, 5, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n  if (__pyx_t_2) {\n/* … */\n  }\n+06:         nb_primes = 1000\n    __Pyx_INCREF(__pyx_mstate_global-&gt;__pyx_int_1000);\n    __Pyx_DECREF_SET(__pyx_v_nb_primes, __pyx_mstate_global-&gt;__pyx_int_1000);\n 07: \n+08:     len_p: cython.int = 0  # The current number of elements in p.\n  __Pyx_INCREF(__pyx_mstate_global-&gt;__pyx_int_0);\n  __pyx_v_len_p = __pyx_mstate_global-&gt;__pyx_int_0;\n+09:     n: cython.int = 2\n  __Pyx_INCREF(__pyx_mstate_global-&gt;__pyx_int_2);\n  __pyx_v_n = __pyx_mstate_global-&gt;__pyx_int_2;\n+10:     while len_p &lt; nb_primes:\n  while (1) {\n    __pyx_t_1 = PyObject_RichCompare(__pyx_v_len_p, __pyx_v_nb_primes, Py_LT); __Pyx_XGOTREF(__pyx_t_1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 10, __pyx_L1_error)\n    __pyx_t_2 = __Pyx_PyObject_IsTrue(__pyx_t_1); if (unlikely((__pyx_t_2 &lt; 0))) __PYX_ERR(0, 10, __pyx_L1_error)\n    __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n    if (!__pyx_t_2) break;\n 11:         # Is n prime?\n+12:         for i in p[:len_p]:\n    __Pyx_INCREF(__pyx_v_len_p);\n    __pyx_t_1 = __pyx_v_len_p;\n    __pyx_t_2 = (__pyx_t_1 == Py_None);\n    if (__pyx_t_2) {\n      __pyx_t_3 = PY_SSIZE_T_MAX;\n    } else {\n      __pyx_t_4 = __Pyx_PyIndex_AsSsize_t(__pyx_t_1); if (unlikely((__pyx_t_4 == (Py_ssize_t)-1) && PyErr_Occurred())) __PYX_ERR(0, 12, __pyx_L1_error)\n      __pyx_t_3 = __pyx_t_4;\n    }\n    __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n    __pyx_t_1 = __Pyx_PyList_GetSlice(__pyx_v_p, 0, __pyx_t_3); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 12, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_1);\n    __pyx_t_5 = __pyx_t_1; __Pyx_INCREF(__pyx_t_5);\n    __pyx_t_3 = 0;\n    __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n    for (;;) {\n      {\n        Py_ssize_t __pyx_temp = __Pyx_PyList_GET_SIZE(__pyx_t_5);\n        #if !CYTHON_ASSUME_SAFE_SIZE\n        if (unlikely((__pyx_temp &lt; 0))) __PYX_ERR(0, 12, __pyx_L1_error)\n        #endif\n        if (__pyx_t_3 &gt;= __pyx_temp) break;\n      }\n      __pyx_t_1 = __Pyx_PyList_GetItemRef(__pyx_t_5, __pyx_t_3);\n      ++__pyx_t_3;\n      if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 12, __pyx_L1_error)\n      __Pyx_GOTREF(__pyx_t_1);\n      __Pyx_XDECREF_SET(__pyx_v_i, __pyx_t_1);\n      __pyx_t_1 = 0;\n/* … */\n    }\n    __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;\n    goto __pyx_L9_for_else;\n    __pyx_L7_break:;\n    __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;\n    goto __pyx_L10_for_end;\n    /*else*/ {\n      __pyx_L9_for_else:;\n+13:             if n % i == 0:\n      __pyx_t_1 = PyNumber_Remainder(__pyx_v_n, __pyx_v_i); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 13, __pyx_L1_error)\n      __Pyx_GOTREF(__pyx_t_1);\n      __pyx_t_2 = (__Pyx_PyLong_BoolEqObjC(__pyx_t_1, __pyx_mstate_global-&gt;__pyx_int_0, 0, 0)); if (unlikely((__pyx_t_2 &lt; 0))) __PYX_ERR(0, 13, __pyx_L1_error)\n      __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n      if (__pyx_t_2) {\n/* … */\n      }\n+14:                 break\n        goto __pyx_L7_break;\n 15: \n 16:         # If no break occurred in the loop, we have a prime.\n 17:         else:\n+18:             p[len_p] = n\n      if (unlikely((PyObject_SetItem(__pyx_v_p, __pyx_v_len_p, __pyx_v_n) &lt; 0))) __PYX_ERR(0, 18, __pyx_L1_error)\n+19:             len_p += 1\n      __pyx_t_5 = __Pyx_PyLong_AddObjC(__pyx_v_len_p, __pyx_mstate_global-&gt;__pyx_int_1, 1, 1, 0); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 19, __pyx_L1_error)\n      __Pyx_GOTREF(__pyx_t_5);\n      __Pyx_DECREF_SET(__pyx_v_len_p, __pyx_t_5);\n      __pyx_t_5 = 0;\n    }\n    __pyx_L10_for_end:;\n+20:         n += 1\n    __pyx_t_5 = __Pyx_PyLong_AddObjC(__pyx_v_n, __pyx_mstate_global-&gt;__pyx_int_1, 1, 1, 0); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 20, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_5);\n    __Pyx_DECREF_SET(__pyx_v_n, __pyx_t_5);\n    __pyx_t_5 = 0;\n  }\n 21: \n 22:     # Let's copy the result into a Python list:\n+23:     result_as_list = [prime for prime in p[:len_p]]\n  { /* enter inner scope */\n    __pyx_t_5 = PyList_New(0); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 23, __pyx_L13_error)\n    __Pyx_GOTREF(__pyx_t_5);\n    __Pyx_INCREF(__pyx_v_len_p);\n    __pyx_t_1 = __pyx_v_len_p;\n    __pyx_t_2 = (__pyx_t_1 == Py_None);\n    if (__pyx_t_2) {\n      __pyx_t_3 = PY_SSIZE_T_MAX;\n    } else {\n      __pyx_t_4 = __Pyx_PyIndex_AsSsize_t(__pyx_t_1); if (unlikely((__pyx_t_4 == (Py_ssize_t)-1) && PyErr_Occurred())) __PYX_ERR(0, 23, __pyx_L13_error)\n      __pyx_t_3 = __pyx_t_4;\n    }\n    __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n    __pyx_t_1 = __Pyx_PyList_GetSlice(__pyx_v_p, 0, __pyx_t_3); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 23, __pyx_L13_error)\n    __Pyx_GOTREF(__pyx_t_1);\n    __pyx_t_6 = __pyx_t_1; __Pyx_INCREF(__pyx_t_6);\n    __pyx_t_3 = 0;\n    __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n    for (;;) {\n      {\n        Py_ssize_t __pyx_temp = __Pyx_PyList_GET_SIZE(__pyx_t_6);\n        #if !CYTHON_ASSUME_SAFE_SIZE\n        if (unlikely((__pyx_temp &lt; 0))) __PYX_ERR(0, 23, __pyx_L13_error)\n        #endif\n        if (__pyx_t_3 &gt;= __pyx_temp) break;\n      }\n      __pyx_t_1 = __Pyx_PyList_GetItemRef(__pyx_t_6, __pyx_t_3);\n      ++__pyx_t_3;\n      if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 23, __pyx_L13_error)\n      __Pyx_GOTREF(__pyx_t_1);\n      __Pyx_XDECREF_SET(__pyx_7genexpr__pyx_v_prime, __pyx_t_1);\n      __pyx_t_1 = 0;\n      if (unlikely(__Pyx_ListComp_Append(__pyx_t_5, (PyObject*)__pyx_7genexpr__pyx_v_prime))) __PYX_ERR(0, 23, __pyx_L13_error)\n    }\n    __Pyx_DECREF(__pyx_t_6); __pyx_t_6 = 0;\n    __Pyx_XDECREF(__pyx_7genexpr__pyx_v_prime); __pyx_7genexpr__pyx_v_prime = 0;\n    goto __pyx_L17_exit_scope;\n    __pyx_L13_error:;\n    __Pyx_XDECREF(__pyx_7genexpr__pyx_v_prime); __pyx_7genexpr__pyx_v_prime = 0;\n    goto __pyx_L1_error;\n    __pyx_L17_exit_scope:;\n  } /* exit inner scope */\n  __pyx_v_result_as_list = ((PyObject*)__pyx_t_5);\n  __pyx_t_5 = 0;\n+24:     return result_as_list\n  __Pyx_XDECREF(__pyx_r);\n  __Pyx_INCREF(__pyx_v_result_as_list);\n  __pyx_r = __pyx_v_result_as_list;\n  goto __pyx_L0;\n\n\n\n\n%timeit primes(10)\n\n1.94 μs ± 19.9 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n%%cython\ndef f_typed(x: cython.double):\n    return x * (x - 1)\n\ndef integrate_f_typed(a: cython.double, b: cython.double, N: cython.int):\n  i: cython.int\n  s: cython.double\n  dx: cython.double\n  s = 0\n  dx = (b - a) / N\n  for i in range(N):\n    s += f_typed(a + i * dx)\n  return s * dx\n\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.pyx:1:21: Unknown type declaration 'cython.double' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.pyx:4:31: Unknown type declaration 'cython.double' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.pyx:4:49: Unknown type declaration 'cython.double' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.pyx:4:67: Unknown type declaration 'cython.int' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.pyx:5:11: Unknown type declaration 'cython.int' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.pyx:6:11: Unknown type declaration 'cython.double' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.pyx:7:12: Unknown type declaration 'cython.double' in annotation, ignoring\n\n\nContent of stderr:\n/Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.c:4663:17: warning: fallthrough annotation in unreachable code [-Wunreachable-code-fallthrough]\n 4663 |                 CYTHON_FALLTHROUGH;\n      |                 ^\n/Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.c:512:34: note: expanded from macro 'CYTHON_FALLTHROUGH'\n  512 |       #define CYTHON_FALLTHROUGH __attribute__((fallthrough))\n      |                                  ^\n/Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.c:4674:17: warning: fallthrough annotation in unreachable code [-Wunreachable-code-fallthrough]\n 4674 |                 CYTHON_FALLTHROUGH;\n      |                 ^\n/Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.c:512:34: note: expanded from macro 'CYTHON_FALLTHROUGH'\n  512 |       #define CYTHON_FALLTHROUGH __attribute__((fallthrough))\n      |                                  ^\n2 warnings generated.\nld: warning: search path 'Modules/_hacl' not found\n\n\n\nf_typed(5)\n\n20\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {\n        \"a\": np.random.randn(1000),\n        \"b\": np.random.randn(1000),\n        \"N\": np.random.randint(100, 1000, (1000)),\n        \"x\": \"x\",\n    }\n)\n\n\n%timeit df.apply(lambda x: integrate_f_typed(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n\n24.5 ms ± 76 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "nb/lab12/Lab12.html",
    "href": "nb/lab12/Lab12.html",
    "title": "Lab 12",
    "section": "",
    "text": "Install Docker from docker.com\nRun MinIO docker image (more details here)\n\n  docker run \\\n   -p 9000:9000 \\\n   -p 9001:9001 \\\n   --name minio1 \\\n   -v &lt;YOUR_DATA_DIR&gt;:/data \\\n   -e \"MINIO_ROOT_USER=ROOTUSER\" \\\n   -e \"MINIO_ROOT_PASSWORD=CHANGEME123\" \\\n   quay.io/minio/minio server /data --console-address \":9001\"\n\nLogin to MinIO console via localhost:9001, and create an access key.\nUsing this key, establish connection to MinIO via Python’s s3fs library:\n\n\nfrom s3fs import S3FileSystem\n\nkey = \"&lt;ACCESS_KEY&gt;\"\nsecret = \"&lt;SECRET_KEY&gt;\"\nendpoint_url = \"localhost:8080\"\n\ns3 = S3FileSystem(anon=False, endpoint_url=endpoint_url,\n                 key=key,\n                 secret=secret,\n                 use_ssl=False)\n\n\nConvert the a result of some SELECT from Postgres DB into a Parquet file and upload it to MinIO\n\nUse Dask’s read_sql\nThen save the resulting DataFrame via to_parquet\nStore the Parquet file via s3.put()"
  },
  {
    "objectID": "nb/lab14/Lab14.html",
    "href": "nb/lab14/Lab14.html",
    "title": "Docker",
    "section": "",
    "text": "Download Docker from https://www.docker.com/products/docker-desktop/.\nInstall it, give it admin permissions if asked.\nMost of the commands listed below can be executed from within Jupyter notebook, by prefixing with an exclamation mark (!). However, some things require running from the shell. On Unix-like systems, it’s called Terminal. On Windows, it’s Command Prompt.\nCheck that Docker is installed by running docker version:\n\n!docker version\n\nClient:\n Version:           27.2.0\n API version:       1.47\n Go version:        go1.21.13\n Git commit:        3ab4256\n Built:             Tue Aug 27 14:14:45 2024\n OS/Arch:           darwin/arm64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.34.3 (170107)\n Engine:\n  Version:          27.2.0\n  API version:      1.47 (minimum version 1.24)\n  Go version:       go1.21.13\n  Git commit:       3ab5c7d\n  Built:            Tue Aug 27 14:15:41 2024\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          1.7.20\n  GitCommit:        8fc6bcff51318944179630522a095cc9dbf9f353\n runc:\n  Version:          1.1.13\n  GitCommit:        v1.1.13-0-g58aa920\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n\n\nAll Docker commands are grouped into categories (e.g. container, image, volume, etc.).\nSo if you want to work on images, the corresponding command will start with docker image, on networks - with docker network, and so on.\nIf not category is listed, container is assumed by default.\nLet’s run an extremely simple container, and execute some command in it.\n\n!docker container run alpine echo \"Hello World\"\n\nUnable to find image 'alpine:latest' locally\nlatest: Pulling from library/alpine\n\nc63912e1: Download complete MB/4.088MBDigest: sha256:beefdbd8a1da6d2915566fde36db9db0b524eb737fc57cd1367effd16dc0d06d\nStatus: Downloaded newer image for alpine:latest\nHello World\n\n\nthe above command has 4 parts:\n\ncontainer - means that we want to do something with a container (run, remove, etc.)\nrun - we will run the container\nalpine - this is the name of the image to run inside the container. Alpine is a simple Linux version optimized for containers, with very small footprint\nand, finally, echo \"Hello World\" is the command we want to immediately execute inside the container.\n\nLet’s try another image, this time centos (another popular Linux distribution):\n\n!docker container run centos ping -c 5 127.0.0.1\n\nUnable to find image 'centos:latest' locally\nlatest: Pulling from library/centos\n\nef134af7: Download complete MB/83.94MBDigest: sha256:a27fd8080b517143cbbbab9dfb7c8571c40d67d534bbdee55bd6c473f432b177\nStatus: Downloaded newer image for centos:latest\nPING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.\n64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.041 ms\n64 bytes from 127.0.0.1: icmp_seq=2 ttl=64 time=0.099 ms\n64 bytes from 127.0.0.1: icmp_seq=3 ttl=64 time=0.087 ms\n64 bytes from 127.0.0.1: icmp_seq=4 ttl=64 time=0.094 ms\n64 bytes from 127.0.0.1: icmp_seq=5 ttl=64 time=0.074 ms\n\n--- 127.0.0.1 ping statistics ---\n5 packets transmitted, 5 received, 0% packet loss, time 4118ms\nrtt min/avg/max/mdev = 0.041/0.079/0.099/0.020 ms\n\n\nNote that it pings 5 times, and this output is shown in the notebook.\nNow let’s try passing some command options:\n\n!docker container run --detach --name trivia fundamentalsofdocker/trivia:ed2\n\nHere --detach means that we leave it running in the background (no messages pushed to the output cell).\nAnd --name is there for optionally specifying a container name. If not explicitly given, Docker will assign some name by itself.\nWe can list currently running containers with ls command. -l means “provide detailed output”.\n\n!docker container ls -l\n\nCONTAINER ID   IMAGE                             COMMAND                  CREATED          STATUS          PORTS     NAMES\nbca55b773358   fundamentalsofdocker/trivia:ed2   \"/bin/sh -c 'source …\"   27 seconds ago   Up 26 seconds             trivia\n\n\nAnd we can remove a container with rm. --force option is there so that Docker does not complain that the container in question is still running. Note that we skip the container part, as by default it’s container anyway.\n\n!docker rm --force trivia\n\nCannot connect to the Docker daemon at unix:///Users/vitvly/.docker/run/docker.sock. Is the docker daemon running?\n\n\nNow we shouldn’t see any running containers:\n\n!docker container ls\n\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n\n\nWe can also passing a -a option. This will list all containers, not only the currently running ones.\n\n!docker container ls -a\n\nCONTAINER ID   IMAGE                 COMMAND                  CREATED          STATUS                      PORTS     NAMES\ne0a8ea8c48e9   centos                \"ping -c 5 127.0.0.1\"    54 minutes ago   Exited (0) 54 minutes ago             nifty_neumann\n98beda218482   alpine                \"echo 'Hello World'\"     56 minutes ago   Exited (0) 56 minutes ago             quizzical_pascal\n44fe2fb9314d   alpine                \"echo 'Hello World'\"     57 minutes ago   Exited (0) 57 minutes ago             crazy_bouman\nab7337959b5e   quay.io/minio/minio   \"/usr/bin/docker-ent…\"   6 days ago       Exited (0) 6 days ago                 minio\n\n\nTake some time to examine the list and columns.\nThe -q option will only list container IDs:\n\n!docker container ls -a -q\n\ne0a8ea8c48e9\n98beda218482\n44fe2fb9314d\nab7337959b5e\n\n\nAlso, you can invoke --help command option to list all available options for particular command:\n\n!docker container ls --help\n\n\nUsage:  docker container ls [OPTIONS]\n\nList containers\n\nAliases:\n  docker container ls, docker container list, docker container ps, docker ps\n\nOptions:\n  -a, --all             Show all containers (default shows just running)\n  -f, --filter filter   Filter output based on conditions provided\n      --format string   Format output using a custom template:\n                        'table':            Print output in table format\n                        with column headers (default)\n                        'table TEMPLATE':   Print output in table format\n                        using the given Go template\n                        'json':             Print in JSON format\n                        'TEMPLATE':         Print output using the given\n                        Go template.\n                        Refer to https://docs.docker.com/go/formatting/\n                        for more information about formatting output with\n                        templates\n  -n, --last int        Show n last created containers (includes all\n                        states) (default -1)\n  -l, --latest          Show the latest created container (includes all\n                        states)\n      --no-trunc        Don't truncate output\n  -q, --quiet           Only display container IDs\n  -s, --size            Display total file sizes\n\n\nThere is also a stop container command that stops a container by ID or name:\n\n!docker container stop trivia\n\ntrivia\n\n\nAfter stopping, container will be down, but not removed, hence visible in the output of ls -a:\n\n!docker container ls -aq\n\n576a24020ffd\n\n\nWe can start the container again via start command:\n\n!docker container start 576a24020ffd\n\n576a24020ffd\n\n\n\n!docker container stop trivia\n\ntrivia\n\n\n\n!docker container ls -a\n\nCONTAINER ID   IMAGE                             COMMAND                  CREATED         STATUS                        PORTS     NAMES\n576a24020ffd   fundamentalsofdocker/trivia:ed2   \"/bin/sh -c 'source …\"   3 minutes ago   Exited (137) 27 seconds ago             trivia\n\n\nNow, we can inspect the container via inspect command:\n\n!docker container inspect trivia\n\n[\n    {\n        \"Id\": \"3eb3a1011d623c9c13c22ba6b0130b34b0db071c5e9f96254a9f76bfe1ffb81d\",\n        \"Created\": \"2024-11-05T10:35:21.679070631Z\",\n        \"Path\": \"/bin/sh\",\n        \"Args\": [\n            \"-c\",\n            \"source script.sh\"\n        ],\n        \"State\": {\n            \"Status\": \"running\",\n            \"Running\": true,\n            \"Paused\": false,\n            \"Restarting\": false,\n            \"OOMKilled\": false,\n            \"Dead\": false,\n            \"Pid\": 1825,\n            \"ExitCode\": 0,\n            \"Error\": \"\",\n            \"StartedAt\": \"2024-11-05T10:35:21.711150631Z\",\n            \"FinishedAt\": \"0001-01-01T00:00:00Z\"\n        },\n        \"Image\": \"sha256:0bb60b6958950c25f7946646eaa6deb9f1db1597d8fbe70d869d67123066dfda\",\n        \"ResolvConfPath\": \"/var/lib/docker/containers/3eb3a1011d623c9c13c22ba6b0130b34b0db071c5e9f96254a9f76bfe1ffb81d/resolv.conf\",\n        \"HostnamePath\": \"/var/lib/docker/containers/3eb3a1011d623c9c13c22ba6b0130b34b0db071c5e9f96254a9f76bfe1ffb81d/hostname\",\n        \"HostsPath\": \"/var/lib/docker/containers/3eb3a1011d623c9c13c22ba6b0130b34b0db071c5e9f96254a9f76bfe1ffb81d/hosts\",\n        \"LogPath\": \"/var/lib/docker/containers/3eb3a1011d623c9c13c22ba6b0130b34b0db071c5e9f96254a9f76bfe1ffb81d/3eb3a1011d623c9c13c22ba6b0130b34b0db071c5e9f96254a9f76bfe1ffb81d-json.log\",\n        \"Name\": \"/trivia\",\n        \"RestartCount\": 0,\n        \"Driver\": \"overlayfs\",\n        \"Platform\": \"linux\",\n        \"MountLabel\": \"\",\n        \"ProcessLabel\": \"\",\n        \"AppArmorProfile\": \"\",\n        \"ExecIDs\": null,\n        \"HostConfig\": {\n            \"Binds\": null,\n            \"ContainerIDFile\": \"\",\n            \"LogConfig\": {\n                \"Type\": \"json-file\",\n                \"Config\": {}\n            },\n            \"NetworkMode\": \"bridge\",\n            \"PortBindings\": {},\n            \"RestartPolicy\": {\n                \"Name\": \"no\",\n                \"MaximumRetryCount\": 0\n            },\n            \"AutoRemove\": false,\n            \"VolumeDriver\": \"\",\n            \"VolumesFrom\": null,\n            \"ConsoleSize\": [\n                24,\n                80\n            ],\n            \"CapAdd\": null,\n            \"CapDrop\": null,\n            \"CgroupnsMode\": \"private\",\n            \"Dns\": [],\n            \"DnsOptions\": [],\n            \"DnsSearch\": [],\n            \"ExtraHosts\": null,\n            \"GroupAdd\": null,\n            \"IpcMode\": \"private\",\n            \"Cgroup\": \"\",\n            \"Links\": null,\n            \"OomScoreAdj\": 0,\n            \"PidMode\": \"\",\n            \"Privileged\": false,\n            \"PublishAllPorts\": false,\n            \"ReadonlyRootfs\": false,\n            \"SecurityOpt\": null,\n            \"UTSMode\": \"\",\n            \"UsernsMode\": \"\",\n            \"ShmSize\": 67108864,\n            \"Runtime\": \"runc\",\n            \"Isolation\": \"\",\n            \"CpuShares\": 0,\n            \"Memory\": 0,\n            \"NanoCpus\": 0,\n            \"CgroupParent\": \"\",\n            \"BlkioWeight\": 0,\n            \"BlkioWeightDevice\": [],\n            \"BlkioDeviceReadBps\": [],\n            \"BlkioDeviceWriteBps\": [],\n            \"BlkioDeviceReadIOps\": [],\n            \"BlkioDeviceWriteIOps\": [],\n            \"CpuPeriod\": 0,\n            \"CpuQuota\": 0,\n            \"CpuRealtimePeriod\": 0,\n            \"CpuRealtimeRuntime\": 0,\n            \"CpusetCpus\": \"\",\n            \"CpusetMems\": \"\",\n            \"Devices\": [],\n            \"DeviceCgroupRules\": null,\n            \"DeviceRequests\": null,\n            \"MemoryReservation\": 0,\n            \"MemorySwap\": 0,\n            \"MemorySwappiness\": null,\n            \"OomKillDisable\": null,\n            \"PidsLimit\": null,\n            \"Ulimits\": [],\n            \"CpuCount\": 0,\n            \"CpuPercent\": 0,\n            \"IOMaximumIOps\": 0,\n            \"IOMaximumBandwidth\": 0,\n            \"MaskedPaths\": [\n                \"/proc/asound\",\n                \"/proc/acpi\",\n                \"/proc/kcore\",\n                \"/proc/keys\",\n                \"/proc/latency_stats\",\n                \"/proc/timer_list\",\n                \"/proc/timer_stats\",\n                \"/proc/sched_debug\",\n                \"/proc/scsi\",\n                \"/sys/firmware\",\n                \"/sys/devices/virtual/powercap\"\n            ],\n            \"ReadonlyPaths\": [\n                \"/proc/bus\",\n                \"/proc/fs\",\n                \"/proc/irq\",\n                \"/proc/sys\",\n                \"/proc/sysrq-trigger\"\n            ]\n        },\n        \"GraphDriver\": {\n            \"Data\": null,\n            \"Name\": \"overlayfs\"\n        },\n        \"Mounts\": [],\n        \"Config\": {\n            \"Hostname\": \"3eb3a1011d62\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n            ],\n            \"Cmd\": [\n                \"/bin/sh\",\n                \"-c\",\n                \"source script.sh\"\n            ],\n            \"Image\": \"fundamentalsofdocker/trivia:ed2\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"/app\",\n            \"Entrypoint\": null,\n            \"OnBuild\": null,\n            \"Labels\": {}\n        },\n        \"NetworkSettings\": {\n            \"Bridge\": \"\",\n            \"SandboxID\": \"6d9a80b1b1af3d5d834c4af2147d93288b33f62c277ae3f2b8ff660e94555d8a\",\n            \"SandboxKey\": \"/var/run/docker/netns/6d9a80b1b1af\",\n            \"Ports\": {},\n            \"HairpinMode\": false,\n            \"LinkLocalIPv6Address\": \"\",\n            \"LinkLocalIPv6PrefixLen\": 0,\n            \"SecondaryIPAddresses\": null,\n            \"SecondaryIPv6Addresses\": null,\n            \"EndpointID\": \"4fa0a42a9e4f9fbbaa976e7b2e7c001a207b4c9ba7d7e375804f2ff155c17575\",\n            \"Gateway\": \"172.17.0.1\",\n            \"GlobalIPv6Address\": \"\",\n            \"GlobalIPv6PrefixLen\": 0,\n            \"IPAddress\": \"172.17.0.2\",\n            \"IPPrefixLen\": 16,\n            \"IPv6Gateway\": \"\",\n            \"MacAddress\": \"02:42:ac:11:00:02\",\n            \"Networks\": {\n                \"bridge\": {\n                    \"IPAMConfig\": null,\n                    \"Links\": null,\n                    \"Aliases\": null,\n                    \"MacAddress\": \"02:42:ac:11:00:02\",\n                    \"DriverOpts\": null,\n                    \"NetworkID\": \"6c12aa86d2281ae2cc59f894cb088749e4d638f3fc679396209fb3e7ffd953be\",\n                    \"EndpointID\": \"4fa0a42a9e4f9fbbaa976e7b2e7c001a207b4c9ba7d7e375804f2ff155c17575\",\n                    \"Gateway\": \"172.17.0.1\",\n                    \"IPAddress\": \"172.17.0.2\",\n                    \"IPPrefixLen\": 16,\n                    \"IPv6Gateway\": \"\",\n                    \"GlobalIPv6Address\": \"\",\n                    \"GlobalIPv6PrefixLen\": 0,\n                    \"DNSNames\": null\n                }\n            }\n        }\n    }\n]\n\n\nAlso, we can execute an arbitrary command inside the running container via exec.\nNote: the below two examples has to be run inside Command Prompt:\n\n\n\ndocker container exec -it trivia /bin/sh\n\n\n\ndocker container exec trivia ps\nNow let’s remove the container (note that we don’t need --force option as it’s already stopped):\n\n!docker container rm 576a24020ffd\n\n576a24020ffd\n\n\n\n!docker container ls -a\n\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n\n\nIn order to wrap up the container section, let’s run a web server inside Docker:\n\n!docker container run -d --name nginx -p 8080:80 nginx:alpine\n\nUnable to find image 'nginx:alpine' locally\nalpine: Pulling from library/nginx\n\nee71aeeb: Pulling fs layer \nc7406b65: Pulling fs layer \n64b3e722: Pulling fs layer \n18b1a2c5: Pulling fs layer \ndf5b8f8a: Pulling fs layer \n342cad58: Pulling fs layer \nee71aeeb: Download complete MB/15.63MBDigest: sha256:2140dad235c130ac861018a4e13a6bc8aea3a35f3a40e20c1b060d51a7efd250\nStatus: Downloaded newer image for nginx:alpine\n29176eb030b5fe2bab3b9f68a00d350058f742926a405c303e9fe34077429afd\n\n\nNote that above -d is a short form of --detach, and -p 8080:80 maps ports: from 80 inside the container to 8080 on your machine.\nIf you now go to http://localhost:8080, you should see a Nginx welcome page.\nNow remove the container and check that no other containers remain:\n\n!docker container rm nginx\n\nnginx\n\n\n\n!docker container ls -a\n\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES"
  },
  {
    "objectID": "nb/lab14/Lab14.html#running",
    "href": "nb/lab14/Lab14.html#running",
    "title": "Docker",
    "section": "",
    "text": "Download Docker from https://www.docker.com/products/docker-desktop/.\nInstall it, give it admin permissions if asked.\nMost of the commands listed below can be executed from within Jupyter notebook, by prefixing with an exclamation mark (!). However, some things require running from the shell. On Unix-like systems, it’s called Terminal. On Windows, it’s Command Prompt.\nCheck that Docker is installed by running docker version:\n\n!docker version\n\nClient:\n Version:           27.2.0\n API version:       1.47\n Go version:        go1.21.13\n Git commit:        3ab4256\n Built:             Tue Aug 27 14:14:45 2024\n OS/Arch:           darwin/arm64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.34.3 (170107)\n Engine:\n  Version:          27.2.0\n  API version:      1.47 (minimum version 1.24)\n  Go version:       go1.21.13\n  Git commit:       3ab5c7d\n  Built:            Tue Aug 27 14:15:41 2024\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          1.7.20\n  GitCommit:        8fc6bcff51318944179630522a095cc9dbf9f353\n runc:\n  Version:          1.1.13\n  GitCommit:        v1.1.13-0-g58aa920\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n\n\nAll Docker commands are grouped into categories (e.g. container, image, volume, etc.).\nSo if you want to work on images, the corresponding command will start with docker image, on networks - with docker network, and so on.\nIf not category is listed, container is assumed by default.\nLet’s run an extremely simple container, and execute some command in it.\n\n!docker container run alpine echo \"Hello World\"\n\nUnable to find image 'alpine:latest' locally\nlatest: Pulling from library/alpine\n\nc63912e1: Download complete MB/4.088MBDigest: sha256:beefdbd8a1da6d2915566fde36db9db0b524eb737fc57cd1367effd16dc0d06d\nStatus: Downloaded newer image for alpine:latest\nHello World\n\n\nthe above command has 4 parts:\n\ncontainer - means that we want to do something with a container (run, remove, etc.)\nrun - we will run the container\nalpine - this is the name of the image to run inside the container. Alpine is a simple Linux version optimized for containers, with very small footprint\nand, finally, echo \"Hello World\" is the command we want to immediately execute inside the container.\n\nLet’s try another image, this time centos (another popular Linux distribution):\n\n!docker container run centos ping -c 5 127.0.0.1\n\nUnable to find image 'centos:latest' locally\nlatest: Pulling from library/centos\n\nef134af7: Download complete MB/83.94MBDigest: sha256:a27fd8080b517143cbbbab9dfb7c8571c40d67d534bbdee55bd6c473f432b177\nStatus: Downloaded newer image for centos:latest\nPING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.\n64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.041 ms\n64 bytes from 127.0.0.1: icmp_seq=2 ttl=64 time=0.099 ms\n64 bytes from 127.0.0.1: icmp_seq=3 ttl=64 time=0.087 ms\n64 bytes from 127.0.0.1: icmp_seq=4 ttl=64 time=0.094 ms\n64 bytes from 127.0.0.1: icmp_seq=5 ttl=64 time=0.074 ms\n\n--- 127.0.0.1 ping statistics ---\n5 packets transmitted, 5 received, 0% packet loss, time 4118ms\nrtt min/avg/max/mdev = 0.041/0.079/0.099/0.020 ms\n\n\nNote that it pings 5 times, and this output is shown in the notebook.\nNow let’s try passing some command options:\n\n!docker container run --detach --name trivia fundamentalsofdocker/trivia:ed2\n\nHere --detach means that we leave it running in the background (no messages pushed to the output cell).\nAnd --name is there for optionally specifying a container name. If not explicitly given, Docker will assign some name by itself.\nWe can list currently running containers with ls command. -l means “provide detailed output”.\n\n!docker container ls -l\n\nCONTAINER ID   IMAGE                             COMMAND                  CREATED          STATUS          PORTS     NAMES\nbca55b773358   fundamentalsofdocker/trivia:ed2   \"/bin/sh -c 'source …\"   27 seconds ago   Up 26 seconds             trivia\n\n\nAnd we can remove a container with rm. --force option is there so that Docker does not complain that the container in question is still running. Note that we skip the container part, as by default it’s container anyway.\n\n!docker rm --force trivia\n\nCannot connect to the Docker daemon at unix:///Users/vitvly/.docker/run/docker.sock. Is the docker daemon running?\n\n\nNow we shouldn’t see any running containers:\n\n!docker container ls\n\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n\n\nWe can also passing a -a option. This will list all containers, not only the currently running ones.\n\n!docker container ls -a\n\nCONTAINER ID   IMAGE                 COMMAND                  CREATED          STATUS                      PORTS     NAMES\ne0a8ea8c48e9   centos                \"ping -c 5 127.0.0.1\"    54 minutes ago   Exited (0) 54 minutes ago             nifty_neumann\n98beda218482   alpine                \"echo 'Hello World'\"     56 minutes ago   Exited (0) 56 minutes ago             quizzical_pascal\n44fe2fb9314d   alpine                \"echo 'Hello World'\"     57 minutes ago   Exited (0) 57 minutes ago             crazy_bouman\nab7337959b5e   quay.io/minio/minio   \"/usr/bin/docker-ent…\"   6 days ago       Exited (0) 6 days ago                 minio\n\n\nTake some time to examine the list and columns.\nThe -q option will only list container IDs:\n\n!docker container ls -a -q\n\ne0a8ea8c48e9\n98beda218482\n44fe2fb9314d\nab7337959b5e\n\n\nAlso, you can invoke --help command option to list all available options for particular command:\n\n!docker container ls --help\n\n\nUsage:  docker container ls [OPTIONS]\n\nList containers\n\nAliases:\n  docker container ls, docker container list, docker container ps, docker ps\n\nOptions:\n  -a, --all             Show all containers (default shows just running)\n  -f, --filter filter   Filter output based on conditions provided\n      --format string   Format output using a custom template:\n                        'table':            Print output in table format\n                        with column headers (default)\n                        'table TEMPLATE':   Print output in table format\n                        using the given Go template\n                        'json':             Print in JSON format\n                        'TEMPLATE':         Print output using the given\n                        Go template.\n                        Refer to https://docs.docker.com/go/formatting/\n                        for more information about formatting output with\n                        templates\n  -n, --last int        Show n last created containers (includes all\n                        states) (default -1)\n  -l, --latest          Show the latest created container (includes all\n                        states)\n      --no-trunc        Don't truncate output\n  -q, --quiet           Only display container IDs\n  -s, --size            Display total file sizes\n\n\nThere is also a stop container command that stops a container by ID or name:\n\n!docker container stop trivia\n\ntrivia\n\n\nAfter stopping, container will be down, but not removed, hence visible in the output of ls -a:\n\n!docker container ls -aq\n\n576a24020ffd\n\n\nWe can start the container again via start command:\n\n!docker container start 576a24020ffd\n\n576a24020ffd\n\n\n\n!docker container stop trivia\n\ntrivia\n\n\n\n!docker container ls -a\n\nCONTAINER ID   IMAGE                             COMMAND                  CREATED         STATUS                        PORTS     NAMES\n576a24020ffd   fundamentalsofdocker/trivia:ed2   \"/bin/sh -c 'source …\"   3 minutes ago   Exited (137) 27 seconds ago             trivia\n\n\nNow, we can inspect the container via inspect command:\n\n!docker container inspect trivia\n\n[\n    {\n        \"Id\": \"3eb3a1011d623c9c13c22ba6b0130b34b0db071c5e9f96254a9f76bfe1ffb81d\",\n        \"Created\": \"2024-11-05T10:35:21.679070631Z\",\n        \"Path\": \"/bin/sh\",\n        \"Args\": [\n            \"-c\",\n            \"source script.sh\"\n        ],\n        \"State\": {\n            \"Status\": \"running\",\n            \"Running\": true,\n            \"Paused\": false,\n            \"Restarting\": false,\n            \"OOMKilled\": false,\n            \"Dead\": false,\n            \"Pid\": 1825,\n            \"ExitCode\": 0,\n            \"Error\": \"\",\n            \"StartedAt\": \"2024-11-05T10:35:21.711150631Z\",\n            \"FinishedAt\": \"0001-01-01T00:00:00Z\"\n        },\n        \"Image\": \"sha256:0bb60b6958950c25f7946646eaa6deb9f1db1597d8fbe70d869d67123066dfda\",\n        \"ResolvConfPath\": \"/var/lib/docker/containers/3eb3a1011d623c9c13c22ba6b0130b34b0db071c5e9f96254a9f76bfe1ffb81d/resolv.conf\",\n        \"HostnamePath\": \"/var/lib/docker/containers/3eb3a1011d623c9c13c22ba6b0130b34b0db071c5e9f96254a9f76bfe1ffb81d/hostname\",\n        \"HostsPath\": \"/var/lib/docker/containers/3eb3a1011d623c9c13c22ba6b0130b34b0db071c5e9f96254a9f76bfe1ffb81d/hosts\",\n        \"LogPath\": \"/var/lib/docker/containers/3eb3a1011d623c9c13c22ba6b0130b34b0db071c5e9f96254a9f76bfe1ffb81d/3eb3a1011d623c9c13c22ba6b0130b34b0db071c5e9f96254a9f76bfe1ffb81d-json.log\",\n        \"Name\": \"/trivia\",\n        \"RestartCount\": 0,\n        \"Driver\": \"overlayfs\",\n        \"Platform\": \"linux\",\n        \"MountLabel\": \"\",\n        \"ProcessLabel\": \"\",\n        \"AppArmorProfile\": \"\",\n        \"ExecIDs\": null,\n        \"HostConfig\": {\n            \"Binds\": null,\n            \"ContainerIDFile\": \"\",\n            \"LogConfig\": {\n                \"Type\": \"json-file\",\n                \"Config\": {}\n            },\n            \"NetworkMode\": \"bridge\",\n            \"PortBindings\": {},\n            \"RestartPolicy\": {\n                \"Name\": \"no\",\n                \"MaximumRetryCount\": 0\n            },\n            \"AutoRemove\": false,\n            \"VolumeDriver\": \"\",\n            \"VolumesFrom\": null,\n            \"ConsoleSize\": [\n                24,\n                80\n            ],\n            \"CapAdd\": null,\n            \"CapDrop\": null,\n            \"CgroupnsMode\": \"private\",\n            \"Dns\": [],\n            \"DnsOptions\": [],\n            \"DnsSearch\": [],\n            \"ExtraHosts\": null,\n            \"GroupAdd\": null,\n            \"IpcMode\": \"private\",\n            \"Cgroup\": \"\",\n            \"Links\": null,\n            \"OomScoreAdj\": 0,\n            \"PidMode\": \"\",\n            \"Privileged\": false,\n            \"PublishAllPorts\": false,\n            \"ReadonlyRootfs\": false,\n            \"SecurityOpt\": null,\n            \"UTSMode\": \"\",\n            \"UsernsMode\": \"\",\n            \"ShmSize\": 67108864,\n            \"Runtime\": \"runc\",\n            \"Isolation\": \"\",\n            \"CpuShares\": 0,\n            \"Memory\": 0,\n            \"NanoCpus\": 0,\n            \"CgroupParent\": \"\",\n            \"BlkioWeight\": 0,\n            \"BlkioWeightDevice\": [],\n            \"BlkioDeviceReadBps\": [],\n            \"BlkioDeviceWriteBps\": [],\n            \"BlkioDeviceReadIOps\": [],\n            \"BlkioDeviceWriteIOps\": [],\n            \"CpuPeriod\": 0,\n            \"CpuQuota\": 0,\n            \"CpuRealtimePeriod\": 0,\n            \"CpuRealtimeRuntime\": 0,\n            \"CpusetCpus\": \"\",\n            \"CpusetMems\": \"\",\n            \"Devices\": [],\n            \"DeviceCgroupRules\": null,\n            \"DeviceRequests\": null,\n            \"MemoryReservation\": 0,\n            \"MemorySwap\": 0,\n            \"MemorySwappiness\": null,\n            \"OomKillDisable\": null,\n            \"PidsLimit\": null,\n            \"Ulimits\": [],\n            \"CpuCount\": 0,\n            \"CpuPercent\": 0,\n            \"IOMaximumIOps\": 0,\n            \"IOMaximumBandwidth\": 0,\n            \"MaskedPaths\": [\n                \"/proc/asound\",\n                \"/proc/acpi\",\n                \"/proc/kcore\",\n                \"/proc/keys\",\n                \"/proc/latency_stats\",\n                \"/proc/timer_list\",\n                \"/proc/timer_stats\",\n                \"/proc/sched_debug\",\n                \"/proc/scsi\",\n                \"/sys/firmware\",\n                \"/sys/devices/virtual/powercap\"\n            ],\n            \"ReadonlyPaths\": [\n                \"/proc/bus\",\n                \"/proc/fs\",\n                \"/proc/irq\",\n                \"/proc/sys\",\n                \"/proc/sysrq-trigger\"\n            ]\n        },\n        \"GraphDriver\": {\n            \"Data\": null,\n            \"Name\": \"overlayfs\"\n        },\n        \"Mounts\": [],\n        \"Config\": {\n            \"Hostname\": \"3eb3a1011d62\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n            ],\n            \"Cmd\": [\n                \"/bin/sh\",\n                \"-c\",\n                \"source script.sh\"\n            ],\n            \"Image\": \"fundamentalsofdocker/trivia:ed2\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"/app\",\n            \"Entrypoint\": null,\n            \"OnBuild\": null,\n            \"Labels\": {}\n        },\n        \"NetworkSettings\": {\n            \"Bridge\": \"\",\n            \"SandboxID\": \"6d9a80b1b1af3d5d834c4af2147d93288b33f62c277ae3f2b8ff660e94555d8a\",\n            \"SandboxKey\": \"/var/run/docker/netns/6d9a80b1b1af\",\n            \"Ports\": {},\n            \"HairpinMode\": false,\n            \"LinkLocalIPv6Address\": \"\",\n            \"LinkLocalIPv6PrefixLen\": 0,\n            \"SecondaryIPAddresses\": null,\n            \"SecondaryIPv6Addresses\": null,\n            \"EndpointID\": \"4fa0a42a9e4f9fbbaa976e7b2e7c001a207b4c9ba7d7e375804f2ff155c17575\",\n            \"Gateway\": \"172.17.0.1\",\n            \"GlobalIPv6Address\": \"\",\n            \"GlobalIPv6PrefixLen\": 0,\n            \"IPAddress\": \"172.17.0.2\",\n            \"IPPrefixLen\": 16,\n            \"IPv6Gateway\": \"\",\n            \"MacAddress\": \"02:42:ac:11:00:02\",\n            \"Networks\": {\n                \"bridge\": {\n                    \"IPAMConfig\": null,\n                    \"Links\": null,\n                    \"Aliases\": null,\n                    \"MacAddress\": \"02:42:ac:11:00:02\",\n                    \"DriverOpts\": null,\n                    \"NetworkID\": \"6c12aa86d2281ae2cc59f894cb088749e4d638f3fc679396209fb3e7ffd953be\",\n                    \"EndpointID\": \"4fa0a42a9e4f9fbbaa976e7b2e7c001a207b4c9ba7d7e375804f2ff155c17575\",\n                    \"Gateway\": \"172.17.0.1\",\n                    \"IPAddress\": \"172.17.0.2\",\n                    \"IPPrefixLen\": 16,\n                    \"IPv6Gateway\": \"\",\n                    \"GlobalIPv6Address\": \"\",\n                    \"GlobalIPv6PrefixLen\": 0,\n                    \"DNSNames\": null\n                }\n            }\n        }\n    }\n]\n\n\nAlso, we can execute an arbitrary command inside the running container via exec.\nNote: the below two examples has to be run inside Command Prompt:\n\n\n\ndocker container exec -it trivia /bin/sh\n\n\n\ndocker container exec trivia ps\nNow let’s remove the container (note that we don’t need --force option as it’s already stopped):\n\n!docker container rm 576a24020ffd\n\n576a24020ffd\n\n\n\n!docker container ls -a\n\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n\n\nIn order to wrap up the container section, let’s run a web server inside Docker:\n\n!docker container run -d --name nginx -p 8080:80 nginx:alpine\n\nUnable to find image 'nginx:alpine' locally\nalpine: Pulling from library/nginx\n\nee71aeeb: Pulling fs layer \nc7406b65: Pulling fs layer \n64b3e722: Pulling fs layer \n18b1a2c5: Pulling fs layer \ndf5b8f8a: Pulling fs layer \n342cad58: Pulling fs layer \nee71aeeb: Download complete MB/15.63MBDigest: sha256:2140dad235c130ac861018a4e13a6bc8aea3a35f3a40e20c1b060d51a7efd250\nStatus: Downloaded newer image for nginx:alpine\n29176eb030b5fe2bab3b9f68a00d350058f742926a405c303e9fe34077429afd\n\n\nNote that above -d is a short form of --detach, and -p 8080:80 maps ports: from 80 inside the container to 8080 on your machine.\nIf you now go to http://localhost:8080, you should see a Nginx welcome page.\nNow remove the container and check that no other containers remain:\n\n!docker container rm nginx\n\nnginx\n\n\n\n!docker container ls -a\n\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES"
  },
  {
    "objectID": "nb/lab14/Lab14.html#images",
    "href": "nb/lab14/Lab14.html#images",
    "title": "Docker",
    "section": "Images",
    "text": "Images\nDocker images are descriptions of what software needs to be run inside a Docker container. In other words, containers are running instances of images.\nInside pinger subdir, there is a Dockerfile with the below contents:\nFROM alpine:3.17\nENTRYPOINT [ \"ping\" ]\nCMD [ \"-c\", \"3\", \"8.8.8.8\" ]\nEach line in the file specifies the steps required to build the image. These steps can also be though of as layers.\n\nWe select alpine:3.17 as the base to start from (this means Alpine Linux version 3.17)\nWe specify the command to run upon container startup via ENTRYPOINT\nAnd specify the parameters to the command via CMD. The arguments to CMD can be overridden on-the-fly when we run the container.\n\nNote: it’s possible to specify everything inside CMD, not using ENTRYPOINT. But in that case we will not be able to override the default CMD.\nWe can now build the image via the below command:\n\n!docker image build -t img_pinger pinger\n\n\n[+] Building 0.0s (0/1)                                    docker:desktop-linux\n\n[+] Building 0.2s (1/2)                                    docker:desktop-linux\n\n =&gt; [internal] load build definition from Dockerfile                       0.0s\n\n =&gt; =&gt; transferring dockerfile: 105B                                       0.0s\n\n =&gt; [internal] load metadata for docker.io/library/alpine:3.17             0.2s\n\n[+] Building 0.3s (1/2)                                    docker:desktop-linux\n\n =&gt; [internal] load build definition from Dockerfile                       0.0s\n\n =&gt; =&gt; transferring dockerfile: 105B                                       0.0s\n\n =&gt; [internal] load metadata for docker.io/library/alpine:3.17             0.3s\n\n[+] Building 0.5s (1/2)                                    docker:desktop-linux\n\n =&gt; [internal] load build definition from Dockerfile                       0.0s\n\n =&gt; =&gt; transferring dockerfile: 105B                                       0.0s\n\n =&gt; [internal] load metadata for docker.io/library/alpine:3.17             0.5s\n\n[+] Building 0.6s (2/2)                                    docker:desktop-linux\n\n =&gt; [internal] load build definition from Dockerfile                       0.0s\n\n =&gt; =&gt; transferring dockerfile: 105B                                       0.0s\n\n =&gt; [internal] load metadata for docker.io/library/alpine:3.17             0.5s\n\n[+] Building 0.6s (5/5) FINISHED                           docker:desktop-linux\n\n =&gt; [internal] load build definition from Dockerfile                       0.0s\n\n =&gt; =&gt; transferring dockerfile: 105B                                       0.0s\n\n =&gt; [internal] load metadata for docker.io/library/alpine:3.17             0.5s\n\n =&gt; [internal] load .dockerignore                                          0.0s\n\n =&gt; =&gt; transferring context: 2B                                            0.0s\n\n =&gt; CACHED [1/1] FROM docker.io/library/alpine:3.17@sha256:8fc3dacfb6d69d  0.0s\n\n =&gt; =&gt; resolve docker.io/library/alpine:3.17@sha256:8fc3dacfb6d69da8d44e4  0.0s\n\n =&gt; exporting to image                                                     0.0s\n\n =&gt; =&gt; exporting layers                                                    0.0s\n\n =&gt; =&gt; exporting manifest sha256:383953e0a21daa0ef49e15ccb95b16baf34206b1  0.0s\n\n =&gt; =&gt; exporting config sha256:6af8d3801a6161970741b5d861640886c38e9f0eb7  0.0s\n\n =&gt; =&gt; exporting attestation manifest sha256:f004624c2b5b8f6ad6f7b7438d77  0.0s\n\n =&gt; =&gt; exporting manifest list sha256:46ffb548a5964a6c10471dc88be75b3aa76  0.0s\n\n =&gt; =&gt; naming to docker.io/library/img_pinger:latest                       0.0s\n\n =&gt; =&gt; unpacking to docker.io/library/img_pinger:latest                    0.0s\n\n\n\nWhat's next:\n\n    View a summary of image vulnerabilities and recommendations → docker scout quickview \n\n\n\n\nIn the above command, -t option specifies the name for the image.\nLast argument, pinger, specifies the folder in which to look for the Dockerfile. Alternatively, you can use Windows Command Prompt and cd to pinger directory and run the build from there. In that case, the command will look like this: !docker image build -t img_pinger . The . (dot) stands for current directory.\nWe can run the image we’ve just built via:\n\n!docker container run --rm img_pinger\n\nPING 8.8.8.8 (8.8.8.8): 56 data bytes\n64 bytes from 8.8.8.8: seq=0 ttl=63 time=24.228 ms\n64 bytes from 8.8.8.8: seq=1 ttl=63 time=30.638 ms\n64 bytes from 8.8.8.8: seq=2 ttl=63 time=26.928 ms\n\n--- 8.8.8.8 ping statistics ---\n3 packets transmitted, 3 packets received, 0% packet loss\nround-trip min/avg/max = 24.228/27.264/30.638 ms\n\n\nThe --rm option in the above command means that we should immediately delete the container after stopping.\nWe can also provide custom arguments to the ping ENTRYPOINT, as discussed above:\n\n!docker container run --rm -it img_pinger -w 5 127.0.0.1\n\nPING 127.0.0.1 (127.0.0.1): 56 data bytes\n64 bytes from 127.0.0.1: seq=0 ttl=64 time=0.035 ms\n64 bytes from 127.0.0.1: seq=1 ttl=64 time=0.138 ms\n64 bytes from 127.0.0.1: seq=2 ttl=64 time=0.105 ms\n64 bytes from 127.0.0.1: seq=3 ttl=64 time=0.137 ms\n64 bytes from 127.0.0.1: seq=4 ttl=64 time=0.136 ms\n\n--- 127.0.0.1 ping statistics ---\n5 packets transmitted, 5 packets received, 0% packet loss\nround-trip min/avg/max = 0.035/0.110/0.138 ms\n\n\nNote that thanks to --rm, we don’t have any containers lying around after stopping:\n\n!docker container ls -a\n\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n\n\nCurrently downloaded (or built) images can be viewed via ls subcommand:\n\n!docker image ls\n\nREPOSITORY                    TAG       IMAGE ID       CREATED       SIZE\nquay.io/minio/minio           latest    9535594ad412   3 weeks ago   217MB\nnginx                         alpine    2140dad235c1   4 weeks ago   76.7MB\npinger                        latest    60cf059f4580   8 weeks ago   11.4MB\nalpine                        latest    beefdbd8a1da   8 weeks ago   13.6MB\ncentos                        latest    a27fd8080b51   3 years ago   380MB\nfundamentalsofdocker/trivia   ed2       0bb60b695895   5 years ago   13.1MB\n\n\nSimilarly to how rm behaves for containers, we can remove images as well:\n\n!docker image rm centos\n\nUntagged: centos:latest\nDeleted: sha256:a27fd8080b517143cbbbab9dfb7c8571c40d67d534bbdee55bd6c473f432b177\n\n\n\n!docker image ls\n\nREPOSITORY                    TAG       IMAGE ID       CREATED       SIZE\nquay.io/minio/minio           latest    9535594ad412   3 weeks ago   217MB\nnginx                         alpine    2140dad235c1   4 weeks ago   76.7MB\npinger                        latest    60cf059f4580   8 weeks ago   11.4MB\nalpine                        latest    beefdbd8a1da   8 weeks ago   13.6MB\nfundamentalsofdocker/trivia   ed2       0bb60b695895   5 years ago   13.1MB\n\n\n\nMulti-stage image builds\nConsider the below, much more complicated, Dockerfile:\nFROM alpine:3.12 AS build\nRUN apk update && \\\napk add --update alpine-sdk\nRUN mkdir /app\nWORKDIR /app\nCOPY . /app\nRUN mkdir bin\nRUN gcc -Wall hello.c -o bin/hello\n\nFROM alpine:3.12\nCOPY --from=build /app/bin/hello /app/hello\nCMD /app/hello\nIt’s located under hello_c subdir.\nThis file describes a two-stage build (it has 2 FROM directives).\nDuring first stage: 1. We use alpine:3.12 as the base image, and name it build. 2. Then, RUN a package update command that will install the C compiler. 3. Then RUN mkdir /app will create a directory inside the image. 4. WORKDIR /app will switch current build dir to /app/, so that all subsequent commands like RUN or COPY are executed from within that dir. 5. COPY . /app will copy the files from . on your machine (the folder in which Dockerfile is located, hello_c) to /app inside the image. 6. RUN mkdir bin will create additional dir named bin. As we are inside /app WORKDIR now, the resulting dir will be /app/bin 7. RUN gcc -Wall hello.c -o bin/hello will compile hello.c and put the result into /app/bin.\nNow, during second stage: 1. We use alpine:3.12 as the base image. 2. We use COPY --from=build so that to tell Docker that we are copying not from our machine, but from the temporary build from stage 1. 3. Then we use CMD to specify which command should run upon container startup.\nWe can build the Dockerfile via docker image build -t hello-c . from within hello_c subdirectory.\nNow we can see the image we just built via ls subcommand:\n\n!docker image ls\n\nREPOSITORY       TAG       IMAGE ID       CREATED          SIZE\nhello-c          latest    4254b85ed709   28 seconds ago   8.72MB\nredis-web        latest    353a1799536c   6 days ago       313MB\nnew-web          latest    fbe8c83422a0   6 days ago       313MB\nredis            alpine    f9f38fd31d4d   6 weeks ago      70.7MB\npostgres         alpine    38a37cc842c0   7 weeks ago      370MB\ndpage/pgadmin4   latest    585350593e8b   2 months ago     732MB\nimg_pinger       latest    46ffb548a596   2 months ago     11.4MB\npinger           latest    9d7e73baada8   2 months ago     11.4MB\nhello-world      latest    305243c73457   18 months ago    21kB\n\n\nAnd we can run it via:\n\n!docker container run hello-c\n\nHello, world!\n\n\nNow remove it:\n\n!docker image rm -f hello-c\n\nUntagged: hello-c:latest\n\n\n\n!docker image ls\n\nREPOSITORY       TAG       IMAGE ID       CREATED              SIZE\n&lt;none&gt;           &lt;none&gt;    4254b85ed709   About a minute ago   8.72MB\nnew-web          latest    fbe8c83422a0   6 days ago           313MB\nredis-web        latest    353a1799536c   6 days ago           313MB\nredis            alpine    f9f38fd31d4d   6 weeks ago          70.7MB\npostgres         alpine    38a37cc842c0   7 weeks ago          370MB\ndpage/pgadmin4   latest    585350593e8b   2 months ago         732MB\nimg_pinger       latest    46ffb548a596   2 months ago         11.4MB\npinger           latest    9d7e73baada8   2 months ago         11.4MB\nhello-world      latest    305243c73457   18 months ago        21kB\n\n\nNote: image names can use a short form, or be fully qualified:\n&lt;registry URL&gt;/&lt;User or Org&gt;/&lt;name&gt;:&lt;tag&gt;\nIn majority of cases, registry URL is hub.docker.com.\n\n\nExample python env\nConsider the below Dockerfile:\nFROM python:3.13\n\nRUN pip install jupyter\n\nRUN pip install numpy\n\nEXPOSE 8888\n\nCMD [\"jupyter\", \"notebook\", \"--ip=0.0.0.0\", \"--no-browser\", \"--allow-root\"]\nThis will install jupyter and numpy, and execute jupyter notebook upon container startup."
  },
  {
    "objectID": "nb/lab14/Lab14.html#volumes",
    "href": "nb/lab14/Lab14.html#volumes",
    "title": "Docker",
    "section": "Volumes",
    "text": "Volumes\nThe purpose of volumes is a long-term storage of data inside a container. Once container is removed, its data are gone. Volumes exist independently of containers.\nFor instance, we can try creating a sample.txt file inside a container:\n\n!docker container run --name demo alpine /bin/sh -c 'echo \"This is a test\" &gt; sample.txt'\n\nUnable to find image 'alpine:latest' locally\nlatest: Pulling from library/alpine\nDigest: sha256:1e42bbe2508154c9126d48c2b8a75420c3544343bf86fd041fb7527e017a4b4a\nStatus: Downloaded newer image for alpine:latest\n\n\nCheck the difference between base image (alpine) and current instance of the container:\n\n!docker container diff demo\n\nA /sample.txt\n\n\nThe file is there, but once container is removed, it will be gone. This is where volumes help.\nCreate a volume via:\n\n!docker volume create sample\n\nsample\n\n\nInspect it:\n\n!docker volume inspect sample\n\n[\n    {\n        \"CreatedAt\": \"2024-11-05T11:47:53Z\",\n        \"Driver\": \"local\",\n        \"Labels\": null,\n        \"Mountpoint\": \"/var/lib/docker/volumes/sample/_data\",\n        \"Name\": \"sample\",\n        \"Options\": null,\n        \"Scope\": \"local\"\n    }\n]\n\n\nNow we can run containers, attach them to the volume, and create some files:\ndocker container run --name test -it -v sample:/data alpine /bin/sh\nWe attach volumes via -v option. In this case, sample volume is mapped to /data dir inside the container.\nNote: the -it means that the container run will run in interactive mode. As Jupyter notebook does not support it, the above command has to be run from the Command Prompt.\nNow remove the container:\n\n!docker container rm test\n\ntest\n\n\nAnd create the same container again, attached to the same sample volume. The files you’ve created during first run should still be there.\nNow remove the sample volume:\n\n!docker volume rm sample\n\nsample\n\n\nWe can also map local dirs to container dirs, by providing their paths instead of the volume name after -v option:\ndocker container run -d --name my-site -v local/html:/usr/share/nginx/html -p 8080:80 nginx:alpine"
  },
  {
    "objectID": "nb/lab14/Lab14.html#networks",
    "href": "nb/lab14/Lab14.html#networks",
    "title": "Docker",
    "section": "Networks",
    "text": "Networks\nDocker specifies a container networking model. This model defines software defined networks, which can be thought of as a way to arrange communications between multiple containers of a single application. Each container, moreover, runs in its own network namespace or sandbox.\nYou can view current networks via network ls command:\n\n!docker network ls\n\nNETWORK ID     NAME      DRIVER    SCOPE\n8d845304bdba   bridge    bridge    local\na44d5472c247   host      host      local\n33c7de9853f4   none      null      local\n\n\nBy default, Docker creates 3 networks: bridge, host, and none.\nThe bridge network is the default network that containers use.\nWe can inspect it:\n\n!docker network inspect bridge\n\n[\n    {\n        \"Name\": \"bridge\",\n        \"Id\": \"f8fae2d5aaa07ed9fb7a8a13c0db9f4f29a67947c62897eb1ac3dfb0f86c1929\",\n        \"Created\": \"2024-11-12T11:39:44.2291835Z\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.17.0.0/16\",\n                    \"Gateway\": \"172.17.0.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Ingress\": false,\n        \"ConfigFrom\": {\n            \"Network\": \"\"\n        },\n        \"ConfigOnly\": false,\n        \"Containers\": {},\n        \"Options\": {\n            \"com.docker.network.bridge.default_bridge\": \"true\",\n            \"com.docker.network.bridge.enable_icc\": \"true\",\n            \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n            \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n            \"com.docker.network.bridge.name\": \"docker0\",\n            \"com.docker.network.driver.mtu\": \"65535\"\n        },\n        \"Labels\": {}\n    }\n]\n\n\nPay attention to IPAM node: IP address management. This specifies which addresses will be assigned to containers using this network.\nThese addresses are local to the network your machine is running inside of, e.g., your WiFi router network.\nCreate a new network:\n\n!docker network create --driver bridge sample-net\n\nf282c4b4cbe2ea77d200f1aea870e07dac85300d6c9adb7d0807a50ba60d14ef\n\n\nAnd check which subnet it gets assigned:\n\n!docker network inspect sample-net | grep Subnet\n\nYou can provide a custom subnet when creating a network:\n!docker network create –subnet “11.11.0.0/16” test-subnet\n\n!!docker network inspect test-subnet | grep Subnet\n\n['                    \"Subnet\": \"11.11.0.0/16\"']\n\n\nNow let’s run a couple of containers and check IP addresses they get:\ndocker container run --name c1 -it --rm alpine:latest /bin/sh\nand inside the container:\n/ # ip addr show eth0\n21: eth0@if22: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 65535 qdisc noqueue state UP\n    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0\n       valid_lft forever preferred_lft forever\nFrom outside:\n\n!docker container inspect c1 | grep IPAddress\n\n            \"SecondaryIPAddresses\": null,\n            \"IPAddress\": \"172.17.0.2\",\n                    \"IPAddress\": \"172.17.0.2\",\n\n\nAnd another container:\n\n!docker container run --name c2 -d --rm alpine:latest ping 127.0.0.1\n\n691dab6d3f35e5f220a89950b7596683acae06dc5ddf7b0594d15269a56297ef\n\n\nCheck its network:\n\n!docker container inspect c2 | grep IPAddress\n\n            \"SecondaryIPAddresses\": null,\n            \"IPAddress\": \"172.17.0.3\",\n                    \"IPAddress\": \"172.17.0.3\",\n\n\nAnd now, if we inspect the network, we’ll see those containers attached (recall that bridge is the default network):\n\n!docker network inspect bridge\n\n[\n    {\n        \"Name\": \"bridge\",\n        \"Id\": \"80f2302e3302f74de65281dc290cf71acf13634784830883436cb072ab9ac294\",\n        \"Created\": \"2024-11-12T12:22:30.525464791Z\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.17.0.0/16\",\n                    \"Gateway\": \"172.17.0.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Ingress\": false,\n        \"ConfigFrom\": {\n            \"Network\": \"\"\n        },\n        \"ConfigOnly\": false,\n        \"Containers\": {\n            \"691dab6d3f35e5f220a89950b7596683acae06dc5ddf7b0594d15269a56297ef\": {\n                \"Name\": \"c2\",\n                \"EndpointID\": \"d11763fcf4b72cf4af8c1fb5f31c5ce3966de61c96791db1e9b2adf3c547bd0f\",\n                \"MacAddress\": \"02:42:ac:11:00:03\",\n                \"IPv4Address\": \"172.17.0.3/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"6e5fad01bb8b9a955b2fc215bf8eb724471e2e7a2fcaf662ea099d15abe22302\": {\n                \"Name\": \"c1\",\n                \"EndpointID\": \"ef027703da6101a7b4e2fd21ec3c71c1de05b9af752ac057d02fe9a37c4347b5\",\n                \"MacAddress\": \"02:42:ac:11:00:02\",\n                \"IPv4Address\": \"172.17.0.2/16\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {\n            \"com.docker.network.bridge.default_bridge\": \"true\",\n            \"com.docker.network.bridge.enable_icc\": \"true\",\n            \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n            \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n            \"com.docker.network.bridge.name\": \"docker0\",\n            \"com.docker.network.driver.mtu\": \"65535\"\n        },\n        \"Labels\": {}\n    }\n]\n\n\nNow let’s create two more containers and attach them via --network option to the sample-net network we created earlier:\n\n!docker container run --name c3 -d --network sample-net alpine ping 127.0.0.1\n\n9ab6bb78688d3a3a5c196acaf735e340f45e080f2391e520448971c134281ed7\n\n\n\n!docker container run --name c4 -d --network sample-net alpine ping 127.0.0.1\n\nc6ec9db2b0799850f002968690908f3b7607183dd91f82cf18117b0343a3a1fd\n\n\nCheck that hosts are mutually reachable via docker container exec -it c3 /bin/sh and try connecting to c4 (should work) and c1 (should fail).\nNetworks can be deleted via rm as usual, or we can use prune:\n\n!docker network prune --force\n\nDeleted Networks:\nsample-net\n\n\n\nprune can also be used for other object types:\ndocker container prune -f\ndocker image prune -af\ndocker system prune -f\nSimilarly to how we can run additional commands via container exec, we can connect currently running to more networks via\ndocker network connect &lt;network&gt; &lt;container&gt;\nAnother network type, --network host is used to run container connectecd to host’s network. You can check it via\ndocker container run --rm -it --name test1 --network host alpine:latest /bin/sh\nand ip addr show eth0 inside.\n\nRunning containers in the same network namespace\nContainer can be attached to the network sandbox of another container, so that they share same IP address:\n\n!docker network create test-net\n\n8f3a7cab15b47ca638554b88ed487982669555104a8538c353371c320bc2944c\n\n\n\n!docker container run --name web -d --network test-net nginx:alpine\n\ncb3c164cc9c444a109cad91fd7a54076946dbc08f66c2472e30e78b49486c826\n\n\nAnd now use the --network container:&lt;name&gt; syntax to connect to the network sandbox of web container:\ndocker container run -it \\\n--network container:web \\\nalpine /bin/sh\nand then wget -qO - localhost inside to verify they indeed share same address.\n\n\nPort mapping\nPort mapping allows us to open specific ports inside the container to the outside world.\n-P parameter will automatically map ports:docker container run --name web -P -d nginx:alpine\ndocker container port web will show ports we just mapped:\n docker container port web\n80/tcp -&gt; 0.0.0.0:55000\nWe can map a specific port via the -p option:\n\n!docker container run --name web2 -p 8080:80 -d nginx:alpine\n\n8f58fb31b2e7efe01e8d7c651279e29223e4acbfdc370f5c2ba80802959a8938"
  },
  {
    "objectID": "nb/lab14/Lab14.html#docker-compose",
    "href": "nb/lab14/Lab14.html#docker-compose",
    "title": "Docker",
    "section": "Docker Compose",
    "text": "Docker Compose\nWith Docker Compose, applications consisting of several containers can be configured in one file.\nIn a sense, Docker Compose allows one to declaratively specify which images and with which options (volumes/environments etc.) should run.\nWhile Dockerfile uses its own format, Docker Compose files are written in YAML (https://en.wikipedia.org/wiki/YAML).\nConsider a sample docker-compose.yml:\nversion: \"3.8\" \nservices: \n  db:\n    image: postgres:alpine\n    environment:\n    - POSTGRES_USER=dockeruser\n    - POSTGRES_PASSWORD=dockerpass\n    - POSTGRES_DB=pets\n    volumes:\n    - pg-data:/var/lib/postgresql/data\n    - ./db:/docker-entrypoint-initdb.d\n  \n  pgadmin:\n    image: dpage/pgadmin4\n    ports:\n      - \"5050:80\"\n    environment:\n      PGADMIN_DEFAULT_EMAIL: admin@acme.com\n      PGADMIN_DEFAULT_PASSWORD: admin\n    volumes:\n      - pgadmin-data:/var/lib/pgadmin\n\nvolumes: \n  pg-data:\n  pgadmin-data:\nThis will run 2 containers: one for PostgreSQL, and another for pgAdmin. This file is located in the pg subdir.\nInside that dir, run\ndocker compose up\nto start the containers, press Ctrl+C to stop it, and\ndocker compose down\nto cleanup.\nWe can also use a build entry to build custom images:\nservices:\n  web:\n    build: .\n    ports:\n      - \"8000:5000\"\n  redis:\n    image: \"redis:alpine\"\nIn the above example, the build: . tells Docker Compose to use the Dockerfile in current directory (.), and for redis, use a stock image named redis:alpine, which it will download from hub.docker.com.\nFull example is in redis subdir.\nIf you want to use a custom Dockerfile name, use this syntax instead:\nbuild:\n  context: web\n  dockerfile: Dockerfile.dev\ndocker compose up, with -d in order to run in detached mode.\nNote that Docker Compose prefixes all names with the name of the parent folder. This can be overridden with the --project-name option:\ndocker compose --project-name demo up"
  },
  {
    "objectID": "nb/lab14/Lab14.html#exercises",
    "href": "nb/lab14/Lab14.html#exercises",
    "title": "Docker",
    "section": "Exercises",
    "text": "Exercises\nFor this Lab, extend the problem from Lab 13: 1. Reproduce Python’s venv in Docker. 2. Run Postgres via Docker 3. Connect Python, Postgres, and MinIO service from Lab12.\nwith 4. Use Docker compose to declaratively specify environment in items 1-3 above"
  },
  {
    "objectID": "applied_lec1.html#what-is-big-data",
    "href": "applied_lec1.html#what-is-big-data",
    "title": "Big Data: Intro",
    "section": "What is Big Data?",
    "text": "What is Big Data?\n\n\n\nDefinition\n\n\nBig Data is a set of technologies designed to store, manage and analyze data that is:\n\ntoo large to fit on a single machine\nwhile accommodating for the issue of growing discrepancy between capacity, throughput and latency."
  },
  {
    "objectID": "applied_lec1.html#prefixes",
    "href": "applied_lec1.html#prefixes",
    "title": "Big Data: Intro",
    "section": "Prefixes",
    "text": "Prefixes\n\n\n\nPrefixes\n\n\n\nkilo (k) 1,000 (3 zeros)\nMega (M) 1,000,000 (6 zeros)\nGiga (G) 1,000,000,000 (9 zeros)\nTera (T) 1,000,000,000,000 (12 zeros)\nPeta (P) 1,000,000,000,000,000 (15 zeros)\nExa (E) 1,000,000,000,000,000,000 (18 zeros)\nZetta (Z) 1,000,000,000,000,000,000,000 (21 zeros)\nYotta (Y) 1,000,000,000,000,000,000,000,000 (24 zeros)\nRonna (R) 1,000,000,000,000,000,000,000,000,000 (27 zeros)\nQuetta (Q) 1,000,000,000,000,000,000,000,000,000,000 (30 zeros)"
  },
  {
    "objectID": "applied_lec1.html#total-estimate",
    "href": "applied_lec1.html#total-estimate",
    "title": "Big Data: Intro",
    "section": "Total estimate",
    "text": "Total estimate\n\n\n\nEstimate\n\n\nThe total amount of data stored digitally worldwide is estimated to be getting close to 100 ZB as of 2021 (zettabytes)"
  },
  {
    "objectID": "applied_lec1.html#three-vs",
    "href": "applied_lec1.html#three-vs",
    "title": "Big Data: Intro",
    "section": "Three Vs",
    "text": "Three Vs\n\n\n\nVolume\nVariety\nVelocity"
  },
  {
    "objectID": "applied_lec1.html#volume",
    "href": "applied_lec1.html#volume",
    "title": "Big Data: Intro",
    "section": "Volume",
    "text": "Volume\n\n\n\nIssue\n\n\nData volume has exponentially increased in recent decades.\n\n\n\n\n\n\nWait but why?\n\n\n\nInternet-of-Things sensor data\nSocial networks\nStorage device progress"
  },
  {
    "objectID": "applied_lec1.html#variety",
    "href": "applied_lec1.html#variety",
    "title": "Big Data: Intro",
    "section": "Variety",
    "text": "Variety\n\n\n\nTypes\n\n\n\ntrees - XML, JSON, Parquet, Avro, etc\nunstructured - text, pictures, audio, video\ndata cubes\ngraphs"
  },
  {
    "objectID": "applied_lec1.html#velocity",
    "href": "applied_lec1.html#velocity",
    "title": "Big Data: Intro",
    "section": "Velocity",
    "text": "Velocity\n\n\n\nDefinion\n\n\nSpeed at which data is being generated, collected, and processed.\n\n\n\n\n\n\nAttributes\n\n\n\nCapacity: how much data can we store per unit of volume?\nThroughput: how many bytes can we read per unit of time?\nLatency: how much time do we need to wait until the bytes start arriving?"
  },
  {
    "objectID": "applied_lec1.html#velocity-1",
    "href": "applied_lec1.html#velocity-1",
    "title": "Big Data: Intro",
    "section": "Velocity",
    "text": "Velocity\nEvolution since 1950s"
  },
  {
    "objectID": "applied_lec1.html#features",
    "href": "applied_lec1.html#features",
    "title": "Big Data: Intro",
    "section": "Features",
    "text": "Features\n\n\n\nFeatures\n\n\n\nReliability\nScalability\nMaintainability"
  },
  {
    "objectID": "applied_lec1.html#reliability",
    "href": "applied_lec1.html#reliability",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nKleppmann’s definition\n\n\nThe system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity\n\nhardware faults\nsoftware faults\nand even human error"
  },
  {
    "objectID": "applied_lec1.html#reliability-1",
    "href": "applied_lec1.html#reliability-1",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nFaults\n\n\nBasically, theses are things that could go wrong.\nSystems that can anticipate faults are called fault-tolerant or resilient.\nFault can be defined as one component of the system deviating from the spec.\n\n\n\n\n\n\nFailures\n\n\nFailures occur when system stops providing services to the user.\n\n\n\nFaults might degenerate into failures."
  },
  {
    "objectID": "applied_lec1.html#reliability-2",
    "href": "applied_lec1.html#reliability-2",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nTypes of errors\n\n\n\nHardware\nSoftware\nHuman"
  },
  {
    "objectID": "applied_lec1.html#reliability-3",
    "href": "applied_lec1.html#reliability-3",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability"
  },
  {
    "objectID": "applied_lec1.html#scalability",
    "href": "applied_lec1.html#scalability",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nKleppmann\n\n\nAs the system grows (in data volume, traffic volume, or complexity), there should be reasonable ways of dealing with that growth.\nIn other words, scalability is a system’s ability to cope with increased load.\n\n\n\n\n\nNote that scalability is a multi-dimensional term. When saying “system scales well”, it’s important to state exactly along which axis."
  },
  {
    "objectID": "applied_lec1.html#scalability-1",
    "href": "applied_lec1.html#scalability-1",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nWhat is load?\n\n\nLoad is described by load parameters. These might include:\n\ndata set size\ndata write speed\ndata read speed\ncomputational complexity\netc…"
  },
  {
    "objectID": "applied_lec1.html#scalability-2",
    "href": "applied_lec1.html#scalability-2",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nPerformance\n\n\nIncreasing load affects performance. There are several meanings to this term:\n\nthroughput – time required to process a dataset of certain size\nresponse time – time between sending a request and receiving a response\nlatency – duration of waiting for a request to be processed. Included in response time.\n\n\n\n\n\n\nPerformance might be more strictly defined by service level objectives (SLOs) and service level agreements (SLAs)."
  },
  {
    "objectID": "applied_lec1.html#scalability-3",
    "href": "applied_lec1.html#scalability-3",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nHow to deal with load\n\n\n\nvertical scaling - scaling up\nhorizontal scaling - scaling out\narchitectural changes\n\n\n\n\n\n\n\nElasticity\n\n\nAn approach to load handling whereby a system automatically adds resources in case of load increase, and can decrease resources if load decreases."
  },
  {
    "objectID": "applied_lec1.html#scalability-4",
    "href": "applied_lec1.html#scalability-4",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nCommon wisdom\n\n\n\nKeep your database on a single node (scale up) until scaling cost or high-availability requirements forces you to make it distributed.\nOptimize code so that it can run on a single node."
  },
  {
    "objectID": "applied_lec1.html#maintainability",
    "href": "applied_lec1.html#maintainability",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nKleppmann\n\n\nOver time, many different people will work on the system\n\nengineering\noperations\nboth maintaining current behavior and adapting the system to new use cases),\n\nand they should all be able to work on it productively."
  },
  {
    "objectID": "applied_lec1.html#maintainability-1",
    "href": "applied_lec1.html#maintainability-1",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nPrinciples\n\n\n\nOperability – make it easy for operations teams to keep the system running smoothly.\nSimplicity – make it easy for new engineers to understand the system, by removing as much complexity as possible from the system.\nEvolvability – Make it easy for engineers to make changes to the system in the future, adapting it for unanticipated use cases as requirements change. Also known as extensibility, modifiability, or plasticity."
  },
  {
    "objectID": "applied_lec1.html#maintainability-2",
    "href": "applied_lec1.html#maintainability-2",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nOperability\n\n\n\nHealth monitoring\nGood deployment practices\nConfiguration management\nVisibility into the internals of the system\nKnowledge preservation – documentation (!).\netc…"
  },
  {
    "objectID": "applied_lec1.html#maintainability-3",
    "href": "applied_lec1.html#maintainability-3",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nComplexity symptoms\n\n\n\nLots of hidden state\nLoose cohesion, tight coupling\nBad naming (!)\nUnnecessary hacks\netc…"
  },
  {
    "objectID": "applied_lec1.html#maintainability-complexity",
    "href": "applied_lec1.html#maintainability-complexity",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity"
  },
  {
    "objectID": "applied_lec1.html#maintainability-complexity-1",
    "href": "applied_lec1.html#maintainability-complexity-1",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity\n\n\n\nTypes\n\n\n\nincidental\naccidental"
  },
  {
    "objectID": "applied_lec1.html#maintainability-complexity-2",
    "href": "applied_lec1.html#maintainability-complexity-2",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity\n\n\n\nIncidental\n\n\n\nEasy things can be complex. There can be complex constructs that are succinctly described, familiar, available and easy to use. That is incidental complexity.\n\nRich Hickey talk “Simple made easy”: https://www.youtube.com/watch?v=SxdOUGdseq4"
  },
  {
    "objectID": "applied_lec1.html#maintainability-complexity-3",
    "href": "applied_lec1.html#maintainability-complexity-3",
    "title": "Big Data: Intro",
    "section": "Maintainability: complexity",
    "text": "Maintainability: complexity\nHowever: Complexity is often caused by\n\n\n\nAccidental complexity\n\n\nMoseley and Marks define complexity as accidental if it is not inherent in the problem that the software solves (as seen by the users) but arises only from the implementation.\n\n\n\n\n\n\nHow to remove?\n\n\nBy providing proper abstractions."
  },
  {
    "objectID": "applied_lec1.html#maintainability-abstractions",
    "href": "applied_lec1.html#maintainability-abstractions",
    "title": "Big Data: Intro",
    "section": "Maintainability: abstractions",
    "text": "Maintainability: abstractions\n\n\n\n\n\n\nDefinition (Ousterhout)\n\n\nAn abstraction is a simplified view of an entity, which omits unimportant details.\nIn modular programming, each module provides an abstraction in the form of its interface."
  },
  {
    "objectID": "applied_lec1.html#maintainability-abstractions-1",
    "href": "applied_lec1.html#maintainability-abstractions-1",
    "title": "Big Data: Intro",
    "section": "Maintainability: abstractions",
    "text": "Maintainability: abstractions\n\n\n\n\n\n\nWhat can abstractions do?\n\n\n\nHide implementation details\nProvide reusable building blocks"
  },
  {
    "objectID": "applied_lec1.html#maintainability-4",
    "href": "applied_lec1.html#maintainability-4",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nEvolvability\n\n\nOne needs to adapt their big data system to possible future requirements changes.\nHowever, keep in mind the following:\n\nInability to foresee exact nature of changes\nNeed to strike the balance of flexibility and fitness for a particular task"
  },
  {
    "objectID": "applied_lec1.html#types-of-big-data-analytics",
    "href": "applied_lec1.html#types-of-big-data-analytics",
    "title": "Big Data: Intro",
    "section": "Types of big data analytics",
    "text": "Types of big data analytics\n\n\n\nTypes\n\n\n\nPrescriptive\nDiagnostic\nDescriptive\nPredictive"
  },
  {
    "objectID": "applied_lec1.html#types-prescriptive",
    "href": "applied_lec1.html#types-prescriptive",
    "title": "Big Data: Intro",
    "section": "Types: Prescriptive",
    "text": "Types: Prescriptive\n\n\n\nPrescriptive\n\n\n\nForward looking\nOptimal decisions for future situations"
  },
  {
    "objectID": "applied_lec1.html#types-diagnostic",
    "href": "applied_lec1.html#types-diagnostic",
    "title": "Big Data: Intro",
    "section": "Types: Diagnostic",
    "text": "Types: Diagnostic\n\n\n\nDiagnostic\n\n\n\nBackward looking\nFocused on causal relationships"
  },
  {
    "objectID": "applied_lec1.html#types-descriptive",
    "href": "applied_lec1.html#types-descriptive",
    "title": "Big Data: Intro",
    "section": "Types: Descriptive",
    "text": "Types: Descriptive\n\n\n\nDescriptive\n\n\n\nBackward looking\nFocused on descriptions and comparisons"
  },
  {
    "objectID": "applied_lec1.html#types-descriptive-1",
    "href": "applied_lec1.html#types-descriptive-1",
    "title": "Big Data: Intro",
    "section": "Types: Descriptive",
    "text": "Types: Descriptive\n\n\n\nPredictive\n\n\n\nForward looking\nFocused on the prediction of future states, relationship, and patterns"
  },
  {
    "objectID": "applied_lec1.html#challenges",
    "href": "applied_lec1.html#challenges",
    "title": "Big Data: Intro",
    "section": "Challenges",
    "text": "Challenges\nThere are 2 main challenges associated with Big Data.\n\n\n\nChallenges\n\n\n\nhow do we store and manage such a huge volume of data efficiently?\nhow do we process and extract valuable information from the data within the given time frame?"
  }
]