[
  {
    "objectID": "applied_lab1.html",
    "href": "applied_lab1.html",
    "title": "Applied Analytics: Lab 1",
    "section": "",
    "text": "This lab is about environment setup. We will use uv, a package and project manager for Python."
  },
  {
    "objectID": "applied_lab1.html#recommended-reading",
    "href": "applied_lab1.html#recommended-reading",
    "title": "Applied Analytics: Lab 1",
    "section": "Recommended reading:",
    "text": "Recommended reading:\n\nGit Book\nGit Magic"
  },
  {
    "objectID": "applied_lab2.html",
    "href": "applied_lab2.html",
    "title": "Applied Analytics: Lab 2",
    "section": "",
    "text": "Previous lab listed some methods of optimizing Pandas work:\n\ndata storage optimizations\nCython conversion\nNumba decorators\n\nIn this lab we’ll start looking into parallelisation as a way of optimizing data analysis workflows.\nWhy - because today’s systems are multicore (sometimes very much so).\nCheck yours:\n\nimport os\nos.cpu_count()\n\n12\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are two kinds of parallelism, distributed and shared-memory. Here we’ll talk about shared-memory version.\nDistributed parallelism is when we use multiple machines/VMs for computations.\n\n\n\n\nIn functional programming paradigm, functions are first-class citizens of the languages. They can be composed, applied, passed as arguments to another functions, or returned as values.\nFunctional programming is inherently better suited for parallel programs.\nBenefits of functional programming:\n\nminimization of state\nno side-effects\npure functions are easier to reason about\nand parallelize (!)\n\nIt is based on lambda calculus. We’ll learn some of its concepts first:\n\nlambda function definition\napplication and partial application\ncurrying\nclosures\n\nFirst off, functions can be values in Python:\n\nadd = lambda x, y: x + y  \nprint (f\"add = {add}\")\n\nadd = &lt;function &lt;lambda&gt; at 0x10efab100&gt;\n\n\nAll variables support some operations on them: addition for integers, or e.g. read_csv on Pandas DataFrame.\nFunction variables support a single operation: application.\n\ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \n  \nk = i + j  \nprint (f\"k = {k}\")  \n  \nz = add (i, j)  \nprint (f\"z = {z}\")\n\nk = 1\nz = 1\n\n\nPartial function application is also supported. We’ll use functools package (https://docs.python.org/3/library/functools.html).\n\nfrom functools import partial  \ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \ninc = partial (add, 1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\")  \n\nii = 1, jj = 2\n\n\nNote: the code above both passes add as an argument to partial, and stores the return value of partial function into a new variable inc.\nThe lambda in the example above accepts two arguments essentially. This can be simplified by currying:\n\ni = 0  \nj = 1    \nadd = lambda x: lambda y: x + y  \ninc = add(1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\")\n\nii = 1, jj = 2\n\n\n\n\n\n\n\n\nTipDefinition for currying\n\n\n\nCurrying is the technique of translating a function that takes multiple arguments into a sequence of families of functions, each taking a single argument.\nAlternatively, we can say that currying is a method to transform a function of arity n to n functions of arity 1.\n\n\nWith currying, we can express partial application without and extra partial function.\n\n\n\n\n\n\nNote\n\n\n\nNote: currying relies on closures - functions that can refer to variables defined in surrounding context (like x in add definition above).\n\n\nA monstrous example:\n\ndef f_5(a, b, c, d, e):\n    return a + b + c + d + e\n\ndef c_5(a):\n    def c_4(b):\n        def c_3(c):\n            def c_2(d):\n                def c_1(e):\n                    return f_5(a, b, c, d, e)\n                return c_1\n            return c_2\n        return c_3\n    return c_4\n\nHow to use?\n\nf_5(1,2,3,4,5)\n\n15\n\n\n\nc_5(1)(2)(3)(4)(5)\n\n15\n\n\n\n\n\nIn functional programming, we do not iterate - we map. Why? Because map can be parallelised.\nIteration (like a for-loop) is sequential. Mapping is not.\nmap takes its name from a mathematical term for functions - mappings.\n\n\n\n\n\n\nWarningMap\n\n\n\nArguments:\n\na sequence to iterate on\na function to apply to each element of the sequence.\n\nReturn value:\n\na processed sequence of the same size as input\n\n\n\nNow, if we want to convert our sequence to a sequence of a different length, or different object altogether, we use reduce.\n\n\n\n\n\n\nTipReduce\n\n\n\nArguments:\n\na sequence to iterate on\naccumulation seed to start reducing on\na function of two arguments, accumulation result and next element\n\nReturn value::\n\nresult of accumulation\n\n\n\nUsing above examples, we can easily implement map using list comprehensions as as:\n\nmap = lambda f, l: [f(x) for x in l]\n\nLet’s use it:\n\nint_list = [1,2,3,4,5,6]\ninc_list = map(inc, int_list)\nprint(inc_list)\n\n[2, 3, 4, 5, 6, 7]\n\n\nAnd reduce can be implemented as:\n\ndef reduce_v1 (f, id, s):  \n    n = len(s)  \n    if n == 0:  \n        return id  \n    elif n == 1:  \n        return s[0]  \n    else:  \n        return f(reduce_v1(f, id, s[1:]), s[0])\n\nUsage example:\n\nadd = lambda x, y: x + y  \nres = reduce_v1(add, 0, int_list)\nprint(res)\n\n21\n\n\n\n\nSome tasks can be trivially parallelised - these are called embarrassingly parallel. inc is a simple example of this type.\nOther tasks have a degree of interdependence. Consider below example, adapted from the reduce above:\n\n# Sum all elements of int_list\ndef sum(lst):\n    sum = 0\n    \n    for i in range(len(lst)):\n      sum += lst[i]\n\n    return sum\n\nprint(sum(int_list))\n\n21\n\n\nHow can it be parallelised? By using a technique called chunking:\n\nchunk1 = int_list[:3]\nchunk2 = int_list[3:]\n\nsum_1 = sum(chunk1)\nsum_2 = sum(chunk2)\n\nresult = sum_1 + sum_2\n\nprint(result)\n\n21\n\n\nThen there are algorithms that are inherently serial. An example is computation of a Fibonacci sequence.\n\n\n\nSuppose we want to square each number in a list. A naive approach would be:\n\nnumber_list = [1,2,3,4]\n\ndef squarev1(lst):\n    squared = [] # sometimes pre-allocation can also work, like [None]*1000\n    for el in lst:\n        squared.append(el*el)\n    return squared\n\nsquarev1(number_list)\n\n[1, 4, 9, 16]\n\n\nOr we can use a built-in (https://docs.python.org/3/library/functions.html#map):\n\n# Note that we return a map object that has to be converted to a list first\ndef squarev2(lst):\n    return list(map(lambda x: x*x, lst))\n\nsquarev2(number_list)\n\n[1, 4, 9, 16]\n\n\n\n\n\nMap function can be made to execute in parallel on multiple CPU cores. How? We need to use multiprocessing library (https://docs.python.org/3/library/multiprocessing.html)\n\nfrom multiprocessing import Pool\n\nThere are limitations though: in order to arrange communication between proceses, Python uses pickling: a method of serializing/deserializing objects so that they can be sent between processes.\nPython cannot pickle closures, nested functions or classes! More info on pickling: https://docs.python.org/3/library/pickle.html.\nMoreover, we have to move sqr definition into a separate file (https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror)\n\nimport _files.defs as defs\n\ndef squarev3(lst):\n    pool = Pool()\n    return pool.map(defs.sqr, number_list)\n\nsquarev3(number_list)\n\n[1, 4, 9, 16]\n\n\n\n\n\n\n\n\nNotePathos map with Dill\n\n\n\nHowever, Dill (https://pypi.org/project/dill/) can overcome this limitation!\nHow to use Dill?\nuv add pathos\n\nuv add toolz\nAnd then\nfrom pathos.multiprocessing import ProcessPool\n\ndef squarev4(lst):\n    pool = ProcessPool()\n    return pool.map(lambda x: x*x, lst)\n\nsquarev4([1,3,5])\n\n\n\n\n\n\nNumpy automatically vectorizes array operations. However, we can explicitly invoke vectorize():\nIn this context vectorization refers to low-level CPU vectorization (single CPU instruction processing items in a vector)\n\nimport numpy as np\n\nnp_array = np.array(number_list)\n\nvectorized_fn = np.vectorize(lambda x: x*x)\n\nresult = vectorized_fn(np_array)\nprint(result)\n\n[ 1  4  9 16]\n\n\n\n\n\n\nWrite a version of curried function c_5 using lambda syntax.\nWrite a program calculating a factorial using parallelisation and map-reduce.\nWrite a parallelised version of reduce (hint: use chunking).\nCompare performance of Numpy vectorization vs parallel map on a really large array of integers.\nUsing computations from previous labs, apply chunking approach to df.apply(...) using parallel map."
  },
  {
    "objectID": "applied_lab2.html#short-intro-to-functional-programming",
    "href": "applied_lab2.html#short-intro-to-functional-programming",
    "title": "Applied Analytics: Lab 2",
    "section": "",
    "text": "In functional programming paradigm, functions are first-class citizens of the languages. They can be composed, applied, passed as arguments to another functions, or returned as values.\nFunctional programming is inherently better suited for parallel programs.\nBenefits of functional programming:\n\nminimization of state\nno side-effects\npure functions are easier to reason about\nand parallelize (!)\n\nIt is based on lambda calculus. We’ll learn some of its concepts first:\n\nlambda function definition\napplication and partial application\ncurrying\nclosures\n\nFirst off, functions can be values in Python:\n\nadd = lambda x, y: x + y  \nprint (f\"add = {add}\")\n\nadd = &lt;function &lt;lambda&gt; at 0x10efab100&gt;\n\n\nAll variables support some operations on them: addition for integers, or e.g. read_csv on Pandas DataFrame.\nFunction variables support a single operation: application.\n\ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \n  \nk = i + j  \nprint (f\"k = {k}\")  \n  \nz = add (i, j)  \nprint (f\"z = {z}\")\n\nk = 1\nz = 1\n\n\nPartial function application is also supported. We’ll use functools package (https://docs.python.org/3/library/functools.html).\n\nfrom functools import partial  \ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \ninc = partial (add, 1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\")  \n\nii = 1, jj = 2\n\n\nNote: the code above both passes add as an argument to partial, and stores the return value of partial function into a new variable inc.\nThe lambda in the example above accepts two arguments essentially. This can be simplified by currying:\n\ni = 0  \nj = 1    \nadd = lambda x: lambda y: x + y  \ninc = add(1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\")\n\nii = 1, jj = 2\n\n\n\n\n\n\n\n\nTipDefinition for currying\n\n\n\nCurrying is the technique of translating a function that takes multiple arguments into a sequence of families of functions, each taking a single argument.\nAlternatively, we can say that currying is a method to transform a function of arity n to n functions of arity 1.\n\n\nWith currying, we can express partial application without and extra partial function.\n\n\n\n\n\n\nNote\n\n\n\nNote: currying relies on closures - functions that can refer to variables defined in surrounding context (like x in add definition above).\n\n\nA monstrous example:\n\ndef f_5(a, b, c, d, e):\n    return a + b + c + d + e\n\ndef c_5(a):\n    def c_4(b):\n        def c_3(c):\n            def c_2(d):\n                def c_1(e):\n                    return f_5(a, b, c, d, e)\n                return c_1\n            return c_2\n        return c_3\n    return c_4\n\nHow to use?\n\nf_5(1,2,3,4,5)\n\n15\n\n\n\nc_5(1)(2)(3)(4)(5)\n\n15"
  },
  {
    "objectID": "applied_lab2.html#map-reduce",
    "href": "applied_lab2.html#map-reduce",
    "title": "Applied Analytics: Lab 2",
    "section": "",
    "text": "In functional programming, we do not iterate - we map. Why? Because map can be parallelised.\nIteration (like a for-loop) is sequential. Mapping is not.\nmap takes its name from a mathematical term for functions - mappings.\n\n\n\n\n\n\nWarningMap\n\n\n\nArguments:\n\na sequence to iterate on\na function to apply to each element of the sequence.\n\nReturn value:\n\na processed sequence of the same size as input\n\n\n\nNow, if we want to convert our sequence to a sequence of a different length, or different object altogether, we use reduce.\n\n\n\n\n\n\nTipReduce\n\n\n\nArguments:\n\na sequence to iterate on\naccumulation seed to start reducing on\na function of two arguments, accumulation result and next element\n\nReturn value::\n\nresult of accumulation\n\n\n\nUsing above examples, we can easily implement map using list comprehensions as as:\n\nmap = lambda f, l: [f(x) for x in l]\n\nLet’s use it:\n\nint_list = [1,2,3,4,5,6]\ninc_list = map(inc, int_list)\nprint(inc_list)\n\n[2, 3, 4, 5, 6, 7]\n\n\nAnd reduce can be implemented as:\n\ndef reduce_v1 (f, id, s):  \n    n = len(s)  \n    if n == 0:  \n        return id  \n    elif n == 1:  \n        return s[0]  \n    else:  \n        return f(reduce_v1(f, id, s[1:]), s[0])\n\nUsage example:\n\nadd = lambda x, y: x + y  \nres = reduce_v1(add, 0, int_list)\nprint(res)\n\n21\n\n\n\n\nSome tasks can be trivially parallelised - these are called embarrassingly parallel. inc is a simple example of this type.\nOther tasks have a degree of interdependence. Consider below example, adapted from the reduce above:\n\n# Sum all elements of int_list\ndef sum(lst):\n    sum = 0\n    \n    for i in range(len(lst)):\n      sum += lst[i]\n\n    return sum\n\nprint(sum(int_list))\n\n21\n\n\nHow can it be parallelised? By using a technique called chunking:\n\nchunk1 = int_list[:3]\nchunk2 = int_list[3:]\n\nsum_1 = sum(chunk1)\nsum_2 = sum(chunk2)\n\nresult = sum_1 + sum_2\n\nprint(result)\n\n21\n\n\nThen there are algorithms that are inherently serial. An example is computation of a Fibonacci sequence.\n\n\n\nSuppose we want to square each number in a list. A naive approach would be:\n\nnumber_list = [1,2,3,4]\n\ndef squarev1(lst):\n    squared = [] # sometimes pre-allocation can also work, like [None]*1000\n    for el in lst:\n        squared.append(el*el)\n    return squared\n\nsquarev1(number_list)\n\n[1, 4, 9, 16]\n\n\nOr we can use a built-in (https://docs.python.org/3/library/functions.html#map):\n\n# Note that we return a map object that has to be converted to a list first\ndef squarev2(lst):\n    return list(map(lambda x: x*x, lst))\n\nsquarev2(number_list)\n\n[1, 4, 9, 16]\n\n\n\n\n\nMap function can be made to execute in parallel on multiple CPU cores. How? We need to use multiprocessing library (https://docs.python.org/3/library/multiprocessing.html)\n\nfrom multiprocessing import Pool\n\nThere are limitations though: in order to arrange communication between proceses, Python uses pickling: a method of serializing/deserializing objects so that they can be sent between processes.\nPython cannot pickle closures, nested functions or classes! More info on pickling: https://docs.python.org/3/library/pickle.html.\nMoreover, we have to move sqr definition into a separate file (https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror)\n\nimport _files.defs as defs\n\ndef squarev3(lst):\n    pool = Pool()\n    return pool.map(defs.sqr, number_list)\n\nsquarev3(number_list)\n\n[1, 4, 9, 16]\n\n\n\n\n\n\n\n\nNotePathos map with Dill\n\n\n\nHowever, Dill (https://pypi.org/project/dill/) can overcome this limitation!\nHow to use Dill?\nuv add pathos\n\nuv add toolz\nAnd then\nfrom pathos.multiprocessing import ProcessPool\n\ndef squarev4(lst):\n    pool = ProcessPool()\n    return pool.map(lambda x: x*x, lst)\n\nsquarev4([1,3,5])"
  },
  {
    "objectID": "applied_lab2.html#numpy-vectorization",
    "href": "applied_lab2.html#numpy-vectorization",
    "title": "Applied Analytics: Lab 2",
    "section": "",
    "text": "Numpy automatically vectorizes array operations. However, we can explicitly invoke vectorize():\nIn this context vectorization refers to low-level CPU vectorization (single CPU instruction processing items in a vector)\n\nimport numpy as np\n\nnp_array = np.array(number_list)\n\nvectorized_fn = np.vectorize(lambda x: x*x)\n\nresult = vectorized_fn(np_array)\nprint(result)\n\n[ 1  4  9 16]"
  },
  {
    "objectID": "applied_lab2.html#exercises",
    "href": "applied_lab2.html#exercises",
    "title": "Applied Analytics: Lab 2",
    "section": "",
    "text": "Write a version of curried function c_5 using lambda syntax.\nWrite a program calculating a factorial using parallelisation and map-reduce.\nWrite a parallelised version of reduce (hint: use chunking).\nCompare performance of Numpy vectorization vs parallel map on a really large array of integers.\nUsing computations from previous labs, apply chunking approach to df.apply(...) using parallel map."
  },
  {
    "objectID": "bigdata.html",
    "href": "bigdata.html",
    "title": "Big Data Analytics",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "bigdata.html#lectures",
    "href": "bigdata.html#lectures",
    "title": "Big Data Analytics",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "bigdata.html#labs",
    "href": "bigdata.html#labs",
    "title": "Big Data Analytics",
    "section": "Labs",
    "text": "Labs\nLab 1\nLab 2\nLab 3"
  },
  {
    "objectID": "bigdata.html#exam-questions",
    "href": "bigdata.html#exam-questions",
    "title": "Big Data Analytics",
    "section": "Exam questions",
    "text": "Exam questions"
  },
  {
    "objectID": "bigdata_lab1.html",
    "href": "bigdata_lab1.html",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "Primary docs:\n\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html\n\nPerformance deps for Pandas:\n\nhttps://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#install-recommended-dependencies\n\n\n\n\n\n\n\nchunksize parameter in Pandas functions.\n\n\n\nUse to_numeric (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html) for downcasting\n\n\n\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n\n\n\n\nuse DataFrame.memory_usage(deep=True) func.\nuse DataFrame.info() func\n\n\n\n\n\nWe’ll get the sample dataset from https://www.kaggle.com/datasets/anthonytherrien/depression-dataset.\n\nimport pandas as pd\n\ndd = pd.read_csv('_files/depression_data.csv')\ndd.head()\n\n\n\n\n\n\n\n\nName\nAge\nMarital Status\nEducation Level\nNumber of Children\nSmoking Status\nPhysical Activity Level\nEmployment Status\nIncome\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\n0\nChristine Barker\n31\nMarried\nBachelor's Degree\n2\nNon-smoker\nActive\nUnemployed\n26265.67\nModerate\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n1\nJacqueline Lewis\n55\nMarried\nHigh School\n1\nNon-smoker\nSedentary\nEmployed\n42710.36\nHigh\nUnhealthy\nFair\nYes\nNo\nNo\nYes\n\n\n2\nShannon Church\n78\nWidowed\nMaster's Degree\n1\nNon-smoker\nSedentary\nEmployed\n125332.79\nLow\nUnhealthy\nGood\nNo\nNo\nYes\nNo\n\n\n3\nCharles Jordan\n58\nDivorced\nMaster's Degree\n3\nNon-smoker\nModerate\nUnemployed\n9992.78\nModerate\nModerate\nPoor\nNo\nNo\nNo\nNo\n\n\n4\nMichael Rich\n18\nSingle\nHigh School\n0\nNon-smoker\nSedentary\nUnemployed\n8595.08\nLow\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n\n\n\n\n\nLet’s see some memory usage stats:\n\ndd.info(memory_usage='deep')\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 413768 entries, 0 to 413767\nData columns (total 16 columns):\n #   Column                        Non-Null Count   Dtype  \n---  ------                        --------------   -----  \n 0   Name                          413768 non-null  object \n 1   Age                           413768 non-null  int64  \n 2   Marital Status                413768 non-null  object \n 3   Education Level               413768 non-null  object \n 4   Number of Children            413768 non-null  int64  \n 5   Smoking Status                413768 non-null  object \n 6   Physical Activity Level       413768 non-null  object \n 7   Employment Status             413768 non-null  object \n 8   Income                        413768 non-null  float64\n 9   Alcohol Consumption           413768 non-null  object \n 10  Dietary Habits                413768 non-null  object \n 11  Sleep Patterns                413768 non-null  object \n 12  History of Mental Illness     413768 non-null  object \n 13  History of Substance Abuse    413768 non-null  object \n 14  Family History of Depression  413768 non-null  object \n 15  Chronic Medical Conditions    413768 non-null  object \ndtypes: float64(1), int64(2), object(13)\nmemory usage: 294.9 MB\n\n\nNote: Pandas stores memory in blocks, managed by BlockManager class. There is a separate block class for each type, like ObjectBlock or FloatBlock.\nNice write-up here: https://uwekorn.com/2020/05/24/the-one-pandas-internal.html.\nLet’s examine memory usage for each type:\n\nfor dtype in ['float','int','object']:\n    selected_dtype = dd.select_dtypes(include=[dtype])\n    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n    mean_usage_mb = mean_usage_b / 1024 ** 2\n    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))\n\nAverage memory usage for float columns: 1.58 MB\nAverage memory usage for int columns: 2.10 MB\nAverage memory usage for object columns: 20.39 MB\n\n\n\n\nLet’s first use iinfo to check ranges for different subtypes:\n\nimport numpy as np\nint_types = [\"uint8\", \"int8\", \"int16\", \"int32\", \"int64\"]\nfor it in int_types:\n    print(np.iinfo(it))\n\nMachine parameters for uint8\n---------------------------------------------------------------\nmin = 0\nmax = 255\n---------------------------------------------------------------\n\nMachine parameters for int8\n---------------------------------------------------------------\nmin = -128\nmax = 127\n---------------------------------------------------------------\n\nMachine parameters for int16\n---------------------------------------------------------------\nmin = -32768\nmax = 32767\n---------------------------------------------------------------\n\nMachine parameters for int32\n---------------------------------------------------------------\nmin = -2147483648\nmax = 2147483647\n---------------------------------------------------------------\n\nMachine parameters for int64\n---------------------------------------------------------------\nmin = -9223372036854775808\nmax = 9223372036854775807\n---------------------------------------------------------------\n\n\n\nWe can use pd.to_numeric() to downcast numeric types.\nFirst, let’s write a helper function for memory usage display:\n\ndef mem_usage(pandas_obj):\n    if isinstance(pandas_obj,pd.DataFrame):\n        usage_b = pandas_obj.memory_usage(deep=True).sum()\n    else: # we assume if not a df it's a series\n        usage_b = pandas_obj.memory_usage(deep=True)\n    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n    return \"{:03.2f} MB\".format(usage_mb)\n\nNote that “Age” and “Number of Children” columns can be presented as unsigned ints. Let’s convert:\n\ndd_int = dd.select_dtypes(include=['int'])\nconverted_int = dd_int.apply(pd.to_numeric,downcast='unsigned')\nprint(\"Before: \", mem_usage(dd_int))\nprint(\"After: \", mem_usage(converted_int))\ncompare_ints = pd.concat([dd_int.dtypes,converted_int.dtypes],axis=1)\ncompare_ints.columns = ['before','after']\ncompare_ints.apply(pd.Series.value_counts)\n\nBefore:  6.31 MB\nAfter:  0.79 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nuint8\nNaN\n2.0\n\n\nint64\n2.0\nNaN\n\n\n\n\n\n\n\nNice. Now let’s process float columns (Income):\n\ndd_float = dd.select_dtypes(include=['float'])\nconverted_float = dd_float.apply(pd.to_numeric,downcast='float')\nprint(\"Before: \", mem_usage(dd_float))\nprint(\"After: \", mem_usage(converted_float))\ncompare_floats = pd.concat([dd_float.dtypes,converted_float.dtypes],axis=1)\ncompare_floats.columns = ['before','after']\ncompare_floats.apply(pd.Series.value_counts)\n\nBefore:  3.16 MB\nAfter:  3.16 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nfloat64\n1\n1\n\n\n\n\n\n\n\nNothing spectacular.\nNow let’s create a new optimized DataFrame:\n\noptimized_dd = dd.copy()\noptimized_dd[converted_int.columns] = converted_int\noptimized_dd[converted_float.columns] = converted_float\nprint(mem_usage(dd))\nprint(mem_usage(optimized_dd))\n\n294.92 MB\n289.40 MB\n\n\nJust a bit. Let’s proceed with object types.\nFirst, read https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/.\nNow, back to the dataset.\nWe can use categoricals (https://pandas.pydata.org/pandas-docs/stable/categorical.html) to optimize object columns.\n\nLet’s look at the number of unique values for each object type:\n\ndd_obj = dd.select_dtypes(include=['object']).copy()\ndd_obj.describe()\n\n\n\n\n\n\n\n\nName\nMarital Status\nEducation Level\nSmoking Status\nPhysical Activity Level\nEmployment Status\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\ncount\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n\n\nunique\n196851\n4\n5\n3\n3\n2\n3\n3\n3\n2\n2\n2\n2\n\n\ntop\nMichael Smith\nMarried\nBachelor's Degree\nNon-smoker\nSedentary\nEmployed\nModerate\nUnhealthy\nFair\nNo\nNo\nNo\nNo\n\n\nfreq\n198\n240444\n124329\n247416\n176850\n265659\n173440\n170817\n196789\n287943\n284880\n302515\n277561\n\n\n\n\n\n\n\nLet’s start with one column first: “Education Level”\n\nedu_level = dd_obj[\"Education Level\"]\nprint(edu_level.head())\nedu_level_cat = edu_level.astype('category')\nprint(edu_level_cat.head())\n\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: object\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: category\nCategories (5, object): ['Associate Degree', 'Bachelor's Degree', 'High School', 'Master's Degree', 'PhD']\n\n\nWe can look at the category codes:\n\nedu_level_cat.head().cat.codes\n\n0    1\n1    2\n2    3\n3    3\n4    2\ndtype: int8\n\n\nCompare memory usage:\n\nprint(mem_usage(edu_level))\nprint(mem_usage(edu_level_cat))\n\n24.92 MB\n0.40 MB\n\n\nWe should only convert objects to categoricals if most values are repeated. We have to pick a threshold, say, 25% of unique values.\nNote: when reading a csv, we can also provide a dtype dictionary param with preferred types.\nConverting all object columns and creating a new optimized DataFrame is left as part of Exercise 1.\n\n\n\n\n\nPick your own dataset from\n\n\nKaggle\nHuggingFace\nor https://archive.ics.uci.edu.\n\n\nUse Dora library installed in Applied lab1 for data cleaning.\n\n\n\n\n\n\n\nPerform numerical and object types conversions aimed at minimizing storage space, as outlined above.\nMeasure performance impact and plot it via e.g. matplotlib."
  },
  {
    "objectID": "bigdata_lab1.html#notes",
    "href": "bigdata_lab1.html#notes",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "chunksize parameter in Pandas functions.\n\n\n\nUse to_numeric (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html) for downcasting\n\n\n\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n\n\n\n\nuse DataFrame.memory_usage(deep=True) func.\nuse DataFrame.info() func"
  },
  {
    "objectID": "bigdata_lab1.html#storage-optimization",
    "href": "bigdata_lab1.html#storage-optimization",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "We’ll get the sample dataset from https://www.kaggle.com/datasets/anthonytherrien/depression-dataset.\n\nimport pandas as pd\n\ndd = pd.read_csv('_files/depression_data.csv')\ndd.head()\n\n\n\n\n\n\n\n\nName\nAge\nMarital Status\nEducation Level\nNumber of Children\nSmoking Status\nPhysical Activity Level\nEmployment Status\nIncome\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\n0\nChristine Barker\n31\nMarried\nBachelor's Degree\n2\nNon-smoker\nActive\nUnemployed\n26265.67\nModerate\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n1\nJacqueline Lewis\n55\nMarried\nHigh School\n1\nNon-smoker\nSedentary\nEmployed\n42710.36\nHigh\nUnhealthy\nFair\nYes\nNo\nNo\nYes\n\n\n2\nShannon Church\n78\nWidowed\nMaster's Degree\n1\nNon-smoker\nSedentary\nEmployed\n125332.79\nLow\nUnhealthy\nGood\nNo\nNo\nYes\nNo\n\n\n3\nCharles Jordan\n58\nDivorced\nMaster's Degree\n3\nNon-smoker\nModerate\nUnemployed\n9992.78\nModerate\nModerate\nPoor\nNo\nNo\nNo\nNo\n\n\n4\nMichael Rich\n18\nSingle\nHigh School\n0\nNon-smoker\nSedentary\nUnemployed\n8595.08\nLow\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n\n\n\n\n\nLet’s see some memory usage stats:\n\ndd.info(memory_usage='deep')\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 413768 entries, 0 to 413767\nData columns (total 16 columns):\n #   Column                        Non-Null Count   Dtype  \n---  ------                        --------------   -----  \n 0   Name                          413768 non-null  object \n 1   Age                           413768 non-null  int64  \n 2   Marital Status                413768 non-null  object \n 3   Education Level               413768 non-null  object \n 4   Number of Children            413768 non-null  int64  \n 5   Smoking Status                413768 non-null  object \n 6   Physical Activity Level       413768 non-null  object \n 7   Employment Status             413768 non-null  object \n 8   Income                        413768 non-null  float64\n 9   Alcohol Consumption           413768 non-null  object \n 10  Dietary Habits                413768 non-null  object \n 11  Sleep Patterns                413768 non-null  object \n 12  History of Mental Illness     413768 non-null  object \n 13  History of Substance Abuse    413768 non-null  object \n 14  Family History of Depression  413768 non-null  object \n 15  Chronic Medical Conditions    413768 non-null  object \ndtypes: float64(1), int64(2), object(13)\nmemory usage: 294.9 MB\n\n\nNote: Pandas stores memory in blocks, managed by BlockManager class. There is a separate block class for each type, like ObjectBlock or FloatBlock.\nNice write-up here: https://uwekorn.com/2020/05/24/the-one-pandas-internal.html.\nLet’s examine memory usage for each type:\n\nfor dtype in ['float','int','object']:\n    selected_dtype = dd.select_dtypes(include=[dtype])\n    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n    mean_usage_mb = mean_usage_b / 1024 ** 2\n    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))\n\nAverage memory usage for float columns: 1.58 MB\nAverage memory usage for int columns: 2.10 MB\nAverage memory usage for object columns: 20.39 MB\n\n\n\n\nLet’s first use iinfo to check ranges for different subtypes:\n\nimport numpy as np\nint_types = [\"uint8\", \"int8\", \"int16\", \"int32\", \"int64\"]\nfor it in int_types:\n    print(np.iinfo(it))\n\nMachine parameters for uint8\n---------------------------------------------------------------\nmin = 0\nmax = 255\n---------------------------------------------------------------\n\nMachine parameters for int8\n---------------------------------------------------------------\nmin = -128\nmax = 127\n---------------------------------------------------------------\n\nMachine parameters for int16\n---------------------------------------------------------------\nmin = -32768\nmax = 32767\n---------------------------------------------------------------\n\nMachine parameters for int32\n---------------------------------------------------------------\nmin = -2147483648\nmax = 2147483647\n---------------------------------------------------------------\n\nMachine parameters for int64\n---------------------------------------------------------------\nmin = -9223372036854775808\nmax = 9223372036854775807\n---------------------------------------------------------------\n\n\n\nWe can use pd.to_numeric() to downcast numeric types.\nFirst, let’s write a helper function for memory usage display:\n\ndef mem_usage(pandas_obj):\n    if isinstance(pandas_obj,pd.DataFrame):\n        usage_b = pandas_obj.memory_usage(deep=True).sum()\n    else: # we assume if not a df it's a series\n        usage_b = pandas_obj.memory_usage(deep=True)\n    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n    return \"{:03.2f} MB\".format(usage_mb)\n\nNote that “Age” and “Number of Children” columns can be presented as unsigned ints. Let’s convert:\n\ndd_int = dd.select_dtypes(include=['int'])\nconverted_int = dd_int.apply(pd.to_numeric,downcast='unsigned')\nprint(\"Before: \", mem_usage(dd_int))\nprint(\"After: \", mem_usage(converted_int))\ncompare_ints = pd.concat([dd_int.dtypes,converted_int.dtypes],axis=1)\ncompare_ints.columns = ['before','after']\ncompare_ints.apply(pd.Series.value_counts)\n\nBefore:  6.31 MB\nAfter:  0.79 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nuint8\nNaN\n2.0\n\n\nint64\n2.0\nNaN\n\n\n\n\n\n\n\nNice. Now let’s process float columns (Income):\n\ndd_float = dd.select_dtypes(include=['float'])\nconverted_float = dd_float.apply(pd.to_numeric,downcast='float')\nprint(\"Before: \", mem_usage(dd_float))\nprint(\"After: \", mem_usage(converted_float))\ncompare_floats = pd.concat([dd_float.dtypes,converted_float.dtypes],axis=1)\ncompare_floats.columns = ['before','after']\ncompare_floats.apply(pd.Series.value_counts)\n\nBefore:  3.16 MB\nAfter:  3.16 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nfloat64\n1\n1\n\n\n\n\n\n\n\nNothing spectacular.\nNow let’s create a new optimized DataFrame:\n\noptimized_dd = dd.copy()\noptimized_dd[converted_int.columns] = converted_int\noptimized_dd[converted_float.columns] = converted_float\nprint(mem_usage(dd))\nprint(mem_usage(optimized_dd))\n\n294.92 MB\n289.40 MB\n\n\nJust a bit. Let’s proceed with object types.\nFirst, read https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/.\nNow, back to the dataset.\nWe can use categoricals (https://pandas.pydata.org/pandas-docs/stable/categorical.html) to optimize object columns.\n\nLet’s look at the number of unique values for each object type:\n\ndd_obj = dd.select_dtypes(include=['object']).copy()\ndd_obj.describe()\n\n\n\n\n\n\n\n\nName\nMarital Status\nEducation Level\nSmoking Status\nPhysical Activity Level\nEmployment Status\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\ncount\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n\n\nunique\n196851\n4\n5\n3\n3\n2\n3\n3\n3\n2\n2\n2\n2\n\n\ntop\nMichael Smith\nMarried\nBachelor's Degree\nNon-smoker\nSedentary\nEmployed\nModerate\nUnhealthy\nFair\nNo\nNo\nNo\nNo\n\n\nfreq\n198\n240444\n124329\n247416\n176850\n265659\n173440\n170817\n196789\n287943\n284880\n302515\n277561\n\n\n\n\n\n\n\nLet’s start with one column first: “Education Level”\n\nedu_level = dd_obj[\"Education Level\"]\nprint(edu_level.head())\nedu_level_cat = edu_level.astype('category')\nprint(edu_level_cat.head())\n\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: object\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: category\nCategories (5, object): ['Associate Degree', 'Bachelor's Degree', 'High School', 'Master's Degree', 'PhD']\n\n\nWe can look at the category codes:\n\nedu_level_cat.head().cat.codes\n\n0    1\n1    2\n2    3\n3    3\n4    2\ndtype: int8\n\n\nCompare memory usage:\n\nprint(mem_usage(edu_level))\nprint(mem_usage(edu_level_cat))\n\n24.92 MB\n0.40 MB\n\n\nWe should only convert objects to categoricals if most values are repeated. We have to pick a threshold, say, 25% of unique values.\nNote: when reading a csv, we can also provide a dtype dictionary param with preferred types.\nConverting all object columns and creating a new optimized DataFrame is left as part of Exercise 1."
  },
  {
    "objectID": "bigdata_lab1.html#exercises",
    "href": "bigdata_lab1.html#exercises",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "Pick your own dataset from\n\n\nKaggle\nHuggingFace\nor https://archive.ics.uci.edu.\n\n\nUse Dora library installed in Applied lab1 for data cleaning.\n\n\n\n\n\n\n\nPerform numerical and object types conversions aimed at minimizing storage space, as outlined above.\nMeasure performance impact and plot it via e.g. matplotlib."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analytics",
    "section": "",
    "text": "Big data analytics / Applied data analytics courses."
  },
  {
    "objectID": "bigdata_lec2.html#moores-law",
    "href": "bigdata_lec2.html#moores-law",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\n\n\nDefinition (1965)\n\n\nNumber of transistors in an integrated circuit (IC) doubles about every two years."
  },
  {
    "objectID": "bigdata_lec2.html#moores-law-1",
    "href": "bigdata_lec2.html#moores-law-1",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law"
  },
  {
    "objectID": "bigdata_lec2.html#moores-law-2",
    "href": "bigdata_lec2.html#moores-law-2",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law"
  },
  {
    "objectID": "bigdata_lec2.html#moores-law-3",
    "href": "bigdata_lec2.html#moores-law-3",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\n\n\nTrends\n\n\n\nGordon Moore: Moore’s law will end by around 2025.\nNvidia CEO Jensen Huang: declared Moore’s law dead in 2022.\n\n\n\n\n\n\n\nPat Gelsinger, Intel CEO, end of 2023\n\n\n\nWe’re no longer in the golden era of Moore’s Law, it’s much, much harder now, so we’re probably doubling effectively closer to every three years now, so we’ve definitely seen a slowing."
  },
  {
    "objectID": "bigdata_lec2.html#edholms-law",
    "href": "bigdata_lec2.html#edholms-law",
    "title": "Big Data: Speeding up computation",
    "section": "Edholm’s law",
    "text": "Edholm’s law\n\n\n\nDefinition (2004)\n\n\n\nthree categories of telecommunication, namely\n\nwireless (mobile),\nnomadic (wireless without mobility)\nand wired networks (fixed),\n\nare in lockstep and gradually converging\nthe bandwidth and data rates double every 18 months, which has proven to be true since the 1970s"
  },
  {
    "objectID": "bigdata_lec2.html#edholms-law-1",
    "href": "bigdata_lec2.html#edholms-law-1",
    "title": "Big Data: Speeding up computation",
    "section": "Edholm’s law",
    "text": "Edholm’s law"
  },
  {
    "objectID": "bigdata_lec2.html#edholms-law-2",
    "href": "bigdata_lec2.html#edholms-law-2",
    "title": "Big Data: Speeding up computation",
    "section": "Edholm’s law",
    "text": "Edholm’s law\n\n\n\n\nData deluge\n\n\n\n90% of data humankind has produced happened in the last two years.\n80% of data could be unstructured\n99% of data produced is never analyzed"
  },
  {
    "objectID": "bigdata_lec2.html#core-counts",
    "href": "bigdata_lec2.html#core-counts",
    "title": "Big Data: Speeding up computation",
    "section": "Core counts",
    "text": "Core counts"
  },
  {
    "objectID": "bigdata_lec2.html#concurrency-vs-parallelism",
    "href": "bigdata_lec2.html#concurrency-vs-parallelism",
    "title": "Big Data: Speeding up computation",
    "section": "Concurrency vs Parallelism",
    "text": "Concurrency vs Parallelism\n\n\n\n\n\nConcurrency Parallelism\n\n\nIndividual steps of both tasks are executed in an interleaved fashion\n\n\n\n\n\n\n\nParallelism\n\n\nTask statements are executed at the same time."
  },
  {
    "objectID": "bigdata_lec2.html#process",
    "href": "bigdata_lec2.html#process",
    "title": "Big Data: Speeding up computation",
    "section": "Process",
    "text": "Process\n\n\n\nDefinition\n\n\nA process can be defined as an instance of a running program with its own memory.\nAlternatively: a context maintained for an executing program.\nProcesses have:\n\nlifetimes\nparents\nchildren.\nmemory/resources allocated."
  },
  {
    "objectID": "bigdata_lec2.html#process-1",
    "href": "bigdata_lec2.html#process-1",
    "title": "Big Data: Speeding up computation",
    "section": "Process",
    "text": "Process"
  },
  {
    "objectID": "bigdata_lec2.html#threads",
    "href": "bigdata_lec2.html#threads",
    "title": "Big Data: Speeding up computation",
    "section": "Threads",
    "text": "Threads\n\n\n\nDefinition\n\n\nA lightweight unit of execution within a process that can operate independently.\nThread is a basic unit to which the operating system allocates processor time.\nManaged with help of thread context, which consists of processor context and information required for thread management."
  },
  {
    "objectID": "bigdata_lec2.html#threads-1",
    "href": "bigdata_lec2.html#threads-1",
    "title": "Big Data: Speeding up computation",
    "section": "Threads",
    "text": "Threads"
  },
  {
    "objectID": "bigdata_lec2.html#green-threadscoroutines",
    "href": "bigdata_lec2.html#green-threadscoroutines",
    "title": "Big Data: Speeding up computation",
    "section": "Green threads/Coroutines",
    "text": "Green threads/Coroutines\n\n\n\nDefinition\n\n\nThese are threads managed by the process runtime, multiplexed onto OS threads."
  },
  {
    "objectID": "bigdata_lec2.html#comparison-table",
    "href": "bigdata_lec2.html#comparison-table",
    "title": "Big Data: Speeding up computation",
    "section": "Comparison table",
    "text": "Comparison table"
  },
  {
    "objectID": "bigdata_lec2.html#imperative",
    "href": "bigdata_lec2.html#imperative",
    "title": "Big Data: Speeding up computation",
    "section": "Imperative",
    "text": "Imperative\n\n\n\nWikipedia definition\n\n\nA programming paradigm of software that uses statements that change a program’s state.\nImperative program is a step-by-step description of program’s algorithm.\n\n\n\n\n\n\nExamples\n\n\n\nFortran\nCOBOL\nC\nPython\nGo"
  },
  {
    "objectID": "bigdata_lec2.html#imperative-1",
    "href": "bigdata_lec2.html#imperative-1",
    "title": "Big Data: Speeding up computation",
    "section": "Imperative",
    "text": "Imperative\n\n\n\nCons\n\n\n\nDifficult to parallelize\nLots of state management\n\n\n\n\n\n\n\nPros\n\n\n\nIntuitive concept, maps well to how people think, program as a recipe.\nEasy to optimize by the compiler"
  },
  {
    "objectID": "bigdata_lec2.html#functional",
    "href": "bigdata_lec2.html#functional",
    "title": "Big Data: Speeding up computation",
    "section": "Functional",
    "text": "Functional\n\n\n\nWikipedia definition\n\n\nA programming paradigm where programs are constructed by applying and composing functions.\n\n\n\n\n\n\nExamples\n\n\n\nML (Ocaml)\nLisp\nHaskell\nJulia"
  },
  {
    "objectID": "bigdata_lec2.html#functional-1",
    "href": "bigdata_lec2.html#functional-1",
    "title": "Big Data: Speeding up computation",
    "section": "Functional",
    "text": "Functional\n\n\n\nCons\n\n\n\nOften non-intuitive to reason about\nDepending on a specific algorithm, might be slower\n\n\n\n\n\n\n\nPros\n\n\n\nEasier to parallelize\nLend themselves beautifully to certain types of problems"
  },
  {
    "objectID": "bigdata_lec2.html#object-oriented",
    "href": "bigdata_lec2.html#object-oriented",
    "title": "Big Data: Speeding up computation",
    "section": "Object-oriented",
    "text": "Object-oriented\n\n\n\nWikipedia definition\n\n\nA programming paradigm based on the concept of objects, which can contain data and code: data in the form of fields (often known as attributes or properties), and code in the form of procedures (often known as methods).\n\n\n\n\n\n\nExamples\n\n\n\nSmalltalk\nJava\nC++\nPython\nC#"
  },
  {
    "objectID": "bigdata_lec2.html#object-oriented-1",
    "href": "bigdata_lec2.html#object-oriented-1",
    "title": "Big Data: Speeding up computation",
    "section": "Object-oriented",
    "text": "Object-oriented"
  },
  {
    "objectID": "bigdata_lec2.html#object-oriented-2",
    "href": "bigdata_lec2.html#object-oriented-2",
    "title": "Big Data: Speeding up computation",
    "section": "Object-oriented",
    "text": "Object-oriented\n\n\n\nCons\n\n\n\nDoes not map well to many problems\nLots of state management\n\n\n\n\n\n\n\nPros\n\n\n\nIntuitively easy to grasp, as human thinking is largely noun-oriented\nUseful for UIs"
  },
  {
    "objectID": "bigdata_lec2.html#symbolic",
    "href": "bigdata_lec2.html#symbolic",
    "title": "Big Data: Speeding up computation",
    "section": "Symbolic",
    "text": "Symbolic\n\n\n\nWikipedia definition\n\n\nA programming paradigm in which the program can manipulate its own formulas and program components as if they were plain data.\n\n\n\n\n\n\nExamples\n\n\n\nLisp\nProlog\nJulia"
  },
  {
    "objectID": "bigdata_lec2.html#lisp",
    "href": "bigdata_lec2.html#lisp",
    "title": "Big Data: Speeding up computation",
    "section": "Lisp",
    "text": "Lisp"
  },
  {
    "objectID": "bigdata_lec2.html#prolog",
    "href": "bigdata_lec2.html#prolog",
    "title": "Big Data: Speeding up computation",
    "section": "Prolog",
    "text": "Prolog\n :::"
  },
  {
    "objectID": "bigdata_lec2.html#typing-1",
    "href": "bigdata_lec2.html#typing-1",
    "title": "Big Data: Speeding up computation",
    "section": "Typing",
    "text": "Typing\n\n\n\nStatic vs dynamic\n\n\n\nstatic: types are known and checked before running the program\ndynamic: types become known when the program is running\n\n\n\n\n\n\n\nStrong vs weak\n\n\n\nstrong: variable types are not changed easily\nweak: types can be changed by the compiler if necessary"
  },
  {
    "objectID": "bigdata_lec2.html#typing-2",
    "href": "bigdata_lec2.html#typing-2",
    "title": "Big Data: Speeding up computation",
    "section": "Typing",
    "text": "Typing"
  },
  {
    "objectID": "bigdata_lab3.html",
    "href": "bigdata_lab3.html",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "Suppose we want to apply several transformations. What can we do?\n\nSeveral maps.\nChain functions with compose.\nCreate a function pipeline with pipe.\n\nSuppose we want to apply \\(f(x) = \\exp(\\sin(x))\\) to a list of numbers.\nOption 1:\n\nimport math\n\nfloat_list = [0.234, -1.96, 5.071]\n\n# several maps\nlst1 = map(math.sin, float_list)\nlst2 = map(math.exp, lst1)\n\nprint(list(lst2))\n\n[1.260956241447609, 0.396447553306686, 0.3920424231508857]\n\n\nOption 2: we’ll use the toolz package we installed in lab2.\n\nfrom toolz.functoolz import compose\n\ncomposed = compose(math.exp, math.sin) # note reverse order\n\nprint(list(map(composed, float_list)))\n\n[1.260956241447609, 0.396447553306686, 0.3920424231508857]\n\n\nOption 3:\nAnd there is also pipe:\n\nfrom toolz.functoolz import pipe\n\npiped = lambda x: pipe(x, math.sin, math.exp) # now the order is proper\n\nprint(list(map(piped, float_list)))\n\n[1.260956241447609, 0.396447553306686, 0.3920424231508857]\n\n\n\n\n\nmap is lazy! This is the opposite to being eager. That’s why we apply list first to materialize results of map application.\nLaziness provides memory efficiency.\n\n\n\nIn addition to map, there are also filter and zip.\nVariations:\n\nfrom itertools import filterfalse\nfrom toolz.dicttoolz import keyfilter, valfilter, itemfilter\n\ndef is_even(x):\n    if x % 2 == 0: return True\n    else: return False\n        \ndef both_are_even(x):\n    k,v = x\n    if is_even(k) and is_even(v): return True\n    else: return False\n        \nprint(list(filterfalse(is_even, range(10))))\n# [1, 3, 5, 7, 9]\nprint(list(keyfilter(is_even, {1:2, 2:3, 3:4, 4:5, 5:6})))\n# [2, 4]\nprint(list(valfilter(is_even, {1:2, 2:3, 3:4, 4:5, 5:6})))\n# [1, 3, 5]\nprint(list(itemfilter(both_are_even, {1:5, 2:4, 3:3, 4:2, 5:1}))) # [2, 4]\n\n[1, 3, 5, 7, 9]\n[2, 4]\n[1, 3, 5]\n[2, 4]\n\n\n\n\n\nprices = [25, 20, 40]\nitems = [\"peas\", \"chicken\", \"beef\"]\n\nzipped = list(zip(prices, items))\n\nimport pandas as pd\n\ndf = pd.DataFrame(list(zipped), columns=['price', 'name'])\n\ndf\n\n\n\n\n\n\n\n\nprice\nname\n\n\n\n\n0\n25\npeas\n\n\n1\n20\nchicken\n\n\n2\n40\nbeef\n\n\n\n\n\n\n\n\n\n\n\nPython documentation defines iterator as:\nAn iterator is an object representing a stream of data; this object returns the data one element at a time.\n\nAn object is called iterable if you can get an iterator for it.\nIterator classes implement __iter__() and __next__() methods. This means one can create their own iterator, like this:\n\nclass SequenceIterator:\n    def __init__(self, sequence):\n        self._sequence = sequence\n        self._index = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self._index &lt; len(self._sequence):\n            item = self._sequence[self._index]\n            self._index += 1\n            return item\n        else:\n            raise StopIteration\n\nAnd then use it:\n\nfor el in SequenceIterator([1,2,3]):\n    print(el)\n\n1\n2\n3\n\n\n\n\n\nGenerators are simply an easy way to create iterators.\nA generator example, using yield keyword:\n\n\n\nGenerators are simply an easy way to create iterators.\nA generator example, using yield keyword:\n\ndef even_numbers(n):\n    i=1\n    while i &lt;= n:\n        yield i*2\n        i += 1\n\nAnd that’s how we use it:\n\nfor i in even_numbers(20):\n    print(i)\n\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\n30\n32\n34\n36\n38\n40\n\n\nA generator example, using an ad-hoc syntax similar to list comprehensions (generator expression):\n\nfirst_100_even = (i*2 for i in range(1,20))\nprint(list(first_100_even))\n\n[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38]\n\n\nMemory usage comparison:\n\n# Generator function\ndef generate_numbers(n):\n    for i in range(n):\n        yield i\n\n# For Loop Example\ndef generate_numbers_list(n):\n    numbers = []\n    for i in range(n):\n        numbers.append(i)\n    return numbers\n\n# comparing memory usage\nimport sys\n\nn = 1000000  # generating 1 million numbers\n\n# memory usage for Generator\ngenerator_memory = sys.getsizeof(generate_numbers(n))\n\n# memory usage for For Loop\nfor_loop_memory = sys.getsizeof(generate_numbers_list(n))\n\nprint(\"memory usage for Generator:\", generator_memory, \"bytes\")\nprint(\"memory usage for For Loop:\", for_loop_memory, \"bytes\")\n\nmemory usage for Generator: 200 bytes\nmemory usage for For Loop: 8448728 bytes\n\n\n\n\n\n\n\n\nNoteWhat to remember\n\n\n\n\nGenerators optimize for memory.\nSometimes lists might be faster (but still use more memory)\nUsed in streaming applications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd now, what is all the fuss about? We can actually use map, reduce, and friends, on iterables, not just on lists:\n\n# map example\ngen_mapped = map(lambda x: x*x, even_numbers(30))\nprint(list(gen_mapped))\n\n[4, 16, 36, 64, 100, 144, 196, 256, 324, 400, 484, 576, 676, 784, 900, 1024, 1156, 1296, 1444, 1600, 1764, 1936, 2116, 2304, 2500, 2704, 2916, 3136, 3364, 3600]\n\n\n\n# reduce example\nfrom functools import reduce\ngen_reduced = reduce(lambda acc, el: acc+el, even_numbers(5), 0)\nprint(gen_reduced)\n\n30\n\n\n\n\n\n\nImplement versions of map, filter, and compose using reduce.\nUsing a csv file you’ve downloaded in lab1, create a generator that will yield first 3 columns of the file. And use this generator to create a Pandas.DataFrame. Hint: use https://docs.python.org/3/library/csv.html\n\nAdditional reading: https://docs.python.org/3/howto/functional.html"
  },
  {
    "objectID": "bigdata_lab3.html#function-pipelines",
    "href": "bigdata_lab3.html#function-pipelines",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "Suppose we want to apply several transformations. What can we do?\n\nSeveral maps.\nChain functions with compose.\nCreate a function pipeline with pipe.\n\nSuppose we want to apply \\(f(x) = \\exp(\\sin(x))\\) to a list of numbers.\nOption 1:\n\nimport math\n\nfloat_list = [0.234, -1.96, 5.071]\n\n# several maps\nlst1 = map(math.sin, float_list)\nlst2 = map(math.exp, lst1)\n\nprint(list(lst2))\n\n[1.260956241447609, 0.396447553306686, 0.3920424231508857]\n\n\nOption 2: we’ll use the toolz package we installed in lab2.\n\nfrom toolz.functoolz import compose\n\ncomposed = compose(math.exp, math.sin) # note reverse order\n\nprint(list(map(composed, float_list)))\n\n[1.260956241447609, 0.396447553306686, 0.3920424231508857]\n\n\nOption 3:\nAnd there is also pipe:\n\nfrom toolz.functoolz import pipe\n\npiped = lambda x: pipe(x, math.sin, math.exp) # now the order is proper\n\nprint(list(map(piped, float_list)))\n\n[1.260956241447609, 0.396447553306686, 0.3920424231508857]"
  },
  {
    "objectID": "bigdata_lab3.html#laziness",
    "href": "bigdata_lab3.html#laziness",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "map is lazy! This is the opposite to being eager. That’s why we apply list first to materialize results of map application.\nLaziness provides memory efficiency."
  },
  {
    "objectID": "bigdata_lab3.html#filter",
    "href": "bigdata_lab3.html#filter",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "In addition to map, there are also filter and zip.\nVariations:\n\nfrom itertools import filterfalse\nfrom toolz.dicttoolz import keyfilter, valfilter, itemfilter\n\ndef is_even(x):\n    if x % 2 == 0: return True\n    else: return False\n        \ndef both_are_even(x):\n    k,v = x\n    if is_even(k) and is_even(v): return True\n    else: return False\n        \nprint(list(filterfalse(is_even, range(10))))\n# [1, 3, 5, 7, 9]\nprint(list(keyfilter(is_even, {1:2, 2:3, 3:4, 4:5, 5:6})))\n# [2, 4]\nprint(list(valfilter(is_even, {1:2, 2:3, 3:4, 4:5, 5:6})))\n# [1, 3, 5]\nprint(list(itemfilter(both_are_even, {1:5, 2:4, 3:3, 4:2, 5:1}))) # [2, 4]\n\n[1, 3, 5, 7, 9]\n[2, 4]\n[1, 3, 5]\n[2, 4]\n\n\n\n\n\nprices = [25, 20, 40]\nitems = [\"peas\", \"chicken\", \"beef\"]\n\nzipped = list(zip(prices, items))\n\nimport pandas as pd\n\ndf = pd.DataFrame(list(zipped), columns=['price', 'name'])\n\ndf\n\n\n\n\n\n\n\n\nprice\nname\n\n\n\n\n0\n25\npeas\n\n\n1\n20\nchicken\n\n\n2\n40\nbeef"
  },
  {
    "objectID": "bigdata_lab3.html#iterators",
    "href": "bigdata_lab3.html#iterators",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "Python documentation defines iterator as:\nAn iterator is an object representing a stream of data; this object returns the data one element at a time.\n\nAn object is called iterable if you can get an iterator for it.\nIterator classes implement __iter__() and __next__() methods. This means one can create their own iterator, like this:\n\nclass SequenceIterator:\n    def __init__(self, sequence):\n        self._sequence = sequence\n        self._index = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self._index &lt; len(self._sequence):\n            item = self._sequence[self._index]\n            self._index += 1\n            return item\n        else:\n            raise StopIteration\n\nAnd then use it:\n\nfor el in SequenceIterator([1,2,3]):\n    print(el)\n\n1\n2\n3"
  },
  {
    "objectID": "bigdata_lab3.html#generators",
    "href": "bigdata_lab3.html#generators",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "Generators are simply an easy way to create iterators.\nA generator example, using yield keyword:"
  },
  {
    "objectID": "bigdata_lab3.html#generators-1",
    "href": "bigdata_lab3.html#generators-1",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "Generators are simply an easy way to create iterators.\nA generator example, using yield keyword:\n\ndef even_numbers(n):\n    i=1\n    while i &lt;= n:\n        yield i*2\n        i += 1\n\nAnd that’s how we use it:\n\nfor i in even_numbers(20):\n    print(i)\n\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\n30\n32\n34\n36\n38\n40\n\n\nA generator example, using an ad-hoc syntax similar to list comprehensions (generator expression):\n\nfirst_100_even = (i*2 for i in range(1,20))\nprint(list(first_100_even))\n\n[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38]\n\n\nMemory usage comparison:\n\n# Generator function\ndef generate_numbers(n):\n    for i in range(n):\n        yield i\n\n# For Loop Example\ndef generate_numbers_list(n):\n    numbers = []\n    for i in range(n):\n        numbers.append(i)\n    return numbers\n\n# comparing memory usage\nimport sys\n\nn = 1000000  # generating 1 million numbers\n\n# memory usage for Generator\ngenerator_memory = sys.getsizeof(generate_numbers(n))\n\n# memory usage for For Loop\nfor_loop_memory = sys.getsizeof(generate_numbers_list(n))\n\nprint(\"memory usage for Generator:\", generator_memory, \"bytes\")\nprint(\"memory usage for For Loop:\", for_loop_memory, \"bytes\")\n\nmemory usage for Generator: 200 bytes\nmemory usage for For Loop: 8448728 bytes\n\n\n\n\n\n\n\n\nNoteWhat to remember\n\n\n\n\nGenerators optimize for memory.\nSometimes lists might be faster (but still use more memory)\nUsed in streaming applications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd now, what is all the fuss about? We can actually use map, reduce, and friends, on iterables, not just on lists:\n\n# map example\ngen_mapped = map(lambda x: x*x, even_numbers(30))\nprint(list(gen_mapped))\n\n[4, 16, 36, 64, 100, 144, 196, 256, 324, 400, 484, 576, 676, 784, 900, 1024, 1156, 1296, 1444, 1600, 1764, 1936, 2116, 2304, 2500, 2704, 2916, 3136, 3364, 3600]\n\n\n\n# reduce example\nfrom functools import reduce\ngen_reduced = reduce(lambda acc, el: acc+el, even_numbers(5), 0)\nprint(gen_reduced)\n\n30"
  },
  {
    "objectID": "bigdata_lab3.html#exercises",
    "href": "bigdata_lab3.html#exercises",
    "title": "Big Data Analytics: Lab 3",
    "section": "",
    "text": "Implement versions of map, filter, and compose using reduce.\nUsing a csv file you’ve downloaded in lab1, create a generator that will yield first 3 columns of the file. And use this generator to create a Pandas.DataFrame. Hint: use https://docs.python.org/3/library/csv.html\n\nAdditional reading: https://docs.python.org/3/howto/functional.html"
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag",
    "href": "bigdata_lec4.html#program-as-dag",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nRepresentation\n\n\nA parallel program can be represented by a node- and edge-weighted directed acyclic graph (DAG), in which:\n\nnode weights represent task processing times\nedge weights represent data dependencies as well as the communication times between tasks."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-1",
    "href": "bigdata_lec4.html#program-as-dag-1",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG"
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-2",
    "href": "bigdata_lec4.html#program-as-dag-2",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nGeneralization\n\n\nMultithreaded computing can be viewed as a natural generalization of sequential computing in the following sense:\n\nin sequential computing, a computation can be defined as a totally ordered set of instructions, where the ordering corresponds to the sequential execution order;\nin multithreaded computing, a computation can be viewed as a partially ordered set of instructions (as specified by the DAG), where the instructions may be executed in any order compatible with the specified partial order."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-3",
    "href": "bigdata_lec4.html#program-as-dag-3",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nOrdering re-cap\n\n\nA binary relation \\(\\preccurlyeq\\) on some set \\(X\\) is called a partial order if \\(\\forall a,b,c \\in X\\) the following is true:\n\nReflexivity: \\(a \\preccurlyeq a\\)\nTransitiity: \\(a \\preccurlyeq b, b \\preccurlyeq c \\Rightarrow a \\preccurlyeq c\\)\nAntisymmetricity: \\(a \\preccurlyeq b, b \\preccurlyeq a \\Rightarrow a = b\\)\n\nIf, additionally, \\(\\forall a,b \\ in X\\) either \\(a \\preccurlyeq b\\) or \\(b \\preccurlyeq a\\), then the order is total."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-4",
    "href": "bigdata_lec4.html#program-as-dag-4",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nDefinitions\n\n\nA parallel program can be represented by a directed acyclic graph (DAG) \\[\nG=(V,E),\n\\]\nwhere \\(V\\) is a set of \\(v\\) nodes and \\(E\\) is a set of \\(e\\) directed edges.\nA node in the DAG represents a task which in turn is a set of instructions which must be executed sequentially without preemption in the same processor.\nThe weight of a node \\(n_i\\) is called the computation cost and is denoted by \\(w(n_i)\\)."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-5",
    "href": "bigdata_lec4.html#program-as-dag-5",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nDefinitions\n\n\nThe edges in the DAG, each of which is denoted by \\((n_i,n_j)\\), correspond to the communication messages and precedence constraints among the nodes.\nThe weight of an edge is called the communication cost of the edge and is denoted by \\(c(n_i, n_j)\\).\nThe source node of an edge is called the parent node while the sink node is called the child node.\nA node with no parent is called an entry node and a node with no child is called an exit node.\nThe communication-to-computation-ratio (CCR) of a parallel program is defined as its average edge weight divided by its average node weight."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-6",
    "href": "bigdata_lec4.html#program-as-dag-6",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nDefinition\n\n\nScheduling involves executing a parallel program by mapping the computation over the processors so that:\n\ncompletion time is minimized\nuse of other resources such as storage as energy is optimal."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-7",
    "href": "bigdata_lec4.html#program-as-dag-7",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nDefinitions\n\n\n\\(ST(n_i)\\) and \\(FT(n_i)\\) denote start time and finish time at some processor.\nAfter all the nodes have been scheduled, the schedule length is defined as \\(\\max_i\\left\\{FT(n_i)\\right\\}\\) across all processors.\nThe goal of scheduling is to minimize \\(\\max_i\\left\\{FT(n_i)\\right\\}\\).\nScheduling is done in such a manner that the precedence constraints among the program tasks are preserved."
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-8",
    "href": "bigdata_lec4.html#program-as-dag-8",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG"
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-9",
    "href": "bigdata_lec4.html#program-as-dag-9",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG"
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-10",
    "href": "bigdata_lec4.html#program-as-dag-10",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG"
  },
  {
    "objectID": "bigdata_lec4.html#program-as-dag-11",
    "href": "bigdata_lec4.html#program-as-dag-11",
    "title": "Big Data: Dask intro",
    "section": "Program as DAG",
    "text": "Program as DAG\n\n\n\nWork\n\n\nWork is defined as the number of vertices in the DAG.\nWork of a computation corresponds to the total number of operations it performs.\n\n\n\n\n\n\nSpan\n\n\nSpan is the length of the longest path in the DAG.\nSpan corresponds to the longest chain of dependencies in the computation.\n\n\n\n\n\n\nWork[make]span\n\n\nThe overall finish-time of a parallel program is commonly called the schedule length or makespan."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-1",
    "href": "bigdata_lec4.html#scheduling-1",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nTaxonomy"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-2",
    "href": "bigdata_lec4.html#scheduling-2",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nTypes\n\n\n\nStatic: the characteristics of a parallel program (such as task processing times, communication, data dependencies, and synchronization requirements) are known before program execution\nDynamic: a few assumptions about the parallel program can be made before execution, and thus, scheduling decisions have to be made on-the-fly."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-3",
    "href": "bigdata_lec4.html#scheduling-3",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nCategories\n\n\n\nJob scheduling: independent jobs are to be scheduled among the processors of a distributed computing system to optimize overall system performance\nScheduling and mapping: allocation of multiple interacting tasks of a single parallel program in order to minimize the completion time on the parallel computer system."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-model-variations",
    "href": "bigdata_lec4.html#scheduling-model-variations",
    "title": "Big Data: Dask intro",
    "section": "Scheduling model variations",
    "text": "Scheduling model variations\n\n\n\nPreemptive vs non-preemptive\n\n\n\npreemptive: execution of the task might be interrupted so that it’s allocated to a different processor\nnon-preemptive: execution must complete on a single processor\n\n\n\n\n\n\n\nParallel vs non-parallel\n\n\nParallel task requires more than one processor for its execution.\n\n\n\n\n\n\nWith vs without conditional branches\n\n\nIn conditional model, each edge in the DAG is associated with a non-zero probability that the child will be executed immediately after the parent."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-4",
    "href": "bigdata_lec4.html#scheduling-4",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nList scheduling\n\n\nThe basic idea of list scheduling is to make a scheduling list (a sequence of nodes for scheduling) by assigning them some priorities, and then repeatedly execute the following two steps until all the nodes in the graph are scheduled:\n\nRemove the first node from the scheduling list;\nAllocate the node to a processor which allows the earliest start-time."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-5",
    "href": "bigdata_lec4.html#scheduling-5",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nGreedy Scheduler\n\n\nWe say that a scheduler is greedy if whenever there is a processor available and a task ready to execute, then it assigns the task to the processor and starts running it immediately. Greedy schedulers have an important property that is summarized by the greedy scheduling principle.\n\n\n\n\n\n\nGreedy Scheduling Principle\n\n\nThe greedy scheduling principle postulates that if a computation is run on \\(P\\) processors using a greedy scheduler, then the total time (clock cycles) for running the computation is bounded by \\[\nT_P &lt; \\frac{W}{P} + S\n\\] where \\(W\\) is the work of the computation, and \\(S\\) is the span of the computation (both measured in units of clock cycles)."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-6",
    "href": "bigdata_lec4.html#scheduling-6",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nOptimality of Greedy Schedulers\n\n\nFirstly, the time to execute the computation cannot be less than \\(\\frac{W}{P}\\) clock cycles since we have a total of \\(W\\) clock cycles of work to do and the best we can possibly do is divide it evenly among the processors. Secondly, the time to execute the computation cannot be any less than \\(S\\) clock cycles, because \\(S\\) represents the longest chain of sequential dependencies. Therefore we have \\[\nT_P \\geq \\max\\left(\\frac{W}{P},S\\right).\n\\] We therefore see that a greedy scheduler does reasonably close to the best possible. In particular, \\(\\frac{W}{P} +S\\) is never more than twice \\(\\max\\left(\\frac{W}{P} ,S\\right)\\)."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-1",
    "href": "bigdata_lec4.html#scheduling-algorithms-1",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-2",
    "href": "bigdata_lec4.html#scheduling-algorithms-2",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms\n\n\n\nWhat’s a heuristic algorithm?\n\n\nAlgorithm used when classic methods are too slow for finding an exact or approximate solution, or when classic methods fail to find any exact solution in a search space.\nThis is achieved by trading\n\noptimality,\ncompleteness,\naccuracy,\nor precision\n\nfor speed.\nIn a way, it can be considered a shortcut."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-3",
    "href": "bigdata_lec4.html#scheduling-algorithms-3",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms\n\n\n\nList scheduling\n\n\nA list-scheduling heuristic maintains a list of all tasks of a given graph according to their priorities. It has two phases:\n\nthe task prioritizing or task selection phase for selecting the highest-priority ready task\nand the processor selection phase for selecting a suitable processor that minimizes a predefined cost function which can be the execution start time.\n\n\n\n\n\n\n\nFeatures\n\n\n\nfor a bounded number of fully connected homogeneous processors\nprovide better performance results at a lower scheduling time than the other groups"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-4",
    "href": "bigdata_lec4.html#scheduling-algorithms-4",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms\n\n\n\nClustering\n\n\n\nMaps the tasks to unlimited number of clusters. The selected tasks for clustering can be any task, not necessarily a ready task.\nEach iteration refines the previous clustering by merging some clusters.\nIf two tasks are assigned to the same cluster, they will be executed on the same processor."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-5",
    "href": "bigdata_lec4.html#scheduling-algorithms-5",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms\n\n\n\nClustering: Extra final steps\n\n\n\na cluster merging step for merging the clusters so that the remaining number of clusters equal the number of processors\na cluster mapping step for mapping the clusters on the available processors\na task ordering step for ordering the mapped tasks within each processor"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-algorithms-6",
    "href": "bigdata_lec4.html#scheduling-algorithms-6",
    "title": "Big Data: Dask intro",
    "section": "Scheduling algorithms",
    "text": "Scheduling algorithms\n\n\n\nGuided random search\n\n\nGuided random search techniques (or randomized search techniques) use random choice to guide themselves through the problem space, which is not the same as performing merely random walks as in the random search methods.\nThese techniques combine the knowledge gained from previous search results with some randomizing features to generate new results."
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop",
    "href": "bigdata_lec4.html#heftcpop",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nDefinition\n\n\nHeterogeneous earliest finish time (HEFT) is a heuristic algorithm to schedule a set of dependent tasks onto a network of heterogeneous workers taking communication time into account.\nCritical-Path-On-a-Processor (CPOP) algorithm uses the summation of upward and downward rank values for prioritizing tasks.\n\n\n\n\n\nH. Topcuoglu, S. Hariri and Min-You Wu, “Performance-effective and low-complexity task scheduling for heterogeneous computing,” in IEEE Transactions on Parallel and Distributed Systems, vol. 13, no. 3, pp. 260-274, March 2002, doi: 10.1109/71.993206."
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-1",
    "href": "bigdata_lec4.html#heftcpop-1",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nDefinitions\n\n\n\n\\(V\\) – set of \\(v\\) nodes\n\\(E\\) – set of \\(e\\) edges\n\\(data\\) – a \\(v \\times v\\) matrix of communication data\n\\(data_{i,k}\\) – amount of data to be transmitted from \\(n_i\\) to \\(n_k\\)\n\\(Q\\) – set of \\(q\\) processors\n\\(W\\) – a \\(v \\times q\\) computation cost matrix, in which each \\(w_{i,j}\\) gives the estimated execution cost to complete task \\(n_i\\) on processor \\(p_j\\)\n\\(B\\) – \\(q \\times q\\) data transfer rates matrix\n\\(L\\) – \\(q\\)-dimensional vector of communication startup costs"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-2",
    "href": "bigdata_lec4.html#heftcpop-2",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nCosts\n\n\nAverage execution cost: \\[\n  \\overline{w_i} = \\sum\\limits_{j=1}^q \\frac{w_{i,j}}{q}.\n\\] Communication cost of the edge \\((i,k)\\) for transfering data from task \\(n_i\\) scheduled on processor \\(p_m\\) to task \\(n_k\\) scheduled on processor \\(p_n\\): \\[\nc_{i,k} = L_m + \\frac{data_{i,k}}{B_{m,n}}\n\\] Average communication cost: \\[\n\\overline{c_{i,k}} = \\overline{L} + \\frac{data_{i,k}}{\\overline{B}}\n\\]"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-3",
    "href": "bigdata_lec4.html#heftcpop-3",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nTimes\n\n\nEarliest execution start time of task \\(n_i\\) on processor \\(p_j\\): \\[\n\\begin{align*}\n& EST(n_{entry}, p_j) = 0, \\\\\n& EST(n_i, p_j) = \\max\\left\\{avail[j], \\max\\limits_{n_m \\in pred(n_i)} (AFT(n_m)+c_{m,i})\\right\\},\n\\end{align*}\n\\] where \\(avail[j]\\) is the earliest time at which processor \\(p_j\\) is ready for task execution.\nEarliest execution finish time of task \\(n_i\\) on processor \\(p_j\\): \\[\nEFT(n_i, p_j) = w_{i,j} + EST(n_i,p_j)\n\\]\nAfter a task \\(n_m\\) is scheduled on processor \\(p_j\\), the earliest start time and the earliest finish time of \\(n_m\\) on processor \\(p_j\\) is equal to the actual start time \\(AST(n_m)\\) and the actual finish time \\(AFT(n_m)\\), respectively."
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-4",
    "href": "bigdata_lec4.html#heftcpop-4",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nWorkspan\n\n\nAfter all tasks in the graph have been scheduled, the schedule length (makespan) will be equal to the actual finish time of the exit task \\(n_{exit}\\). In case of several exits: \\[\nmakespan = \\max \\left\\{AFT(n_{exit})\\right\\}\n\\]\n\n\n\n\n\n\nDefinition\n\n\nThe objective function of the task scheduling problem is to determine the assignment of tasks to processors such that its makespan is minimized."
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-5",
    "href": "bigdata_lec4.html#heftcpop-5",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nUpward rank\n\n\n\\[\n\\begin{align*}\n  rank_u(n_i) = \\overline{w_i} + \\max\\limits_{n_j \\in succ(n_i)} \\left(\\overline{c_{i,j}} + rank_u(n_j)\\right)\n\\end{align*}\n\\]\n\n\\(succ(n_i)\\) – set of immediate successors of \\(n_i\\)\n\\(\\overline{c_{i,j}}\\) – average communication cost of edge \\((i,j)\\)\n\\(\\overline{w_i}\\) – average computation cost of task \\(n_i\\)\n\n\\[\nrank_u(n_{exit}) = \\overline{w_{exit}}\n\\]"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-6",
    "href": "bigdata_lec4.html#heftcpop-6",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nDownward rank\n\n\n\\[\n\\begin{align*}\n  rank_d(n_i) = \\max\\limits_{n_j \\in pred(n_i)} \\left(rank_d(n_j)+ \\overline{w_j} + \\overline{c_{j,i}}\\right)\n  \\end{align*}\n\\]\n\n\\(pred(n_i)\\) – set of immediate predecessors of \\(n_i\\)\n\\(rank_d(n_{entry}) = 0\\)\ncan be thought of as the longest distance from the entry task to \\(n_i\\) without computation costs"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-7",
    "href": "bigdata_lec4.html#heftcpop-7",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nHEFT\n\n\n\nSet the computation costs of tasks and communication costs of edges with mean values\nCompute \\(rank_u\\) for all tasks by traversing graph upward, starting from the exit task\nSort the tasks in the scheduling list by non-increasing order of \\(rank_u\\) values\nwhile there are unscheduled tasks in the list do:\n   select the task \\(n_i\\) from the scheduling list\n   for each processor \\(p_k\\) in the processor set \\(Q\\) do:\n     Compute \\(EFT(n_i,p_k)\\) using the insertion-based scheduling policy\n   Assign task \\(n_i\\) to the processor \\(p_j\\) minimizing \\(EFT(n_i, p_j)\\)"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-8",
    "href": "bigdata_lec4.html#heftcpop-8",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nCPOP\n\n\n\nSet the computation costs of tasks and communication costs of edges with mean values\nCompute \\(rank_u\\) for all tasks by traversing graph upward, starting from the exit task\nCompute \\(rank_d\\) for all tasks by traversing graph downward, starting from the entry task\nCompute \\(priority(n_i) = rank_d(n_i) + rank_u(n_i) \\; \\forall n_i\\)\n\\(|CP| = priority(n_{entry})\\)\n\\(SET_{CP} = \\{n_{entry}\\}\\), where \\(SET_{CP}\\) is a set of tasks on a critical path\n\\(n_k \\leftarrow n_{entry}\\)"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-9",
    "href": "bigdata_lec4.html#heftcpop-9",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nCPOP\n\n\n\nwhile \\(n_k \\neq n_{exit}\\) do\n   select \\(n_j: n_j \\in succ(n_k) \\textbf{ and } priority(n_j) = |CP|\\)\n   \\(SET_{CP} = SET_{CP} \\cup {n_j}\\)\n   \\(n_k \\leftarrow n_j\\)\nselect \\(p_{CP}\\) minimizing \\(\\sum\\limits_{n_i \\in SET_{CP}} w_{i,j} \\; \\forall p_j \\in Q\\)\ninitialize priority queue \\(PQ\\) with the entry task"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-10",
    "href": "bigdata_lec4.html#heftcpop-10",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\n\n\nCPOP\n\n\n\nwhile there are unscheduled tasks in the \\(PQ\\)do\n   select the highest priority task \\(n_i\\) from \\(PQ\\)\n   if \\(n_i \\ in SET_{CP}\\) then\n     assign \\(n_i\\) to \\(p_{CP}\\)\n   else\n     assign \\(n_i\\) to \\(p_j\\) minimizing \\(EFT(n_i,p_j)\\)\n   update \\(PQ\\) with the successors of \\(n_i\\) if ready"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-11",
    "href": "bigdata_lec4.html#heftcpop-11",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP"
  },
  {
    "objectID": "bigdata_lec4.html#heftcpop-12",
    "href": "bigdata_lec4.html#heftcpop-12",
    "title": "Big Data: Dask intro",
    "section": "HEFT/CPOP",
    "text": "HEFT/CPOP\n\nHEFT - (a), CPOP - (b)"
  },
  {
    "objectID": "bigdata_lec4.html#dask",
    "href": "bigdata_lec4.html#dask",
    "title": "Big Data: Dask intro",
    "section": "Dask",
    "text": "Dask\nDask is a library to perform parallel computation for analytics."
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-7",
    "href": "bigdata_lec4.html#scheduling-7",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nWhat does Dask scheduler do?\n\n\n\nexecute DAGs on parallel hardware\nmanage resource allocation across DAG nodes"
  },
  {
    "objectID": "bigdata_lec4.html#dask-scheduling-1",
    "href": "bigdata_lec4.html#dask-scheduling-1",
    "title": "Big Data: Dask intro",
    "section": "Dask Scheduling",
    "text": "Dask Scheduling\n\n\n\nDask Scheduler types\n\n\n\nSingle-machine scheduler: This scheduler provides basic features on a local process or thread pool. This scheduler was made first and is the default. It is simple and cheap to use, although it can only be used on a single machine and does not scale\nDistributed scheduler: This scheduler is more sophisticated, offers more features, but also requires a bit more effort to set up. It can run locally or distributed across a cluster"
  },
  {
    "objectID": "bigdata_lec4.html#dag",
    "href": "bigdata_lec4.html#dag",
    "title": "Big Data: Dask intro",
    "section": "DAG",
    "text": "DAG"
  },
  {
    "objectID": "bigdata_lec4.html#dask-scheduling-2",
    "href": "bigdata_lec4.html#dask-scheduling-2",
    "title": "Big Data: Dask intro",
    "section": "Dask Scheduling",
    "text": "Dask Scheduling"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-8",
    "href": "bigdata_lec4.html#scheduling-8",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nSingle-thread scheduler\n\n\nimport dask\ndask.config.set(scheduler='synchronous')  # overwrite default with single-threaded scheduler\n\n\n\n\n\n\nNotes\n\n\n\nUseful for debugging or profiling\nNo parallelism at all"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-9",
    "href": "bigdata_lec4.html#scheduling-9",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nThread scheduler\n\n\nimport dask\ndask.config.set(scheduler='threads')  # overwrite default with threaded scheduler \n\n\n\n\n\n\nNotes\n\n\n\nSmall overhead of 50 microseconds per task\nOnly provides parallelism when executing non-Python code (because of GIL)"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-10",
    "href": "bigdata_lec4.html#scheduling-10",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nProcess scheduler\n\n\nimport dask\ndask.config.set(scheduler='processes')  # overwrite default with multiprocessing scheduler\n\n\n\n\n\n\nNotes\n\n\n\nPerformance penalties when inter-process communication is heavy\nCan provide parallelism when executing Python code"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-11",
    "href": "bigdata_lec4.html#scheduling-11",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nDistributed (local) scheduler\n\n\nfrom dask.distributed import Client\nclient = Client()\n# or\nclient = Client(processes=False)\n\n\n\n\n\n\nNotes\n\n\n\nCan be more efficient than the multiprocessing scheduler on workloads that require multiple processes\nDiagnostic dashboard and async API"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-12",
    "href": "bigdata_lec4.html#scheduling-12",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling\n\n\n\nDistributed (cluster) scheduler\n\n\n# You can swap out LocalCluster for other cluster types\n\nfrom dask.distributed import LocalCluster\nfrom dask_kubernetes import KubeCluster\n\n# cluster = LocalCluster()\ncluster = KubeCluster()  # example, you can swap out for Kubernetes\n\nclient = cluster.get_client()\n\n\n\n\n\n\nNotes\n\n\n\nCan be setup either locally, or e.g. on a pre-existing Kubernetes cluster\nDifferent cluster backends easy to swap"
  },
  {
    "objectID": "bigdata_lec4.html#scheduling-13",
    "href": "bigdata_lec4.html#scheduling-13",
    "title": "Big Data: Dask intro",
    "section": "Scheduling",
    "text": "Scheduling"
  },
  {
    "objectID": "bigdata_lec4.html#issues",
    "href": "bigdata_lec4.html#issues",
    "title": "Big Data: Dask intro",
    "section": "Issues",
    "text": "Issues\n\n\n\nIssues\n\n\n\nResource starvation\nWorker failures\nData loss"
  },
  {
    "objectID": "bigdata_lec5.html#dask",
    "href": "bigdata_lec5.html#dask",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nParallel\n\n\nWe refer to algorithms that use multiple cores simultaneously as parallel.\n\n\n\n\n\n\nOut-of-core\n\n\nWe refer to systems that efficiently use disk as extensions of memory as out-of-core."
  },
  {
    "objectID": "bigdata_lec5.html#dask-1",
    "href": "bigdata_lec5.html#dask-1",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nHow to execute parallel code?\n\n\n\nrepresent the structure of our program explicitly as data within the program itself\nencode task schedules programmatically within a framework"
  },
  {
    "objectID": "bigdata_lec5.html#dask-2",
    "href": "bigdata_lec5.html#dask-2",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask Graph definition\n\n\n\nA Python dictionary mapping keys to tasks or values.\nA key is any Python hashable\na value is any Python object that is not a task\na task is a Python tuple with a callable first element."
  },
  {
    "objectID": "bigdata_lec5.html#dask-3",
    "href": "bigdata_lec5.html#dask-3",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\ndef inc(i):\n  return i + 1\n\ndef add(a, b):\n  return a + b\n\nx = 1\ny = inc(x)\nz = add(y, 10)"
  },
  {
    "objectID": "bigdata_lec5.html#dask-4",
    "href": "bigdata_lec5.html#dask-4",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask"
  },
  {
    "objectID": "bigdata_lec5.html#dask-5",
    "href": "bigdata_lec5.html#dask-5",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDictionary representation\n\n\nd = {'x': 1,\n     'y': (inc, 'x'),\n     'z': (add, 'y', 10)}"
  },
  {
    "objectID": "bigdata_lec5.html#dask-6",
    "href": "bigdata_lec5.html#dask-6",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask computation\n\n\nDask represents a computation as a directed acyclic graph of tasks with data dependencies.\nIt can be said that Dask is a specification to encode such a graph using ordinary Python data structures, namely dicts, tuples, functions, and arbitrary Python values."
  },
  {
    "objectID": "bigdata_lec5.html#dask-7",
    "href": "bigdata_lec5.html#dask-7",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n{'x': 1,\n 'y': 2,\n 'z': (add, 'x', 'y'),\n 'w': (sum, ['x', 'y', 'z'])}\n\n\n\nExamples\n\n\n\nkey: 'x', ('x', 2, 3)\ntask: (add, 'x', 'y')\ntask argument: 'x', 1, (inc, 'x'), [1, 'x', (inc, 'x')]"
  },
  {
    "objectID": "bigdata_lec5.html#dask-8",
    "href": "bigdata_lec5.html#dask-8",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nValid tasks in a Dask graph\n\n\n(add, 1, 2)\n(add, 'x' , 2)\n(add, (inc, 'x'), 2)\n(sum, [1, 2])\n(sum, ['x', (inc, 'x')])\n(np.dot, np.array([...]), np.array([...]))"
  },
  {
    "objectID": "bigdata_lec5.html#dask-9",
    "href": "bigdata_lec5.html#dask-9",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask Array\n\n\nThe dask.array submodule uses dask graphs to create a NumPy-like library that uses all of your cores and operates on datasets that do not fit in memory.\nIt does this by building up a dask graph of blocked array algorithms.\nDask array functions produce Array objects that hold on to Dask graphs. These Dask graphs use several NumPy functions to achieve the full result."
  },
  {
    "objectID": "bigdata_lec5.html#dask-10",
    "href": "bigdata_lec5.html#dask-10",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nBlocked Array Algorithms\n\n\nBlocked algorithms compute a large result like\n\n“take the sum of these trillion numbers”\n\nwith many small computations like\n\n“break up the trillion numbers into one million chunks of size one million”,\n“sum each chunk”,\n“then sum all of the intermediate sums.”\n\nThrough tricks like this we can evaluate one large problem by solving very many small problems.\nBlocked algorithm organizes a computation so that it works on contiguous chunks of data."
  },
  {
    "objectID": "bigdata_lec5.html#dask-11",
    "href": "bigdata_lec5.html#dask-11",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nBlocked Array Algorithms"
  },
  {
    "objectID": "bigdata_lec5.html#dask-12",
    "href": "bigdata_lec5.html#dask-12",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nUnblocked\n\n\nfor i in range(N):\n for k in range(N):\n   r = X[i,k]\n   for j in range(N):\n     Z[i,j] += r*Y[k,j]\n\n\n\n\n\n\nBlocked\n\n\nfor kk in range(N/B):\n for jj in range(N/B): \n   for i in range(N):\n     for k in range(kk, min(kk+B-1, N)):\n       r = X[i,k]\n       for j in range(jj, min(jj+B-1,N)):\n         Z[i,j] += r*Y[k,j]"
  },
  {
    "objectID": "bigdata_lec5.html#dask-13",
    "href": "bigdata_lec5.html#dask-13",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\n\n\n\nNote\n\n\nIt is well known that the memory hierarchy can be better utilized if scientific algorithms are blocked.\nBlocking is also known as tiling.\nIn matrix multiplication example: instead of operating on individual matrix entries, the calculation is performed on submatrices.\nB is a blocking factor."
  },
  {
    "objectID": "bigdata_lec5.html#dask-14",
    "href": "bigdata_lec5.html#dask-14",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nBlocking features\n\n\n\nBlocking is a general optimization technique for increasing the effectiveness of a memory hierarchy.\nBy reusing data in the faster level of the hierarchy, it cuts down the average access latency.\nIt also reduces the number of references made to slower levels of the hierarchy.\nBlocking is superior to optimization such as prefetching, which hides the latency but does not reduce the memory bandwidth requirement.\nThis reduction is especially im portant for multiprocessors since memory bandwidth is often the bottleneck of the system."
  },
  {
    "objectID": "bigdata_lec5.html#dask-15",
    "href": "bigdata_lec5.html#dask-15",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\nimport dask.array as da\nx = da.arange(15, chunks=(5,))"
  },
  {
    "objectID": "bigdata_lec5.html#dask-16",
    "href": "bigdata_lec5.html#dask-16",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nMetadata\n\n\nx # Array object metadata\ndask.array&lt;x-1, shape=(15,), chunks=((5, 5, 5)), dtype=int64&gt;\n\n\n\n\n\n\nDask Graph\n\n\nx.dask # Every dask array holds a dask graph\n{('x' , 0): (np.arange, 0, 5),\n ('x', 1): (np.arange, 5, 10),\n ('x' , 2): (np.arange, 10, 15)}"
  },
  {
    "objectID": "bigdata_lec5.html#dask-17",
    "href": "bigdata_lec5.html#dask-17",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nMore complex graph\n\n\nz = (x + 100).sum()\nz.dask\n{('x', 0): (np.arange, 0, 5),\n('x', 1): (np.arange, 5, 10),\n('x', 2): (np.arange, 10, 15),\n('y', 0): (add, ('x', 0), 100),\n('y', 1): (add, ('x', 1), 100),\n('y', 2): (add, ('x', 2), 100),\n('z', 0): (np.sum, ('y', 0)),\n('z', 1): (np.sum, ('y', 1)),\n('z', 2): (np.sum, ('y', 2)),\n('z',): (sum, [('z', 0), ('z', 1), ('z', 2)])}\n\n\n\n\n\n\nExecute the Graph\n\n\nz.compute()\n1605"
  },
  {
    "objectID": "bigdata_lec5.html#dask-18",
    "href": "bigdata_lec5.html#dask-18",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\ndask.array.Array objects\n\n\nx and z are both dask.array.Array objects containing:\n\nDask graph .dask\narray shape and chunk shape .chunks\na name identifying which keys in the graph correspond to the result, .name\na dtype"
  },
  {
    "objectID": "bigdata_lec5.html#dask-19",
    "href": "bigdata_lec5.html#dask-19",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nChunks\n\n\nA normal NumPy array knows its shape, a dask array must know its shape and the shape of all of the internal NumPy blocks that make up the larger array.\nThese shapes can be concisely described by a tuple of tuples of integers, where each internal tuple corresponds to the lengths along a single dimension.\nIn the example above we have a 20 by 24 array cut into uniform blocks of size 5 by 8. The chunks attribute describing this array is the following:\nchunks = ((5, 5, 5, 5), (8, 8, 8))"
  },
  {
    "objectID": "bigdata_lec5.html#dask-20",
    "href": "bigdata_lec5.html#dask-20",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nChunks need not be uniform!\n\n\nx[::2].chunks\n((3, 2, 3, 2), (8, 8, 8))\nx[::2].T.chunks\n((8, 8, 8), (3, 2, 3, 2))"
  },
  {
    "objectID": "bigdata_lec5.html#dask-21",
    "href": "bigdata_lec5.html#dask-21",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask Array operations\n\n\n\narithmetic and scalar math: +, *, exp, log\nreductions along axes: sum(), mean(), std(), sum(axis=0)\ntensor contractions / dot products / matrix multiplication: tensordot\naxis reordering / transposition: transpose\nslicing: x[:100, 500:100:-2]\nutility functions: bincount, where"
  },
  {
    "objectID": "bigdata_lec5.html#dask-22",
    "href": "bigdata_lec5.html#dask-22",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nAhead-of-time shape limitations\n\n\nx[x &gt; 0]"
  },
  {
    "objectID": "bigdata_lec5.html#dask-23",
    "href": "bigdata_lec5.html#dask-23",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\nGraph creation and graph scheduling are separate problems!\nCurrent Dask scheduler is dynamic.\n\n\n\nCurrent Dask scheduler logic\n\n\n\nA worker reports that it has completed a task and that it is ready for another.\nWe update runtime state to record the finished task,\nmark which new tasks can be run, which data can be released, etc.\nWe then choose a task to give to this worker from among the set of ready-to-run tasks. This small choice governs the macroscale performance of the scheduler."
  },
  {
    "objectID": "bigdata_lec5.html#dask-24",
    "href": "bigdata_lec5.html#dask-24",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nOut-of-core computation - which task to choose?\n\n\n\nlast in, first out\nselect tasks whose data dependencies were most recently made available.\nthis causes a behavior where long chains of related tasks trigger each other\nit forces the scheduler to finish related tasks before starting new ones.\nimplementation: a simple stack, which can operate in constant time."
  },
  {
    "objectID": "bigdata_lec5.html#dask-25",
    "href": "bigdata_lec5.html#dask-25",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask"
  },
  {
    "objectID": "bigdata_lec5.html#dask-26",
    "href": "bigdata_lec5.html#dask-26",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask"
  },
  {
    "objectID": "bigdata_lec5.html#taskvine",
    "href": "bigdata_lec5.html#taskvine",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nDescription\n\n\nTaskVine is a framework for building large scale data intensive dynamic workflows for:\n\nhigh performance computing (HPC) clusters\nGPU clusters\ncloud service providers\nand other distributed computing systems.\n\nTaskVine is our third-generation workflow system, built on our twenty years of experience creating scalable applications in fields such as\n\nhigh energy physics\nbioinformatics\nmolecular dynamics\nand machine learning."
  },
  {
    "objectID": "bigdata_lec5.html#taskvine-1",
    "href": "bigdata_lec5.html#taskvine-1",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nWorkflow\n\n\nA workflow is a collection of programs and files that are organized in a graph structure, allowing parts of the workflow to run in a parallel, reproducible way."
  },
  {
    "objectID": "bigdata_lec5.html#taskvine-2",
    "href": "bigdata_lec5.html#taskvine-2",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nSteps\n\n\n\nA TaskVine workflow requires:\n\na manager\nand a large number of worker processes.\n\nThe application generates a large number of small tasks, which are distributed to workers.\nAs tasks access external data sources and produce their own outputs, more and more data is pulled into local storage on cluster nodes.\nThis data is used to accelerate future tasks and avoid re-computing existing results. The application gradually grows “like a vine” through the cluster."
  },
  {
    "objectID": "bigdata_lec5.html#taskvine-3",
    "href": "bigdata_lec5.html#taskvine-3",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nArchitecture"
  },
  {
    "objectID": "bigdata_lec5.html#taskvine-4",
    "href": "bigdata_lec5.html#taskvine-4",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nFeatures and error handling\n\n\n\nWhile an application is running, workers may be added or removed as computing resources become available. (elasticity)\nNewly added workers will gradually accumulate data within the cluster.\nRemoved (or failed) workers are handled gracefully, and tasks will be retried elsewhere as needed.\nIf a worker failure results in the loss of files, tasks will be re-executed as necessary to re-create them."
  },
  {
    "objectID": "bigdata_lec5.html#taskvine-5",
    "href": "bigdata_lec5.html#taskvine-5",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nCoding it\n\n\n\nIndividual tasks can be simple Python functions, complex Unix applications, or serverless function invocations.\nThe key idea is that you declare file objects, and then declare tasks that consume them and produce new file objects. \n\nf = m.declare_url(\"https://www.gutenberg.org/cache/epub/2600/pg2600.txt\")\ng = m.declare_file(\"myoutput.txt\")\n\nt = Task(\"grep needle warandpeace.txt &gt; output.txt\")\nt.add_input(f, \"warandpeace.txt\")\nt.add_output(g, \"output.txt\")"
  },
  {
    "objectID": "bigdata_lec5.html#taskvine-6",
    "href": "bigdata_lec5.html#taskvine-6",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nTask options\n\n\n\nEach task can be labelled with the resources (CPU cores, GPU devices, memory, disk space) that it needs to execute.\nIf you don’t know the resources needed, you can enable a resource monitor to automatically track, report, and allocate what each task uses.\nThis allows each worker to pack the appropriate number of tasks. For example, a worker running on a 64-core machine could run 32 dual-core tasks, 16 four-core tasks, or any other combination that adds up to 64 cores."
  },
  {
    "objectID": "bigdata_lec5.html#taskvine-7",
    "href": "bigdata_lec5.html#taskvine-7",
    "title": "Big Data: Dask intro part2",
    "section": "Taskvine",
    "text": "Taskvine\n\n\n\nAs Dask Scheduler\n\n\nTaskVine manager can be used as Dask scheduler:\ntry:\n    import dask\n    import awkward as ak\n    import dask_awkward as dak\n    import numpy as np\nexcept ImportError:\n    print(\"You need dask, awkward, and numpy installed\")\n    print(\"(e.g. conda install -c conda-forge dask dask-awkward numpy) to run this example.\")\n\nbehavior: dict = {}\n\n@ak.mixin_class(behavior)\nclass Point:\n    def distance(self, other):\n    return np.sqrt((self.x - other.x) ** 2 + (self.y - other.y) ** 2)\n\n\nif __name__ == \"__main__\":\n    # data arrays\n    points1 = ak.Array([\n    [{\"x\": 1.0, \"y\": 1.1}, {\"x\": 2.0, \"y\": 2.2}, {\"x\": 3, \"y\": 3.3}],\n    [],\n    [{\"x\": 4.0, \"y\": 4.4}, {\"x\": 5.0, \"y\": 5.5}],\n    [{\"x\": 6.0, \"y\": 6.6}],\n    [{\"x\": 7.0, \"y\": 7.7}, {\"x\": 8.0, \"y\": 8.8}, {\"x\": 9, \"y\": 9.9}],\n    ])\n\n    points2 = ak.Array([\n    [{\"x\": 0.9, \"y\": 1.0}, {\"x\": 2.0, \"y\": 2.2}, {\"x\": 2.9, \"y\": 3.0}],\n    [],\n    [{\"x\": 3.9, \"y\": 4.0}, {\"x\": 5.0, \"y\": 5.5}],\n    [{\"x\": 5.9, \"y\": 6.0}],\n    [{\"x\": 6.9, \"y\": 7.0}, {\"x\": 8.0, \"y\": 8.8}, {\"x\": 8.9, \"y\": 9.0}],\n    ])\n    array1 = dak.from_awkward(points1, npartitions=3)\n    array2 = dak.from_awkward(points2, npartitions=3)\n\n    array1 = dak.with_name(array1, name=\"Point\", behavior=behavior)\n    array2 = dak.with_name(array2, name=\"Point\", behavior=behavior)\n\n    distance = array1.distance(array2)\n\n    m = vine.DaskVine(port=9123, ssl=True)\n    m.set_name(\"test_manager\")\n    print(f\"Listening for workers at port: {m.port}\")\n\n    f = vine.Factory(manager=m)\n    f.cores = 4\n    f.max_workers = 1\n    f.min_workers = 1\n    with f:\n        with dask.config.set(scheduler=m.get):\n            result = distance.compute(resources={\"cores\": 1}, resources_mode=\"max\", lazy_transfers=True)\n            print(f\"distance = {result}\")\n        print(\"Terminating workers...\", end=\"\")\n    print(\"done!\")"
  },
  {
    "objectID": "bigdata_lec5.html#dask-27",
    "href": "bigdata_lec5.html#dask-27",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nCollections\n\n\n\ndask.array = numpy+ threading\ndask.bag = toolz+ multiprocessing\ndask.dataframe = pandas+ threading"
  },
  {
    "objectID": "bigdata_lec5.html#dask-28",
    "href": "bigdata_lec5.html#dask-28",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask Bag - Definition\n\n\nA bag is an unordered collection with repeats.\nIt is like a Python list but does not guarantee the order of elements.\nThe dask.bag API contains functions like map and filter and generally follows the PyToolz API."
  },
  {
    "objectID": "bigdata_lec5.html#dask-29",
    "href": "bigdata_lec5.html#dask-29",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n&gt;&gt;&gt; import dask.bag as db\n&gt;&gt;&gt; import json\n&gt;&gt;&gt; b = db.from_filenames('2014-*.json.gz').map(json.loads)\n&gt;&gt;&gt; alices = b.filter(lambda d: d['name'] == 'Alice')\n&gt;&gt;&gt; alices.take(3)\n({'name': 'Alice', 'city': 'LA', 'balance': 100},\n{'name': 'Alice', 'city': 'LA', 'balance': 200},\n{'name': 'Alice', 'city': 'NYC', 'balance': 300},)\n\n&gt;&gt;&gt; dict(alices.pluck('city').frequencies())\n{'LA': 10000, 'NYC': 20000, ...}"
  },
  {
    "objectID": "bigdata_lec5.html#dask-30",
    "href": "bigdata_lec5.html#dask-30",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nS3 example\n\n\n&gt;&gt;&gt; import dask.bag as db\n&gt;&gt;&gt; b = db.from_s3('githubarchive-data', '2015-01-01-*.json.gz')\n          .map(json.loads)\n          .map(lambda d: d['type'] == 'PushEvent')\n          .count()"
  },
  {
    "objectID": "bigdata_lec5.html#dask-31",
    "href": "bigdata_lec5.html#dask-31",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask"
  },
  {
    "objectID": "bigdata_lec5.html#dask-32",
    "href": "bigdata_lec5.html#dask-32",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask DataFrame - Definition\n\n\nThe dask.dataframe module implements a large dataframe out of many Pandas DataFrames.\nIt uses a threaded scheduler."
  },
  {
    "objectID": "bigdata_lec5.html#dask-33",
    "href": "bigdata_lec5.html#dask-33",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nPartitioned datasets\n\n\nThe dask dataframe can compute efficiently on partitioned datasets where the different blocks are well separated along an index.\nFor example in time series data we may know that all of January is in one block while all of February is in another.\nJoin, groupby, and range queries along this index are significantly faster when working on partitioned datasets."
  },
  {
    "objectID": "bigdata_lec5.html#dask-34",
    "href": "bigdata_lec5.html#dask-34",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nDask DataFrame join"
  },
  {
    "objectID": "bigdata_lec5.html#dask-35",
    "href": "bigdata_lec5.html#dask-35",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nOut-of-core parallel SVD example\n\n\n&gt;&gt;&gt; import dask.array as da\n&gt;&gt;&gt; x = da.ones((5000, 1000), chunks=(1000, 1000))\n&gt;&gt;&gt; u, s, v = da.svd(x)\nOut-of-core parallel non-negative matrix factorizations on top of dask.array."
  },
  {
    "objectID": "bigdata_lec5.html#dask-36",
    "href": "bigdata_lec5.html#dask-36",
    "title": "Big Data: Dask intro part2",
    "section": "Dask",
    "text": "Dask\n\n\n\nOut-of-core parallel SVD"
  },
  {
    "objectID": "bigdata_lec5.html#usage-1",
    "href": "bigdata_lec5.html#usage-1",
    "title": "Big Data: Dask intro part2",
    "section": "Usage",
    "text": "Usage\n\nscida.io - astrophysical simulations"
  },
  {
    "objectID": "bigdata_lec5.html#usage-2",
    "href": "bigdata_lec5.html#usage-2",
    "title": "Big Data: Dask intro part2",
    "section": "Usage",
    "text": "Usage\n\nPangeo - open, reproducible, scalable geoscience. A global slice of Sea Water Temperature"
  },
  {
    "objectID": "bigdata_lec5.html#usage-3",
    "href": "bigdata_lec5.html#usage-3",
    "title": "Big Data: Dask intro part2",
    "section": "Usage",
    "text": "Usage"
  },
  {
    "objectID": "bigdata_lec5.html#comparison-with-spark",
    "href": "bigdata_lec5.html#comparison-with-spark",
    "title": "Big Data: Dask intro part2",
    "section": "Comparison with Spark",
    "text": "Comparison with Spark\n\n\n\nSetup\n\n\n\nBigBrain20, a 3-D image of the human brain, total data size of 606 GiB.\ndataset provided by the Consortium for Reliability and Reproducibility, entire dataset is 379.83 GiB, used all 3,491 anatomical images, representing 26.67 GiB overall."
  },
  {
    "objectID": "bigdata_lec5.html#comparison-with-spark-1",
    "href": "bigdata_lec5.html#comparison-with-spark-1",
    "title": "Big Data: Dask intro part2",
    "section": "Comparison with Spark",
    "text": "Comparison with Spark"
  },
  {
    "objectID": "bigdata_lec5.html#comparison-with-spark-2",
    "href": "bigdata_lec5.html#comparison-with-spark-2",
    "title": "Big Data: Dask intro part2",
    "section": "Comparison with Spark",
    "text": "Comparison with Spark"
  },
  {
    "objectID": "bigdata_lec5.html#comparison-with-spark-3",
    "href": "bigdata_lec5.html#comparison-with-spark-3",
    "title": "Big Data: Dask intro part2",
    "section": "Comparison with Spark",
    "text": "Comparison with Spark"
  },
  {
    "objectID": "applied.html",
    "href": "applied.html",
    "title": "Applied Data Analytics",
    "section": "",
    "text": "Course will cover the following:\n\ndata fetching/preprocessing\nspecialized data types:\n\ngeospatial\nfinancial\nsupply chain management\nmore\n\nvisualization/presentation (e.g. Gradio)\nLLM toolkits (LangChain/LangFlow)\nML Pipelines (MLFlow/KubeFlow/AirFlow/Prefect/Metaflow)\nMLOps\nAmazon Web Services\nDocker"
  },
  {
    "objectID": "applied.html#overview",
    "href": "applied.html#overview",
    "title": "Applied Data Analytics",
    "section": "",
    "text": "Course will cover the following:\n\ndata fetching/preprocessing\nspecialized data types:\n\ngeospatial\nfinancial\nsupply chain management\nmore\n\nvisualization/presentation (e.g. Gradio)\nLLM toolkits (LangChain/LangFlow)\nML Pipelines (MLFlow/KubeFlow/AirFlow/Prefect/Metaflow)\nMLOps\nAmazon Web Services\nDocker"
  },
  {
    "objectID": "applied.html#lectures",
    "href": "applied.html#lectures",
    "title": "Applied Data Analytics",
    "section": "Lectures",
    "text": "Lectures\nIntro to Docker"
  },
  {
    "objectID": "applied.html#labs",
    "href": "applied.html#labs",
    "title": "Applied Data Analytics",
    "section": "Labs",
    "text": "Labs\nLab 1\nLab 2\nLab 3"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management",
    "href": "bigdata_lec3.html#memory-management",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\n\n\nManual\n\n\n\nC/C++\nPascal\nForth\nFortran\nZig\n\n\n\n\n\n\n\n\nAutomatic\n\n\n\nLisp\nJava\nPython\nGo\nJulia"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-1",
    "href": "bigdata_lec3.html#memory-management-1",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\nCode\n\n\nimport os\nimport gc\nimport psutil\n\nproc = psutil.Process(os.getpid())\ngc.collect()\ninitial_memory = proc.memory_info().rss\n\n## Allocate memory by creating large lists\nfoo = ['abc' for _ in range(10**7)]\nallocated_memory = proc.memory_info().rss\n\n## Deallocate memory\ndel foo\ngc.collect()\nfinal_memory = proc.memory_info().rss"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-2",
    "href": "bigdata_lec3.html#memory-management-2",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\nPrint memory statistics\n\n\nincrease = lambda x2, x1: 100.0 * (x2 - x1) / initial_memory\nprint(\"Allocated Memory Increase: %0.2f%%\" % increase(allocated_memory, initial_memory))\nprint(\"Memory After Deletion: %0.2f%%\" % increase(final_memory, allocated_memory))\n\n&gt;&gt;&gt; Allocated Memory Increase: 23.35%\n&gt;&gt;&gt; Memory After Deletion: -10.78%"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-3",
    "href": "bigdata_lec3.html#memory-management-3",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\n\n\n\n\nPython internals\n\n\n\npools\nblocks\narenas\n\n\n\n\n\n\n\nhttps://docs.python.org/3/c-api/memory.html\nhttps://realpython.com/python-memory-management"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-4",
    "href": "bigdata_lec3.html#memory-management-4",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management"
  },
  {
    "objectID": "bigdata_lec3.html#python-options",
    "href": "bigdata_lec3.html#python-options",
    "title": "Big Data: Speeding up computation",
    "section": "Python Options",
    "text": "Python Options\n\n\n\n\n\n\n\n\n\nLibraries\nLow-level langs\nAlt Python Impls\nJIT\n\n\n\n\nNumPy,  SciPy\nC, Rust, Cython, PyO3\nPyPy, Jython\nNumba, PyPy\n\n\n\n\n\nOptions above are not mutually exclusive!"
  },
  {
    "objectID": "bigdata_lec3.html#interpreters",
    "href": "bigdata_lec3.html#interpreters",
    "title": "Big Data: Speeding up computation",
    "section": "Interpreters",
    "text": "Interpreters\n\n\n\nWikipedia definition\n\n\nAn interpreter is a computer program that directly executes instructions written in a programming or scripting language, without requiring them previously to have been compiled into a machine language program.\n\n\n\n\n\n\nExamples\n\n\n\nPython\nRuby\nLua\nJavascript"
  },
  {
    "objectID": "bigdata_lec3.html#cpython",
    "href": "bigdata_lec3.html#cpython",
    "title": "Big Data: Speeding up computation",
    "section": "CPython",
    "text": "CPython\n\n\n\n\nFlow\n\n\n\nRead Python code\nConvert Python into bytecode\nExecute bytecode inside a VM\nVM converts bytecode to machine code"
  },
  {
    "objectID": "bigdata_lec3.html#cpython-1",
    "href": "bigdata_lec3.html#cpython-1",
    "title": "Big Data: Speeding up computation",
    "section": "CPython",
    "text": "CPython"
  },
  {
    "objectID": "bigdata_lec3.html#compilers",
    "href": "bigdata_lec3.html#compilers",
    "title": "Big Data: Speeding up computation",
    "section": "Compilers",
    "text": "Compilers\n\n\n\nWikipedia definition\n\n\nSource code is compiled - in this context, translated into machine code for better performance.\n\n\n\n\n\n\nExamples\n\n\n\nC/C++\nGo\nPython (to intermediate VM code)\nJava\nCython"
  },
  {
    "objectID": "bigdata_lec3.html#compilers-1",
    "href": "bigdata_lec3.html#compilers-1",
    "title": "Big Data: Speeding up computation",
    "section": "Compilers",
    "text": "Compilers"
  },
  {
    "objectID": "bigdata_lec3.html#cython",
    "href": "bigdata_lec3.html#cython",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nDefinition\n\n\nCython is an optimising static compiler for the Python programming language.\n\nconverts Python code to C\nsupports static type declarations"
  },
  {
    "objectID": "bigdata_lec3.html#cython-1",
    "href": "bigdata_lec3.html#cython-1",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython"
  },
  {
    "objectID": "bigdata_lec3.html#cython-2",
    "href": "bigdata_lec3.html#cython-2",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython"
  },
  {
    "objectID": "bigdata_lec3.html#cython-3",
    "href": "bigdata_lec3.html#cython-3",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nPython code"
  },
  {
    "objectID": "bigdata_lec3.html#cython-4",
    "href": "bigdata_lec3.html#cython-4",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nAnnotated Python code"
  },
  {
    "objectID": "bigdata_lec3.html#cython-5",
    "href": "bigdata_lec3.html#cython-5",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nCython code"
  },
  {
    "objectID": "bigdata_lec3.html#parallel-cython",
    "href": "bigdata_lec3.html#parallel-cython",
    "title": "Big Data: Speeding up computation",
    "section": "Parallel Cython",
    "text": "Parallel Cython"
  },
  {
    "objectID": "bigdata_lec3.html#parallel-cython-1",
    "href": "bigdata_lec3.html#parallel-cython-1",
    "title": "Big Data: Speeding up computation",
    "section": "Parallel Cython",
    "text": "Parallel Cython"
  },
  {
    "objectID": "bigdata_lec3.html#jit",
    "href": "bigdata_lec3.html#jit",
    "title": "Big Data: Speeding up computation",
    "section": "JIT",
    "text": "JIT\n\n\n\nWikipedia definition\n\n\nA compilation (of computer code) during execution of a program (at run time) rather than before execution.\n\n\n\n\n\n\nFeatures\n\n\n\nwarm-up time: JIT causes a slight to noticeable delay in the initial execution of an application, due to the time taken to load and compile the input code.\nstatistics collection: performed by the system during runtime, shows how the program is actually running in the environment it is in; helps JIT to rearrange and recompile for optimum performance.\nparticularly suited for dynamic programming languages"
  },
  {
    "objectID": "bigdata_lec3.html#jit-1",
    "href": "bigdata_lec3.html#jit-1",
    "title": "Big Data: Speeding up computation",
    "section": "JIT",
    "text": "JIT\n\n\n\nExamples\n\n\n\nHotSpot Java Virtual Machine\nLuaJIT\nNumba\nPyPy"
  },
  {
    "objectID": "bigdata_lec3.html#numba",
    "href": "bigdata_lec3.html#numba",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba"
  },
  {
    "objectID": "bigdata_lec3.html#numba-1",
    "href": "bigdata_lec3.html#numba-1",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nDescription\n\n\n\nNumba translates Python byte-code to machine code immediately before execution to improve the execution speed.\nFor that we add a @jit decorator\nWorks well for numeric operations, NumPy, and loops"
  },
  {
    "objectID": "bigdata_lec3.html#numba-2",
    "href": "bigdata_lec3.html#numba-2",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nSteps\n\n\n\nread the Python bytecode for a decorated function\ncombine it with information about the types of the input arguments to the function\nanalyze and optimize the code\nuse the LLVM compiler library to generate a machine code version of the function, tailored to specific CPU capabilities."
  },
  {
    "objectID": "bigdata_lec3.html#numba-3",
    "href": "bigdata_lec3.html#numba-3",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nWorks great"
  },
  {
    "objectID": "bigdata_lec3.html#numba-4",
    "href": "bigdata_lec3.html#numba-4",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nNope"
  },
  {
    "objectID": "bigdata_lec3.html#numba-5",
    "href": "bigdata_lec3.html#numba-5",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba"
  },
  {
    "objectID": "bigdata_lec3.html#numpy",
    "href": "bigdata_lec3.html#numpy",
    "title": "Big Data: Speeding up computation",
    "section": "Numpy",
    "text": "Numpy\n\n\n\nWhy so fast?\n\n\n\nOptimized C code\nDensely packed arrays\nUses BLAS - Basic Linear Algebra Subroutines."
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-1",
    "href": "bigdata_lec3.html#rustpyo3-example-1",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nDescription\n\n\nWe show an example of a simple algebraic cipher that utilizes PyO3 bindings to speed up encoding/decoding.\n\n\n\n\n\n\nCipher definition\n\n\nThe basic mechanism for encrypting a message using a shared secret key is called a cipher (or encryption scheme)"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-2",
    "href": "bigdata_lec3.html#rustpyo3-example-2",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\n\n\n\nDefinition\n\n\nEncryption and decryption use the same secret key.\n\n\n\n\n\n\nExamples\n\n\n\nAES\nSalsa20\nTwofish\nDES"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-3",
    "href": "bigdata_lec3.html#rustpyo3-example-3",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nTypes\n\n\n\nblock ciphers\nstream ciphers\nhash functions"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-4",
    "href": "bigdata_lec3.html#rustpyo3-example-4",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nOverview\n\n\n\\[\nf: \\mathcal{K}\\times\\mathcal{D} \\rightarrow C\n\\] where\n\n\\(\\mathcal{K}\\) is key space\n\\(\\mathcal{D}\\) is domain (or message space)\n\\(\\mathcal{C}\\) is co-domain (or ciphertext space)"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-5",
    "href": "bigdata_lec3.html#rustpyo3-example-5",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nShannon cipher\n\n\nA Shannon cipher is a pair \\(\\mathcal{E} = (E,D)\\) of functions:\n\nThe function \\(E\\) (the encryption function) takes as input a key \\(k\\) and message \\(m\\) (also called plaintext) and produces as output a ciphertext \\(c):\\)$ c = E(k,m) $$\n\n\\(c\\) is the encryption of \\(m\\) under \\(k\\).\n\nThe function \\(D\\) (the decryption function) takes as input a key \\(k\\) and ciphertext \\(c\\), and produces a message \\(m\\): \\[\nm = D(k,c)\n\\]\n\n\\(m\\) is the decryption of \\(c\\) under \\(k\\)."
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-6",
    "href": "bigdata_lec3.html#rustpyo3-example-6",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nCorrectness property\n\n\nFor all keys \\(k\\) and messages \\(m\\), we have \\[\nD(k, E(k,m)) = m\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-7",
    "href": "bigdata_lec3.html#rustpyo3-example-7",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nParameters\n\n\nNow we describe the cipher.\nFirst, we define cipher parameters:\n\nAn alphabet \\(A\\) with size \\(L \\equiv |A|\\).\nA matrix \\(M\\) with size \\(N \\gg L\\)\n\\(\\sigma_1, \\sigma_2\\) are some permutations \\(N \\rightarrow N\\)\n\\(\\phi\\) is some bit sequence of length \\(P\\): \\(\\phi \\in \\{0,1\\}^P\\)\n\nA triple \\((\\sigma_1, \\sigma_2, \\phi)\\) will be our secret key.\nWe define each symbol \\(z\\) by a corresponding set of diagonals \\(D\\) in the matrix \\(M\\), so that \\(\\forall (x,y) \\in D: x - y = z (\\mod L)\\) (see Figure 1)."
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-8",
    "href": "bigdata_lec3.html#rustpyo3-example-8",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-9",
    "href": "bigdata_lec3.html#rustpyo3-example-9",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nEncoder flow\n\n\nSuppose we receive some text \\(T\\) containing symbols to be encoded.\nFor each \\(t_i \\in T\\), obtain its numeric representation \\(z_i\\). \\[\nz_i \\in [0,L)\n\\] Then we map each \\(z_i\\) to a pair of matrix coordinates \\((x_i, y_i)\\) such that:\n\nFirst, we pick a random \\(x_i \\in [0,N)\\) (e.g., horizontal coordinate in a matrix)\nThen, we randomly pick some \\(y_i \\in [0,N)\\) such that: \\[\nx_i - y_i = z_i (\\mod L)\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-10",
    "href": "bigdata_lec3.html#rustpyo3-example-10",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nEncoder flow\n\n\nHaving thus obtained a sequence \\(\\{(x_i, y_i), \\, i \\in [0, |T|) \\}\\), we now apply permutations \\(\\sigma_k: [0,N) \\rightarrow [0,N), \\, k=1,2\\): \\[\n\\text{ciphertext } (\\xi,\\eta) := (\\sigma_j(x),\\sigma_{j+1}(y))\n\\] where \\[\n\\sigma_j = \\begin{cases}\n\\sigma_1, \\; \\text{if}\\; \\phi_j=0,\\\\\n\\sigma_2\\; \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-11",
    "href": "bigdata_lec3.html#rustpyo3-example-11",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nDecoder flow\n\n\nBelow are steps executing during decoding phase:\n\nReceive encoded ciphertext \\(\\{(\\xi_i, \\eta_i)\\}\\).\nApply inverse permutations \\(\\sigma_j^{-1}, \\sigma_{j+1}^{-1}\\): \\[\n(x_i,y_i) = (\\sigma_j^{-1}(\\xi_i), \\sigma_{j+1}^{-1}(\\eta_i))\n\\]\nFind \\(z_i\\): \\[\nz_i = x_i - y_i \\; (\\mod L)\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-12",
    "href": "bigdata_lec3.html#rustpyo3-example-12",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nImplementation: PyO3\n\n\n\nRust bindings for Python extension modules\n\n\n\n\n\n\n\nUsers\n\n\n\nQiskit https://www.ibm.com/quantum/qiskit\nPython Cryptography package https://github.com/pyca/cryptography\nScallop https://www.scallop-lang.org\nHuggingFace Tokenizers https://github.com/huggingface/tokenizers"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-13",
    "href": "bigdata_lec3.html#rustpyo3-example-13",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nRust wrapper\n\n\n/// A Python module implemented in Rust.\n#[pymodule]\nfn cipher(m: &Bound&lt;'_, PyModule&gt;) -&gt; PyResult&lt;()&gt; {\n    m.add_function(wrap_pyfunction!(encode, m)?)?;\n    m.add_function(wrap_pyfunction!(decode, m)?)?;\n    Ok(())\n}"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-14",
    "href": "bigdata_lec3.html#rustpyo3-example-14",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nEncoding\n\n\n// Generate random permutation of N integers\nlet mut numbers: Vec&lt;usize&gt; = (0..N).collect();\nlet mut rng = thread_rng();\nnumbers.shuffle(&mut rng);\nlet permutation = numbers;\n\n// Generate pairs for each z\nlet mut pairs = Vec::with_capacity(zs.len());\nfor &z in &zs {\n    // Generate random x between 0 and N-1\n    let x = rng.gen_range(0..N);\n\n    // Compute y such that x - y = z (mod L)\n    let y = if x &gt;= z {\n        (x - z) % L\n    } else {\n        ((x + L) - z) % L\n    };\n\n    // Apply permutation to x and y\n    let px = permutation[x];\n    let py = permutation[y];\n    pairs.push((px, py));"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-15",
    "href": "bigdata_lec3.html#rustpyo3-example-15",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nDecoding\n\n\n// Create inverse permutation\nlet mut inverse = vec![0; N];\nfor (i, &p) in permutation.iter().enumerate() {\n    inverse[p] = i;\n}\n\n// Recover z for each pair\nlet mut zs = Vec::with_capacity(pairs.len());\nfor &(px, py) in pairs {\n    // Apply inverse permutation to get x and y\n    let x = inverse[px];\n    let y = inverse[py];\n\n    // Compute z = x - y (mod L)\n    let z = if x &gt;= y {\n        (x - y) % L\n    } else {\n        ((x + L) - y) % L\n    };\n    zs.push(z);\n}"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-16",
    "href": "bigdata_lec3.html#rustpyo3-example-16",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nPython\n\n\nmkdir cipher; cd cipher\npython -m venv venv\n. venv/bin/activate\n!pip install maturin\nmaturin init\nmaturin develop\n\n\n\n\n\n\nUsage\n\n\nimport cipher\nencoded = cipher.encode(\"Whazzuuupppp??!!\")\ndecoded = cipher.decode(encoded)"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-1",
    "href": "bigdata_lec3.html#distributed-computing-1",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nTypes\n\n\n\nCluster computing: collection of similar workstations\nGrid computing: federation of different computer systems\nCloud computing: provide the facilities to dynamically construct an infrastructure and compose what is needed from available services. Not only providing lots of resources."
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-2",
    "href": "bigdata_lec3.html#distributed-computing-2",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nOriginal Beowulf cluster at NASA (1994)"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-3",
    "href": "bigdata_lec3.html#distributed-computing-3",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nBeowulf cluster diagram"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-4",
    "href": "bigdata_lec3.html#distributed-computing-4",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\n\n\nGrid architecture diagram (Foster et al. 2001)\n\n\n\n\n\n\n\n\n\n\nNotation\n\n\n\n\nfabric: interfaces to local resources at a specific site\nconnectivity: communication protocols for supporting grid transactions that span the usage of multiple resources\nresource: responsible for managing a single resource\ncollective: handling access to multiple resources and typically consists of services for resource discovery, allocation and scheduling of tasks onto multiple resources, data replication, and so on\napplication: applications that operate within a virtual organization"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-5",
    "href": "bigdata_lec3.html#distributed-computing-5",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nCloud architecture"
  },
  {
    "objectID": "bigdata_lab2.html",
    "href": "bigdata_lab2.html",
    "title": "Big Data Analytics: Lab 2",
    "section": "",
    "text": "First, install Cython and Numba:\nuv add cython\nuv add numba\nThen we can load Cython extension in Jupyter:\n\n%load_ext Cython\n\nDefine a function that generates a list of first \\(n\\) primes:\n\n\n\n\n\n\nImportantPure Python\n\n\n\n\ndef primes1(nb_primes):\n    p = [0] * 1000\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nImportantPure Python results\n\n\n\nWe can measure its execution time:\n\n%timeit primes1(100)\n\n222 μs ± 4.37 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n\nNow, define a function using Cython type annotations. Note that we use %%cython magic before function definition:\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\n\n%%cython\ndef primes2(nb_primes: cython.int):\n    i: cython.int\n    p: cython.int[1000] = [0] * 1000\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p: cython.int = 0  # The current number of elements in p.\n    n: cython.int = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nNotePython with Cython type hints: results\n\n\n\nWe measure its execution time:\n\n%timeit primes2(100)\n\n85 μs ± 2.04 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nDifferent syntax:\n\n\n\n\n\n\nTipCython syntax\n\n\n\n\n%%cython\ndef primes3(int nb_primes):\n    cdef int n, i, len_p\n    cdef int[1000] p\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nTipCython syntax results\n\n\n\nWe measure its execution time:\n\n%timeit primes3(100)\n\n4.54 μs ± 195 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPassing --annotate flag will produce more relevant info."
  },
  {
    "objectID": "bigdata_lab2.html#cython",
    "href": "bigdata_lab2.html#cython",
    "title": "Big Data Analytics: Lab 2",
    "section": "",
    "text": "First, install Cython and Numba:\nuv add cython\nuv add numba\nThen we can load Cython extension in Jupyter:\n\n%load_ext Cython\n\nDefine a function that generates a list of first \\(n\\) primes:\n\n\n\n\n\n\nImportantPure Python\n\n\n\n\ndef primes1(nb_primes):\n    p = [0] * 1000\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nImportantPure Python results\n\n\n\nWe can measure its execution time:\n\n%timeit primes1(100)\n\n222 μs ± 4.37 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n\nNow, define a function using Cython type annotations. Note that we use %%cython magic before function definition:\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\n\n%%cython\ndef primes2(nb_primes: cython.int):\n    i: cython.int\n    p: cython.int[1000] = [0] * 1000\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p: cython.int = 0  # The current number of elements in p.\n    n: cython.int = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nNotePython with Cython type hints: results\n\n\n\nWe measure its execution time:\n\n%timeit primes2(100)\n\n85 μs ± 2.04 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nDifferent syntax:\n\n\n\n\n\n\nTipCython syntax\n\n\n\n\n%%cython\ndef primes3(int nb_primes):\n    cdef int n, i, len_p\n    cdef int[1000] p\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nTipCython syntax results\n\n\n\nWe measure its execution time:\n\n%timeit primes3(100)\n\n4.54 μs ± 195 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPassing --annotate flag will produce more relevant info."
  },
  {
    "objectID": "bigdata_lab2.html#using-cython-in-pandas",
    "href": "bigdata_lab2.html#using-cython-in-pandas",
    "title": "Big Data Analytics: Lab 2",
    "section": "Using Cython in Pandas",
    "text": "Using Cython in Pandas\nDefine a random DataFrame:\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {\n        \"a\": np.random.randn(1000),\n        \"b\": np.random.randn(1000),\n        \"N\": np.random.randint(100, 1000, (1000)),\n        \"x\": \"x\",\n    }\n)\n\nDefine functions that will be applied to the DataFrame:\n\n\n\n\n\n\nImportantPure Python\n\n\n\n\ndef f(x):\n    return x * (x - 1)\n\n\ndef integrate_f(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f(a + i * dx)\n    return s * dx\n\n\n\n\n\n\n\n\n\nImportantPure Python results\n\n\n\nApply functions to DataFrame:\n\n%timeit df.apply(lambda x: integrate_f(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n\n39 ms ± 539 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nNow let’s use Cython-annotated functions:\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\n\n%%cython\ndef f_typed2(x: cython.double) -&gt; cython.double:\n    return x * (x - 1)\n\ndef integrate_f_typed2(a: cython.double, b: cython.double, N: cython.int) -&gt; cython.double:\n    i: cython.int\n    s: cython.double\n    dx: cython.double\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f_typed2(a + i * dx)\n    return s * dx\n\n\n\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\nApply annotated functions to DataFrame:\n\n%timeit df.apply(lambda x: integrate_f_typed2(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n\n29.9 ms ± 223 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nWith a different syntax:\n\n\n\n\n\n\nTipCython syntax\n\n\n\n\n%%cython\ncdef double f_typed(double x) except? -2:\n   return x * (x - 1)\ncpdef double integrate_f_typed(double a, double b, int N):\n   cdef int i\n   cdef double s, dx\n   s = 0\n   dx = (b - a) / N\n   for i in range(N):\n       s += f_typed(a + i * dx)\n   return s * dx\n\n\n\n\n\n\n\n\n\nTipCython syntax results\n\n\n\n\n%timeit df.apply(lambda x: integrate_f_typed(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n\n3.74 ms ± 101 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n\n\n\n\n\nNotePandas\n\n\n\nRead more about type annotations for Pandas: https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html."
  },
  {
    "objectID": "bigdata_lab2.html#numba",
    "href": "bigdata_lab2.html#numba",
    "title": "Big Data Analytics: Lab 2",
    "section": "Numba",
    "text": "Numba\nEither add engine=\"numba\" parameter to Pandas functions, or add a @jit annotation to Python funcs.\n\n\n\n\n\n\nWarningNumba-annotated code\n\n\n\n\nimport numba\n\n\n@numba.jit\ndef f_numba(x):\n    return x * (x - 1)\n\n\n@numba.jit\ndef integrate_f_numba(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f_numba(a + i * dx)\n    return s * dx\n\n\n@numba.jit\ndef apply_integrate_f_numba(col_a, col_b, col_N):\n    n = len(col_N)\n    result = np.empty(n, dtype=\"float64\")\n    assert len(col_a) == len(col_b) == n\n    for i in range(n):\n        result[i] = integrate_f_numba(col_a[i], col_b[i], col_N[i])\n    return result\n\n\ndef compute_numba(df):\n    result = apply_integrate_f_numba(\n        df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy()\n    )\n    return pd.Series(result, index=df.index, name=\"result\")\n\n\n\n\n\n\n\n\n\nWarningNumba Results\n\n\n\n\n%timeit compute_numba(df)\n\n390 μs ± 52 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "bigdata_lab2.html#exercises",
    "href": "bigdata_lab2.html#exercises",
    "title": "Big Data Analytics: Lab 2",
    "section": "Exercises",
    "text": "Exercises\n\nApply Cython/Numba optimizations to some computations on your dataframe from Lab 1.\nGo through the tutorial on Black-Scholes option pricing (https://louis-finegan.github.io/2024/10/10/Black-Scholes.html). Modify the code so that:\n\n\nit works for some different company\nhas 2 versions: using Cython and Numba\nmeasure results"
  },
  {
    "objectID": "bigdata_lab2.html#references",
    "href": "bigdata_lab2.html#references",
    "title": "Big Data Analytics: Lab 2",
    "section": "References",
    "text": "References\n\nCython tutorial\nCython build instructions\nCython language\nPandas optimization"
  },
  {
    "objectID": "bigdata_lec1.html#what-is-big-data",
    "href": "bigdata_lec1.html#what-is-big-data",
    "title": "Big Data: Intro",
    "section": "What is Big Data?",
    "text": "What is Big Data?\n\n\n\nDefinition\n\n\nBig Data is a set of technologies designed to store, manage and analyze data that is:\n\ntoo large to fit on a single machine\nwhile accommodating for the issue of growing discrepancy between capacity, throughput and latency."
  },
  {
    "objectID": "bigdata_lec1.html#prefixes",
    "href": "bigdata_lec1.html#prefixes",
    "title": "Big Data: Intro",
    "section": "Prefixes",
    "text": "Prefixes\n\n\n\nPrefixes\n\n\n\nkilo (k) 1,000 (3 zeros)\nMega (M) 1,000,000 (6 zeros)\nGiga (G) 1,000,000,000 (9 zeros)\nTera (T) 1,000,000,000,000 (12 zeros)\nPeta (P) 1,000,000,000,000,000 (15 zeros)\nExa (E) 1,000,000,000,000,000,000 (18 zeros)\nZetta (Z) 1,000,000,000,000,000,000,000 (21 zeros)\nYotta (Y) 1,000,000,000,000,000,000,000,000 (24 zeros)\nRonna (R) 1,000,000,000,000,000,000,000,000,000 (27 zeros)\nQuetta (Q) 1,000,000,000,000,000,000,000,000,000,000 (30 zeros)"
  },
  {
    "objectID": "bigdata_lec1.html#total-estimate",
    "href": "bigdata_lec1.html#total-estimate",
    "title": "Big Data: Intro",
    "section": "Total estimate",
    "text": "Total estimate\n\n\n\nEstimate\n\n\nThe total amount of data stored digitally worldwide is estimated to be getting close to 100 ZB as of 2021 (zettabytes)"
  },
  {
    "objectID": "bigdata_lec1.html#three-vs",
    "href": "bigdata_lec1.html#three-vs",
    "title": "Big Data: Intro",
    "section": "Three Vs",
    "text": "Three Vs\n\n\n\nVolume\nVariety\nVelocity"
  },
  {
    "objectID": "bigdata_lec1.html#volume",
    "href": "bigdata_lec1.html#volume",
    "title": "Big Data: Intro",
    "section": "Volume",
    "text": "Volume\n\n\n\nIssue\n\n\nData volume has exponentially increased in recent decades.\n\n\n\n\n\n\nWait but why?\n\n\n\nInternet-of-Things sensor data\nSocial networks\nStorage device progress"
  },
  {
    "objectID": "bigdata_lec1.html#variety",
    "href": "bigdata_lec1.html#variety",
    "title": "Big Data: Intro",
    "section": "Variety",
    "text": "Variety\n\n\n\nTypes\n\n\n\ntrees - XML, JSON, Parquet, Avro, etc\nunstructured - text, pictures, audio, video\ndata cubes\ngraphs"
  },
  {
    "objectID": "bigdata_lec1.html#velocity",
    "href": "bigdata_lec1.html#velocity",
    "title": "Big Data: Intro",
    "section": "Velocity",
    "text": "Velocity\n\n\n\nDefinion\n\n\nSpeed at which data is being generated, collected, and processed.\n\n\n\n\n\n\nAttributes\n\n\n\nCapacity: how much data can we store per unit of volume?\nThroughput: how many bytes can we read per unit of time?\nLatency: how much time do we need to wait until the bytes start arriving?"
  },
  {
    "objectID": "bigdata_lec1.html#velocity-1",
    "href": "bigdata_lec1.html#velocity-1",
    "title": "Big Data: Intro",
    "section": "Velocity",
    "text": "Velocity\nEvolution since 1950s"
  },
  {
    "objectID": "bigdata_lec1.html#features",
    "href": "bigdata_lec1.html#features",
    "title": "Big Data: Intro",
    "section": "Features",
    "text": "Features\n\n\n\nFeatures\n\n\n\nReliability\nScalability\nMaintainability"
  },
  {
    "objectID": "bigdata_lec1.html#reliability",
    "href": "bigdata_lec1.html#reliability",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nKleppmann’s definition\n\n\nThe system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity\n\nhardware faults\nsoftware faults\nand even human error"
  },
  {
    "objectID": "bigdata_lec1.html#reliability-1",
    "href": "bigdata_lec1.html#reliability-1",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nFaults\n\n\nBasically, theses are things that could go wrong.\nSystems that can anticipate faults are called fault-tolerant or resilient.\nFault can be defined as one component of the system deviating from the spec.\n\n\n\n\n\n\nFailures\n\n\nFailures occur when system stops providing services to the user.\n\n\n\nFaults might degenerate into failures."
  },
  {
    "objectID": "bigdata_lec1.html#reliability-2",
    "href": "bigdata_lec1.html#reliability-2",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nTypes of errors\n\n\n\nHardware\nSoftware\nHuman"
  },
  {
    "objectID": "bigdata_lec1.html#reliability-3",
    "href": "bigdata_lec1.html#reliability-3",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability"
  },
  {
    "objectID": "bigdata_lec1.html#scalability",
    "href": "bigdata_lec1.html#scalability",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nKleppmann\n\n\nAs the system grows (in data volume, traffic volume, or complexity), there should be reasonable ways of dealing with that growth.\nIn other words, scalability is a system’s ability to cope with increased load.\n\n\n\n\n\nNote that scalability is a multi-dimensional term. When saying “system scales well”, it’s important to state exactly along which axis."
  },
  {
    "objectID": "bigdata_lec1.html#scalability-1",
    "href": "bigdata_lec1.html#scalability-1",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nWhat is load?\n\n\nLoad is described by load parameters. These might include:\n\ndata set size\ndata write speed\ndata read speed\ncomputational complexity\netc…"
  },
  {
    "objectID": "bigdata_lec1.html#scalability-2",
    "href": "bigdata_lec1.html#scalability-2",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nPerformance\n\n\nIncreasing load affects performance. There are several meanings to this term:\n\nthroughput – time required to process a dataset of certain size\nresponse time – time between sending a request and receiving a response\nlatency – duration of waiting for a request to be processed. Included in response time.\n\n\n\n\n\n\nPerformance might be more strictly defined by service level objectives (SLOs) and service level agreements (SLAs)."
  },
  {
    "objectID": "bigdata_lec1.html#scalability-3",
    "href": "bigdata_lec1.html#scalability-3",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nHow to deal with load\n\n\n\nvertical scaling - scaling up\nhorizontal scaling - scaling out\narchitectural changes\n\n\n\n\n\n\n\nElasticity\n\n\nAn approach to load handling whereby a system automatically adds resources in case of load increase, and can decrease resources if load decreases."
  },
  {
    "objectID": "bigdata_lec1.html#scalability-4",
    "href": "bigdata_lec1.html#scalability-4",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nCommon wisdom\n\n\n\nKeep your database on a single node (scale up) until scaling cost or high-availability requirements forces you to make it distributed.\nOptimize code so that it can run on a single node."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability",
    "href": "bigdata_lec1.html#maintainability",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nKleppmann\n\n\nOver time, many different people will work on the system\n\nengineering\noperations\nboth maintaining current behavior and adapting the system to new use cases),\n\nand they should all be able to work on it productively."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-1",
    "href": "bigdata_lec1.html#maintainability-1",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nPrinciples\n\n\n\nOperability – make it easy for operations teams to keep the system running smoothly.\nSimplicity – make it easy for new engineers to understand the system, by removing as much complexity as possible from the system.\nEvolvability – Make it easy for engineers to make changes to the system in the future, adapting it for unanticipated use cases as requirements change. Also known as extensibility, modifiability, or plasticity."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-2",
    "href": "bigdata_lec1.html#maintainability-2",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nOperability\n\n\n\nHealth monitoring\nGood deployment practices\nConfiguration management\nVisibility into the internals of the system\nKnowledge preservation – documentation (!).\netc…"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-3",
    "href": "bigdata_lec1.html#maintainability-3",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nComplexity symptoms\n\n\n\nLots of hidden state\nLoose cohesion, tight coupling\nBad naming (!)\nUnnecessary hacks\netc…"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity",
    "href": "bigdata_lec1.html#maintainability-complexity",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity-1",
    "href": "bigdata_lec1.html#maintainability-complexity-1",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity\n\n\n\nTypes\n\n\n\nincidental\naccidental"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity-2",
    "href": "bigdata_lec1.html#maintainability-complexity-2",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity\n\n\n\nIncidental\n\n\n\nEasy things can be complex. There can be complex constructs that are succinctly described, familiar, available and easy to use. That is incidental complexity.\n\nRich Hickey talk “Simple made easy”: https://www.youtube.com/watch?v=SxdOUGdseq4"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity-3",
    "href": "bigdata_lec1.html#maintainability-complexity-3",
    "title": "Big Data: Intro",
    "section": "Maintainability: complexity",
    "text": "Maintainability: complexity\nHowever: Complexity is often caused by\n\n\n\nAccidental complexity\n\n\nMoseley and Marks define complexity as accidental if it is not inherent in the problem that the software solves (as seen by the users) but arises only from the implementation.\n\n\n\n\n\n\nHow to remove?\n\n\nBy providing proper abstractions."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-abstractions",
    "href": "bigdata_lec1.html#maintainability-abstractions",
    "title": "Big Data: Intro",
    "section": "Maintainability: abstractions",
    "text": "Maintainability: abstractions\n\n\n\n\n\n\nDefinition (Ousterhout)\n\n\nAn abstraction is a simplified view of an entity, which omits unimportant details.\nIn modular programming, each module provides an abstraction in the form of its interface."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-abstractions-1",
    "href": "bigdata_lec1.html#maintainability-abstractions-1",
    "title": "Big Data: Intro",
    "section": "Maintainability: abstractions",
    "text": "Maintainability: abstractions\n\n\n\n\n\n\nWhat can abstractions do?\n\n\n\nHide implementation details\nProvide reusable building blocks"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-4",
    "href": "bigdata_lec1.html#maintainability-4",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nEvolvability\n\n\nOne needs to adapt their big data system to possible future requirements changes.\nHowever, keep in mind the following:\n\nInability to foresee exact nature of changes\nNeed to strike the balance of flexibility and fitness for a particular task"
  },
  {
    "objectID": "bigdata_lec1.html#types-of-big-data-analytics",
    "href": "bigdata_lec1.html#types-of-big-data-analytics",
    "title": "Big Data: Intro",
    "section": "Types of big data analytics",
    "text": "Types of big data analytics\n\n\n\nTypes\n\n\n\nPrescriptive\nDiagnostic\nDescriptive\nPredictive"
  },
  {
    "objectID": "bigdata_lec1.html#types-prescriptive",
    "href": "bigdata_lec1.html#types-prescriptive",
    "title": "Big Data: Intro",
    "section": "Types: Prescriptive",
    "text": "Types: Prescriptive\n\n\n\nPrescriptive\n\n\n\nForward looking\nOptimal decisions for future situations"
  },
  {
    "objectID": "bigdata_lec1.html#types-diagnostic",
    "href": "bigdata_lec1.html#types-diagnostic",
    "title": "Big Data: Intro",
    "section": "Types: Diagnostic",
    "text": "Types: Diagnostic\n\n\n\nDiagnostic\n\n\n\nBackward looking\nFocused on causal relationships"
  },
  {
    "objectID": "bigdata_lec1.html#types-descriptive",
    "href": "bigdata_lec1.html#types-descriptive",
    "title": "Big Data: Intro",
    "section": "Types: Descriptive",
    "text": "Types: Descriptive\n\n\n\nDescriptive\n\n\n\nBackward looking\nFocused on descriptions and comparisons"
  },
  {
    "objectID": "bigdata_lec1.html#types-descriptive-1",
    "href": "bigdata_lec1.html#types-descriptive-1",
    "title": "Big Data: Intro",
    "section": "Types: Descriptive",
    "text": "Types: Descriptive\n\n\n\nPredictive\n\n\n\nForward looking\nFocused on the prediction of future states, relationship, and patterns"
  },
  {
    "objectID": "bigdata_lec1.html#challenges",
    "href": "bigdata_lec1.html#challenges",
    "title": "Big Data: Intro",
    "section": "Challenges",
    "text": "Challenges\nThere are 2 main challenges associated with Big Data.\n\n\n\nChallenges\n\n\n\nhow do we store and manage such a huge volume of data efficiently?\nhow do we process and extract valuable information from the data within the given time frame?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "applied_lab3.html",
    "href": "applied_lab3.html",
    "title": "Applied Analytics: Lab 3",
    "section": "",
    "text": "Overview\nThis lab is about completing Docker installation/first steps from Docker intro lecture."
  },
  {
    "objectID": "applied_lec1.html#why-now",
    "href": "applied_lec1.html#why-now",
    "title": "Intro to Docker",
    "section": "Why now?",
    "text": "Why now?\n\n\n\n\n\nBefore\n\n\n\nmonolithic applications\nlong development cycles\nsingle environment\nslowly scaling up\n\n\n\n\n\n\n\n\nNow:\n\n\n\ndecoupled services\nfast, iterative improvements\nmultiple environments\nquickly scaling out"
  },
  {
    "objectID": "applied_lec1.html#vms",
    "href": "applied_lec1.html#vms",
    "title": "Intro to Docker",
    "section": "VMs",
    "text": "VMs\n\n\n\n\n\n\nDescription\n\n\n\nVirtual machines emulate physical computers by running operating systems in isolated instances.\nMultiple VMs are commonly hosted on a single server, with a hypervisor acting as a lightweight software layer positioned between the physical host and the VMs.\nThis hypervisor efficiently manages access to resources, enabling virtual machines to function as distinct servers while offering enhanced flexibility and agility.\nGained popularity in the 2000s due to consolidation and cost saving initiatives"
  },
  {
    "objectID": "applied_lec1.html#containers",
    "href": "applied_lec1.html#containers",
    "title": "Intro to Docker",
    "section": "Containers",
    "text": "Containers\n\n\n\n\n\n\nDescription\n\n\n\nA container is an isolated, lightweight silo for running an application on the host operating system.\nContainers build on top of the host operating system’s kernel\nContainers contain only apps and some lightweight operating system APIs and services"
  },
  {
    "objectID": "applied_lec1.html#vms-vs-containers",
    "href": "applied_lec1.html#vms-vs-containers",
    "title": "Intro to Docker",
    "section": "VMs vs Containers",
    "text": "VMs vs Containers"
  },
  {
    "objectID": "applied_lec1.html#deployment",
    "href": "applied_lec1.html#deployment",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment\n\n\n\n\n\n\nComplexity\n\n\nMany different stacks:\n\nlanguages\nframeworks\ndatabases\n\nMany different targets:\n\nindividual development environments\npre-production, QA, staging…\nproduction: on prem, cloud, hybrid"
  },
  {
    "objectID": "applied_lec1.html#deployment-1",
    "href": "applied_lec1.html#deployment-1",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#deployment-2",
    "href": "applied_lec1.html#deployment-2",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#deployment-3",
    "href": "applied_lec1.html#deployment-3",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#deployment-4",
    "href": "applied_lec1.html#deployment-4",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#deployment-5",
    "href": "applied_lec1.html#deployment-5",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#deployment-6",
    "href": "applied_lec1.html#deployment-6",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#deployment-7",
    "href": "applied_lec1.html#deployment-7",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#deployment-8",
    "href": "applied_lec1.html#deployment-8",
    "title": "Intro to Docker",
    "section": "Deployment",
    "text": "Deployment"
  },
  {
    "objectID": "applied_lec1.html#docker",
    "href": "applied_lec1.html#docker",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nResults\n\n\n\nDev-to-prod reduced from 9 months to 15 minutes (ING)\nContinuous integration job time reduced by more than 60% (BBC)\nDeploy 100 times a day instead of once a week (GILT)\n70% infrastructure consolidation (MetLife)"
  },
  {
    "objectID": "applied_lec1.html#docker-1",
    "href": "applied_lec1.html#docker-1",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\n\n\n\nHow to deploy now?\n\n\nEscape dependency hell:\n\nWrite installation instructions into an INSTALL.txt file\nUsing this file, write an install.sh script that works for you\nTurn this file into a Dockerfile, test it on your machine\nIf the Dockerfile builds on your machine, it will build anywhere\nRejoice as you escape dependency hell and “works on my machine”\nNever again “worked in dev - ops problem now!”"
  },
  {
    "objectID": "applied_lec1.html#docker-2",
    "href": "applied_lec1.html#docker-2",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\n\n\n\nHow to deploy now?\n\n\nQuick onboarding\n\nWrite Dockerfiles for your application components\nUse pre-made images from the Docker Hub (mysql, redis…)\nDescribe your stack with a Compose file\nOn-board somebody with two commands:\n\ngit clone ...\ndocker-compose up"
  },
  {
    "objectID": "applied_lec1.html#docker-3",
    "href": "applied_lec1.html#docker-3",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\n\n\n\nImplement reliable CI easily\n\n\n\nBuild test environment with a Dockerfile or Compose file\nFor each test run, stage up a new container or stack\nEach run is now in a clean environment\nNo pollution from previous tests"
  },
  {
    "objectID": "applied_lec1.html#docker-4",
    "href": "applied_lec1.html#docker-4",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\n\n\n\nUse container images as build artefacts\n\n\n\nBuild your app from Dockerfiles\nStore the resulting images in a registry\nKeep them forever (or as long as necessary)\nTest those images in QA, CI, integration…\nRun the same images in production\nSomething goes wrong? Rollback to previous image\nInvestigating old regression? Old image has your back!\nImages contain all the libraries, dependencies, etc. needed to run the app."
  },
  {
    "objectID": "applied_lec1.html#docker-formats",
    "href": "applied_lec1.html#docker-formats",
    "title": "Intro to Docker",
    "section": "Docker: Formats",
    "text": "Docker: Formats\n\n\n\n\n\nBefore\n\n\n\nNo standardized exchange format. \nContainers are hard to use for developers. \nAs a result, they are hidden from the end users.\nNo re-usable components, APIs, tools. \n\n\n\n\n\n\n\n\n\n\n\nAfter\n\n\n\nStandardize the container format, because containers were not portable.\nMake containers easy to use for developers.\nEmphasis on re-usable components, APIs, ecosystem of standard tools.\nImprovement over ad-hoc, in-house, specific tools."
  },
  {
    "objectID": "applied_lec1.html#docker-deployment",
    "href": "applied_lec1.html#docker-deployment",
    "title": "Intro to Docker",
    "section": "Docker: Deployment",
    "text": "Docker: Deployment\n\n\n\n\n\nBefore\n\n\n\nShip packages: deb, rpm, gem, jar, homebrew…\nDependency hell.\n“Works on my machine.”\nBase deployment often done from scratch and unreliable.\n\n\n\n\n\n\n\n\nAfter\n\n\n\nShip container images with all their dependencies.\nImages are bigger, but they are broken down into layers.\nOnly ship layers that have changed.\nSave disk, network, memory usage."
  },
  {
    "objectID": "applied_lec1.html#docker-5",
    "href": "applied_lec1.html#docker-5",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nExample\n\n\nLayers:\n\nCentOS\nJRE\nTomcat\nDependencies\nApplication JAR\nConfiguration"
  },
  {
    "objectID": "applied_lec1.html#docker-devops",
    "href": "applied_lec1.html#docker-devops",
    "title": "Intro to Docker",
    "section": "Docker: Devops",
    "text": "Docker: Devops\n\n\n\n\n\nBefore\n\n\n\nDrop a tarball (or a commit hash) with instructions.\nDev environment very different from production.\nOps don’t always have a dev environment themselves …\n… and when they do, it can differ from the devs’.\nOps have to sort out differences and make it work …\n… or bounce it back to devs.\nShipping code causes frictions and delays.\n\n\n\n\n\n\n\n\nAfter\n\n\n\nDrop a container image or a Compose file.\nOps can always run that container image.\nOps can always run that Compose file.\nOps still have to adapt to prod environment, but at least they have a reference point.\nOps have tools allowing to use the same image in dev and prod.\nDevs can be empowered to make releases themselves more easily."
  },
  {
    "objectID": "applied_lec1.html#docker-history",
    "href": "applied_lec1.html#docker-history",
    "title": "Intro to Docker",
    "section": "Docker history",
    "text": "Docker history"
  },
  {
    "objectID": "applied_lec1.html#docker-history-1",
    "href": "applied_lec1.html#docker-history-1",
    "title": "Intro to Docker",
    "section": "Docker history",
    "text": "Docker history\n\n\n\ndotCloud\n\n\n\ndotCloud was operating a PaaS, using a custom container engine.\nThis engine was based on OpenVZ (and later, LXC) and AUFS.\nIt started (circa 2008) as a single Python script.\nBy 2012, the engine had multiple (~10) Python components. (and ~100 other micro-services!)\nEnd of 2012, dotCloud refactors this container engine.\nThe codename for this project is “Docker.”"
  },
  {
    "objectID": "applied_lec1.html#docker-history-2",
    "href": "applied_lec1.html#docker-history-2",
    "title": "Intro to Docker",
    "section": "Docker history",
    "text": "Docker history\n\n\n\nFirst public release\n\n\n\nMarch 2013, PyCon, Santa Clara:\n\n“Docker” is shown to a public audience for the first time.\n\nIt is released with an open source license.\nVery positive reactions and feedback!\nThe dotCloud team progressively shifts to Docker development.\nThe same year, dotCloud changes name to Docker."
  },
  {
    "objectID": "applied_lec1.html#docker-history-3",
    "href": "applied_lec1.html#docker-history-3",
    "title": "Intro to Docker",
    "section": "Docker history",
    "text": "Docker history\n\n\n\nAfter release\n\n\n\n2013: fixing bugs around OS support\n2014: Docker Compose v1 (written in Python)\n2015: version 1.0, Open Containers Initiative\n2015: creation of the Cloud Native Computing Foundation\n2020: Docker Compose v2 (re-written in Go)"
  },
  {
    "objectID": "applied_lec1.html#containerd",
    "href": "applied_lec1.html#containerd",
    "title": "Intro to Docker",
    "section": "containerd",
    "text": "containerd"
  },
  {
    "objectID": "applied_lec1.html#docker-installation",
    "href": "applied_lec1.html#docker-installation",
    "title": "Intro to Docker",
    "section": "Docker installation",
    "text": "Docker installation\n\n\n\n\n\n\nWhat is Docker?\n\n\n\n“Installing Docker” really means “Installing the Docker Engine and CLI”.\nThe Docker Engine is a daemon (a service running in the background).\nThis daemon manages containers, the same way that a hypervisor manages VMs.\nWe interact with the Docker Engine by using the Docker CLI.\nThe Docker CLI and the Docker Engine communicate through an API.\nThere are many other programs and client libraries which use that API."
  },
  {
    "objectID": "applied_lec1.html#docker-installation-1",
    "href": "applied_lec1.html#docker-installation-1",
    "title": "Intro to Docker",
    "section": "Docker installation",
    "text": "Docker installation\n\n\n\nDocker Desktop\n\n\n\nLeverages the host OS virtualization subsystem\nUnder the hood, runs a tiny VM\nAccesses network resources like normal applications\nSupports filesystem sharing through volumes"
  },
  {
    "objectID": "applied_lec1.html#docker-installation-2",
    "href": "applied_lec1.html#docker-installation-2",
    "title": "Intro to Docker",
    "section": "Docker installation",
    "text": "Docker installation\n\n\n\nDocker Desktop\n\n\nWhen you execute docker version from the terminal:\n\nthe CLI connects to the Docker Engine over a standard socket,\nthe Docker Engine is, in fact, running in a VM,\n… but the CLI doesn’t know or care about that,\nthe CLI sends a request using the REST API,\nthe Docker Engine in the VM processes the request,\nthe CLI gets the response and displays it to you."
  },
  {
    "objectID": "applied_lec1.html#docker-installation-3",
    "href": "applied_lec1.html#docker-installation-3",
    "title": "Intro to Docker",
    "section": "Docker installation",
    "text": "Docker installation\n\n\n\nCheck that it works\n\n\n$ docker version\n\nClient:\n Version:           28.2.2\n API version:       1.50\n Go version:        go1.24.3\n Git commit:        e6534b4\n Built:             Fri May 30 12:07:35 2025\n OS/Arch:           darwin/arm64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.42.1 (196648)\n Engine:\n  Version:          28.2.2\n  API version:      1.50 (minimum version 1.24)\n  Go version:       go1.24.3\n  Git commit:       45873be\n  Built:            Fri May 30 12:07:27 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\n containerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\n runc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0"
  },
  {
    "objectID": "applied_lec1.html#docker-installation-busybox",
    "href": "applied_lec1.html#docker-installation-busybox",
    "title": "Intro to Docker",
    "section": "Docker installation: Busybox",
    "text": "Docker installation: Busybox\n\n\n\n\nWhat is it?\n\n\n\nprovides several Unix utilities in a single executable file.\nvery space-efficient\ncreated for embedded operating systems with very limited resources.\n\n\n\n\n\n\n\nCheck that it works\n\n\n$ docker run busybox echo hello world\nUnable to find image 'busybox:latest' locally\nlatest: Pulling from library/busybox\n499bcf3c8ead: Pull complete\nDigest: sha256:d82f458899c9696cb26a7c02d5568f81c8c8223f8661bb2a7988b269c8b9051e\nStatus: Downloaded newer image for busybox:latest\nhello world"
  },
  {
    "objectID": "applied_lec1.html#docker-installation-ubuntu",
    "href": "applied_lec1.html#docker-installation-ubuntu",
    "title": "Intro to Docker",
    "section": "Docker installation: Ubuntu",
    "text": "Docker installation: Ubuntu\n\n\n\n\nRunning Ubuntu\n\n\n$ docker run -it ubuntu\nUnable to find image 'ubuntu:latest' locally\nlatest: Pulling from library/ubuntu\n59a5d47f84c3: Pull complete\nDigest: sha256:353675e2a41babd526e2b837d7ec780c2a05bca0164f7ea5dbbd433d21d166fc\nStatus: Downloaded newer image for ubuntu:latest\nroot@014ed1f2eac1:/# sudo apt-get moo\nbash: sudo: command not found\nroot@014ed1f2eac1:/# apt-get moo\n                 (__)\n                 (oo)\n           /------\\/\n          / |    ||\n         *  /\\---/\\\n            ~~   ~~\n...\"Have you mooed today?\"...\nroot@014ed1f2eac1:/#"
  },
  {
    "objectID": "applied_lec1.html#docker-installation-ubuntu-1",
    "href": "applied_lec1.html#docker-installation-ubuntu-1",
    "title": "Intro to Docker",
    "section": "Docker installation: Ubuntu",
    "text": "Docker installation: Ubuntu\n\n\n\n\n\n\nWhat does this mean?\n\n\n\nIt runs a bare-bones, no-frills ubuntu system.\n-it is shorthand for -i -t.\n-i tells Docker to connect us to the container’s stdin.: e.g. interactive mode.\n-t tells Docker that we want a pseudo-terminal."
  },
  {
    "objectID": "applied_lec1.html#docker-installation-ubuntu-2",
    "href": "applied_lec1.html#docker-installation-ubuntu-2",
    "title": "Intro to Docker",
    "section": "Docker installation: Ubuntu",
    "text": "Docker installation: Ubuntu\n\n\n\nRun something\n\n\nroot@014ed1f2eac1:/# figlet hello\nbash: figlet: command not found\n\nroot@014ed1f2eac1:/# apt-get update && apt-get install figlet\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  figlet\n...\nUnpacking figlet (2.2.5-3) ...\nroot@014ed1f2eac1:/# figlet hello\n _          _ _\n| |__   ___| | | ___\n| '_ \\ / _ \\ | |/ _ \\\n| | | |  __/ | | (_) |\n|_| |_|\\___|_|_|\\___/\n\nroot@014ed1f2eac1:/#"
  },
  {
    "objectID": "applied_lec1.html#docker-installation-ubuntu-3",
    "href": "applied_lec1.html#docker-installation-ubuntu-3",
    "title": "Intro to Docker",
    "section": "Docker installation: Ubuntu",
    "text": "Docker installation: Ubuntu\n\n\n\n\n\n\nImportant\n\n\nExit the container via exit or Ctrl-D.\nIf we try running figlet again, this won’t work - it’s only installed inside the container."
  },
  {
    "objectID": "applied_lec1.html#docker-installation-ubuntu-4",
    "href": "applied_lec1.html#docker-installation-ubuntu-4",
    "title": "Intro to Docker",
    "section": "Docker installation: Ubuntu",
    "text": "Docker installation: Ubuntu\n\n\n\nHosts vs Containers\n\n\n\nWe ran an ubuntu container on an Linux/Windows/macOS host.\nThey have different, independent packages.\nInstalling something on the host doesn’t expose it to the container.\nAnd vice-versa.\nEven if both the host and the container have the same Linux distro!\nWe can run any container on any host."
  },
  {
    "objectID": "applied_lec1.html#docker-6",
    "href": "applied_lec1.html#docker-6",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nWhere’s our container now\n\n\n\nin a stopped state\nusing disk storage\nNOT using CPU or memory"
  },
  {
    "objectID": "applied_lec1.html#docker-7",
    "href": "applied_lec1.html#docker-7",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nStart a new container\n\n\ndocker run -it ubuntu\nroot@5c6dc90eb867:/# figlet\nbash: figlet: command not found\nroot@5c6dc90eb867:/#\nWhy?\n\nWe started a brand new container.\nThe basic Ubuntu image was used, and figlet is not here."
  },
  {
    "objectID": "applied_lec1.html#docker-8",
    "href": "applied_lec1.html#docker-8",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nCan we restore our container somehow?\n\n\nWe can, but that’s not the default workflow with Docker.\n\n\n\n\n\n\nWhat’s the default workflow, then?\n\n\n\nAlways start with a fresh container.\nIf we need something installed in our container, build a custom image.\n\n\n\n\n\n\n\nWhy so complicated?\n\n\n\nIt’s quite easy actually\nThis puts a strong emphasis on automation and repeatability. Let’s see why …"
  },
  {
    "objectID": "applied_lec1.html#docker-9",
    "href": "applied_lec1.html#docker-9",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker"
  },
  {
    "objectID": "applied_lec1.html#docker-10",
    "href": "applied_lec1.html#docker-10",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\n\n\nPets\n\n\n\nhave distinctive names and unique configurations\nwhen they have an outage, we do everything we can to fix them\n\n\n\n\n\n\n\n\nCattle\n\n\n\nhave generic names (e.g. with numbers) and generic configuration\nconfiguration is enforced by configuration management, golden images …\nwhen they have an outage, we can replace them immediately with a new server"
  },
  {
    "objectID": "applied_lec1.html#docker-11",
    "href": "applied_lec1.html#docker-11",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nPet VM\n\n\nWhen we use local VMs (with e.g. VirtualBox or VMware), our workflow looks like this:\n\ncreate VM from base template (Ubuntu, CentOS…)\ninstall packages, set up environment\nwork on project\nwhen done, shut down VM\nnext time we need to work on project, restart VM as we left it\nif we need to tweak the environment, we do it live\nOver time, the VM configuration evolves, diverges.\nWe don’t have a clean, reliable, deterministic way to provision that environment."
  },
  {
    "objectID": "applied_lec1.html#docker-12",
    "href": "applied_lec1.html#docker-12",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nCattle container\n\n\nWith Docker, the workflow looks like this:\n\ncreate container image with our dev environment\nrun container with that image\nwork on project\nwhen done, shut down container\nnext time we need to work on project, start a new container\nif we need to tweak the environment, we create a new image\nWe have a clear definition of our environment, and can share it reliably with others."
  },
  {
    "objectID": "applied_lec1.html#docker-13",
    "href": "applied_lec1.html#docker-13",
    "title": "Intro to Docker",
    "section": "Docker",
    "text": "Docker\n\n\n\nNon-interactive containers\n\n\nOur first containers were interactive.\nWe will now see how to:\n\nRun a non-interactive container.\nRun a container in the background.\nList running containers.\nCheck the logs of a container.\nStop a container.\nList stopped containers."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-1",
    "href": "applied_lec1.html#non-interactive-containers-1",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nExample\n\n\nWe will run a small custom container. This container just displays the time every second.\n$ docker run jpetazzo/clock\nUnable to find image 'jpetazzo/clock:latest' locally\nlatest: Pulling from jpetazzo/clock\n36fbfd22ebfc: Pull complete\nDigest: sha256:dc06bbc3744f7200404bff0bbb2516925e7adea115e07de9da8b36bf15fe3dd3\nStatus: Downloaded newer image for jpetazzo/clock:latest\nSat Sep 20 11:00:45 UTC 2025\nSat Sep 20 11:00:46 UTC 2025\nSat Sep 20 11:00:47 UTC 2025\n^C%\n\nThis container will run forever.\nTo stop it, press ^C.\nDocker has automatically downloaded the image jpetazzo/clock.\nThis image is a user image, created by jpetazzo."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-2",
    "href": "applied_lec1.html#non-interactive-containers-2",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nCtrl-C might now always work!\n\n\nWhat happens when we hit Ctrl-C:\n\nSIGINT gets sent to the container, which means:\nSIGINT gets sent to PID 1 (default case)\nSIGINT gets sent to foreground processes when running with -ti\n\nBut there is a special case for PID 1: it ignores all signals!\n\nexcept SIGKILL and SIGSTOP\nexcept signals handled explicitly\n\nTL,DR: there are many circumstances when Ctrl-C won’t stop the container."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-3",
    "href": "applied_lec1.html#non-interactive-containers-3",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\n\n\n\nWhy is PID 1 special?\n\n\nPID 1 has some extra responsibilities:\n\nit starts (directly or indirectly) every other process\nwhen a process exits, its processes are “reparented” under PID 1\nWhen PID 1 exits, everything stops:\non a “regular” machine, it causes a kernel panic\nin a container, it kills all the processes\n\nErgo: We don’t want PID 1 to stop accidentally. That’s why it has these extra protections."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-4",
    "href": "applied_lec1.html#non-interactive-containers-4",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nSolution"
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-5",
    "href": "applied_lec1.html#non-interactive-containers-5",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nDaemon mode\n\n\nContainers can be started in the background, with the -d flag (daemon mode):\n$ docker run -d jpetazzo/clock\n896ffc453901fc7d7c417381c8bde9a8911182d07b819dc988aa0b4d1c298d3e\n\nWe don’t see the output of the container.\nBut don’t worry: Docker collects that output and logs it!\nDocker gives us the ID of the container."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-6",
    "href": "applied_lec1.html#non-interactive-containers-6",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nMaxwell demon: MIT’s Project MAC\n\n\n\n\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Maxwell’s_demon"
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-7",
    "href": "applied_lec1.html#non-interactive-containers-7",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nList running containers\n\n\nHow can we check that our container is still running?\nWith docker ps, just like the UNIX ps command, lists running processes.\n$ docker ps\nCONTAINER ID   IMAGE            COMMAND                  CREATED         STATUS                PORTS                                             NAMES\n896ffc453901   jpetazzo/clock   \"/bin/sh -c 'while d…\"   5 minutes ago   Up 5 minutes                                                            quirky_wilson\n\nDocker tells us:\n\nThe (truncated) ID of our container.\nThe image used to start the container.\nThat our container has been running (Up) for a couple of minutes.\nOther information (COMMAND, PORTS, NAMES) that we will explain later."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-8",
    "href": "applied_lec1.html#non-interactive-containers-8",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nMore containers\n\n\nRun 2 more:\n$ docker run -d jpetazzo/clock\n\n42518eae35544162179d3f7086410949256a767244e36e40518f0f9d1dd223ae\n$ docker run -d jpetazzo/clock\n\n31a2d9cc7e40d58280b9e5cdd6135cf824559bc1ab6409f026c3a0c82419273e\nCheck running:\n$ docker ps\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS                PORTS                                             NAMES\n31a2d9cc7e40   jpetazzo/clock   \"/bin/sh -c 'while d…\"   30 seconds ago   Up 29 seconds                                                           optimistic_payne\n42518eae3554   jpetazzo/clock   \"/bin/sh -c 'while d…\"   31 seconds ago   Up 30 seconds                                                           beautiful_kalam\n896ffc453901   jpetazzo/clock   \"/bin/sh -c 'while d…\"   7 minutes ago    Up 7 minutes                                                            quirky_wilson"
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-9",
    "href": "applied_lec1.html#non-interactive-containers-9",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nLast run container\n\n\n$ docker ps -l\nCONTAINER ID   IMAGE            COMMAND                  CREATED              STATUS              PORTS     NAMES\n31a2d9cc7e40   jpetazzo/clock   \"/bin/sh -c 'while d…\"   About a minute ago   Up About a minute             optimistic_payne\n\n\n\n\n\n\nIDs only\n\n\n$ docker ps -q\n31a2d9cc7e40\n42518eae3554\n896ffc453901"
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-10",
    "href": "applied_lec1.html#non-interactive-containers-10",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nContainer logs\n\n\n$ docker logs 31a\n...\n...\n...\nSat Sep 20 11:18:46 UTC 2025\nSat Sep 20 11:18:47 UTC 2025\nSat Sep 20 11:18:48 UTC 2025\nSat Sep 20 11:18:49 UTC 2025\nSat Sep 20 11:18:50 UTC 2025\nSat Sep 20 11:18:51 UTC 2025\nSat Sep 20 11:18:52 UTC 2025\nSat Sep 20 11:18:53 UTC 2025\nSat Sep 20 11:18:54 UTC 2025\nSat Sep 20 11:18:55 UTC 2025\nSat Sep 20 11:18:56 UTC 2025\nSat Sep 20 11:18:57 UTC 2025\nSat Sep 20 11:18:58 UTC 2025\nSat Sep 20 11:18:59 UTC 2025\nSat Sep 20 11:19:00 UTC 2025\n\nAll logs are dumped - a bit too much."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-11",
    "href": "applied_lec1.html#non-interactive-containers-11",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\nContainer logs tail\n\n\n$ docker logs --tail 5 31a\nSat Sep 20 11:19:55 UTC 2025\nSat Sep 20 11:19:56 UTC 2025\nSat Sep 20 11:19:57 UTC 2025\nSat Sep 20 11:19:58 UTC 2025\nSat Sep 20 11:19:59 UTC 2025\n\n\n\n\n\n\nContainer logs tail & follow\n\n\n$ docker logs --tail 1 --follow 31a\nSat Sep 20 11:21:45 UTC 2025\nSat Sep 20 11:21:46 UTC 2025\nSat Sep 20 11:21:47 UTC 2025\nSat Sep 20 11:21:48 UTC 2025\nSat Sep 20 11:21:49 UTC 2025"
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-12",
    "href": "applied_lec1.html#non-interactive-containers-12",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\n\n\n\nStopping\n\n\nThere are two ways we can terminate our detached container.\n\nKilling it using the docker kill command.\n\nstops the container immediately, by using the KILL signal.\n\nStopping it using the docker stop command.\n\nsends a TERM signal, and after 10 seconds, if the container has not stopped, it sends KILL.\n\n\nReminder: the KILL signal cannot be intercepted, and will forcibly terminate the container."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-13",
    "href": "applied_lec1.html#non-interactive-containers-13",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\n\n\n\nStopping: Example\n\n\n$ docker stop 31a\n&lt;10 seconds pass&gt;\n31a\n\nDocker sends the TERM signal;\nthe container doesn’t react to this signal (it’s a simple Shell script with no special signal handling);\n10 seconds later, since the container is still running, Docker sends the KILL signal;\nthis terminates the container."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-14",
    "href": "applied_lec1.html#non-interactive-containers-14",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\n\n\n\nKilling: Example\n\n\n$ docker kill 425 896\n425\n896\nThose containers will be terminated immediately (without the 10-second delay)."
  },
  {
    "objectID": "applied_lec1.html#non-interactive-containers-15",
    "href": "applied_lec1.html#non-interactive-containers-15",
    "title": "Intro to Docker",
    "section": "Non-interactive containers",
    "text": "Non-interactive containers\n\n\n\n\n\n\nList stopped containers\n\n\nWe can also see stopped containers, with the -a (–all) option.\n$ docker ps -a\nCONTAINER ID   IMAGE                                          COMMAND                  CREATED             STATUS                            PORTS     NAMES\n31a2d9cc7e40   jpetazzo/clock                                 \"/bin/sh -c 'while d…\"   12 minutes ago      Exited (137) 2 minutes ago                  optimistic_payne\n42518eae3554   jpetazzo/clock                                 \"/bin/sh -c 'while d…\"   12 minutes ago      Exited (137) About a minute ago             beautiful_kalam\n896ffc453901   jpetazzo/clock                                 \"/bin/sh -c 'while d…\"   19 minutes ago      Exited (137) About a minute ago             quirky_wilson\n74b84530ad71   jpetazzo/clock                                 \"/bin/sh -c 'while d…\"   26 minutes ago      Exited (130) 26 minutes ago                 amazing_cohen\n5c6dc90eb867   ubuntu                                         \"/bin/bash\"              40 minutes ago      Exited (130) 26 minutes ago                 thirsty_bardeen\n014ed1f2eac1   ubuntu                                         \"/bin/bash\"              52 minutes ago      Exited (0) 40 minutes ago                   elated_darwin\n42f574fdf1af   busybox                                        \"echo hello world\"       About an hour ago   Exited (0) About an hour ago                suspicious_franklin"
  },
  {
    "objectID": "applied_lec1.html#restarting-and-attaching",
    "href": "applied_lec1.html#restarting-and-attaching",
    "title": "Intro to Docker",
    "section": "Restarting and attaching",
    "text": "Restarting and attaching\n\n\n\n\n\n\nBackground vs foreground\n\n\n\nThe distinction between foreground and background containers is arbitrary.\nFrom Docker’s point of view, all containers are the same.\nAll containers run the same way, whether there is a client attached to them or not.\nIt is always possible to detach from a container, and to reattach to a container.\nAnalogy: attaching to a container is like plugging a keyboard and screen to a physical server."
  },
  {
    "objectID": "applied_lec1.html#restarting-and-attaching-1",
    "href": "applied_lec1.html#restarting-and-attaching-1",
    "title": "Intro to Docker",
    "section": "Restarting and attaching",
    "text": "Restarting and attaching\n\n\n\nDetaching from containers\n\n\nIf you have started an interactive container (with option -it), you can detach from it.\nThe “detach” sequence is Ctrl-P Ctrl-Q or Ctrl-C on Windows.\nOtherwise you can detach by killing the Docker client."
  },
  {
    "objectID": "applied_lec1.html#restarting-and-attaching-2",
    "href": "applied_lec1.html#restarting-and-attaching-2",
    "title": "Intro to Docker",
    "section": "Restarting and attaching",
    "text": "Restarting and attaching\n\n\n\nCustom detach\n\n\nYou can change the sequence with docker run --detach-keys.\nThis can also be passed as a global option to the engine.\nStart a container with a custom detach command:\n$ docker run -ti --detach-keys ctrl-x,x jpetazzo/clock\nDetach by hitting Ctrl-X x.\n$ docker ps -l"
  },
  {
    "objectID": "applied_lec1.html#restarting-and-attaching-3",
    "href": "applied_lec1.html#restarting-and-attaching-3",
    "title": "Intro to Docker",
    "section": "Restarting and attaching",
    "text": "Restarting and attaching\n\n\n\nAttaching\n\n\nYou can attach to a container:\n$ docker attach &lt;containerID&gt;\nThe container must be running.\nThere can be multiple clients attached to the same container.\nIf you don’t specify --detach-keys when attaching, it defaults back to Ctrl-P Ctrl-Q.\nTry it on our previous container:\n$ docker attach $(docker ps -lq)"
  },
  {
    "objectID": "applied_lec1.html#restarting-and-attaching-4",
    "href": "applied_lec1.html#restarting-and-attaching-4",
    "title": "Intro to Docker",
    "section": "Restarting and attaching",
    "text": "Restarting and attaching\n\n\n\nDetaching from non-interactive containers\n\n\nWarning: if the container was started without -it..., you won’t be able to detach with Ctrl-P Ctrl-Q. If you hit Ctrl-C, the signal will be proxied to the container.\nRemember: you can always detach by killing the Docker client."
  },
  {
    "objectID": "applied_lec1.html#restarting-and-attaching-5",
    "href": "applied_lec1.html#restarting-and-attaching-5",
    "title": "Intro to Docker",
    "section": "Restarting and attaching",
    "text": "Restarting and attaching\n\n\n\nRestarting a container\n\n\nWhen a container has exited, it is in stopped state.\nIt can then be restarted with the start command.\n$ docker start &lt;yourContainerID&gt;\nThe container will be restarted using the same options you launched it with.\nYou can re-attach to it if you want to interact with it:\n$ docker attach &lt;yourContainerID&gt;\nUse docker ps -a to identify the container ID of a previous jpetazzo/clock container, and try those commands."
  },
  {
    "objectID": "applied_lec1.html#docker-images-1",
    "href": "applied_lec1.html#docker-images-1",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\n\nOutline\n\n\nWhat we will go through now:\n\nWhat is an image.\nWhat is a layer.\nThe various image namespaces.\nHow to search and download images.\nImage tags and when to use them."
  },
  {
    "objectID": "applied_lec1.html#docker-images-2",
    "href": "applied_lec1.html#docker-images-2",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nWhat is an image?\n\n\n\nImage = files + metadata\nThese files form the root filesystem of our container.\nThe metadata can indicate a number of things, e.g.:\n\nthe author of the image\nthe command to execute in the container when starting it\nenvironment variables to be set\netc.\n\nImages are made of layers, conceptually stacked on top of each other.\nEach layer can add, change, and remove files and/or metadata.\nImages can share layers to optimize disk usage, transfer times, and memory use."
  },
  {
    "objectID": "applied_lec1.html#docker-images-3",
    "href": "applied_lec1.html#docker-images-3",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nDocker Image Example: Website\n\n\nThe images will contain these layers:\n\nCentOS base layer\nPackages and configuration files added by our local IT\nPython installation\nFlask/Django\nOur application’s dependencies\nOur application code and assets\nOur application configuration\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nApp config is generally added by orchestration facilities."
  },
  {
    "objectID": "applied_lec1.html#docker-images-4",
    "href": "applied_lec1.html#docker-images-4",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nRead-write layer\n\n\nExists on top of image layers."
  },
  {
    "objectID": "applied_lec1.html#docker-images-5",
    "href": "applied_lec1.html#docker-images-5",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\n\n\n\nContainers vs Images\n\n\n\nan image is a read-only filesystem.\na container is an encapsulated set of processes,\nrunning in a read-write copy of that filesystem.\nto optimize container boot time, copy-on-write is used instead of regular copy.\ndocker run starts a container from a given image."
  },
  {
    "objectID": "applied_lec1.html#docker-images-6",
    "href": "applied_lec1.html#docker-images-6",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images"
  },
  {
    "objectID": "applied_lec1.html#docker-images-7",
    "href": "applied_lec1.html#docker-images-7",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nCompared to Python objects/classes\n\n\n\nImages are conceptually similar to classes.\nLayers are conceptually similar to inheritance.\nContainers are conceptually similar to instances."
  },
  {
    "objectID": "applied_lec1.html#docker-images-8",
    "href": "applied_lec1.html#docker-images-8",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nHow do change read-only images?\n\n\nIf an image is read-only, how do we change it?\n\nWe don’t.\nWe create a new container from that image.\nThen we make changes to that container.\nWhen we are satisfied with those changes, we transform them into a new layer.\nA new image is created by stacking the new layer on top of the old image."
  },
  {
    "objectID": "applied_lec1.html#docker-images-9",
    "href": "applied_lec1.html#docker-images-9",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nHow do we create empty images?\n\n\n\nThere is a special empty image called scratch.  It allows to build from scratch.\nThe docker import command loads a tarball into Docker.  The imported tarball becomes a standalone image.  That new image has a single layer.\n\nNote: you will probably never have to do this yourself."
  },
  {
    "objectID": "applied_lec1.html#docker-images-10",
    "href": "applied_lec1.html#docker-images-10",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nHow do we create other images?\n\n\ndocker commit\n\nSaves all the changes made to a container into a new layer.\nCreates a new image (effectively a copy of the container).\n\n\n\n\n\n\n\nAnother option\n\n\ndocker build (used 99% of the time)\n\nPerforms a repeatable build sequence.\nThis is the preferred method!"
  },
  {
    "objectID": "applied_lec1.html#docker-images-11",
    "href": "applied_lec1.html#docker-images-11",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nImage namespaces\n\n\nImages have names, and these names can belong to three namespaces:\n\nOfficial images (root namespace)\n\n e.g. ubuntu, busybox …\n\nUser (and organizations) images (user namespace)\n\n e.g. jpetazzo/clock\n\nSelf-hosted images (self-hosted namespace)\n\n e.g. registry.example.com:5000/my-private/image"
  },
  {
    "objectID": "applied_lec1.html#docker-images-12",
    "href": "applied_lec1.html#docker-images-12",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nRoot namespace\n\n\n\nThe root namespace is for official images.\nThey are gated by Docker Inc.\nThey are generally authored and maintained by third parties.\nThose images include:\nSmall, “swiss-army-knife” images like busybox.\nDistro images to be used as bases for your builds, like ubuntu, fedora...\nReady-to-use components and services, like redis, postgresql..."
  },
  {
    "objectID": "applied_lec1.html#docker-images-13",
    "href": "applied_lec1.html#docker-images-13",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nUser namespace\n\n\nThe user namespace holds images for Docker Hub users and organizations. For example:\njpetazzo/clock\nThe Docker Hub user is:\njpetazzo\nThe image name is:\nclock"
  },
  {
    "objectID": "applied_lec1.html#docker-images-14",
    "href": "applied_lec1.html#docker-images-14",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nSelf-hosted namespace\n\n\nThis namespace holds images which are not hosted on Docker Hub, but on third party registries.\nThey contain the hostname (or IP address), and optionally the port, of the registry server.\nFor example:\nlocalhost:5000/wordpress\n\nlocalhost:5000 is the host and port of the registry\nwordpress is the name of the image\n\nOther examples:\nquay.io/coreos/etcd\ngcr.io/google-containers/hugo"
  },
  {
    "objectID": "applied_lec1.html#docker-images-15",
    "href": "applied_lec1.html#docker-images-15",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nImage storage\n\n\nImages can be stored:\n\nOn your Docker host.\nIn a Docker registry.\n\nYou can use the Docker client to download (pull) or upload (push) images."
  },
  {
    "objectID": "applied_lec1.html#docker-images-16",
    "href": "applied_lec1.html#docker-images-16",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nCurrent images\n\n\n$ docker images\nREPOSITORY                                     TAG            IMAGE ID       CREATED         SIZE\nubuntu                                         latest         353675e2a41b   11 days ago     139MB\nmongo                                          latest         a6bda40d00e5   8 weeks ago     1.19GB\nmysql                                          8.0            18dee92bbc23   2 months ago    1.06GB\nbitnami/redis                                  latest         5927ff3702df   2 months ago    253MB\npostgres                                       14             563a4985838f   3 months ago    623MB\nnode                                           18.20.5        8b7f2b36c945   10 months ago   1.56GB\nbusybox                                        latest         d82f458899c9   11 months ago   6.21MB\nmysql                                          8.0.35         c6812f0dcd97   21 months ago   809MB\npostgres                                       11-alpine      ea50b9fd617b   21 months ago   337MB\nnode                                           16             f77a1aef2da8   2 years ago     1.27GB\nmaven                                          3-openjdk-11   805f366910ae   3 years ago     1.03GB\njpetazzo/clock                                 latest         dc06bbc3744f   4 years ago     2.32MB"
  },
  {
    "objectID": "applied_lec1.html#docker-images-17",
    "href": "applied_lec1.html#docker-images-17",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nImage search\n\n\nWe cannot list all images on a remote registry, but we can search for a specific keyword:\n$ docker search jupyter\nNAME                               DESCRIPTION                                     STARS     OFFICIAL\nislasgeci/jupyter                  Jupyter para Ciencia de Datos • GECI            0\nopendatacube/jupyter               An image with OpenDataCube and Jupyter          1\ndatajoint/jupyter                  **Deprecated**: Official DataJoint Jupyter n…   0\njupyter/scipy-notebook             Scientific Jupyter Notebook Python Stack fro…   467\njupyter/all-spark-notebook         Python, Scala, R and Spark Jupyter Notebook …   439\njupyter/pyspark-notebook           Python and Spark Jupyter Notebook Stack from…   316\njupyter/tensorflow-notebook        Scientific Jupyter Notebook Python Stack w/ …   372\nbiocontainers/jupyter                                                              0\njupyter/datascience-notebook       Data Science Jupyter Notebook Python Stack f…   1092\njupyter/minimal-notebook           Minimal Jupyter Notebook Python Stack from h…   199\njupyter/base-notebook              Base image for Jupyter Notebook stacks from …   237\njupyter/nbviewer                   Jupyter Notebook Viewer                         34\njupyter/r-notebook                 R Jupyter Notebook Stack from https://github…   62\njupyter/repo2docker                Turn git repositories into Jupyter enabled D…   22\njupyter/docker-stacks-foundation   Tiny base image on which Jupyter apps can be…   6\njupyter/demo                       (DEPRECATED) Demo of the IPython/Jupyter Not…   16\njupyter/julia-notebook             Julia Jupyter Notebook Stack from https://gi…   4\n\n\nStars indicate the popularity of the image.\nOfficial images are those in the root namespace."
  },
  {
    "objectID": "applied_lec1.html#docker-images-18",
    "href": "applied_lec1.html#docker-images-18",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nDownloading images\n\n\nThere are two ways to download images.\n\nExplicitly, with docker pull.\nImplicitly, when executing docker run and the image is not found locally."
  },
  {
    "objectID": "applied_lec1.html#docker-images-19",
    "href": "applied_lec1.html#docker-images-19",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nPulling an image\n\n\n$ docker pull debian:jessie\njessie: Pulling from library/debian\nf24aff4096a5: Pull complete\nDigest: sha256:32ad5050caffb2c7e969dac873bce2c370015c2256ff984b70c1c08b3a2816a0\nStatus: Downloaded newer image for debian:jessie\ndocker.io/library/debian:jessie\n\nAs seen previously, images are made up of layers.\nDocker has downloaded all the necessary layers.\nIn this example, :jessie indicates which exact version of Debian we would like.\n\nIt is a version tag."
  },
  {
    "objectID": "applied_lec1.html#docker-images-20",
    "href": "applied_lec1.html#docker-images-20",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nImage tags\n\n\nImages can have tags.\n\nTags define image versions or variants.\ndocker pull ubuntu will refer to ubuntu:latest.\nThe :latest tag is generally updated often."
  },
  {
    "objectID": "applied_lec1.html#docker-images-21",
    "href": "applied_lec1.html#docker-images-21",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nDon’t specify tags:\n\n\n\nWhen doing rapid testing and prototyping.\nWhen experimenting.\nWhen you want the latest version.\n\n\n\n\n\n\n\nDo specify tags:\n\n\n\nWhen recording a procedure into a script.\nWhen going to production.\nTo ensure that the same version will be used everywhere.\nTo ensure repeatability later.\nThis is similar to what we would do with pip install, npm install, etc."
  },
  {
    "objectID": "applied_lec1.html#docker-images-22",
    "href": "applied_lec1.html#docker-images-22",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nMulti-arch images\n\n\nAn image can support multiple architectures\nMore precisely, a specific tag in a given repository can have either:\n\na single manifest referencing an image for a single architecture\na manifest list (or fat manifest) referencing multiple images\n\nIn a manifest list, each image is identified by a combination of:\n\nos (linux, windows)\narchitecture (amd64, arm, arm64…)\noptional fields like variant (for arm and arm64), os.version (for windows)"
  },
  {
    "objectID": "applied_lec1.html#docker-images-23",
    "href": "applied_lec1.html#docker-images-23",
    "title": "Intro to Docker",
    "section": "Docker Images",
    "text": "Docker Images\n\n\n\nWorking with multi-arch images\n\n\n\nThe Docker Engine will pull “native” images when available (images matching its own os/architecture/variant)\nWe can ask for a specific image platform with --platform\nThe Docker Engine can run non-native images thanks to QEMU+binfmt (automatically on Docker Desktop; with a bit of setup on Linux)"
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-1",
    "href": "applied_lec1.html#building-images-interactively-1",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nWhat we will do\n\n\nWe will create our first container image. It will be a basic distribution image, but we will pre-install the package figlet.\nWe will:\n\nCreate a container from a base image.\nInstall software manually in the container, and turn it into a new image.\nLearn about new commands:\n\ndocker commit\ndocker tag\nand docker diff."
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-2",
    "href": "applied_lec1.html#building-images-interactively-2",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nPlan\n\n\n\nCreate a container (with docker run) using our base distro of choice.\nRun a bunch of commands to install and set up our software in the container.\n(Optionally) review changes in the container with docker diff.\nTurn the container into a new image with docker commit.\n(Optionally) add tags to the image with docker tag."
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-3",
    "href": "applied_lec1.html#building-images-interactively-3",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nSetting up container\n\n\n\nStart an Ubuntu container:\n\n$ docker run -it ubuntu\nroot@65d17729ff6e:/#\n\n\nRun the commands:\n\n\napt-get update to refresh the list of packages available to install.\napt-get install figlet to install the program we are interested in."
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-4",
    "href": "applied_lec1.html#building-images-interactively-4",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nSetting up container\n\n\nroot@65d17729ff6e:/# apt-get update\nGet:1 http://ports.ubuntu.com/ubuntu-ports noble InRelease [256 kB]\n...\nGet:18 http://ports.ubuntu.com/ubuntu-ports noble-security/restricted arm64 Packages [3119 kB]\nFetched 34.7 MB in 2s (14.7 MB/s)\nReading package lists... Done\nroot@65d17729ff6e:/# apt-get install figlet\n...\nAfter this operation, 745 kB of additional disk space will be used.\nGet:1 http://ports.ubuntu.com/ubuntu-ports noble/universe arm64 figlet arm64 2.2.5-3 [130 kB]\nUnpacking figlet (2.2.5-3) ...\nSetting up figlet (2.2.5-3) ...\n...\nroot@65d17729ff6e:/# exit"
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-5",
    "href": "applied_lec1.html#building-images-interactively-5",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nCheck changes\n\n\n\nType exit at the container prompt to leave the interactive session.\nNow let’s run docker diff to see the difference between the base image and our container\n\ndocker diff 65d1\nC /usr\nC /usr/bin\nA /usr/bin/chkfont\nA /usr/bin/figlist\nA /usr/bin/figlet-figlet\nA /usr/bin/figlet\nA /usr/bin/showfigfonts\nC /usr/share\nC /usr/share/doc\nA /usr/share/doc/figlet\nA /usr/share/doc/figlet/examples\nA /usr/share/doc/figlet/changelog.Debian.gz\nA /usr/share/doc/figlet/copyright\nA /usr/share/emacs\nA /usr/share/emacs/site-lisp\nA /usr/share/emacs/site-lisp/figlet\n..."
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-6",
    "href": "applied_lec1.html#building-images-interactively-6",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nDocker tracks filesystem changes\n\n\n\nAn image is read-only.\nWhen we make changes, they happen in a copy of the image.\nDocker can show the difference between the image, and its copy.\nFor performance, Docker uses copy-on-write systems. (i.e. starting a container based on a big image doesn’t incur a huge copy.)\nContainers can also be started in read-only mode (their root filesystem will be read-only, but they can still have read-write data volumes)"
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-7",
    "href": "applied_lec1.html#building-images-interactively-7",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nCommitting changes\n\n\nThe docker commit command will:\n\nCreate a new layer with those changes\nAnd a new image using this new layer.\n\n$ docker commit 65d1\nsha256:289e61ad4776d701b9133249e91c481f6597b4a178f050d1bcc5df171a2a5bec\nThe output of the docker commit command will be the ID for your newly created image. We can use it as an argument to docker run."
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-8",
    "href": "applied_lec1.html#building-images-interactively-8",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nTesting new image\n\n\n$ docker run -it 289e6\nroot@e88f50c79e4c:/# figlet whazzzuuuppp\n          _\n__      _| |__   __ _ _____________   _ _   _ _   _ _ __  _ __  _ __\n\\ \\ /\\ / / '_ \\ / _` |_  /_  /_  / | | | | | | | | | '_ \\| '_ \\| '_ \\\n \\ V  V /| | | | (_| |/ / / / / /| |_| | |_| | |_| | |_) | |_) | |_) |\n  \\_/\\_/ |_| |_|\\__,_/___/___/___|\\__,_|\\__,_|\\__,_| .__/| .__/| .__/\n                                                   |_|   |_|   |_|\nroot@e88f50c79e4c:/#"
  },
  {
    "objectID": "applied_lec1.html#building-images-interactively-9",
    "href": "applied_lec1.html#building-images-interactively-9",
    "title": "Intro to Docker",
    "section": "Building images interactively",
    "text": "Building images interactively\n\n\n\nTagging images\n\n\nReferring to an image by its ID is not convenient. Let’s tag it instead.\nWe can use the tag command:\n$ docker tag 289e6 figlet\nBut we can also specify the tag as an extra argument to commit:\n$ docker commit 65d1 figlet\nsha256:b2e5078491301fbba4fdb585e2e7a826998e438dd698d1c96547df72ce22f470\nAnd then run it using its tag:\n$ docker run -it figlet"
  },
  {
    "objectID": "applied_lec1.html#dockerfile-1",
    "href": "applied_lec1.html#dockerfile-1",
    "title": "Intro to Docker",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nWhat is it?\n\n\nAn automated way to construct images, as opposed to manually executing commands in a shell.\n\nA Dockerfile is a build recipe for a Docker image.\nIt contains a series of instructions telling Docker how an image is constructed.\nThe docker build command builds an image from a Dockerfile."
  },
  {
    "objectID": "applied_lec1.html#dockerfile-2",
    "href": "applied_lec1.html#dockerfile-2",
    "title": "Intro to Docker",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nCreating a Dockerfile\n\n\nOur Dockerfile must be in a new, empty directory.\nCreate a directory to hold our Dockerfile.\n$ mkdir myimage\nCreate a Dockerfile inside this directory.\n$ cd myimage\n$ nvim Dockerfile\nYou can use whatever editor you like."
  },
  {
    "objectID": "applied_lec1.html#dockerfile-3",
    "href": "applied_lec1.html#dockerfile-3",
    "title": "Intro to Docker",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nContents\n\n\nFROM ubuntu\nRUN apt-get update\nRUN apt-get install figlet\n\nFROM indicates the base image for our build.\nEach RUN line will be executed by Docker during the build.\nOur RUN commands must be non-interactive. (No input can be provided to Docker during the build.)\n\nIn many cases, we will add the -y flag to apt-get."
  },
  {
    "objectID": "applied_lec1.html#dockerfile-4",
    "href": "applied_lec1.html#dockerfile-4",
    "title": "Intro to Docker",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nBuilding\n\n\nSave our file, then execute:\n$ docker build -t figlet .\n\n-t indicates the tag to apply to the image.\n. indicates the location of the build context. We will talk more about the build context later. To keep things simple for now: this is the directory where our Dockerfile is located."
  },
  {
    "objectID": "applied_lec1.html#dockerfile-5",
    "href": "applied_lec1.html#dockerfile-5",
    "title": "Intro to Docker",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nFull log\n\n\n[+] Building 9.3s (8/8) FINISHED                                                                                                                                                                docker:desktop-linux\n =&gt; [internal] load build definition from Dockerfile                                                                                                                                                            0.0s\n =&gt; =&gt; transferring dockerfile: 95B                                                                                                                                                                             0.0s\n =&gt; [internal] load metadata for docker.io/library/ubuntu:latest                                                                                                                                                0.1s\n =&gt; [internal] load .dockerignore                                                                                                                                                                               0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                                                                                 0.0s\n =&gt; [1/3] FROM docker.io/library/ubuntu:latest@sha256:353675e2a41babd526e2b837d7ec780c2a05bca0164f7ea5dbbd433d21d166fc                                                                                          3.5s\n =&gt; =&gt; resolve docker.io/library/ubuntu:latest@sha256:353675e2a41babd526e2b837d7ec780c2a05bca0164f7ea5dbbd433d21d166fc                                                                                          3.4s\n =&gt; [auth] library/ubuntu:pull token for registry-1.docker.io                                                                                                                                                   0.0s\n =&gt; [2/3] RUN apt-get update                                                                                                                                                                                    2.9s\n =&gt; [3/3] RUN apt-get install figlet                                                                                                                                                                            1.3s\n =&gt; exporting to image                                                                                                                                                                                          1.6s\n =&gt; =&gt; exporting layers                                                                                                                                                                                         1.3s\n =&gt; =&gt; exporting manifest sha256:0d82650ef2fb2b1107ee3332b4e9167a3da20e4368b8a74764827a3091819ec9                                                                                                               0.0s\n =&gt; =&gt; exporting config sha256:474cb85cf40523c5ac0e1d337e8d97e2d3f9a3e24407a8680fd033d1820e9644                                                                                                                 0.0s\n =&gt; =&gt; exporting attestation manifest sha256:8c2c8be4e7e3253c54cdc1b68d50dd79dad45c73625a3517c7e79ec4ae2220ea                                                                                                   0.0s\n =&gt; =&gt; exporting manifest list sha256:6447daabcad8e301a8e7920b201c41f4ffc9889445c10d06050c085fd73ab8ea                                                                                                          0.0s\n =&gt; =&gt; naming to docker.io/library/figlet:latest                                                                                                                                                                0.0s\n =&gt; =&gt; unpacking to docker.io/library/figlet:latest"
  },
  {
    "objectID": "applied_lec1.html#dockerfile-6",
    "href": "applied_lec1.html#dockerfile-6",
    "title": "Intro to Docker",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nSteps\n\n\n\nBuildKit transfers the Dockerfile and the build context (these are the first two [internal] stages)\nThen it executes the steps defined in the Dockerfile ([1/3], [2/3], [3/3])\nFinally, it exports the result of the build (image definition + collection of layers)\n\n\n\n\n\n\nIn as CI, the output will be different. Revert to old output with --progress=plain."
  },
  {
    "objectID": "applied_lec1.html#dockerfile-7",
    "href": "applied_lec1.html#dockerfile-7",
    "title": "Intro to Docker",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nCaching system\n\n\n\nAfter each build step, Docker takes a snapshot of the resulting image.\nBefore executing a step, Docker checks if it has already built the same sequence.\nDocker uses the exact strings defined in your Dockerfile, so:\n\nRUN apt-get install figlet cowsay\nis different from\nRUN apt-get install cowsay figlet\n\n\nRUN apt-get update is not re-executed when the mirrors are updated\nYou can force a rebuild with docker build --no-cache ...."
  },
  {
    "objectID": "applied_lec1.html#dockerfile-8",
    "href": "applied_lec1.html#dockerfile-8",
    "title": "Intro to Docker",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nResult\n\n\nIdentical to manual:\ndocker run -it figlet\nroot@25ac5862b142:/# figlet hey\n _\n| |__   ___ _   _\n| '_ \\ / _ \\ | | |\n| | | |  __/ |_| |\n|_| |_|\\___|\\__, |\n            |___/\nroot@25ac5862b142:/#"
  },
  {
    "objectID": "applied_lec1.html#dockerfile-9",
    "href": "applied_lec1.html#dockerfile-9",
    "title": "Intro to Docker",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nImage history\n\n\n\nThe history command lists all the layers composing an image.\nFor each layer, it shows its creation time, size, and creation command.\nWhen an image was built with a Dockerfile, each layer corresponds to a line of the Dockerfile.\n\ndocker history figlet\nIMAGE          CREATED        CREATED BY                                      SIZE      COMMENT\n6447daabcad8   19 hours ago   RUN /bin/sh -c apt-get install figlet # buil…   1.34MB    buildkit.dockerfile.v0\n&lt;missing&gt;      19 hours ago   RUN /bin/sh -c apt-get update # buildkit        56.4MB    buildkit.dockerfile.v0\n&lt;missing&gt;      12 days ago    /bin/sh -c #(nop)  CMD [\"/bin/bash\"]            0B\n&lt;missing&gt;      12 days ago    /bin/sh -c #(nop) ADD file:4e55519deacaaab35…   110MB\n&lt;missing&gt;      12 days ago    /bin/sh -c #(nop)  LABEL org.opencontainers.…   0B\n&lt;missing&gt;      12 days ago    /bin/sh -c #(nop)  LABEL org.opencontainers.…   0B\n&lt;missing&gt;      12 days ago    /bin/sh -c #(nop)  ARG LAUNCHPAD_BUILD_ARCH     0B\n&lt;missing&gt;      12 days ago    /bin/sh -c #(nop)  ARG RELEASE                  0B"
  },
  {
    "objectID": "applied_lec1.html#dockerfile-10",
    "href": "applied_lec1.html#dockerfile-10",
    "title": "Intro to Docker",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\n\n\n\nWhy sh -c?\n\n\n\nOn UNIX, to start a new program, we need two system calls:\n\nfork(), to create a new child process;\nexecve(), to replace the new child process with the program to run.\n\nConceptually, execve() works like this:\n\nexecve(program, [list, of, arguments])\n\nWhen we run a command, e.g. ls -l /tmp, something needs to parse the command. (i.e. split the program and its arguments into a list.)\nThe shell is usually doing that. (It also takes care of expanding environment variables and special things like ~.)"
  },
  {
    "objectID": "applied_lec1.html#dockerfile-11",
    "href": "applied_lec1.html#dockerfile-11",
    "title": "Intro to Docker",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nExec syntax\n\n\nDocker can parse the command by itself.\nInstead of plain string, or shell syntax:\nRUN apt-get install figlet\nwe can use JSON list, or exec syntax:\nRUN [\"apt-get\", \"install\", \"figlet\"]"
  },
  {
    "objectID": "applied_lec1.html#dockerfile-12",
    "href": "applied_lec1.html#dockerfile-12",
    "title": "Intro to Docker",
    "section": "Dockerfile",
    "text": "Dockerfile\n\n\n\nCheck exec syntax\n\n\n\nChange Dockerfile to\n\nFROM ubuntu\nRUN apt-get update\nRUN [\"apt-get\", \"install\", \"figlet\"]\n\nBuild it:\n\ndocker build -t figlet .\n\nCheck history:\n\n$ docker history figlet\nIMAGE          CREATED         CREATED BY                                      SIZE      COMMENT\n18c1be63d556   4 seconds ago   RUN apt-get install figlet # buildkit           1.34MB    buildkit.dockerfile.v0\n&lt;missing&gt;      19 hours ago    RUN /bin/sh -c apt-get update # buildkit        56.4MB    buildkit.dockerfile.v0\n...\nExact command!"
  },
  {
    "objectID": "applied_lec1.html#cmd-and-entrypoint-1",
    "href": "applied_lec1.html#cmd-and-entrypoint-1",
    "title": "Intro to Docker",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nDefault commands\n\n\nWhen people run our container, we want to greet them with a nice hello message, and using a custom font.\nFor that, we will execute:\nfiglet -f script hello\n-f script tells figlet to use a fancy font.\nhello is the message that we want it to display."
  },
  {
    "objectID": "applied_lec1.html#cmd-and-entrypoint-2",
    "href": "applied_lec1.html#cmd-and-entrypoint-2",
    "title": "Intro to Docker",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nAdding CMD to Dockerfile\n\n\nOur new Dockerfile will look like this:\nFROM ubuntu\nRUN apt-get update\nRUN [\"apt-get\", \"install\", \"figlet\"]\nCMD figlet -f script hello\nCMD defines a default command to run when none is given.\nIt can appear at any point in the file.\nEach CMD will replace and override the previous one. As a result, while you can have multiple CMD lines, it is useless."
  },
  {
    "objectID": "applied_lec1.html#cmd-and-entrypoint-3",
    "href": "applied_lec1.html#cmd-and-entrypoint-3",
    "title": "Intro to Docker",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nBuild and run\n\n\n$ docker build -t figlet .\n[+] Building 3.4s (8/8) FINISHED                                                                                                                                                                docker:desktop-linux\n...\n$ docker run -it figlet\n _          _   _\n| |        | | | |\n| |     _  | | | |  __\n|/ \\   |/  |/  |/  /  \\_\n|   |_/|__/|__/|__/\\__/"
  },
  {
    "objectID": "applied_lec1.html#cmd-and-entrypoint-4",
    "href": "applied_lec1.html#cmd-and-entrypoint-4",
    "title": "Intro to Docker",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nCMD override\n\n\nIf we want to get a shell into our container (instead of running figlet), we just have to specify a different program to run:\n$ docker run -it figlet bash\nroot@3e95f6bafdd9:/#\nWe specified bash.\nIt replaced the value of CMD."
  },
  {
    "objectID": "applied_lec1.html#cmd-and-entrypoint-5",
    "href": "applied_lec1.html#cmd-and-entrypoint-5",
    "title": "Intro to Docker",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nUsing ENTRYPOINT\n\n\nObjective: we want to be able to specify a different message on the command line, while retaining figlet and some default parameters.\nIn other words, we would like to be able to do this:\n$ docker run figlet salut\n           _            \n          | |           \n ,   __,  | |       _|_ \n/ \\_/  |  |/  |   |  |  \n \\/ \\_/|_/|__/ \\_/|_/|_/\nWe will use the ENTRYPOINT verb in Dockerfile."
  },
  {
    "objectID": "applied_lec1.html#cmd-and-entrypoint-6",
    "href": "applied_lec1.html#cmd-and-entrypoint-6",
    "title": "Intro to Docker",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nDockerfile with ENTRYPOINT\n\n\nOur new Dockerfile will look like this:\nFROM ubuntu\nRUN apt-get update\nRUN [\"apt-get\", \"install\", \"figlet\"]\nENTRYPOINT [\"figlet\", \"-f\", \"script\"]\n\nENTRYPOINT defines a base command (and its parameters) for the container.\nThe command line arguments are appended to those parameters.\nLike CMD, ENTRYPOINT can appear anywhere, and replaces the previous value."
  },
  {
    "objectID": "applied_lec1.html#cmd-and-entrypoint-7",
    "href": "applied_lec1.html#cmd-and-entrypoint-7",
    "title": "Intro to Docker",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nBuild\n\n\ndocker build -t figlet .\n[+] Building 0.1s (7/7) FINISHED                                                                                                                                                                docker:desktop-linux\n...\n$ docker run figlet salve\n           _\n          | |\n ,   __,  | |       _\n/ \\_/  |  |/  |  |_|/\n \\/ \\_/|_/|__/ \\/  |__/"
  },
  {
    "objectID": "applied_lec1.html#cmd-and-entrypoint-8",
    "href": "applied_lec1.html#cmd-and-entrypoint-8",
    "title": "Intro to Docker",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nUsing CMD and ENTRYPOING together\n\n\nIf we use ENTRYPOINT and CMD together:\n\nENTRYPOINT will define the base command for our container.\nCMD will define the default parameter(s) for this command.\nThey both have to use JSON syntax."
  },
  {
    "objectID": "applied_lec1.html#cmd-and-entrypoint-9",
    "href": "applied_lec1.html#cmd-and-entrypoint-9",
    "title": "Intro to Docker",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nDockerfile\n\n\nFROM ubuntu\nRUN apt-get update\nRUN [\"apt-get\", \"install\", \"figlet\"]\nENTRYPOINT [\"figlet\", \"-f\", \"script\"]\nCMD [\"hello world\"]\n\nENTRYPOINT defines a base command (and its parameters) for the container.\nIf we don’t specify extra command-line arguments when starting the container, the value of CMD is appended.\nOtherwise, our extra command-line arguments are used instead of CMD."
  },
  {
    "objectID": "applied_lec1.html#cmd-and-entrypoint-10",
    "href": "applied_lec1.html#cmd-and-entrypoint-10",
    "title": "Intro to Docker",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nBuild and run\n\n\n\nBuild:\n\n$ docker build -t myfiglet .\n[+] Building 0.1s (7/7) FINISHED\n...\n\nRun without parameters:\n\n$ docker run myfiglet\n _          _   _                             _\n| |        | | | |                           | |    |\n| |     _  | | | |  __             __   ,_   | |  __|\n|/ \\   |/  |/  |/  /  \\_  |  |  |_/  \\_/  |  |/  /  |\n|   |_/|__/|__/|__/\\__/    \\/ \\/  \\__/    |_/|__/\\_/|_/"
  },
  {
    "objectID": "applied_lec1.html#cmd-and-entrypoint-11",
    "href": "applied_lec1.html#cmd-and-entrypoint-11",
    "title": "Intro to Docker",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nBuild and run\n\n\n\nRun with parameters:\n\n$ docker run myfiglet hey\n _\n| |\n| |     _\n|/ \\   |/  |   |\n|   |_/|__/ \\_/|/\n              /|\n              \\|"
  },
  {
    "objectID": "applied_lec1.html#cmd-and-entrypoint-12",
    "href": "applied_lec1.html#cmd-and-entrypoint-12",
    "title": "Intro to Docker",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nENTRYPOINT override\n\n\nWhat if we want to run a shell in our container?\nWe cannot just do docker run myfiglet bash because that would just tell figlet to display the word “bash.”\nWe use the --entrypoint parameter:\n$ docker run -it --entrypoint bash myfiglet\nroot@0e2f53d52f7d:/#"
  },
  {
    "objectID": "applied_lec1.html#cmd-and-entrypoint-13",
    "href": "applied_lec1.html#cmd-and-entrypoint-13",
    "title": "Intro to Docker",
    "section": "CMD and ENTRYPOINT",
    "text": "CMD and ENTRYPOINT\n\n\n\nCMD and ENTRYPOINT recap\n\n\n\ndocker run myimage executes ENTRYPOINT + CMD\ndocker run myimage args executes ENTRYPOINT + args (overriding CMD)\ndocker run --entrypoint prog myimage executes prog (overriding both)"
  }
]