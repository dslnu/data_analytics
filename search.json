[
  {
    "objectID": "applied_lab1.html",
    "href": "applied_lab1.html",
    "title": "Applied Analytics: Lab 1",
    "section": "",
    "text": "This lab is about environment setup. We will use uv, a package and project manager for Python."
  },
  {
    "objectID": "applied_lab1.html#recommended-reading",
    "href": "applied_lab1.html#recommended-reading",
    "title": "Applied Analytics: Lab 1",
    "section": "Recommended reading:",
    "text": "Recommended reading:\n\nGit Book\nGit Magic"
  },
  {
    "objectID": "applied_lab1.html#basic-steps",
    "href": "applied_lab1.html#basic-steps",
    "title": "Applied Analytics: Lab 1",
    "section": "Basic steps",
    "text": "Basic steps\n\nInstall Git for Windows.\nRun Git Bash.\nCreate a test directory: mkdir test.\nNavigate to test directory: cd test.\nCreate a test file: echo \"test contents\" &gt; test.txt.\nInitialize a Git repo: git init.\nCheck Git repo status: git status.\nAdd the test file: git add test.txt.\nCommit the test file: git commit -m \"Commit message\"\n\n\nGit will complain that it doesn’t know who you are. Update your name and email per it’s instructions.\n\n\nNavigate to GitHub and create your account and repository.\nCopy repository URL and set it in your local repo via git remote add origin &lt;GITHUB_REPO_URL&gt;.\nPush your local changes to remote repo: git push -u origin master.\n\n\nNote that -u option is only needed the first time you’re doing the push in any local branch."
  },
  {
    "objectID": "nb/bigdata_lab2.html",
    "href": "nb/bigdata_lab2.html",
    "title": "Big Data analytics / Applied Data analytics",
    "section": "",
    "text": "import pandas as pd\n\ndd = pd.read_csv('../files/depression_data.csv')\ndd.head()\n\n\n\n\n\n\n\n\nName\nAge\nMarital Status\nEducation Level\nNumber of Children\nSmoking Status\nPhysical Activity Level\nEmployment Status\nIncome\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\n0\nChristine Barker\n31\nMarried\nBachelor's Degree\n2\nNon-smoker\nActive\nUnemployed\n26265.67\nModerate\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n1\nJacqueline Lewis\n55\nMarried\nHigh School\n1\nNon-smoker\nSedentary\nEmployed\n42710.36\nHigh\nUnhealthy\nFair\nYes\nNo\nNo\nYes\n\n\n2\nShannon Church\n78\nWidowed\nMaster's Degree\n1\nNon-smoker\nSedentary\nEmployed\n125332.79\nLow\nUnhealthy\nGood\nNo\nNo\nYes\nNo\n\n\n3\nCharles Jordan\n58\nDivorced\nMaster's Degree\n3\nNon-smoker\nModerate\nUnemployed\n9992.78\nModerate\nModerate\nPoor\nNo\nNo\nNo\nNo\n\n\n4\nMichael Rich\n18\nSingle\nHigh School\n0\nNon-smoker\nSedentary\nUnemployed\n8595.08\nLow\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n\n\n\n\n\n\nimport math\ndef process(x):\n    if x &lt;= 9:\n        return x\n    lst = [int(i) for i in str(x).replace(\".\", \"\")]\n    result = 0\n    for el in lst:\n        result = result + el\n    return process(result)\n\n\nprocess(123688)\n\n1\n\n\n\n%timeit dd.apply(lambda x: process(x[\"Income\"]), axis=1)\n\n931 ms ± 7.41 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n%prun -l 4 dd.apply(lambda x: process(x[\"Income\"]), axis=1)\n\n \n\n\n         12191139 function calls (11268225 primitive calls) in 2.512 seconds\n\n   Ordered by: internal time\n   List reduced from 289 to 4 due to restriction &lt;4&gt;\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n1336657/413768    0.571    0.000    0.632    0.000 3049257351.py:2(process)\n   413769    0.245    0.000    0.544    0.000 apply.py:1242(series_generator)\n   413768    0.212    0.000    0.827    0.000 series.py:1104(__getitem__)\n2482837/2482833    0.159    0.000    0.282    0.000 {built-in method builtins.isinstance}\n\n\n\nimport os\nimport gc\nimport psutil\n\nproc = psutil.Process(os.getpid())\ngc.collect()\ninitial_memory = proc.memory_info().rss\n\n## Allocate memory by creating large lists\nfoo = ['abc' for _ in range(10**7)]\nallocated_memory = proc.memory_info().rss\n\n## Deallocate memory\ndel foo\ngc.collect()\nfinal_memory = proc.memory_info().rss\n\n## Print memory statistics\nincrease = lambda x2, x1: 100.0 * (x2 - x1) / initial_memory\nprint(\"Allocated Memory Increase: %0.2f%%\" % increase(allocated_memory, initial_memory))\nprint(\"Memory After Deletion: %0.2f%%\" % increase(final_memory, allocated_memory))\n\nAllocated Memory Increase: 23.35%\nMemory After Deletion: -10.78%\n\n\n\n%load_ext cython\n\n\n%%cython --annotate\ndef process_cython(x: cython.int):\n    if x &lt;= 9:\n        return x\n    lst = [int(i) for i in str(x).replace(\".\", \"\")]\n    result: cython.int = 0\n    for el in lst:\n        result = result + el\n    return process_cython(result)\n\n\n\nGenerated by Cython 3.1.3\n\n    Yellow lines hint at Python interaction.\n    Click on a line that starts with a \"+\" to see the C code that Cython generated for it.\n\n+1: def process_cython(x: cython.int):\n/* Python wrapper */\nstatic PyObject *__pyx_pw_78_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6_1process_cython(PyObject *__pyx_self, \n#if CYTHON_METH_FASTCALL\nPyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds\n#else\nPyObject *__pyx_args, PyObject *__pyx_kwds\n#endif\n); /*proto*/\nstatic PyMethodDef __pyx_mdef_78_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6_1process_cython = {\"process_cython\", (PyCFunction)(void(*)(void))(__Pyx_PyCFunction_FastCallWithKeywords)__pyx_pw_78_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6_1process_cython, __Pyx_METH_FASTCALL|METH_KEYWORDS, 0};\nstatic PyObject *__pyx_pw_78_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6_1process_cython(PyObject *__pyx_self, \n#if CYTHON_METH_FASTCALL\nPyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds\n#else\nPyObject *__pyx_args, PyObject *__pyx_kwds\n#endif\n) {\n  PyObject *__pyx_v_x = 0;\n  #if !CYTHON_METH_FASTCALL\n  CYTHON_UNUSED Py_ssize_t __pyx_nargs;\n  #endif\n  CYTHON_UNUSED PyObject *const *__pyx_kwvalues;\n  PyObject *__pyx_r = 0;\n  __Pyx_RefNannyDeclarations\n  __Pyx_RefNannySetupContext(\"process_cython (wrapper)\", 0);\n  #if !CYTHON_METH_FASTCALL\n  #if CYTHON_ASSUME_SAFE_SIZE\n  __pyx_nargs = PyTuple_GET_SIZE(__pyx_args);\n  #else\n  __pyx_nargs = PyTuple_Size(__pyx_args); if (unlikely(__pyx_nargs &lt; 0)) return NULL;\n  #endif\n  #endif\n  __pyx_kwvalues = __Pyx_KwValues_FASTCALL(__pyx_args, __pyx_nargs);\n  {\n    PyObject ** const __pyx_pyargnames[] = {&__pyx_mstate_global-&gt;__pyx_n_u_x,0};\n  PyObject* values[1] = {0};\n    const Py_ssize_t __pyx_kwds_len = (__pyx_kwds) ? __Pyx_NumKwargs_FASTCALL(__pyx_kwds) : 0;\n    if (unlikely(__pyx_kwds_len) &lt; 0) __PYX_ERR(0, 1, __pyx_L3_error)\n    if (__pyx_kwds_len &gt; 0) {\n      switch (__pyx_nargs) {\n        case  1:\n        values[0] = __Pyx_ArgRef_FASTCALL(__pyx_args, 0);\n        if (!CYTHON_ASSUME_SAFE_MACROS && unlikely(!values[0])) __PYX_ERR(0, 1, __pyx_L3_error)\n        CYTHON_FALLTHROUGH;\n        case  0: break;\n        default: goto __pyx_L5_argtuple_error;\n      }\n      const Py_ssize_t kwd_pos_args = __pyx_nargs;\n      if (__Pyx_ParseKeywords(__pyx_kwds, __pyx_kwvalues, __pyx_pyargnames, 0, values, kwd_pos_args, __pyx_kwds_len, \"process_cython\", 0) &lt; 0) __PYX_ERR(0, 1, __pyx_L3_error)\n      for (Py_ssize_t i = __pyx_nargs; i &lt; 1; i++) {\n        if (unlikely(!values[i])) { __Pyx_RaiseArgtupleInvalid(\"process_cython\", 1, 1, 1, i); __PYX_ERR(0, 1, __pyx_L3_error) }\n      }\n    } else if (unlikely(__pyx_nargs != 1)) {\n      goto __pyx_L5_argtuple_error;\n    } else {\n      values[0] = __Pyx_ArgRef_FASTCALL(__pyx_args, 0);\n      if (!CYTHON_ASSUME_SAFE_MACROS && unlikely(!values[0])) __PYX_ERR(0, 1, __pyx_L3_error)\n    }\n    __pyx_v_x = values[0];\n  }\n  goto __pyx_L6_skip;\n  __pyx_L5_argtuple_error:;\n  __Pyx_RaiseArgtupleInvalid(\"process_cython\", 1, 1, 1, __pyx_nargs); __PYX_ERR(0, 1, __pyx_L3_error)\n  __pyx_L6_skip:;\n  goto __pyx_L4_argument_unpacking_done;\n  __pyx_L3_error:;\n  for (Py_ssize_t __pyx_temp=0; __pyx_temp &lt; (Py_ssize_t)(sizeof(values)/sizeof(values[0])); ++__pyx_temp) {\n    Py_XDECREF(values[__pyx_temp]);\n  }\n  __Pyx_AddTraceback(\"_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6.process_cython\", __pyx_clineno, __pyx_lineno, __pyx_filename);\n  __Pyx_RefNannyFinishContext();\n  return NULL;\n  __pyx_L4_argument_unpacking_done:;\n  __pyx_r = __pyx_pf_78_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6_process_cython(__pyx_self, __pyx_v_x);\n  int __pyx_lineno = 0;\n  const char *__pyx_filename = NULL;\n  int __pyx_clineno = 0;\n\n  /* function exit code */\n  for (Py_ssize_t __pyx_temp=0; __pyx_temp &lt; (Py_ssize_t)(sizeof(values)/sizeof(values[0])); ++__pyx_temp) {\n    Py_XDECREF(values[__pyx_temp]);\n  }\n  __Pyx_RefNannyFinishContext();\n  return __pyx_r;\n}\n\nstatic PyObject *__pyx_pf_78_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6_process_cython(CYTHON_UNUSED PyObject *__pyx_self, PyObject *__pyx_v_x) {\n  PyObject *__pyx_v_lst = NULL;\n  PyObject *__pyx_v_result = NULL;\n  PyObject *__pyx_v_el = NULL;\n  Py_UCS4 __pyx_7genexpr__pyx_v_i;\n  PyObject *__pyx_r = NULL;\n/* … */\n  __pyx_t_2 = __Pyx_PyDict_NewPresized(1); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_2);\n  if (PyDict_SetItem(__pyx_t_2, __pyx_mstate_global-&gt;__pyx_n_u_x, __pyx_mstate_global-&gt;__pyx_kp_u_cython_int) &lt; 0) __PYX_ERR(0, 1, __pyx_L1_error)\n  __pyx_t_3 = __Pyx_CyFunction_New(&__pyx_mdef_78_cython_magic_bd7b64be6d8b40f63beab335e9e6f2740716b69b162d86cefee16936a01b95b6_1process_cython, 0, __pyx_mstate_global-&gt;__pyx_n_u_process_cython, NULL, __pyx_mstate_global-&gt;__pyx_n_u_cython_magic_bd7b64be6d8b40f63b, __pyx_mstate_global-&gt;__pyx_d, ((PyObject *)__pyx_mstate_global-&gt;__pyx_codeobj_tab[0])); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_3);\n  __Pyx_CyFunction_SetAnnotationsDict(__pyx_t_3, __pyx_t_2);\n  __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;\n  if (PyDict_SetItem(__pyx_mstate_global-&gt;__pyx_d, __pyx_mstate_global-&gt;__pyx_n_u_process_cython, __pyx_t_3) &lt; 0) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;\n  __pyx_t_3 = __Pyx_PyDict_NewPresized(0); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_3);\n  if (PyDict_SetItem(__pyx_mstate_global-&gt;__pyx_d, __pyx_mstate_global-&gt;__pyx_n_u_test, __pyx_t_3) &lt; 0) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;\n+2:     if x &lt;= 9:\n  __pyx_t_1 = PyObject_RichCompare(__pyx_v_x, __pyx_mstate_global-&gt;__pyx_int_9, Py_LE); __Pyx_XGOTREF(__pyx_t_1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 2, __pyx_L1_error)\n  __pyx_t_2 = __Pyx_PyObject_IsTrue(__pyx_t_1); if (unlikely((__pyx_t_2 &lt; 0))) __PYX_ERR(0, 2, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n  if (__pyx_t_2) {\n/* … */\n  }\n+3:         return x\n    __Pyx_XDECREF(__pyx_r);\n    __Pyx_INCREF(__pyx_v_x);\n    __pyx_r = __pyx_v_x;\n    goto __pyx_L0;\n+4:     lst = [int(i) for i in str(x).replace(\".\", \"\")]\n  { /* enter inner scope */\n    __pyx_t_1 = PyList_New(0); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 4, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_1);\n    __pyx_t_3 = __Pyx_PyObject_Unicode(__pyx_v_x); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 4, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_3);\n    __pyx_t_4 = PyUnicode_Replace(((PyObject*)__pyx_t_3), __pyx_mstate_global-&gt;__pyx_kp_u_, __pyx_mstate_global-&gt;__pyx_kp_u__2, -1L); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 4, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_4);\n    __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;\n    __pyx_t_9 = __Pyx_init_unicode_iteration(__pyx_t_4, (&__pyx_t_6), (&__pyx_t_7), (&__pyx_t_8)); if (unlikely(__pyx_t_9 == ((int)-1))) __PYX_ERR(0, 4, __pyx_L1_error)\n    for (__pyx_t_10 = 0; __pyx_t_10 &lt; __pyx_t_6; __pyx_t_10++) {\n      __pyx_t_5 = __pyx_t_10;\n      __pyx_7genexpr__pyx_v_i = __Pyx_PyUnicode_READ(__pyx_t_8, __pyx_t_7, __pyx_t_5);\n      __pyx_t_11 = NULL;\n      __Pyx_INCREF((PyObject *)(&PyLong_Type));\n      __pyx_t_12 = ((PyObject *)(&PyLong_Type)); \n      __pyx_t_13 = __Pyx_PyUnicode_FromOrdinal(__pyx_7genexpr__pyx_v_i); if (unlikely(!__pyx_t_13)) __PYX_ERR(0, 4, __pyx_L1_error)\n      __Pyx_GOTREF(__pyx_t_13);\n      __pyx_t_14 = 1;\n      {\n        PyObject *__pyx_callargs[2] = {__pyx_t_11, __pyx_t_13};\n        __pyx_t_3 = __Pyx_PyObject_FastCall(__pyx_t_12, __pyx_callargs+__pyx_t_14, (2-__pyx_t_14) | (__pyx_t_14*__Pyx_PY_VECTORCALL_ARGUMENTS_OFFSET));\n        __Pyx_XDECREF(__pyx_t_11); __pyx_t_11 = 0;\n        __Pyx_DECREF(__pyx_t_13); __pyx_t_13 = 0;\n        __Pyx_DECREF(__pyx_t_12); __pyx_t_12 = 0;\n        if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 4, __pyx_L1_error)\n        __Pyx_GOTREF(__pyx_t_3);\n      }\n      if (unlikely(__Pyx_ListComp_Append(__pyx_t_1, (PyObject*)__pyx_t_3))) __PYX_ERR(0, 4, __pyx_L1_error)\n      __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;\n    }\n    __Pyx_DECREF(__pyx_t_4); __pyx_t_4 = 0;\n  } /* exit inner scope */\n  __pyx_v_lst = ((PyObject*)__pyx_t_1);\n  __pyx_t_1 = 0;\n+5:     result: cython.int = 0\n  __Pyx_INCREF(__pyx_mstate_global-&gt;__pyx_int_0);\n  __pyx_v_result = __pyx_mstate_global-&gt;__pyx_int_0;\n+6:     for el in lst:\n  __pyx_t_1 = __pyx_v_lst; __Pyx_INCREF(__pyx_t_1);\n  __pyx_t_6 = 0;\n  for (;;) {\n    {\n      Py_ssize_t __pyx_temp = __Pyx_PyList_GET_SIZE(__pyx_t_1);\n      #if !CYTHON_ASSUME_SAFE_SIZE\n      if (unlikely((__pyx_temp &lt; 0))) __PYX_ERR(0, 6, __pyx_L1_error)\n      #endif\n      if (__pyx_t_6 &gt;= __pyx_temp) break;\n    }\n    __pyx_t_4 = __Pyx_PyList_GetItemRef(__pyx_t_1, __pyx_t_6);\n    ++__pyx_t_6;\n    if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 6, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_4);\n    __Pyx_XDECREF_SET(__pyx_v_el, __pyx_t_4);\n    __pyx_t_4 = 0;\n/* … */\n  }\n  __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n+7:         result = result + el\n    __pyx_t_4 = PyNumber_Add(__pyx_v_result, __pyx_v_el); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 7, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_4);\n    __Pyx_DECREF_SET(__pyx_v_result, __pyx_t_4);\n    __pyx_t_4 = 0;\n+8:     return process_cython(result)\n  __Pyx_XDECREF(__pyx_r);\n  __pyx_t_4 = NULL;\n  __Pyx_GetModuleGlobalName(__pyx_t_3, __pyx_mstate_global-&gt;__pyx_n_u_process_cython); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 8, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_3);\n  __pyx_t_14 = 1;\n  #if CYTHON_UNPACK_METHODS\n  if (unlikely(PyMethod_Check(__pyx_t_3))) {\n    __pyx_t_4 = PyMethod_GET_SELF(__pyx_t_3);\n    assert(__pyx_t_4);\n    PyObject* __pyx__function = PyMethod_GET_FUNCTION(__pyx_t_3);\n    __Pyx_INCREF(__pyx_t_4);\n    __Pyx_INCREF(__pyx__function);\n    __Pyx_DECREF_SET(__pyx_t_3, __pyx__function);\n    __pyx_t_14 = 0;\n  }\n  #endif\n  {\n    PyObject *__pyx_callargs[2] = {__pyx_t_4, __pyx_v_result};\n    __pyx_t_1 = __Pyx_PyObject_FastCall(__pyx_t_3, __pyx_callargs+__pyx_t_14, (2-__pyx_t_14) | (__pyx_t_14*__Pyx_PY_VECTORCALL_ARGUMENTS_OFFSET));\n    __Pyx_XDECREF(__pyx_t_4); __pyx_t_4 = 0;\n    __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;\n    if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 8, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_1);\n  }\n  __pyx_r = __pyx_t_1;\n  __pyx_t_1 = 0;\n  goto __pyx_L0;\n\n\n\n\n%%cython --annotate\ndef primes(nb_primes: cython.int):\n    i: cython.int\n    p: cython.int[1000] = [0] * 1000\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p: cython.int = 0  # The current number of elements in p.\n    n: cython.int = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.pyx:1:28: Unknown type declaration 'cython.int' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.pyx:2:13: Unknown type declaration 'cython.int' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.pyx:8:17: Unknown type declaration 'cython.int' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.pyx:9:13: Unknown type declaration 'cython.int' in annotation, ignoring\n\n\nContent of stderr:\n/Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.c:4550:17: warning: fallthrough annotation in unreachable code [-Wunreachable-code-fallthrough]\n 4550 |                 CYTHON_FALLTHROUGH;\n      |                 ^\n/Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.c:512:34: note: expanded from macro 'CYTHON_FALLTHROUGH'\n  512 |       #define CYTHON_FALLTHROUGH __attribute__((fallthrough))\n      |                                  ^\n/Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.c:4561:17: warning: fallthrough annotation in unreachable code [-Wunreachable-code-fallthrough]\n 4561 |                 CYTHON_FALLTHROUGH;\n      |                 ^\n/Users/vitvly/.cache/ipython/cython/_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.c:512:34: note: expanded from macro 'CYTHON_FALLTHROUGH'\n  512 |       #define CYTHON_FALLTHROUGH __attribute__((fallthrough))\n      |                                  ^\n2 warnings generated.\nld: warning: search path 'Modules/_hacl' not found\n\n\n\n\nGenerated by Cython 3.1.3\n\n    Yellow lines hint at Python interaction.\n    Click on a line that starts with a \"+\" to see the C code that Cython generated for it.\n\n+01: def primes(nb_primes: cython.int):\n/* Python wrapper */\nstatic PyObject *__pyx_pw_78_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1_1primes(PyObject *__pyx_self, \n#if CYTHON_METH_FASTCALL\nPyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds\n#else\nPyObject *__pyx_args, PyObject *__pyx_kwds\n#endif\n); /*proto*/\nstatic PyMethodDef __pyx_mdef_78_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1_1primes = {\"primes\", (PyCFunction)(void(*)(void))(__Pyx_PyCFunction_FastCallWithKeywords)__pyx_pw_78_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1_1primes, __Pyx_METH_FASTCALL|METH_KEYWORDS, 0};\nstatic PyObject *__pyx_pw_78_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1_1primes(PyObject *__pyx_self, \n#if CYTHON_METH_FASTCALL\nPyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds\n#else\nPyObject *__pyx_args, PyObject *__pyx_kwds\n#endif\n) {\n  PyObject *__pyx_v_nb_primes = 0;\n  #if !CYTHON_METH_FASTCALL\n  CYTHON_UNUSED Py_ssize_t __pyx_nargs;\n  #endif\n  CYTHON_UNUSED PyObject *const *__pyx_kwvalues;\n  PyObject *__pyx_r = 0;\n  __Pyx_RefNannyDeclarations\n  __Pyx_RefNannySetupContext(\"primes (wrapper)\", 0);\n  #if !CYTHON_METH_FASTCALL\n  #if CYTHON_ASSUME_SAFE_SIZE\n  __pyx_nargs = PyTuple_GET_SIZE(__pyx_args);\n  #else\n  __pyx_nargs = PyTuple_Size(__pyx_args); if (unlikely(__pyx_nargs &lt; 0)) return NULL;\n  #endif\n  #endif\n  __pyx_kwvalues = __Pyx_KwValues_FASTCALL(__pyx_args, __pyx_nargs);\n  {\n    PyObject ** const __pyx_pyargnames[] = {&__pyx_mstate_global-&gt;__pyx_n_u_nb_primes,0};\n  PyObject* values[1] = {0};\n    const Py_ssize_t __pyx_kwds_len = (__pyx_kwds) ? __Pyx_NumKwargs_FASTCALL(__pyx_kwds) : 0;\n    if (unlikely(__pyx_kwds_len) &lt; 0) __PYX_ERR(0, 1, __pyx_L3_error)\n    if (__pyx_kwds_len &gt; 0) {\n      switch (__pyx_nargs) {\n        case  1:\n        values[0] = __Pyx_ArgRef_FASTCALL(__pyx_args, 0);\n        if (!CYTHON_ASSUME_SAFE_MACROS && unlikely(!values[0])) __PYX_ERR(0, 1, __pyx_L3_error)\n        CYTHON_FALLTHROUGH;\n        case  0: break;\n        default: goto __pyx_L5_argtuple_error;\n      }\n      const Py_ssize_t kwd_pos_args = __pyx_nargs;\n      if (__Pyx_ParseKeywords(__pyx_kwds, __pyx_kwvalues, __pyx_pyargnames, 0, values, kwd_pos_args, __pyx_kwds_len, \"primes\", 0) &lt; 0) __PYX_ERR(0, 1, __pyx_L3_error)\n      for (Py_ssize_t i = __pyx_nargs; i &lt; 1; i++) {\n        if (unlikely(!values[i])) { __Pyx_RaiseArgtupleInvalid(\"primes\", 1, 1, 1, i); __PYX_ERR(0, 1, __pyx_L3_error) }\n      }\n    } else if (unlikely(__pyx_nargs != 1)) {\n      goto __pyx_L5_argtuple_error;\n    } else {\n      values[0] = __Pyx_ArgRef_FASTCALL(__pyx_args, 0);\n      if (!CYTHON_ASSUME_SAFE_MACROS && unlikely(!values[0])) __PYX_ERR(0, 1, __pyx_L3_error)\n    }\n    __pyx_v_nb_primes = values[0];\n  }\n  goto __pyx_L6_skip;\n  __pyx_L5_argtuple_error:;\n  __Pyx_RaiseArgtupleInvalid(\"primes\", 1, 1, 1, __pyx_nargs); __PYX_ERR(0, 1, __pyx_L3_error)\n  __pyx_L6_skip:;\n  goto __pyx_L4_argument_unpacking_done;\n  __pyx_L3_error:;\n  for (Py_ssize_t __pyx_temp=0; __pyx_temp &lt; (Py_ssize_t)(sizeof(values)/sizeof(values[0])); ++__pyx_temp) {\n    Py_XDECREF(values[__pyx_temp]);\n  }\n  __Pyx_AddTraceback(\"_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1.primes\", __pyx_clineno, __pyx_lineno, __pyx_filename);\n  __Pyx_RefNannyFinishContext();\n  return NULL;\n  __pyx_L4_argument_unpacking_done:;\n  __pyx_r = __pyx_pf_78_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1_primes(__pyx_self, __pyx_v_nb_primes);\n  int __pyx_lineno = 0;\n  const char *__pyx_filename = NULL;\n  int __pyx_clineno = 0;\n\n  /* function exit code */\n  for (Py_ssize_t __pyx_temp=0; __pyx_temp &lt; (Py_ssize_t)(sizeof(values)/sizeof(values[0])); ++__pyx_temp) {\n    Py_XDECREF(values[__pyx_temp]);\n  }\n  __Pyx_RefNannyFinishContext();\n  return __pyx_r;\n}\n\nstatic PyObject *__pyx_pf_78_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1_primes(CYTHON_UNUSED PyObject *__pyx_self, PyObject *__pyx_v_nb_primes) {\n  PyObject *__pyx_v_i = 0;\n  PyObject *__pyx_v_p = NULL;\n  PyObject *__pyx_v_len_p = NULL;\n  PyObject *__pyx_v_n = NULL;\n  PyObject *__pyx_v_result_as_list = NULL;\n  PyObject *__pyx_7genexpr__pyx_v_prime = NULL;\n  PyObject *__pyx_r = NULL;\n  __Pyx_INCREF(__pyx_v_nb_primes);\n/* … */\n  __pyx_t_2 = __Pyx_PyDict_NewPresized(1); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_2);\n  if (PyDict_SetItem(__pyx_t_2, __pyx_mstate_global-&gt;__pyx_n_u_nb_primes, __pyx_mstate_global-&gt;__pyx_kp_u_cython_int) &lt; 0) __PYX_ERR(0, 1, __pyx_L1_error)\n  __pyx_t_3 = __Pyx_CyFunction_New(&__pyx_mdef_78_cython_magic_3354ed55144f0f98204f256a085dfc94005bea5ac92946ad6c437e18ea8febb1_1primes, 0, __pyx_mstate_global-&gt;__pyx_n_u_primes, NULL, __pyx_mstate_global-&gt;__pyx_n_u_cython_magic_3354ed55144f0f9820, __pyx_mstate_global-&gt;__pyx_d, ((PyObject *)__pyx_mstate_global-&gt;__pyx_codeobj_tab[0])); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_3);\n  __Pyx_CyFunction_SetAnnotationsDict(__pyx_t_3, __pyx_t_2);\n  __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;\n  if (PyDict_SetItem(__pyx_mstate_global-&gt;__pyx_d, __pyx_mstate_global-&gt;__pyx_n_u_primes, __pyx_t_3) &lt; 0) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;\n  __pyx_t_3 = __Pyx_PyDict_NewPresized(0); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_3);\n  if (PyDict_SetItem(__pyx_mstate_global-&gt;__pyx_d, __pyx_mstate_global-&gt;__pyx_n_u_test, __pyx_t_3) &lt; 0) __PYX_ERR(0, 1, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;\n 02:     i: cython.int\n+03:     p: cython.int[1000] = [0] * 1000\n  __pyx_t_1 = PyList_New(1 * 1000); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 3, __pyx_L1_error)\n  __Pyx_GOTREF(__pyx_t_1);\n  { Py_ssize_t __pyx_temp;\n    for (__pyx_temp=0; __pyx_temp &lt; 0x3E8; __pyx_temp++) {\n      __Pyx_INCREF(__pyx_mstate_global-&gt;__pyx_int_0);\n      __Pyx_GIVEREF(__pyx_mstate_global-&gt;__pyx_int_0);\n      if (__Pyx_PyList_SET_ITEM(__pyx_t_1, __pyx_temp, __pyx_mstate_global-&gt;__pyx_int_0) != (0)) __PYX_ERR(0, 3, __pyx_L1_error);\n    }\n  }\n  __pyx_v_p = ((PyObject*)__pyx_t_1);\n  __pyx_t_1 = 0;\n 04: \n+05:     if nb_primes &gt; 1000:\n  __pyx_t_1 = PyObject_RichCompare(__pyx_v_nb_primes, __pyx_mstate_global-&gt;__pyx_int_1000, Py_GT); __Pyx_XGOTREF(__pyx_t_1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 5, __pyx_L1_error)\n  __pyx_t_2 = __Pyx_PyObject_IsTrue(__pyx_t_1); if (unlikely((__pyx_t_2 &lt; 0))) __PYX_ERR(0, 5, __pyx_L1_error)\n  __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n  if (__pyx_t_2) {\n/* … */\n  }\n+06:         nb_primes = 1000\n    __Pyx_INCREF(__pyx_mstate_global-&gt;__pyx_int_1000);\n    __Pyx_DECREF_SET(__pyx_v_nb_primes, __pyx_mstate_global-&gt;__pyx_int_1000);\n 07: \n+08:     len_p: cython.int = 0  # The current number of elements in p.\n  __Pyx_INCREF(__pyx_mstate_global-&gt;__pyx_int_0);\n  __pyx_v_len_p = __pyx_mstate_global-&gt;__pyx_int_0;\n+09:     n: cython.int = 2\n  __Pyx_INCREF(__pyx_mstate_global-&gt;__pyx_int_2);\n  __pyx_v_n = __pyx_mstate_global-&gt;__pyx_int_2;\n+10:     while len_p &lt; nb_primes:\n  while (1) {\n    __pyx_t_1 = PyObject_RichCompare(__pyx_v_len_p, __pyx_v_nb_primes, Py_LT); __Pyx_XGOTREF(__pyx_t_1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 10, __pyx_L1_error)\n    __pyx_t_2 = __Pyx_PyObject_IsTrue(__pyx_t_1); if (unlikely((__pyx_t_2 &lt; 0))) __PYX_ERR(0, 10, __pyx_L1_error)\n    __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n    if (!__pyx_t_2) break;\n 11:         # Is n prime?\n+12:         for i in p[:len_p]:\n    __Pyx_INCREF(__pyx_v_len_p);\n    __pyx_t_1 = __pyx_v_len_p;\n    __pyx_t_2 = (__pyx_t_1 == Py_None);\n    if (__pyx_t_2) {\n      __pyx_t_3 = PY_SSIZE_T_MAX;\n    } else {\n      __pyx_t_4 = __Pyx_PyIndex_AsSsize_t(__pyx_t_1); if (unlikely((__pyx_t_4 == (Py_ssize_t)-1) && PyErr_Occurred())) __PYX_ERR(0, 12, __pyx_L1_error)\n      __pyx_t_3 = __pyx_t_4;\n    }\n    __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n    __pyx_t_1 = __Pyx_PyList_GetSlice(__pyx_v_p, 0, __pyx_t_3); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 12, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_1);\n    __pyx_t_5 = __pyx_t_1; __Pyx_INCREF(__pyx_t_5);\n    __pyx_t_3 = 0;\n    __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n    for (;;) {\n      {\n        Py_ssize_t __pyx_temp = __Pyx_PyList_GET_SIZE(__pyx_t_5);\n        #if !CYTHON_ASSUME_SAFE_SIZE\n        if (unlikely((__pyx_temp &lt; 0))) __PYX_ERR(0, 12, __pyx_L1_error)\n        #endif\n        if (__pyx_t_3 &gt;= __pyx_temp) break;\n      }\n      __pyx_t_1 = __Pyx_PyList_GetItemRef(__pyx_t_5, __pyx_t_3);\n      ++__pyx_t_3;\n      if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 12, __pyx_L1_error)\n      __Pyx_GOTREF(__pyx_t_1);\n      __Pyx_XDECREF_SET(__pyx_v_i, __pyx_t_1);\n      __pyx_t_1 = 0;\n/* … */\n    }\n    __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;\n    goto __pyx_L9_for_else;\n    __pyx_L7_break:;\n    __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;\n    goto __pyx_L10_for_end;\n    /*else*/ {\n      __pyx_L9_for_else:;\n+13:             if n % i == 0:\n      __pyx_t_1 = PyNumber_Remainder(__pyx_v_n, __pyx_v_i); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 13, __pyx_L1_error)\n      __Pyx_GOTREF(__pyx_t_1);\n      __pyx_t_2 = (__Pyx_PyLong_BoolEqObjC(__pyx_t_1, __pyx_mstate_global-&gt;__pyx_int_0, 0, 0)); if (unlikely((__pyx_t_2 &lt; 0))) __PYX_ERR(0, 13, __pyx_L1_error)\n      __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n      if (__pyx_t_2) {\n/* … */\n      }\n+14:                 break\n        goto __pyx_L7_break;\n 15: \n 16:         # If no break occurred in the loop, we have a prime.\n 17:         else:\n+18:             p[len_p] = n\n      if (unlikely((PyObject_SetItem(__pyx_v_p, __pyx_v_len_p, __pyx_v_n) &lt; 0))) __PYX_ERR(0, 18, __pyx_L1_error)\n+19:             len_p += 1\n      __pyx_t_5 = __Pyx_PyLong_AddObjC(__pyx_v_len_p, __pyx_mstate_global-&gt;__pyx_int_1, 1, 1, 0); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 19, __pyx_L1_error)\n      __Pyx_GOTREF(__pyx_t_5);\n      __Pyx_DECREF_SET(__pyx_v_len_p, __pyx_t_5);\n      __pyx_t_5 = 0;\n    }\n    __pyx_L10_for_end:;\n+20:         n += 1\n    __pyx_t_5 = __Pyx_PyLong_AddObjC(__pyx_v_n, __pyx_mstate_global-&gt;__pyx_int_1, 1, 1, 0); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 20, __pyx_L1_error)\n    __Pyx_GOTREF(__pyx_t_5);\n    __Pyx_DECREF_SET(__pyx_v_n, __pyx_t_5);\n    __pyx_t_5 = 0;\n  }\n 21: \n 22:     # Let's copy the result into a Python list:\n+23:     result_as_list = [prime for prime in p[:len_p]]\n  { /* enter inner scope */\n    __pyx_t_5 = PyList_New(0); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 23, __pyx_L13_error)\n    __Pyx_GOTREF(__pyx_t_5);\n    __Pyx_INCREF(__pyx_v_len_p);\n    __pyx_t_1 = __pyx_v_len_p;\n    __pyx_t_2 = (__pyx_t_1 == Py_None);\n    if (__pyx_t_2) {\n      __pyx_t_3 = PY_SSIZE_T_MAX;\n    } else {\n      __pyx_t_4 = __Pyx_PyIndex_AsSsize_t(__pyx_t_1); if (unlikely((__pyx_t_4 == (Py_ssize_t)-1) && PyErr_Occurred())) __PYX_ERR(0, 23, __pyx_L13_error)\n      __pyx_t_3 = __pyx_t_4;\n    }\n    __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n    __pyx_t_1 = __Pyx_PyList_GetSlice(__pyx_v_p, 0, __pyx_t_3); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 23, __pyx_L13_error)\n    __Pyx_GOTREF(__pyx_t_1);\n    __pyx_t_6 = __pyx_t_1; __Pyx_INCREF(__pyx_t_6);\n    __pyx_t_3 = 0;\n    __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;\n    for (;;) {\n      {\n        Py_ssize_t __pyx_temp = __Pyx_PyList_GET_SIZE(__pyx_t_6);\n        #if !CYTHON_ASSUME_SAFE_SIZE\n        if (unlikely((__pyx_temp &lt; 0))) __PYX_ERR(0, 23, __pyx_L13_error)\n        #endif\n        if (__pyx_t_3 &gt;= __pyx_temp) break;\n      }\n      __pyx_t_1 = __Pyx_PyList_GetItemRef(__pyx_t_6, __pyx_t_3);\n      ++__pyx_t_3;\n      if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 23, __pyx_L13_error)\n      __Pyx_GOTREF(__pyx_t_1);\n      __Pyx_XDECREF_SET(__pyx_7genexpr__pyx_v_prime, __pyx_t_1);\n      __pyx_t_1 = 0;\n      if (unlikely(__Pyx_ListComp_Append(__pyx_t_5, (PyObject*)__pyx_7genexpr__pyx_v_prime))) __PYX_ERR(0, 23, __pyx_L13_error)\n    }\n    __Pyx_DECREF(__pyx_t_6); __pyx_t_6 = 0;\n    __Pyx_XDECREF(__pyx_7genexpr__pyx_v_prime); __pyx_7genexpr__pyx_v_prime = 0;\n    goto __pyx_L17_exit_scope;\n    __pyx_L13_error:;\n    __Pyx_XDECREF(__pyx_7genexpr__pyx_v_prime); __pyx_7genexpr__pyx_v_prime = 0;\n    goto __pyx_L1_error;\n    __pyx_L17_exit_scope:;\n  } /* exit inner scope */\n  __pyx_v_result_as_list = ((PyObject*)__pyx_t_5);\n  __pyx_t_5 = 0;\n+24:     return result_as_list\n  __Pyx_XDECREF(__pyx_r);\n  __Pyx_INCREF(__pyx_v_result_as_list);\n  __pyx_r = __pyx_v_result_as_list;\n  goto __pyx_L0;\n\n\n\n\n%timeit primes(10)\n\n1.94 μs ± 19.9 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n%%cython\ndef f_typed(x: cython.double):\n    return x * (x - 1)\n\ndef integrate_f_typed(a: cython.double, b: cython.double, N: cython.int):\n  i: cython.int\n  s: cython.double\n  dx: cython.double\n  s = 0\n  dx = (b - a) / N\n  for i in range(N):\n    s += f_typed(a + i * dx)\n  return s * dx\n\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.pyx:1:21: Unknown type declaration 'cython.double' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.pyx:4:31: Unknown type declaration 'cython.double' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.pyx:4:49: Unknown type declaration 'cython.double' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.pyx:4:67: Unknown type declaration 'cython.int' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.pyx:5:11: Unknown type declaration 'cython.int' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.pyx:6:11: Unknown type declaration 'cython.double' in annotation, ignoring\nwarning: /Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.pyx:7:12: Unknown type declaration 'cython.double' in annotation, ignoring\n\n\nContent of stderr:\n/Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.c:4663:17: warning: fallthrough annotation in unreachable code [-Wunreachable-code-fallthrough]\n 4663 |                 CYTHON_FALLTHROUGH;\n      |                 ^\n/Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.c:512:34: note: expanded from macro 'CYTHON_FALLTHROUGH'\n  512 |       #define CYTHON_FALLTHROUGH __attribute__((fallthrough))\n      |                                  ^\n/Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.c:4674:17: warning: fallthrough annotation in unreachable code [-Wunreachable-code-fallthrough]\n 4674 |                 CYTHON_FALLTHROUGH;\n      |                 ^\n/Users/vitvly/.cache/ipython/cython/_cython_magic_1195bae15d7f73d98e6e65f310e6446ef19a3abaaf4e064ee6cf87a68dbfdc5c.c:512:34: note: expanded from macro 'CYTHON_FALLTHROUGH'\n  512 |       #define CYTHON_FALLTHROUGH __attribute__((fallthrough))\n      |                                  ^\n2 warnings generated.\nld: warning: search path 'Modules/_hacl' not found\n\n\n\nf_typed(5)\n\n20\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {\n        \"a\": np.random.randn(1000),\n        \"b\": np.random.randn(1000),\n        \"N\": np.random.randint(100, 1000, (1000)),\n        \"x\": \"x\",\n    }\n)\n\n\n%timeit df.apply(lambda x: integrate_f_typed(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n\n24.5 ms ± 76 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "nb/lab4/00_overview.html",
    "href": "nb/lab4/00_overview.html",
    "title": "Welcome to the Dask Tutorial",
    "section": "",
    "text": "Dask is a parallel and distributed computing library that scales the existing Python and PyData ecosystem.\nDask can scale up to your full laptop capacity and out to a cloud cluster."
  },
  {
    "objectID": "nb/lab4/00_overview.html#an-example-dask-computation",
    "href": "nb/lab4/00_overview.html#an-example-dask-computation",
    "title": "Welcome to the Dask Tutorial",
    "section": "An example Dask computation",
    "text": "An example Dask computation\nIn the following lines of code, we’re reading the NYC taxi cab data from 2015 and finding the mean tip amount. Don’t worry about the code, this is just for a quick demonstration. We’ll go over all of this in the next notebook. :)\n\nimport dask.dataframe as dd\nfrom dask.distributed import Client\n\n\nclient = Client()\nclient\n\n\nddf = dd.read_parquet(\n    \"./data/taxi.parquet\",\n    #\"s3://dask-data/nyc-taxi/nyc-2015.parquet/part.*.parquet\",\n    columns=[\"passenger_count\", \"tip_amount\"],\n    storage_options={\"anon\": True},\n)\n\n\nresult = ddf.groupby(\"passenger_count\").tip_amount.mean().compute()\nresult"
  },
  {
    "objectID": "nb/lab4/00_overview.html#what-is-dask",
    "href": "nb/lab4/00_overview.html#what-is-dask",
    "title": "Welcome to the Dask Tutorial",
    "section": "What is Dask?",
    "text": "What is Dask?\nThere are many parts to the “Dask” the project: * Collections/API also known as “core-library”. * Distributed – to create clusters * Intergrations and broader ecosystem\n\nDask Collections\nDask provides multi-core and distributed+parallel execution on larger-than-memory datasets\nWe can think of Dask’s APIs (also called collections) at a high and a low level:\n\n\n\n\nHigh-level collections: Dask provides high-level Array, Bag, and DataFrame collections that mimic NumPy, lists, and pandas but can operate in parallel on datasets that don’t fit into memory.\nLow-level collections: Dask also provides low-level Delayed and Futures collections that give you finer control to build custom parallel and distributed computations.\n\n\n\nDask Cluster\nMost of the times when you are using Dask, you will be using a distributed scheduler, which exists in the context of a Dask cluster. The Dask cluster is structured as:\n\n\n\n\n\nDask Ecosystem\nIn addition to the core Dask library and its distributed scheduler, the Dask ecosystem connects several additional initiatives, including:\n\nDask-ML (parallel scikit-learn-style API)\nDask-image\nDask-cuDF\nDask-sql\nDask-snowflake\nDask-mongo\nDask-bigquery\n\nCommunity libraries that have built-in dask integrations like:\n\nXarray\nXGBoost\nPrefect\nAirflow\n\nDask deployment libraries - Dask-kubernetes - Dask-YARN - Dask-gateway - Dask-cloudprovider - jobqueue\n… When we talk about the Dask project we include all these efforts as part of the community."
  },
  {
    "objectID": "nb/lab4/00_overview.html#dask-use-cases",
    "href": "nb/lab4/00_overview.html#dask-use-cases",
    "title": "Welcome to the Dask Tutorial",
    "section": "Dask Use Cases",
    "text": "Dask Use Cases\nDask is used in multiple fields such as:\n\nGeospatial\nFinance\nAstrophysics\nMicrobiology\nEnvironmental science\n\nCheck out the Dask use cases page that provides a number of sample workflows."
  },
  {
    "objectID": "nb/lab4/00_overview.html#prepare",
    "href": "nb/lab4/00_overview.html#prepare",
    "title": "Welcome to the Dask Tutorial",
    "section": "Prepare",
    "text": "Prepare\n\n1. You should clone this repository\ngit clone http://github.com/dask/dask-tutorial\nand then install necessary packages. There are three different ways to achieve this, pick the one that best suits you, and only pick one option. They are, in order of preference:\n\n\n2a) Create a conda environment (preferred)\nIn the main repo directory\nconda env create -f binder/environment.yml\nconda activate dask-tutorial\n\n\n2b) Install into an existing environment\nYou will need the following core libraries\nconda install -c conda-forge ipycytoscape jupyterlab python-graphviz matplotlib zarr xarray pooch pyarrow s3fs scipy dask distributed dask-labextension\nNote that these options will alter your existing environment, potentially changing the versions of packages you already have installed.\n\n\n2c) Use Python venv\n  - python -m venv venv_dask\n  - pip install \"dask[complete]\"\n  - pip install jupyter\n  - pip install s3fs\n  - pip install graphviz\n  - pip install zarr\n  - pip install ipycytoscape\n  - pip install xarray\n  - pip install pooch\n  - pip install scipy\nGraphviz installation notes: https://pypi.org/project/graphviz/\nPython virtual environments: https://python.land/virtual-environments/virtualenv"
  },
  {
    "objectID": "nb/lab4/00_overview.html#tutorial-structure",
    "href": "nb/lab4/00_overview.html#tutorial-structure",
    "title": "Welcome to the Dask Tutorial",
    "section": "Tutorial Structure",
    "text": "Tutorial Structure\nEach section is a Jupyter notebook. There’s a mixture of text, code, and exercises.\n\nOverview - dask’s place in the universe.\nDataframe - parallelized operations on many pandas dataframes spread across your cluster.\nArray - blocked numpy-like functionality with a collection of numpy arrays spread across your cluster.\nDelayed - the single-function way to parallelize general python code.\nDeployment/Distributed - Dask’s scheduler for clusters, with details of how to view the UI.\nDistributed Futures - non-blocking results that compute asynchronously.\nConclusion\n\nIf you haven’t used Jupyterlab, it’s similar to the Jupyter Notebook. If you haven’t used the Notebook, the quick intro is\n\nThere are two modes: command and edit\nFrom command mode, press Enter to edit a cell (like this markdown cell)\nFrom edit mode, press Esc to change to command mode\nPress shift+enter to execute a cell and move to the next cell.\n\nThe toolbar has commands for executing, converting, and creating cells.\n\nExercise: Print Hello, world!\nEach notebook will have exercises for you to solve. You’ll be given a blank or partially completed cell, followed by a hidden cell with a solution. For example.\nPrint the text “Hello, world!”.\n\n# Your code here\n\nThe next cell has the solution. Click the ellipses to expand the solution, and always make sure to run the solution cell, in case later sections of the notebook depend on the output from the solution.\n\nprint(\"Hello, world!\")\n\n\n\nDon’t forget to shutdown the client\n\nclient.shutdown()"
  },
  {
    "objectID": "nb/lab4/00_overview.html#useful-links",
    "href": "nb/lab4/00_overview.html#useful-links",
    "title": "Welcome to the Dask Tutorial",
    "section": "Useful Links",
    "text": "Useful Links\n\nReference\n\nDocs\nExamples\nCode\nBlog\n\nAsk for help\n\ndask tag on Stack Overflow, for usage questions\ngithub issues for bug reports and feature requests\ndiscourse forum for general, non-bug, questions and discussion\nAttend a live tutorial"
  },
  {
    "objectID": "nb/lab2/lab2.html",
    "href": "nb/lab2/lab2.html",
    "title": "Lab2: Parallelisation",
    "section": "",
    "text": "Previous lab listed some methods of optimizing Pandas work - data storage optimizations, Cython conversion, Numba annotations.\nIn this lab we’ll start looking into parallelisation as a way of optimizing data analysis workflows.\nWhy - because today’s systems are multicore (sometimes very much so).\nCheck yours:\nimport os\nos.cpu_count()\nNote: there are two kinds of parallelism, distributed and shared-memory. Here we’ll talk about shared-memory version.\nDistributed parallelism is when we use multiple machines/VMs for computations."
  },
  {
    "objectID": "nb/lab2/lab2.html#short-intro-to-functional-programming",
    "href": "nb/lab2/lab2.html#short-intro-to-functional-programming",
    "title": "Lab2: Parallelisation",
    "section": "Short intro to functional programming",
    "text": "Short intro to functional programming\nIn functional programming paradigm, functions are first-class citizens of the languages. They can be composed, applied, passed as arguments to another functions, or returned as values.\nFunctional programming is inherently better suited for parallel programs.\nBenefits of functional programming: - minimization of state - no side-effects - pure functions are easier to reason about - and parallelize (!)\nIt is based on lambda calculus. We’ll learn some of its concepts first: - lambda function definition - application and partial application - currying - closures\nFirst off, functions can be values in Python:\n\nadd = lambda x, y: x + y  \nprint (f\"add = {add}\")  \n\nAll variables support some operations on them: addition for integers, or e.g. read_csv on Pandas DataFrame.\nFunction variables support a single operation: application.\n\ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \n  \nk = i + j  \nprint (f\"k = {k}\")  \n  \nz = add (i, j)  \nprint (f\"z = {z}\") \n\nPartial function application is also supported. We’ll use functools package (https://docs.python.org/3/library/functools.html).\n\nfrom functools import partial  \ni = 0  \nj = 1    \nadd = lambda x, y: x + y  \ninc = partial (add, 1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\")  \n\nNote: the code above both passes add as an argument to partial, and stores the return value of partial function into a new variable inc.\nThe lambda in the example above accepts two arguments essentially. This can be simplified by currying:\n\ni = 0  \nj = 1    \nadd = lambda x: lambda y: x + y  \ninc = add(1)  \n  \nii = inc (i)  \njj = inc (j)  \nprint (f\"ii = {ii}, jj = {jj}\") \n\nDefinition for currying: currying is the technique of translating a function that takes multiple arguments into a sequence of families of functions, each taking a single argument.\nAlternatively, we can say that currying is the mean to transform a function of arity n to n functions of arity 1.\nWith currying, we can express partial application without and extra partial function.\nNote: currying relies on closures - functions that can refer to variables defined in surrounding context (like x in add definition above).\nA monstrous example:\n\ndef f_5(a, b, c, d, e):\n    return a + b + c + d + e\n\ndef c_5(a):\n    def c_4(b):\n        def c_3(c):\n            def c_2(d):\n                def c_1(e):\n                    return f_5(a, b, c, d, e)\n                return c_1\n            return c_2\n        return c_3\n    return c_4\n\nHow to use?\n\nf_5(1,2,3,4,5)\n\n\nc_5(1)(2)(3)(4)(5)"
  },
  {
    "objectID": "nb/lab2/lab2.html#map-reduce",
    "href": "nb/lab2/lab2.html#map-reduce",
    "title": "Lab2: Parallelisation",
    "section": "map-reduce",
    "text": "map-reduce\nIn functional programming, we do not iterate - we map. Why? Because map can be parallelised.\nIteration (like a for-loop) is sequential. Mapping is not.\nmap takes its name from a mathematical term for functions - mappings.\nmap’s arguments: - a sequence to iterate on - a function to apply to each element of the sequence.\nmap’s return value: - a processed sequence of the same size as input\nNow, if we want to convert our sequence to a sequence of a different length, or different object altogether, we use reduce.\nreduce’s arguments: - a sequence to iterate on - accumulation seed to start reducing on - a function of two arguments, accumulation result and next element\nUsing above examples, we can easily implement map using list comprehensions as as:\n\nmap = lambda f, l: [f(x) for x in l]\n\nLet’s use it:\n\nint_list = [1,2,3,4,5,6]\ninc_list = map(inc, int_list)\nprint(inc_list)\n\nAnd reduce can be implemented as:\n\ndef reduce_v1 (f, id, s):  \n    n = len(s)  \n    if n == 0:  \n        return id  \n    elif n == 1:  \n        return s[0]  \n    else:  \n        return f(reduce_v1(f, id, s[1:]), s[0]) \n\nUsage example:\n\nadd = lambda x, y: x + y  \nres = reduce_v1(add, 0, int_list)\nprint(res)\n\n\nLimitations and parallelisation types\nSome tasks can be trivially parallelised - these are called embarrassingly parallel. inc is a simple example of this type.\nOther tasks have a degree of interdependence. Consider below example, adapted from the reduce above:\n\n# Sum all elements of int_list\ndef sum(lst):\n    sum = 0\n    \n    for i in range(len(lst)):\n      sum += lst[i]\n\n    return sum\n\nprint(sum(int_list))\n\nHow can it be parallelised? By using a technique called chunking:\n\nchunk1 = int_list[:3]\nchunk2 = int_list[3:]\n\nsum_1 = sum(chunk1)\nsum_2 = sum(chunk2)\n\nresult = sum_1 + sum_2\n\nprint(result)\n\nThen there algorithms that are inherently serial. An example is computation of a Fibonacci sequence.\n\n\nPython’s builtin map\nSuppose we want to square each number in a list. A naive approach would be:\n\nnumber_list = [1,2,3,4]\n\ndef squarev1(lst):\n    squared = [] # sometimes pre-allocation can also work, like [None]*1000\n    for el in lst:\n        squared.append(el*el)\n    return squared\n\nsquarev1(number_list)\n\n\n# Built in map: https://docs.python.org/3/library/functions.html#map\n# Note that we return a map object that has to be converted to a list first\ndef squarev2(lst):\n    return list(map(lambda x: x*x, lst))\n\n\nsquarev2(number_list)\n\n\n\nPython parallel map\nWe need to use multiprocessing library (https://docs.python.org/3/library/multiprocessing.html)\n\nfrom multiprocessing import Pool\n\nThere are limitations though: in order to arrange communication between proceses, Python uses pickling: a method of serializing/deserializing objects so that they can be sent between processes.\nPython cannot pickle closures, nested functions or classes! More info on pickling: https://docs.python.org/3/library/pickle.html\nMoreover, we have to move sqr definition into a separate file (https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror)\n\nimport defs\n\n\ndef squarev3(lst):\n    pool = Pool()\n    return pool.map(defs.sqr, number_list)\n\n\nsquarev3(number_list)\n\n\n\nPathos map\nNote:: however, Dill (https://pypi.org/project/dill/) can overcome this limitation!\nHow to use Dill?\npip3 install pathos\n\npip3 install toolz\n\nfrom pathos.multiprocessing import ProcessPool\n\ndef squarev4(lst):\n    pool = ProcessPool()\n    return pool.map(lambda x: x*x, lst)\n\n\nsquarev4([1,3,5])\n\n\n\nNumpy vectorization\nNumpy automatically vectorizes array operations. However, we can explicitly invoke vectorize():\n\nimport numpy as np\n\nnp_array = np.array(number_list)\n\nvectorized_fn = np.vectorize(lambda x: x*x)\n\nresult = vectorized_fn(np_array)\nprint(result)"
  },
  {
    "objectID": "nb/lab2/lab2.html#exercises",
    "href": "nb/lab2/lab2.html#exercises",
    "title": "Lab2: Parallelisation",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a version of curried function c_5 using lambda syntax.\nWrite a program calculating a factorial using parallelisation and map-reduce.\nWrite a parallelised version of reduce (hint: use chunking).\nCompare performance of Numpy vectorization vs parallel map on a really large array of integers."
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#a-typical-workflow",
    "href": "nb/lab6/03_dask.delayed.html#a-typical-workflow",
    "title": "dask.delayed - parallelize any code",
    "section": "A Typical Workflow",
    "text": "A Typical Workflow\nTypically if a workflow contains a for-loop it can benefit from delayed. The following example outlines a read-transform-write:\nimport dask\n    \n@dask.delayed\ndef process_file(filename):\n    data = read_a_file(filename)\n    data = do_a_transformation(data)\n    destination = f\"results/{filename}\"\n    write_out_data(data, destination)\n    return destination\n\nresults = []\nfor filename in filenames:\n    results.append(process_file(filename))\n    \ndask.compute(results)"
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#basics",
    "href": "nb/lab6/03_dask.delayed.html#basics",
    "title": "dask.delayed - parallelize any code",
    "section": "Basics",
    "text": "Basics\nFirst let’s make some toy functions, inc and add, that sleep for a while to simulate work. We’ll then time running these functions normally.\nIn the next section we’ll parallelize this code.\n\nfrom time import sleep\n\n\ndef inc(x):\n    sleep(1)\n    return x + 1\n\n\ndef add(x, y):\n    sleep(1)\n    return x + y\n\nWe time the execution of this normal code using the %%time magic, which is a special function of the Jupyter Notebook.\n\n%%time\n# This takes three seconds to run because we call each\n# function sequentially, one after the other\n\nx = inc(1)\ny = inc(2)\nz = add(x, y)\n\n\nParallelize with the dask.delayed decorator\nThose two increment calls could be called in parallel, because they are totally independent of one-another.\nWe’ll make the inc and add functions lazy using the dask.delayed decorator. When we call the delayed version by passing the arguments, exactly as before, the original function isn’t actually called yet - which is why the cell execution finishes very quickly. Instead, a delayed object is made, which keeps track of the function to call and the arguments to pass to it.\n\nimport dask\n\n\n@dask.delayed\ndef inc(x):\n    sleep(1)\n    return x + 1\n\n\n@dask.delayed\ndef add(x, y):\n    sleep(1)\n    return x + y\n\n\n%%time\n# This runs immediately, all it does is build a graph\n\nx = inc(1)\ny = inc(2)\nz = add(x, y)\n\nThis ran immediately, since nothing has really happened yet.\nTo get the result, call compute. Notice that this runs faster than the original code.\n\n%%time\n# This actually runs our computation using a local thread pool\n\nz.compute()\n\n\n\nAlternate syntax\nInstead of a decorator, one can use functions instead:\n\nimport dask\n\nx1 = dask.delayed(inc)(1)\n\ny1 = dask.delayed(inc)(2)\n\nz1 = dask.delayed(add)(x1, y1)\n\nz1.compute()\n5\n\nz1.visualize()"
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#what-just-happened",
    "href": "nb/lab6/03_dask.delayed.html#what-just-happened",
    "title": "dask.delayed - parallelize any code",
    "section": "What just happened?",
    "text": "What just happened?\nThe z object is a lazy Delayed object. This object holds everything we need to compute the final result, including references to all of the functions that are required and their inputs and relationship to one-another. We can evaluate the result with .compute() as above or we can visualize the task graph for this value with .visualize().\n\nz\n\n\n# Look at the task graph for `z`\nz.visualize()\n\nNotice that this includes the names of the functions from before, and the logical flow of the outputs of the inc functions to the inputs of add.\n\nSome questions to consider:\n\nWhy did we go from 3s to 2s? Why weren’t we able to parallelize down to 1s?\nWhat would have happened if the inc and add functions didn’t include the sleep(1)? Would Dask still be able to speed up this code?\nWhat if we have multiple outputs or also want to get access to x or y?"
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#exercise-parallelize-a-for-loop",
    "href": "nb/lab6/03_dask.delayed.html#exercise-parallelize-a-for-loop",
    "title": "dask.delayed - parallelize any code",
    "section": "Exercise: Parallelize a for loop",
    "text": "Exercise: Parallelize a for loop\nfor loops are one of the most common things that we want to parallelize. Use dask.delayed on inc and sum to parallelize the computation below:\n\ndata = [1, 2, 3, 4, 5, 6, 7, 8]\n\n\n%%time\n# Sequential code\n\n\ndef inc(x):\n    sleep(1)\n    return x + 1\n\n\nresults = []\nfor x in data:\n    y = inc(x)\n    results.append(y)\n\ntotal = sum(results)\n\n\ntotal\n\n\n%%time\n# Your parallel code here...\n\nHow do the graph visualizations compare with the given solution, compared to a version with the sum function used directly rather than wrapped with delayed? Can you explain the latter version? You might find the result of the following expression illuminating\ninc(1) + inc(2)"
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#exercise-parallelize-a-for-loop-code-with-control-flow",
    "href": "nb/lab6/03_dask.delayed.html#exercise-parallelize-a-for-loop-code-with-control-flow",
    "title": "dask.delayed - parallelize any code",
    "section": "Exercise: Parallelize a for-loop code with control flow",
    "text": "Exercise: Parallelize a for-loop code with control flow\nOften we want to delay only some functions, running a few of them immediately. This is especially helpful when those functions are fast and help us to determine what other slower functions we should call. This decision, to delay or not to delay, is usually where we need to be thoughtful when using dask.delayed.\nIn the example below we iterate through a list of inputs. If that input is even then we want to call inc. If the input is odd then we want to call double. This is_even decision to call inc or double has to be made immediately (not lazily) in order for our graph-building Python code to proceed.\n\ndef double(x):\n    sleep(1)\n    return 2 * x\n\n\ndef is_even(x):\n    return not x % 2\n\n\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n%%time\n# Sequential code\n\nresults = []\nfor x in data:\n    if is_even(x):\n        y = double(x)\n    else:\n        y = inc(x)\n    results.append(y)\n\ntotal = sum(results)\nprint(total)\n\n\n%%time\n# Your parallel code here...\n# TODO: parallelize the sequential code above using dask.delayed\n# You will need to delay some functions, but not all\n\n\n%time total.compute()\n\n\ntotal.visualize()\n\n\nSome questions to consider:\n\nWhat are other examples of control flow where we can’t use delayed?\nWhat would have happened if we had delayed the evaluation of is_even(x) in the example above?\nWhat are your thoughts on delaying sum? This function is both computational but also fast to run."
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#exercise-parallelize-a-pandas-groupby-reduction",
    "href": "nb/lab6/03_dask.delayed.html#exercise-parallelize-a-pandas-groupby-reduction",
    "title": "dask.delayed - parallelize any code",
    "section": "Exercise: Parallelize a Pandas Groupby Reduction",
    "text": "Exercise: Parallelize a Pandas Groupby Reduction\nIn this exercise we read several CSV files and perform a groupby operation in parallel. We are given sequential code to do this and parallelize it with dask.delayed.\nThe computation we will parallelize is to compute the mean departure delay per airport from some historical flight data. We will do this by using dask.delayed together with pandas. In a future section we will do this same exercise with dask.dataframe."
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#create-data",
    "href": "nb/lab6/03_dask.delayed.html#create-data",
    "title": "dask.delayed - parallelize any code",
    "section": "Create data",
    "text": "Create data\nRun this code to prep some data.\nThis downloads and extracts some historical flight data for flights out of NYC between 1990 and 2000. The data is originally from here.\n\n#%run prep.py -d flights # not necessary to manually invoke this\n\n\nInspect data\n\nimport os\n\nsorted(os.listdir(os.path.join(\"data\", \"nycflights\")))\n\n\n\nRead one file with pandas.read_csv and compute mean departure delay\n\nimport pandas as pd\n\ndf = pd.read_csv(os.path.join(\"data\", \"nycflights\", \"1990.csv\"))\ndf.head()\n\n\n# What is the schema?\ndf.dtypes\n\n\n# What originating airports are in the data?\ndf.Origin.unique()\n\n\n# Mean departure delay per-airport for one year\ndf.groupby(\"Origin\").DepDelay.mean()\n\n\n\nSequential code: Mean Departure Delay Per Airport\nThe above cell computes the mean departure delay per-airport for one year. Here we expand that to all years using a sequential for loop.\n\nfrom glob import glob\n\nfilenames = sorted(glob(os.path.join(\"data\", \"nycflights\", \"*.csv\")))\n\n\n%%time\n\nsums = []\ncounts = []\nfor fn in filenames:\n    # Read in file\n    df = pd.read_csv(fn)\n\n    # Groupby origin airport\n    by_origin = df.groupby(\"Origin\")\n\n    # Sum of all departure delays by origin\n    total = by_origin.DepDelay.sum()\n\n    # Number of flights by origin\n    count = by_origin.DepDelay.count()\n\n    # Save the intermediates\n    sums.append(total)\n    counts.append(count)\n\n# Combine intermediates to get total mean-delay-per-origin\ntotal_delays = sum(sums)\nn_flights = sum(counts)\nmean = total_delays / n_flights\n\n\nmean\n\n\n\nParallelize the code above\nUse dask.delayed to parallelize the code above. Some extra things you will need to know.\n\nMethods and attribute access on delayed objects work automatically, so if you have a delayed object you can perform normal arithmetic, slicing, and method calls on it and it will produce the correct delayed calls.\nCalling the .compute() method works well when you have a single output. When you have multiple outputs you might want to use the dask.compute function. This way Dask can share the intermediate values.\n\nSo your goal is to parallelize the code above (which has been copied below) using dask.delayed. You may also want to visualize a bit of the computation to see if you’re doing it correctly.\n\n%%time\n# your code here\n\nIf you load the solution, add %%time to the top of the cell to measure the running time.\n\n(sum(sums)).visualize()\n\n\n# ensure the results still match\nmean\n\n\n\nSome questions to consider:\n\nHow much speedup did you get? Is this how much speedup you’d expect?\nExperiment with where to call compute. What happens when you call it on sums and counts? What happens if you wait and call it on mean?\nExperiment with delaying the call to sum. What does the graph look like if sum is delayed? What does the graph look like if it isn’t?\nCan you think of any reason why you’d want to do the reduction one way over the other?\n\n\n\nLearn More\nVisit the Delayed documentation. In particular, this delayed screencast will reinforce the concepts you learned here and the delayed best practices document collects advice on using dask.delayed well."
  },
  {
    "objectID": "nb/lab6/03_dask.delayed.html#close-the-client",
    "href": "nb/lab6/03_dask.delayed.html#close-the-client",
    "title": "dask.delayed - parallelize any code",
    "section": "Close the Client",
    "text": "Close the Client\nBefore moving on to the next exercise, make sure to close your client or stop this kernel.\n\nclient.close()"
  },
  {
    "objectID": "nb/lab1/Lab1.html",
    "href": "nb/lab1/Lab1.html",
    "title": "Pandas optimization",
    "section": "",
    "text": "Primary docs: - https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html#numba-jit-compilation - https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html\nPerformance deps for Pandas: - https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#install-recommended-dependencies\nUse Numba for JIT optimizations.\nhttps://github.com/modin-project/modin"
  },
  {
    "objectID": "nb/lab1/Lab1.html#notes",
    "href": "nb/lab1/Lab1.html#notes",
    "title": "Pandas optimization",
    "section": "Notes",
    "text": "Notes\n\nChunking\nchunksize parameter in Pandas functions.\n\n\nData types\nUse to_numeric (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html) for downcasting\n\n\nCategoricals\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n\n\nMemory usage\nUse DataFrame.memory_usage(deep=True) func. Use DataFrame.info() func"
  },
  {
    "objectID": "nb/lab1/Lab1.html#storage-optimization",
    "href": "nb/lab1/Lab1.html#storage-optimization",
    "title": "Pandas optimization",
    "section": "Storage optimization",
    "text": "Storage optimization\nWe’ll get the sample dataset from https://www.kaggle.com/datasets/anthonytherrien/depression-dataset.\n\nimport pandas as pd\n\ndd = pd.read_csv('depression_data.csv')\ndd.head()\n\n\n\n\n\n\n\n\nName\nAge\nMarital Status\nEducation Level\nNumber of Children\nSmoking Status\nPhysical Activity Level\nEmployment Status\nIncome\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\n0\nChristine Barker\n31\nMarried\nBachelor's Degree\n2\nNon-smoker\nActive\nUnemployed\n26265.67\nModerate\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n1\nJacqueline Lewis\n55\nMarried\nHigh School\n1\nNon-smoker\nSedentary\nEmployed\n42710.36\nHigh\nUnhealthy\nFair\nYes\nNo\nNo\nYes\n\n\n2\nShannon Church\n78\nWidowed\nMaster's Degree\n1\nNon-smoker\nSedentary\nEmployed\n125332.79\nLow\nUnhealthy\nGood\nNo\nNo\nYes\nNo\n\n\n3\nCharles Jordan\n58\nDivorced\nMaster's Degree\n3\nNon-smoker\nModerate\nUnemployed\n9992.78\nModerate\nModerate\nPoor\nNo\nNo\nNo\nNo\n\n\n4\nMichael Rich\n18\nSingle\nHigh School\n0\nNon-smoker\nSedentary\nUnemployed\n8595.08\nLow\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n\n\n\n\n\nLet’s see some memory usage stats:\n\ndd.info(memory_usage='deep')\n\nNote: Pandas stores memory in blocks, managed by BlockManager class. There is a separate block class for each type, like ObjectBlock or FloatBlock.\nNice write-up here: https://uwekorn.com/2020/05/24/the-one-pandas-internal.html\nLet’s examine memory usage for each type:\n\nfor dtype in ['float','int','object']:\n    selected_dtype = dd.select_dtypes(include=[dtype])\n    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n    mean_usage_mb = mean_usage_b / 1024 ** 2\n    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))\n\n\nNumeric types optimization\nLet’s first use iinfo to check ranges for different subtypes:\n\nimport numpy as np\nint_types = [\"uint8\", \"int8\", \"int16\", \"int32\", \"int64\"]\nfor it in int_types:\n    print(np.iinfo(it))\n\nWe can use pd.to_numeric() to downcast numeric types.\nFirst, let’s write a helper function for memory usage display:\n\ndef mem_usage(pandas_obj):\n    if isinstance(pandas_obj,pd.DataFrame):\n        usage_b = pandas_obj.memory_usage(deep=True).sum()\n    else: # we assume if not a df it's a series\n        usage_b = pandas_obj.memory_usage(deep=True)\n    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n    return \"{:03.2f} MB\".format(usage_mb)\n\nNote that “Age” and “Number of Children” columns can be presented as unsigned ints. Let’s convert:\n\ndd_int = dd.select_dtypes(include=['int'])\nconverted_int = dd_int.apply(pd.to_numeric,downcast='unsigned')\nprint(mem_usage(dd_int))\nprint(mem_usage(converted_int))\ncompare_ints = pd.concat([dd_int.dtypes,converted_int.dtypes],axis=1)\ncompare_ints.columns = ['before','after']\ncompare_ints.apply(pd.Series.value_counts)\n\nNice. Now let’s process float columns (Income):\n\ndd_float = dd.select_dtypes(include=['float'])\nconverted_float = dd_float.apply(pd.to_numeric,downcast='float')\nprint(mem_usage(dd_float))\nprint(mem_usage(converted_float))\ncompare_floats = pd.concat([dd_float.dtypes,converted_float.dtypes],axis=1)\ncompare_floats.columns = ['before','after']\ncompare_floats.apply(pd.Series.value_counts)\n\nNow let’s create a new optimized DataFrame:\n\noptimized_dd = dd.copy()\noptimized_dd[converted_int.columns] = converted_int\noptimized_dd[converted_float.columns] = converted_float\nprint(mem_usage(dd))\nprint(mem_usage(optimized_dd))\n\nJust a bit. Let’s proceed with object types.\nFirst, read https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/.\nNow, back to the dataset.\nWe can use categoricals (https://pandas.pydata.org/pandas-docs/stable/categorical.html) to optimize object columns.\n\n\n\ncategoricals\n\n\nLet’s look at the number of unique values for each object type:\n\ndd_obj = dd.select_dtypes(include=['object']).copy()\ndd_obj.describe()\n\nLet’s start with one column first: “Education Level”\n\nedu_level = dd_obj[\"Education Level\"]\nprint(edu_level.head())\nedu_level_cat = edu_level.astype('category')\nprint(edu_level_cat.head())\n\nWe can look at the category codes:\n\nedu_level_cat.head().cat.codes\n\nCompare memory usage:\n\nprint(mem_usage(edu_level))\nprint(mem_usage(edu_level_cat))\n\nWe should only convert objects to categoricals if most values are repeated. We have to pick a threshold, say, 25% of unique values.\nNote: when reading a csv, we can also provide a dtype dictionary param with preferred types.\nConverting all object columns and creating a new optimized DataFrame is left as part of Exercise 1."
  },
  {
    "objectID": "nb/lab1/Lab1.html#computation-optimization",
    "href": "nb/lab1/Lab1.html#computation-optimization",
    "title": "Pandas optimization",
    "section": "Computation optimization",
    "text": "Computation optimization\n\nCython\n\nimport math\ndef process(x):\n    if x &lt;= 9:\n        return x\n    lst = [int(i) for i in str(x).replace(\".\", \"\")]\n    result = 0\n    for el in lst:\n        result = result + el\n    return process(result)\n\n\nprocess(123688)\n\nObtain timing:\n\n%timeit dd.apply(lambda x: process(x[\"Income\"]), axis=1)\n\nSee more detailed breakdown:\n\n%prun -l 4 dd.apply(lambda x: process(x[\"Income\"]), axis=1)\n\n\n%load_ext Cython\n\n\n%%cython\ndef process_cython(x):\n    if x &lt;= 9:\n        return x\n    lst = [int(i) for i in str(x).replace(\".\", \"\")]\n    result = 0\n    for el in lst:\n        result = result + el\n    return process_cython(result)\n\n\n%timeit dd.apply(lambda x: process_cython(x[\"Income\"]), axis=1)\n\n\n%prun -l 8 dd.apply(lambda x: process_cython(x[\"Income\"]), axis=1)\n\nRead more about type annotations: https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html#declaring-c-types\n\n\nNumba\nEither add engine=\"numba\" parameter to Pandas functions, or add a @jit annotation to Python funcs.\n\nimport numba\n\n@numba.jit\ndef process_jit(x):\n    if x &lt;= 9:\n        return x\n    char_lst = [i for i in str(x).replace(\".\", \"\")]\n    result = 0\n    for el in char_lst:\n        i = int(el)\n        result = result + i\n    return process_jit(result)\n\n\n%prun -l 4 dd.apply(lambda x: process_jit(x[\"Income\"]), axis=1)"
  },
  {
    "objectID": "nb/lab1/Lab1.html#problems",
    "href": "nb/lab1/Lab1.html#problems",
    "title": "Pandas optimization",
    "section": "Problems",
    "text": "Problems\n\nPick your own dataset from Kaggle or HuggingFace or https://archive.ics.uci.edu. Perform numerical and object types conversions aimed at minimizing storage space, as outlined above.\nFix the Numba issue related to missing int() (more context here: https://github.com/numba/numba/issues/5650)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "bigdata_lec1.html#what-is-big-data",
    "href": "bigdata_lec1.html#what-is-big-data",
    "title": "Big Data: Intro",
    "section": "What is Big Data?",
    "text": "What is Big Data?\n\n\n\nDefinition\n\n\nBig Data is a set of technologies designed to store, manage and analyze data that is:\n\ntoo large to fit on a single machine\nwhile accommodating for the issue of growing discrepancy between capacity, throughput and latency."
  },
  {
    "objectID": "bigdata_lec1.html#prefixes",
    "href": "bigdata_lec1.html#prefixes",
    "title": "Big Data: Intro",
    "section": "Prefixes",
    "text": "Prefixes\n\n\n\nPrefixes\n\n\n\nkilo (k) 1,000 (3 zeros)\nMega (M) 1,000,000 (6 zeros)\nGiga (G) 1,000,000,000 (9 zeros)\nTera (T) 1,000,000,000,000 (12 zeros)\nPeta (P) 1,000,000,000,000,000 (15 zeros)\nExa (E) 1,000,000,000,000,000,000 (18 zeros)\nZetta (Z) 1,000,000,000,000,000,000,000 (21 zeros)\nYotta (Y) 1,000,000,000,000,000,000,000,000 (24 zeros)\nRonna (R) 1,000,000,000,000,000,000,000,000,000 (27 zeros)\nQuetta (Q) 1,000,000,000,000,000,000,000,000,000,000 (30 zeros)"
  },
  {
    "objectID": "bigdata_lec1.html#total-estimate",
    "href": "bigdata_lec1.html#total-estimate",
    "title": "Big Data: Intro",
    "section": "Total estimate",
    "text": "Total estimate\n\n\n\nEstimate\n\n\nThe total amount of data stored digitally worldwide is estimated to be getting close to 100 ZB as of 2021 (zettabytes)"
  },
  {
    "objectID": "bigdata_lec1.html#three-vs",
    "href": "bigdata_lec1.html#three-vs",
    "title": "Big Data: Intro",
    "section": "Three Vs",
    "text": "Three Vs\n\n\n\nVolume\nVariety\nVelocity"
  },
  {
    "objectID": "bigdata_lec1.html#volume",
    "href": "bigdata_lec1.html#volume",
    "title": "Big Data: Intro",
    "section": "Volume",
    "text": "Volume\n\n\n\nIssue\n\n\nData volume has exponentially increased in recent decades.\n\n\n\n\n\n\nWait but why?\n\n\n\nInternet-of-Things sensor data\nSocial networks\nStorage device progress"
  },
  {
    "objectID": "bigdata_lec1.html#variety",
    "href": "bigdata_lec1.html#variety",
    "title": "Big Data: Intro",
    "section": "Variety",
    "text": "Variety\n\n\n\nTypes\n\n\n\ntrees - XML, JSON, Parquet, Avro, etc\nunstructured - text, pictures, audio, video\ndata cubes\ngraphs"
  },
  {
    "objectID": "bigdata_lec1.html#velocity",
    "href": "bigdata_lec1.html#velocity",
    "title": "Big Data: Intro",
    "section": "Velocity",
    "text": "Velocity\n\n\n\nDefinion\n\n\nSpeed at which data is being generated, collected, and processed.\n\n\n\n\n\n\nAttributes\n\n\n\nCapacity: how much data can we store per unit of volume?\nThroughput: how many bytes can we read per unit of time?\nLatency: how much time do we need to wait until the bytes start arriving?"
  },
  {
    "objectID": "bigdata_lec1.html#velocity-1",
    "href": "bigdata_lec1.html#velocity-1",
    "title": "Big Data: Intro",
    "section": "Velocity",
    "text": "Velocity\nEvolution since 1950s"
  },
  {
    "objectID": "bigdata_lec1.html#features",
    "href": "bigdata_lec1.html#features",
    "title": "Big Data: Intro",
    "section": "Features",
    "text": "Features\n\n\n\nFeatures\n\n\n\nReliability\nScalability\nMaintainability"
  },
  {
    "objectID": "bigdata_lec1.html#reliability",
    "href": "bigdata_lec1.html#reliability",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nKleppmann’s definition\n\n\nThe system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity\n\nhardware faults\nsoftware faults\nand even human error"
  },
  {
    "objectID": "bigdata_lec1.html#reliability-1",
    "href": "bigdata_lec1.html#reliability-1",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nFaults\n\n\nBasically, theses are things that could go wrong.\nSystems that can anticipate faults are called fault-tolerant or resilient.\nFault can be defined as one component of the system deviating from the spec.\n\n\n\n\n\n\nFailures\n\n\nFailures occur when system stops providing services to the user.\n\n\n\nFaults might degenerate into failures."
  },
  {
    "objectID": "bigdata_lec1.html#reliability-2",
    "href": "bigdata_lec1.html#reliability-2",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nTypes of errors\n\n\n\nHardware\nSoftware\nHuman"
  },
  {
    "objectID": "bigdata_lec1.html#reliability-3",
    "href": "bigdata_lec1.html#reliability-3",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability"
  },
  {
    "objectID": "bigdata_lec1.html#scalability",
    "href": "bigdata_lec1.html#scalability",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nKleppmann\n\n\nAs the system grows (in data volume, traffic volume, or complexity), there should be reasonable ways of dealing with that growth.\nIn other words, scalability is a system’s ability to cope with increased load.\n\n\n\n\n\nNote that scalability is a multi-dimensional term. When saying “system scales well”, it’s important to state exactly along which axis."
  },
  {
    "objectID": "bigdata_lec1.html#scalability-1",
    "href": "bigdata_lec1.html#scalability-1",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nWhat is load?\n\n\nLoad is described by load parameters. These might include:\n\ndata set size\ndata write speed\ndata read speed\ncomputational complexity\netc…"
  },
  {
    "objectID": "bigdata_lec1.html#scalability-2",
    "href": "bigdata_lec1.html#scalability-2",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nPerformance\n\n\nIncreasing load affects performance. There are several meanings to this term:\n\nthroughput – time required to process a dataset of certain size\nresponse time – time between sending a request and receiving a response\nlatency – duration of waiting for a request to be processed. Included in response time.\n\n\n\n\n\n\nPerformance might be more strictly defined by service level objectives (SLOs) and service level agreements (SLAs)."
  },
  {
    "objectID": "bigdata_lec1.html#scalability-3",
    "href": "bigdata_lec1.html#scalability-3",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nHow to deal with load\n\n\n\nvertical scaling - scaling up\nhorizontal scaling - scaling out\narchitectural changes\n\n\n\n\n\n\n\nElasticity\n\n\nAn approach to load handling whereby a system automatically adds resources in case of load increase, and can decrease resources if load decreases."
  },
  {
    "objectID": "bigdata_lec1.html#scalability-4",
    "href": "bigdata_lec1.html#scalability-4",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nCommon wisdom\n\n\n\nKeep your database on a single node (scale up) until scaling cost or high-availability requirements forces you to make it distributed.\nOptimize code so that it can run on a single node."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability",
    "href": "bigdata_lec1.html#maintainability",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nKleppmann\n\n\nOver time, many different people will work on the system\n\nengineering\noperations\nboth maintaining current behavior and adapting the system to new use cases),\n\nand they should all be able to work on it productively."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-1",
    "href": "bigdata_lec1.html#maintainability-1",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nPrinciples\n\n\n\nOperability – make it easy for operations teams to keep the system running smoothly.\nSimplicity – make it easy for new engineers to understand the system, by removing as much complexity as possible from the system.\nEvolvability – Make it easy for engineers to make changes to the system in the future, adapting it for unanticipated use cases as requirements change. Also known as extensibility, modifiability, or plasticity."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-2",
    "href": "bigdata_lec1.html#maintainability-2",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nOperability\n\n\n\nHealth monitoring\nGood deployment practices\nConfiguration management\nVisibility into the internals of the system\nKnowledge preservation – documentation (!).\netc…"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-3",
    "href": "bigdata_lec1.html#maintainability-3",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nComplexity symptoms\n\n\n\nLots of hidden state\nLoose cohesion, tight coupling\nBad naming (!)\nUnnecessary hacks\netc…"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity",
    "href": "bigdata_lec1.html#maintainability-complexity",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity-1",
    "href": "bigdata_lec1.html#maintainability-complexity-1",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity\n\n\n\nTypes\n\n\n\nincidental\naccidental"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity-2",
    "href": "bigdata_lec1.html#maintainability-complexity-2",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity\n\n\n\nIncidental\n\n\n\nEasy things can be complex. There can be complex constructs that are succinctly described, familiar, available and easy to use. That is incidental complexity.\n\nRich Hickey talk “Simple made easy”: https://www.youtube.com/watch?v=SxdOUGdseq4"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-complexity-3",
    "href": "bigdata_lec1.html#maintainability-complexity-3",
    "title": "Big Data: Intro",
    "section": "Maintainability: complexity",
    "text": "Maintainability: complexity\nHowever: Complexity is often caused by\n\n\n\nAccidental complexity\n\n\nMoseley and Marks define complexity as accidental if it is not inherent in the problem that the software solves (as seen by the users) but arises only from the implementation.\n\n\n\n\n\n\nHow to remove?\n\n\nBy providing proper abstractions."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-abstractions",
    "href": "bigdata_lec1.html#maintainability-abstractions",
    "title": "Big Data: Intro",
    "section": "Maintainability: abstractions",
    "text": "Maintainability: abstractions\n\n\n\n\n\n\nDefinition (Ousterhout)\n\n\nAn abstraction is a simplified view of an entity, which omits unimportant details.\nIn modular programming, each module provides an abstraction in the form of its interface."
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-abstractions-1",
    "href": "bigdata_lec1.html#maintainability-abstractions-1",
    "title": "Big Data: Intro",
    "section": "Maintainability: abstractions",
    "text": "Maintainability: abstractions\n\n\n\n\n\n\nWhat can abstractions do?\n\n\n\nHide implementation details\nProvide reusable building blocks"
  },
  {
    "objectID": "bigdata_lec1.html#maintainability-4",
    "href": "bigdata_lec1.html#maintainability-4",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nEvolvability\n\n\nOne needs to adapt their big data system to possible future requirements changes.\nHowever, keep in mind the following:\n\nInability to foresee exact nature of changes\nNeed to strike the balance of flexibility and fitness for a particular task"
  },
  {
    "objectID": "bigdata_lec1.html#types-of-big-data-analytics",
    "href": "bigdata_lec1.html#types-of-big-data-analytics",
    "title": "Big Data: Intro",
    "section": "Types of big data analytics",
    "text": "Types of big data analytics\n\n\n\nTypes\n\n\n\nPrescriptive\nDiagnostic\nDescriptive\nPredictive"
  },
  {
    "objectID": "bigdata_lec1.html#types-prescriptive",
    "href": "bigdata_lec1.html#types-prescriptive",
    "title": "Big Data: Intro",
    "section": "Types: Prescriptive",
    "text": "Types: Prescriptive\n\n\n\nPrescriptive\n\n\n\nForward looking\nOptimal decisions for future situations"
  },
  {
    "objectID": "bigdata_lec1.html#types-diagnostic",
    "href": "bigdata_lec1.html#types-diagnostic",
    "title": "Big Data: Intro",
    "section": "Types: Diagnostic",
    "text": "Types: Diagnostic\n\n\n\nDiagnostic\n\n\n\nBackward looking\nFocused on causal relationships"
  },
  {
    "objectID": "bigdata_lec1.html#types-descriptive",
    "href": "bigdata_lec1.html#types-descriptive",
    "title": "Big Data: Intro",
    "section": "Types: Descriptive",
    "text": "Types: Descriptive\n\n\n\nDescriptive\n\n\n\nBackward looking\nFocused on descriptions and comparisons"
  },
  {
    "objectID": "bigdata_lec1.html#types-descriptive-1",
    "href": "bigdata_lec1.html#types-descriptive-1",
    "title": "Big Data: Intro",
    "section": "Types: Descriptive",
    "text": "Types: Descriptive\n\n\n\nPredictive\n\n\n\nForward looking\nFocused on the prediction of future states, relationship, and patterns"
  },
  {
    "objectID": "bigdata_lec1.html#challenges",
    "href": "bigdata_lec1.html#challenges",
    "title": "Big Data: Intro",
    "section": "Challenges",
    "text": "Challenges\nThere are 2 main challenges associated with Big Data.\n\n\n\nChallenges\n\n\n\nhow do we store and manage such a huge volume of data efficiently?\nhow do we process and extract valuable information from the data within the given time frame?"
  },
  {
    "objectID": "bigdata_lab2.html",
    "href": "bigdata_lab2.html",
    "title": "Big Data Analytics: Lab 2",
    "section": "",
    "text": "First, install Cython and Numba:\nuv add cython\nuv add numba\nThen we can load Cython extension in Jupyter:\n\n%load_ext Cython\n\nDefine a function that generates a list of first \\(n\\) primes:\n\n\n\n\n\n\nImportantPure Python\n\n\n\n\ndef primes1(nb_primes):\n    p = [0] * 1000\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nImportantPure Python results\n\n\n\nWe can measure its execution time:\n\n%timeit primes1(100)\n\n193 μs ± 3.49 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nNow, define a function using Cython type annotations. Note that we use %%cython magic before function definition:\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\n\n%%cython\ndef primes2(nb_primes: cython.int):\n    i: cython.int\n    p: cython.int[1000] = [0] * 1000\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p: cython.int = 0  # The current number of elements in p.\n    n: cython.int = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nNotePython with Cython type hints: results\n\n\n\nWe measure its execution time:\n\n%timeit primes2(100)\n\n82.2 μs ± 748 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nDifferent syntax:\n\n\n\n\n\n\nTipCython syntax\n\n\n\n\n%%cython\ndef primes3(int nb_primes):\n    cdef int n, i, len_p\n    cdef int[1000] p\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nTipCython syntax results\n\n\n\nWe measure its execution time:\n\n%timeit primes3(100)\n\n4.32 μs ± 131 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPassing --annotate flag will produce more relevant info."
  },
  {
    "objectID": "bigdata_lab2.html#cython",
    "href": "bigdata_lab2.html#cython",
    "title": "Big Data Analytics: Lab 2",
    "section": "",
    "text": "First, install Cython and Numba:\nuv add cython\nuv add numba\nThen we can load Cython extension in Jupyter:\n\n%load_ext Cython\n\nDefine a function that generates a list of first \\(n\\) primes:\n\n\n\n\n\n\nImportantPure Python\n\n\n\n\ndef primes1(nb_primes):\n    p = [0] * 1000\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nImportantPure Python results\n\n\n\nWe can measure its execution time:\n\n%timeit primes1(100)\n\n193 μs ± 3.49 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nNow, define a function using Cython type annotations. Note that we use %%cython magic before function definition:\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\n\n%%cython\ndef primes2(nb_primes: cython.int):\n    i: cython.int\n    p: cython.int[1000] = [0] * 1000\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p: cython.int = 0  # The current number of elements in p.\n    n: cython.int = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nNotePython with Cython type hints: results\n\n\n\nWe measure its execution time:\n\n%timeit primes2(100)\n\n82.2 μs ± 748 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nDifferent syntax:\n\n\n\n\n\n\nTipCython syntax\n\n\n\n\n%%cython\ndef primes3(int nb_primes):\n    cdef int n, i, len_p\n    cdef int[1000] p\n\n    if nb_primes &gt; 1000:\n        nb_primes = 1000\n\n    len_p = 0  # The current number of elements in p.\n    n = 2\n    while len_p &lt; nb_primes:\n        # Is n prime?\n        for i in p[:len_p]:\n            if n % i == 0:\n                break\n\n        # If no break occurred in the loop, we have a prime.\n        else:\n            p[len_p] = n\n            len_p += 1\n        n += 1\n\n    # Let's copy the result into a Python list:\n    result_as_list = [prime for prime in p[:len_p]]\n    return result_as_list\n\n\n\n\n\n\n\n\n\nTipCython syntax results\n\n\n\nWe measure its execution time:\n\n%timeit primes3(100)\n\n4.32 μs ± 131 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPassing --annotate flag will produce more relevant info."
  },
  {
    "objectID": "bigdata_lab2.html#using-cython-in-pandas",
    "href": "bigdata_lab2.html#using-cython-in-pandas",
    "title": "Big Data Analytics: Lab 2",
    "section": "Using Cython in Pandas",
    "text": "Using Cython in Pandas\nDefine a random DataFrame:\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {\n        \"a\": np.random.randn(1000),\n        \"b\": np.random.randn(1000),\n        \"N\": np.random.randint(100, 1000, (1000)),\n        \"x\": \"x\",\n    }\n)\n\nDefine functions that will be applied to the DataFrame:\n\n\n\n\n\n\nImportantPure Python\n\n\n\n\ndef f(x):\n    return x * (x - 1)\n\n\ndef integrate_f(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f(a + i * dx)\n    return s * dx\n\n\n\n\n\n\n\n\n\nImportantPure Python results\n\n\n\nApply functions to DataFrame:\n\n%timeit df.apply(lambda x: integrate_f(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n\n24.8 ms ± 85.3 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nNow let’s use Cython-annotated functions:\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\n\n%%cython\ndef f_typed2(x: cython.double) -&gt; cython.double:\n    return x * (x - 1)\n\ndef integrate_f_typed2(a: cython.double, b: cython.double, N: cython.int) -&gt; cython.double:\n    i: cython.int\n    s: cython.double\n    dx: cython.double\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f_typed2(a + i * dx)\n    return s * dx\n\n\n\n\n\n\n\n\n\nNotePython with Cython type hints\n\n\n\nApply annotated functions to DataFrame:\n\n%timeit df.apply(lambda x: integrate_f_typed2(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n\n26 ms ± 118 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nWith a different syntax:\n\n\n\n\n\n\nTipCython syntax\n\n\n\n\n%%cython\ncdef double f_typed(double x) except? -2:\n   return x * (x - 1)\ncpdef double integrate_f_typed(double a, double b, int N):\n   cdef int i\n   cdef double s, dx\n   s = 0\n   dx = (b - a) / N\n   for i in range(N):\n       s += f_typed(a + i * dx)\n   return s * dx\n\n\n\n\n\n\n\n\n\nTipCython syntax results\n\n\n\n\n%timeit df.apply(lambda x: integrate_f_typed(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n\n2.95 ms ± 24.5 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n\n\n\n\n\nNotePandas\n\n\n\nRead more about type annotations for Pandas: https://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html."
  },
  {
    "objectID": "bigdata_lab2.html#numba",
    "href": "bigdata_lab2.html#numba",
    "title": "Big Data Analytics: Lab 2",
    "section": "Numba",
    "text": "Numba\nEither add engine=\"numba\" parameter to Pandas functions, or add a @jit annotation to Python funcs.\n\n\n\n\n\n\nWarningNumba-annotated code\n\n\n\n\nimport numba\n\n\n@numba.jit\ndef f_numba(x):\n    return x * (x - 1)\n\n\n@numba.jit\ndef integrate_f_numba(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f_numba(a + i * dx)\n    return s * dx\n\n\n@numba.jit\ndef apply_integrate_f_numba(col_a, col_b, col_N):\n    n = len(col_N)\n    result = np.empty(n, dtype=\"float64\")\n    assert len(col_a) == len(col_b) == n\n    for i in range(n):\n        result[i] = integrate_f_numba(col_a[i], col_b[i], col_N[i])\n    return result\n\n\ndef compute_numba(df):\n    result = apply_integrate_f_numba(\n        df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy()\n    )\n    return pd.Series(result, index=df.index, name=\"result\")\n\n\n\n\n\n\n\n\n\nWarningNumba Results\n\n\n\n\n%timeit compute_numba(df)\n\n380 μs ± 27.6 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "bigdata_lab2.html#exercises",
    "href": "bigdata_lab2.html#exercises",
    "title": "Big Data Analytics: Lab 2",
    "section": "Exercises",
    "text": "Exercises\n\nApply Cython/Numba optimizations to some computations on your dataframe from Lab 1.\nGo through the tutorial on Black-Scholes option pricing (https://louis-finegan.github.io/2024/10/10/Black-Scholes.html). Modify the code so that:\n\n\nit works for some different company\nhas 2 versions: using Cython and Numba\nmeasure results"
  },
  {
    "objectID": "bigdata_lab2.html#references",
    "href": "bigdata_lab2.html#references",
    "title": "Big Data Analytics: Lab 2",
    "section": "References",
    "text": "References\n\nCython tutorial\nCython build instructions\nCython language\nPandas optimization"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management",
    "href": "bigdata_lec3.html#memory-management",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\n\n\nManual\n\n\n\nC/C++\nPascal\nForth\nFortran\nZig\n\n\n\n\n\n\n\n\nAutomatic\n\n\n\nLisp\nJava\nPython\nGo\nJulia"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-1",
    "href": "bigdata_lec3.html#memory-management-1",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\nCode\n\n\nimport os\nimport gc\nimport psutil\n\nproc = psutil.Process(os.getpid())\ngc.collect()\ninitial_memory = proc.memory_info().rss\n\n## Allocate memory by creating large lists\nfoo = ['abc' for _ in range(10**7)]\nallocated_memory = proc.memory_info().rss\n\n## Deallocate memory\ndel foo\ngc.collect()\nfinal_memory = proc.memory_info().rss"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-2",
    "href": "bigdata_lec3.html#memory-management-2",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\nPrint memory statistics\n\n\nincrease = lambda x2, x1: 100.0 * (x2 - x1) / initial_memory\nprint(\"Allocated Memory Increase: %0.2f%%\" % increase(allocated_memory, initial_memory))\nprint(\"Memory After Deletion: %0.2f%%\" % increase(final_memory, allocated_memory))\n\n&gt;&gt;&gt; Allocated Memory Increase: 23.35%\n&gt;&gt;&gt; Memory After Deletion: -10.78%"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-3",
    "href": "bigdata_lec3.html#memory-management-3",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management\n\n\n\n\n\n\n\nPython internals\n\n\n\npools\nblocks\narenas\n\n\n\n\n\n\n\nhttps://docs.python.org/3/c-api/memory.html\nhttps://realpython.com/python-memory-management"
  },
  {
    "objectID": "bigdata_lec3.html#memory-management-4",
    "href": "bigdata_lec3.html#memory-management-4",
    "title": "Big Data: Speeding up computation",
    "section": "Memory management",
    "text": "Memory management"
  },
  {
    "objectID": "bigdata_lec3.html#python-options",
    "href": "bigdata_lec3.html#python-options",
    "title": "Big Data: Speeding up computation",
    "section": "Python Options",
    "text": "Python Options\n\n\n\n\n\n\n\n\n\nLibraries\nLow-level langs\nAlt Python Impls\nJIT\n\n\n\n\nNumPy,  SciPy\nC, Rust, Cython, PyO3\nPyPy, Jython\nNumba, PyPy\n\n\n\n\n\nOptions above are not mutually exclusive!"
  },
  {
    "objectID": "bigdata_lec3.html#interpreters",
    "href": "bigdata_lec3.html#interpreters",
    "title": "Big Data: Speeding up computation",
    "section": "Interpreters",
    "text": "Interpreters\n\n\n\nWikipedia definition\n\n\nAn interpreter is a computer program that directly executes instructions written in a programming or scripting language, without requiring them previously to have been compiled into a machine language program.\n\n\n\n\n\n\nExamples\n\n\n\nPython\nRuby\nLua\nJavascript"
  },
  {
    "objectID": "bigdata_lec3.html#cpython",
    "href": "bigdata_lec3.html#cpython",
    "title": "Big Data: Speeding up computation",
    "section": "CPython",
    "text": "CPython\n\n\n\n\nFlow\n\n\n\nRead Python code\nConvert Python into bytecode\nExecute bytecode inside a VM\nVM converts bytecode to machine code"
  },
  {
    "objectID": "bigdata_lec3.html#cpython-1",
    "href": "bigdata_lec3.html#cpython-1",
    "title": "Big Data: Speeding up computation",
    "section": "CPython",
    "text": "CPython"
  },
  {
    "objectID": "bigdata_lec3.html#compilers",
    "href": "bigdata_lec3.html#compilers",
    "title": "Big Data: Speeding up computation",
    "section": "Compilers",
    "text": "Compilers\n\n\n\nWikipedia definition\n\n\nSource code is compiled - in this context, translated into machine code for better performance.\n\n\n\n\n\n\nExamples\n\n\n\nC/C++\nGo\nPython (to intermediate VM code)\nJava\nCython"
  },
  {
    "objectID": "bigdata_lec3.html#compilers-1",
    "href": "bigdata_lec3.html#compilers-1",
    "title": "Big Data: Speeding up computation",
    "section": "Compilers",
    "text": "Compilers"
  },
  {
    "objectID": "bigdata_lec3.html#cython",
    "href": "bigdata_lec3.html#cython",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nDefinition\n\n\nCython is an optimising static compiler for the Python programming language.\n\nconverts Python code to C\nsupports static type declarations"
  },
  {
    "objectID": "bigdata_lec3.html#cython-1",
    "href": "bigdata_lec3.html#cython-1",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython"
  },
  {
    "objectID": "bigdata_lec3.html#cython-2",
    "href": "bigdata_lec3.html#cython-2",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython"
  },
  {
    "objectID": "bigdata_lec3.html#cython-3",
    "href": "bigdata_lec3.html#cython-3",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nPython code"
  },
  {
    "objectID": "bigdata_lec3.html#cython-4",
    "href": "bigdata_lec3.html#cython-4",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nAnnotated Python code"
  },
  {
    "objectID": "bigdata_lec3.html#cython-5",
    "href": "bigdata_lec3.html#cython-5",
    "title": "Big Data: Speeding up computation",
    "section": "Cython",
    "text": "Cython\n\n\n\nCython code"
  },
  {
    "objectID": "bigdata_lec3.html#parallel-cython",
    "href": "bigdata_lec3.html#parallel-cython",
    "title": "Big Data: Speeding up computation",
    "section": "Parallel Cython",
    "text": "Parallel Cython"
  },
  {
    "objectID": "bigdata_lec3.html#parallel-cython-1",
    "href": "bigdata_lec3.html#parallel-cython-1",
    "title": "Big Data: Speeding up computation",
    "section": "Parallel Cython",
    "text": "Parallel Cython"
  },
  {
    "objectID": "bigdata_lec3.html#jit",
    "href": "bigdata_lec3.html#jit",
    "title": "Big Data: Speeding up computation",
    "section": "JIT",
    "text": "JIT\n\n\n\nWikipedia definition\n\n\nA compilation (of computer code) during execution of a program (at run time) rather than before execution.\n\n\n\n\n\n\nFeatures\n\n\n\nwarm-up time: JIT causes a slight to noticeable delay in the initial execution of an application, due to the time taken to load and compile the input code.\nstatistics collection: performed by the system during runtime, shows how the program is actually running in the environment it is in; helps JIT to rearrange and recompile for optimum performance.\nparticularly suited for dynamic programming languages"
  },
  {
    "objectID": "bigdata_lec3.html#jit-1",
    "href": "bigdata_lec3.html#jit-1",
    "title": "Big Data: Speeding up computation",
    "section": "JIT",
    "text": "JIT\n\n\n\nExamples\n\n\n\nHotSpot Java Virtual Machine\nLuaJIT\nNumba\nPyPy"
  },
  {
    "objectID": "bigdata_lec3.html#numba",
    "href": "bigdata_lec3.html#numba",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba"
  },
  {
    "objectID": "bigdata_lec3.html#numba-1",
    "href": "bigdata_lec3.html#numba-1",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nDescription\n\n\n\nNumba translates Python byte-code to machine code immediately before execution to improve the execution speed.\nFor that we add a @jit decorator\nWorks well for numeric operations, NumPy, and loops"
  },
  {
    "objectID": "bigdata_lec3.html#numba-2",
    "href": "bigdata_lec3.html#numba-2",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nSteps\n\n\n\nread the Python bytecode for a decorated function\ncombine it with information about the types of the input arguments to the function\nanalyze and optimize the code\nuse the LLVM compiler library to generate a machine code version of the function, tailored to specific CPU capabilities."
  },
  {
    "objectID": "bigdata_lec3.html#numba-3",
    "href": "bigdata_lec3.html#numba-3",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nWorks great"
  },
  {
    "objectID": "bigdata_lec3.html#numba-4",
    "href": "bigdata_lec3.html#numba-4",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba\n\n\n\nNope"
  },
  {
    "objectID": "bigdata_lec3.html#numba-5",
    "href": "bigdata_lec3.html#numba-5",
    "title": "Big Data: Speeding up computation",
    "section": "Numba",
    "text": "Numba"
  },
  {
    "objectID": "bigdata_lec3.html#numpy",
    "href": "bigdata_lec3.html#numpy",
    "title": "Big Data: Speeding up computation",
    "section": "Numpy",
    "text": "Numpy\n\n\n\nWhy so fast?\n\n\n\nOptimized C code\nDensely packed arrays\nUses BLAS - Basic Linear Algebra Subroutines."
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-1",
    "href": "bigdata_lec3.html#rustpyo3-example-1",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nDescription\n\n\nWe show an example of a simple algebraic cipher that utilizes PyO3 bindings to speed up encoding/decoding.\n\n\n\n\n\n\nCipher definition\n\n\nThe basic mechanism for encrypting a message using a shared secret key is called a cipher (or encryption scheme)"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-2",
    "href": "bigdata_lec3.html#rustpyo3-example-2",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\n\n\n\nDefinition\n\n\nEncryption and decryption use the same secret key.\n\n\n\n\n\n\nExamples\n\n\n\nAES\nSalsa20\nTwofish\nDES"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-3",
    "href": "bigdata_lec3.html#rustpyo3-example-3",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nTypes\n\n\n\nblock ciphers\nstream ciphers\nhash functions"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-4",
    "href": "bigdata_lec3.html#rustpyo3-example-4",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nOverview\n\n\n\\[\nf: \\mathcal{K}\\times\\mathcal{D} \\rightarrow C\n\\] where\n\n\\(\\mathcal{K}\\) is key space\n\\(\\mathcal{D}\\) is domain (or message space)\n\\(\\mathcal{C}\\) is co-domain (or ciphertext space)"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-5",
    "href": "bigdata_lec3.html#rustpyo3-example-5",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nShannon cipher\n\n\nA Shannon cipher is a pair \\(\\mathcal{E} = (E,D)\\) of functions:\n\nThe function \\(E\\) (the encryption function) takes as input a key \\(k\\) and message \\(m\\) (also called plaintext) and produces as output a ciphertext \\(c):\\)$ c = E(k,m) $$\n\n\\(c\\) is the encryption of \\(m\\) under \\(k\\).\n\nThe function \\(D\\) (the decryption function) takes as input a key \\(k\\) and ciphertext \\(c\\), and produces a message \\(m\\): \\[\nm = D(k,c)\n\\]\n\n\\(m\\) is the decryption of \\(c\\) under \\(k\\)."
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-6",
    "href": "bigdata_lec3.html#rustpyo3-example-6",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nCorrectness property\n\n\nFor all keys \\(k\\) and messages \\(m\\), we have \\[\nD(k, E(k,m)) = m\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-7",
    "href": "bigdata_lec3.html#rustpyo3-example-7",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nParameters\n\n\nNow we describe the cipher.\nFirst, we define cipher parameters:\n\nAn alphabet \\(A\\) with size \\(L \\equiv |A|\\).\nA matrix \\(M\\) with size \\(N \\gg L\\)\n\\(\\sigma_1, \\sigma_2\\) are some permutations \\(N \\rightarrow N\\)\n\\(\\phi\\) is some bit sequence of length \\(P\\): \\(\\phi \\in \\{0,1\\}^P\\)\n\nA triple \\((\\sigma_1, \\sigma_2, \\phi)\\) will be our secret key.\nWe define each symbol \\(z\\) by a corresponding set of diagonals \\(D\\) in the matrix \\(M\\), so that \\(\\forall (x,y) \\in D: x - y = z (\\mod L)\\) (see Figure 1)."
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-8",
    "href": "bigdata_lec3.html#rustpyo3-example-8",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-9",
    "href": "bigdata_lec3.html#rustpyo3-example-9",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nEncoder flow\n\n\nSuppose we receive some text \\(T\\) containing symbols to be encoded.\nFor each \\(t_i \\in T\\), obtain its numeric representation \\(z_i\\). \\[\nz_i \\in [0,L)\n\\] Then we map each \\(z_i\\) to a pair of matrix coordinates \\((x_i, y_i)\\) such that:\n\nFirst, we pick a random \\(x_i \\in [0,N)\\) (e.g., horizontal coordinate in a matrix)\nThen, we randomly pick some \\(y_i \\in [0,N)\\) such that: \\[\nx_i - y_i = z_i (\\mod L)\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-10",
    "href": "bigdata_lec3.html#rustpyo3-example-10",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nEncoder flow\n\n\nHaving thus obtained a sequence \\(\\{(x_i, y_i), \\, i \\in [0, |T|) \\}\\), we now apply permutations \\(\\sigma_k: [0,N) \\rightarrow [0,N), \\, k=1,2\\): \\[\n\\text{ciphertext } (\\xi,\\eta) := (\\sigma_j(x),\\sigma_{j+1}(y))\n\\] where \\[\n\\sigma_j = \\begin{cases}\n\\sigma_1, \\; \\text{if}\\; \\phi_j=0,\\\\\n\\sigma_2\\; \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-11",
    "href": "bigdata_lec3.html#rustpyo3-example-11",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nDecoder flow\n\n\nBelow are steps executing during decoding phase:\n\nReceive encoded ciphertext \\(\\{(\\xi_i, \\eta_i)\\}\\).\nApply inverse permutations \\(\\sigma_j^{-1}, \\sigma_{j+1}^{-1}\\): \\[\n(x_i,y_i) = (\\sigma_j^{-1}(\\xi_i), \\sigma_{j+1}^{-1}(\\eta_i))\n\\]\nFind \\(z_i\\): \\[\nz_i = x_i - y_i \\; (\\mod L)\n\\]"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-12",
    "href": "bigdata_lec3.html#rustpyo3-example-12",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nImplementation: PyO3\n\n\n\nRust bindings for Python extension modules\n\n\n\n\n\n\n\nUsers\n\n\n\nQiskit https://www.ibm.com/quantum/qiskit\nPython Cryptography package https://github.com/pyca/cryptography\nScallop https://www.scallop-lang.org\nHuggingFace Tokenizers https://github.com/huggingface/tokenizers"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-13",
    "href": "bigdata_lec3.html#rustpyo3-example-13",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nRust wrapper\n\n\n/// A Python module implemented in Rust.\n#[pymodule]\nfn cipher(m: &Bound&lt;'_, PyModule&gt;) -&gt; PyResult&lt;()&gt; {\n    m.add_function(wrap_pyfunction!(encode, m)?)?;\n    m.add_function(wrap_pyfunction!(decode, m)?)?;\n    Ok(())\n}"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-14",
    "href": "bigdata_lec3.html#rustpyo3-example-14",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nEncoding\n\n\n// Generate random permutation of N integers\nlet mut numbers: Vec&lt;usize&gt; = (0..N).collect();\nlet mut rng = thread_rng();\nnumbers.shuffle(&mut rng);\nlet permutation = numbers;\n\n// Generate pairs for each z\nlet mut pairs = Vec::with_capacity(zs.len());\nfor &z in &zs {\n    // Generate random x between 0 and N-1\n    let x = rng.gen_range(0..N);\n\n    // Compute y such that x - y = z (mod L)\n    let y = if x &gt;= z {\n        (x - z) % L\n    } else {\n        ((x + L) - z) % L\n    };\n\n    // Apply permutation to x and y\n    let px = permutation[x];\n    let py = permutation[y];\n    pairs.push((px, py));"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-15",
    "href": "bigdata_lec3.html#rustpyo3-example-15",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nDecoding\n\n\n// Create inverse permutation\nlet mut inverse = vec![0; N];\nfor (i, &p) in permutation.iter().enumerate() {\n    inverse[p] = i;\n}\n\n// Recover z for each pair\nlet mut zs = Vec::with_capacity(pairs.len());\nfor &(px, py) in pairs {\n    // Apply inverse permutation to get x and y\n    let x = inverse[px];\n    let y = inverse[py];\n\n    // Compute z = x - y (mod L)\n    let z = if x &gt;= y {\n        (x - y) % L\n    } else {\n        ((x + L) - y) % L\n    };\n    zs.push(z);\n}"
  },
  {
    "objectID": "bigdata_lec3.html#rustpyo3-example-16",
    "href": "bigdata_lec3.html#rustpyo3-example-16",
    "title": "Big Data: Speeding up computation",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nPython\n\n\nmkdir cipher; cd cipher\npython -m venv venv\n. venv/bin/activate\n!pip install maturin\nmaturin init\nmaturin develop\n\n\n\n\n\n\nUsage\n\n\nimport cipher\nencoded = cipher.encode(\"Whazzuuupppp??!!\")\ndecoded = cipher.decode(encoded)"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-1",
    "href": "bigdata_lec3.html#distributed-computing-1",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nTypes\n\n\n\nCluster computing: collection of similar workstations\nGrid computing: federation of different computer systems\nCloud computing: provide the facilities to dynamically construct an infrastructure and compose what is needed from available services. Not only providing lots of resources."
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-2",
    "href": "bigdata_lec3.html#distributed-computing-2",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nOriginal Beowulf cluster at NASA (1994)"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-3",
    "href": "bigdata_lec3.html#distributed-computing-3",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nBeowulf cluster diagram"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-4",
    "href": "bigdata_lec3.html#distributed-computing-4",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\n\n\nGrid architecture diagram (Foster et al. 2001)\n\n\n\n\n\n\n\n\n\n\nNotation\n\n\n\n\nfabric: interfaces to local resources at a specific site\nconnectivity: communication protocols for supporting grid transactions that span the usage of multiple resources\nresource: responsible for managing a single resource\ncollective: handling access to multiple resources and typically consists of services for resource discovery, allocation and scheduling of tasks onto multiple resources, data replication, and so on\napplication: applications that operate within a virtual organization"
  },
  {
    "objectID": "bigdata_lec3.html#distributed-computing-5",
    "href": "bigdata_lec3.html#distributed-computing-5",
    "title": "Big Data: Speeding up computation",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nCloud architecture"
  },
  {
    "objectID": "bigdata_lec4.html#memory-management",
    "href": "bigdata_lec4.html#memory-management",
    "title": "Big Data: Dask intro",
    "section": "Memory management",
    "text": "Memory management\n\n\n\n\n\nManual\n\n\n\nC/C++\nPascal\nForth\nFortran\nZig\n\n\n\n\n\n\n\n\nAutomatic\n\n\n\nLisp\nJava\nPython\nGo\nJulia"
  },
  {
    "objectID": "bigdata_lec4.html#python-options",
    "href": "bigdata_lec4.html#python-options",
    "title": "Big Data: Dask intro",
    "section": "Python Options",
    "text": "Python Options\n\n\n\n\n\n\n\n\n\nLibraries\nLow-level langs\nAlt Python Impls\nJIT\n\n\n\n\nNumPy,  SciPy\nC, Rust, Cython, PyO3\nPyPy, Jython\nNumba, PyPy\n\n\n\n\n\nOptions above are not mutually exclusive!"
  },
  {
    "objectID": "bigdata_lec4.html#interpreters",
    "href": "bigdata_lec4.html#interpreters",
    "title": "Big Data: Dask intro",
    "section": "Interpreters",
    "text": "Interpreters\n\n\n\nWikipedia definition\n\n\nAn interpreter is a computer program that directly executes instructions written in a programming or scripting language, without requiring them previously to have been compiled into a machine language program.\n\n\n\n\n\n\nExamples\n\n\n\nPython\nRuby\nLua\nJavascript"
  },
  {
    "objectID": "bigdata_lec4.html#cpython",
    "href": "bigdata_lec4.html#cpython",
    "title": "Big Data: Dask intro",
    "section": "CPython",
    "text": "CPython\n\n\n\n\nFlow\n\n\n\nRead Python code\nConvert Python into bytecode\nExecute bytecode inside a VM\nVM converts bytecode to machine code"
  },
  {
    "objectID": "bigdata_lec4.html#cpython-1",
    "href": "bigdata_lec4.html#cpython-1",
    "title": "Big Data: Dask intro",
    "section": "CPython",
    "text": "CPython"
  },
  {
    "objectID": "bigdata_lec4.html#compilers",
    "href": "bigdata_lec4.html#compilers",
    "title": "Big Data: Dask intro",
    "section": "Compilers",
    "text": "Compilers\n\n\n\nWikipedia definition\n\n\nSource code is compiled - in this context, translated into machine code for better performance.\n\n\n\n\n\n\nExamples\n\n\n\nC/C++\nGo\nPython (to intermediate VM code)\nJava\nCython"
  },
  {
    "objectID": "bigdata_lec4.html#compilers-1",
    "href": "bigdata_lec4.html#compilers-1",
    "title": "Big Data: Dask intro",
    "section": "Compilers",
    "text": "Compilers"
  },
  {
    "objectID": "bigdata_lec4.html#cython",
    "href": "bigdata_lec4.html#cython",
    "title": "Big Data: Dask intro",
    "section": "Cython",
    "text": "Cython\n\n\n\nDefinition\n\n\nCython is an optimising static compiler for the Python programming language.\n\nconverts Python code to C\nsupports static type declarations"
  },
  {
    "objectID": "bigdata_lec4.html#cython-1",
    "href": "bigdata_lec4.html#cython-1",
    "title": "Big Data: Dask intro",
    "section": "Cython",
    "text": "Cython"
  },
  {
    "objectID": "bigdata_lec4.html#cython-2",
    "href": "bigdata_lec4.html#cython-2",
    "title": "Big Data: Dask intro",
    "section": "Cython",
    "text": "Cython"
  },
  {
    "objectID": "bigdata_lec4.html#cython-3",
    "href": "bigdata_lec4.html#cython-3",
    "title": "Big Data: Dask intro",
    "section": "Cython",
    "text": "Cython\n\n\n\nPython code"
  },
  {
    "objectID": "bigdata_lec4.html#cython-4",
    "href": "bigdata_lec4.html#cython-4",
    "title": "Big Data: Dask intro",
    "section": "Cython",
    "text": "Cython\n\n\n\nAnnotated Python code"
  },
  {
    "objectID": "bigdata_lec4.html#cython-5",
    "href": "bigdata_lec4.html#cython-5",
    "title": "Big Data: Dask intro",
    "section": "Cython",
    "text": "Cython\n\n\n\nCython code"
  },
  {
    "objectID": "bigdata_lec4.html#parallel-cython",
    "href": "bigdata_lec4.html#parallel-cython",
    "title": "Big Data: Dask intro",
    "section": "Parallel Cython",
    "text": "Parallel Cython"
  },
  {
    "objectID": "bigdata_lec4.html#parallel-cython-1",
    "href": "bigdata_lec4.html#parallel-cython-1",
    "title": "Big Data: Dask intro",
    "section": "Parallel Cython",
    "text": "Parallel Cython"
  },
  {
    "objectID": "bigdata_lec4.html#jit",
    "href": "bigdata_lec4.html#jit",
    "title": "Big Data: Dask intro",
    "section": "JIT",
    "text": "JIT\n\n\n\nWikipedia definition\n\n\nA compilation (of computer code) during execution of a program (at run time) rather than before execution.\n\n\n\n\n\n\nFeatures\n\n\n\nwarm-up time: JIT causes a slight to noticeable delay in the initial execution of an application, due to the time taken to load and compile the input code.\nstatistics collection: performed by the system during runtime, shows how the program is actually running in the environment it is in; helps JIT to rearrange and recompile for optimum performance.\nparticularly suited for dynamic programming languages"
  },
  {
    "objectID": "bigdata_lec4.html#jit-1",
    "href": "bigdata_lec4.html#jit-1",
    "title": "Big Data: Dask intro",
    "section": "JIT",
    "text": "JIT\n\n\n\nExamples\n\n\n\nHotSpot Java Virtual Machine\nLuaJIT\nNumba\nPyPy"
  },
  {
    "objectID": "bigdata_lec4.html#numba",
    "href": "bigdata_lec4.html#numba",
    "title": "Big Data: Dask intro",
    "section": "Numba",
    "text": "Numba"
  },
  {
    "objectID": "bigdata_lec4.html#numba-1",
    "href": "bigdata_lec4.html#numba-1",
    "title": "Big Data: Dask intro",
    "section": "Numba",
    "text": "Numba\n\n\n\nDescription\n\n\n\nNumba translates Python byte-code to machine code immediately before execution to improve the execution speed.\nFor that we add a @jit decorator\nWorks well for numeric operations, NumPy, and loops"
  },
  {
    "objectID": "bigdata_lec4.html#numba-2",
    "href": "bigdata_lec4.html#numba-2",
    "title": "Big Data: Dask intro",
    "section": "Numba",
    "text": "Numba\n\n\n\nSteps\n\n\n\nread the Python bytecode for a decorated function\ncombine it with information about the types of the input arguments to the function\nanalyze and optimize the code\nuse the LLVM compiler library to generate a machine code version of the function, tailored to specific CPU capabilities."
  },
  {
    "objectID": "bigdata_lec4.html#numba-3",
    "href": "bigdata_lec4.html#numba-3",
    "title": "Big Data: Dask intro",
    "section": "Numba",
    "text": "Numba\n\n\n\nWorks great"
  },
  {
    "objectID": "bigdata_lec4.html#numba-4",
    "href": "bigdata_lec4.html#numba-4",
    "title": "Big Data: Dask intro",
    "section": "Numba",
    "text": "Numba\n\n\n\nNope"
  },
  {
    "objectID": "bigdata_lec4.html#numba-5",
    "href": "bigdata_lec4.html#numba-5",
    "title": "Big Data: Dask intro",
    "section": "Numba",
    "text": "Numba"
  },
  {
    "objectID": "bigdata_lec4.html#numba-6",
    "href": "bigdata_lec4.html#numba-6",
    "title": "Big Data: Dask intro",
    "section": "Numba",
    "text": "Numba\n\n\n\nSequential"
  },
  {
    "objectID": "bigdata_lec4.html#numba-7",
    "href": "bigdata_lec4.html#numba-7",
    "title": "Big Data: Dask intro",
    "section": "Numba",
    "text": "Numba\n\n\n\nParallel"
  },
  {
    "objectID": "bigdata_lec4.html#numpy",
    "href": "bigdata_lec4.html#numpy",
    "title": "Big Data: Dask intro",
    "section": "Numpy",
    "text": "Numpy\n\n\n\nWhy so fast?\n\n\n\nOptimized C code\nDensely packed arrays\nUses BLAS - Basic Linear Algebra Subroutines."
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-1",
    "href": "bigdata_lec4.html#rustpyo3-example-1",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nDescription\n\n\nWe show an example of a simple algebraic cipher that utilizes PyO3 bindings to speed up encoding/decoding.\n\n\n\n\n\n\nCipher definition\n\n\nThe basic mechanism for encrypting a message using a shared secret key is called a cipher (or encryption scheme)"
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-2",
    "href": "bigdata_lec4.html#rustpyo3-example-2",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\n\n\n\nDefinition\n\n\nEncryption and decryption use the same secret key.\n\n\n\n\n\n\nExamples\n\n\n\nAES\nSalsa20\nTwofish\nDES"
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-3",
    "href": "bigdata_lec4.html#rustpyo3-example-3",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nTypes\n\n\n\nblock ciphers\nstream ciphers\nhash functions"
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-4",
    "href": "bigdata_lec4.html#rustpyo3-example-4",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nOverview\n\n\n\\[\nf: \\mathcal{K}\\times\\mathcal{D} \\rightarrow C\n\\] where\n\n\\(\\mathcal{K}\\) is key space\n\\(\\mathcal{D}\\) is domain (or message space)\n\\(\\mathcal{C}\\) is co-domain (or ciphertext space)"
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-5",
    "href": "bigdata_lec4.html#rustpyo3-example-5",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nShannon cipher\n\n\nA Shannon cipher is a pair \\(\\mathcal{E} = (E,D)\\) of functions:\n\nThe function \\(E\\) (the encryption function) takes as input a key \\(k\\) and message \\(m\\) (also called plaintext) and produces as output a ciphertext \\(c):\\)$ c = E(k,m) $$\n\n\\(c\\) is the encryption of \\(m\\) under \\(k\\).\n\nThe function \\(D\\) (the decryption function) takes as input a key \\(k\\) and ciphertext \\(c\\), and produces a message \\(m\\): \\[\nm = D(k,c)\n\\]\n\n\\(m\\) is the decryption of \\(c\\) under \\(k\\)."
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-6",
    "href": "bigdata_lec4.html#rustpyo3-example-6",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nCorrectness property\n\n\nFor all keys \\(k\\) and messages \\(m\\), we have \\[\nD(k, E(k,m)) = m\n\\]"
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-7",
    "href": "bigdata_lec4.html#rustpyo3-example-7",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nParameters\n\n\nNow we describe the cipher.\nFirst, we define cipher parameters:\n\nAn alphabet \\(A\\) with size \\(L \\equiv |A|\\).\nA matrix \\(M\\) with size \\(N \\gg L\\)\n\\(\\sigma_1, \\sigma_2\\) are some permutations \\(N \\rightarrow N\\)\n\\(\\phi\\) is some bit sequence of length \\(P\\): \\(\\phi \\in \\{0,1\\}^P\\)\n\nA triple \\((\\sigma_1, \\sigma_2, \\phi)\\) will be our secret key.\nWe define each symbol \\(z\\) by a corresponding set of diagonals \\(D\\) in the matrix \\(M\\), so that \\(\\forall (x,y) \\in D: x - y = z (\\mod L)\\) (see Figure 1)."
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-8",
    "href": "bigdata_lec4.html#rustpyo3-example-8",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example"
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-9",
    "href": "bigdata_lec4.html#rustpyo3-example-9",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nEncoder flow\n\n\nSuppose we receive some text \\(T\\) containing symbols to be encoded.\nFor each \\(t_i \\in T\\), obtain its numeric representation \\(z_i\\). \\[\nz_i \\in [0,L)\n\\] Then we map each \\(z_i\\) to a pair of matrix coordinates \\((x_i, y_i)\\) such that:\n\nFirst, we pick a random \\(x_i \\in [0,N)\\) (e.g., horizontal coordinate in a matrix)\nThen, we randomly pick some \\(y_i \\in [0,N)\\) such that: \\[\nx_i - y_i = z_i (\\mod L)\n\\]"
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-10",
    "href": "bigdata_lec4.html#rustpyo3-example-10",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nEncoder flow\n\n\nHaving thus obtained a sequence \\(\\{(x_i, y_i), \\, i \\in [0, |T|) \\}\\), we now apply permutations \\(\\sigma_k: [0,N) \\rightarrow [0,N), \\, k=1,2\\): \\[\n\\text{ciphertext } (\\xi,\\eta) := (\\sigma_j(x),\\sigma_{j+1}(y))\n\\] where \\[\n\\sigma_j = \\begin{cases}\n\\sigma_1, \\; \\text{if}\\; \\phi_j=0,\\\\\n\\sigma_2\\; \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-11",
    "href": "bigdata_lec4.html#rustpyo3-example-11",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nDecoder flow\n\n\nBelow are steps executing during decoding phase:\n\nReceive encoded ciphertext \\(\\{(\\xi_i, \\eta_i)\\}\\).\nApply inverse permutations \\(\\sigma_j^{-1}, \\sigma_{j+1}^{-1}\\): \\[\n(x_i,y_i) = (\\sigma_j^{-1}(\\xi_i), \\sigma_{j+1}^{-1}(\\eta_i))\n\\]\nFind \\(z_i\\): \\[\nz_i = x_i - y_i \\; (\\mod L)\n\\]"
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-12",
    "href": "bigdata_lec4.html#rustpyo3-example-12",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nImplementation: PyO3\n\n\n\nRust bindings for Python extension modules\n\n\n\n\n\n\n\nUsers\n\n\n\nQiskit https://www.ibm.com/quantum/qiskit\nPython Cryptography package https://github.com/pyca/cryptography\nScallop https://www.scallop-lang.org\nHuggingFace Tokenizers https://github.com/huggingface/tokenizers"
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-13",
    "href": "bigdata_lec4.html#rustpyo3-example-13",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nRust wrapper\n\n\n/// A Python module implemented in Rust.\n#[pymodule]\nfn cipher(m: &Bound&lt;'_, PyModule&gt;) -&gt; PyResult&lt;()&gt; {\n    m.add_function(wrap_pyfunction!(encode, m)?)?;\n    m.add_function(wrap_pyfunction!(decode, m)?)?;\n    Ok(())\n}"
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-14",
    "href": "bigdata_lec4.html#rustpyo3-example-14",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nEncoding\n\n\n// Generate random permutation of N integers\nlet mut numbers: Vec&lt;usize&gt; = (0..N).collect();\nlet mut rng = thread_rng();\nnumbers.shuffle(&mut rng);\nlet permutation = numbers;\n\n// Generate pairs for each z\nlet mut pairs = Vec::with_capacity(zs.len());\nfor &z in &zs {\n    // Generate random x between 0 and N-1\n    let x = rng.gen_range(0..N);\n\n    // Compute y such that x - y = z (mod L)\n    let y = if x &gt;= z {\n        (x - z) % L\n    } else {\n        ((x + L) - z) % L\n    };\n\n    // Apply permutation to x and y\n    let px = permutation[x];\n    let py = permutation[y];\n    pairs.push((px, py));"
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-15",
    "href": "bigdata_lec4.html#rustpyo3-example-15",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nDecoding\n\n\n// Create inverse permutation\nlet mut inverse = vec![0; N];\nfor (i, &p) in permutation.iter().enumerate() {\n    inverse[p] = i;\n}\n\n// Recover z for each pair\nlet mut zs = Vec::with_capacity(pairs.len());\nfor &(px, py) in pairs {\n    // Apply inverse permutation to get x and y\n    let x = inverse[px];\n    let y = inverse[py];\n\n    // Compute z = x - y (mod L)\n    let z = if x &gt;= y {\n        (x - y) % L\n    } else {\n        ((x + L) - y) % L\n    };\n    zs.push(z);\n}"
  },
  {
    "objectID": "bigdata_lec4.html#rustpyo3-example-16",
    "href": "bigdata_lec4.html#rustpyo3-example-16",
    "title": "Big Data: Dask intro",
    "section": "Rust/PyO3 example",
    "text": "Rust/PyO3 example\n\n\n\nPython\n\n\nmkdir cipher; cd cipher\npython -m venv venv\n. venv/bin/activate\n!pip install maturin\nmaturin init\nmaturin develop\n\n\n\n\n\n\nUsage\n\n\nimport cipher\nencoded = cipher.encode(\"Whazzuuupppp??!!\")\ndecoded = cipher.decode(encoded)"
  },
  {
    "objectID": "bigdata_lec4.html#distributed-computing-1",
    "href": "bigdata_lec4.html#distributed-computing-1",
    "title": "Big Data: Dask intro",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nTypes\n\n\n\nCluster computing: collection of similar workstations\nGrid computing: federation of different computer systems\nCloud computing: provide the facilities to dynamically construct an infrastructure and compose what is needed from available services. Not only providing lots of resources."
  },
  {
    "objectID": "bigdata_lec4.html#distributed-computing-2",
    "href": "bigdata_lec4.html#distributed-computing-2",
    "title": "Big Data: Dask intro",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nOriginal Beowulf cluster at NASA (1994)"
  },
  {
    "objectID": "bigdata_lec4.html#distributed-computing-3",
    "href": "bigdata_lec4.html#distributed-computing-3",
    "title": "Big Data: Dask intro",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nBeowulf cluster diagram"
  },
  {
    "objectID": "bigdata_lec4.html#distributed-computing-4",
    "href": "bigdata_lec4.html#distributed-computing-4",
    "title": "Big Data: Dask intro",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\n\n\nGrid architecture diagram (Foster et al. 2001)\n\n\n\n\n\n\n\n\n\n\nNotation\n\n\n\n\nfabric: interfaces to local resources at a specific site\nconnectivity: communication protocols for supporting grid transactions that span the usage of multiple resources\nresource: responsible for managing a single resource\ncollective: handling access to multiple resources and typically consists of services for resource discovery, allocation and scheduling of tasks onto multiple resources, data replication, and so on\napplication: applications that operate within a virtual organization"
  },
  {
    "objectID": "bigdata_lec4.html#distributed-computing-5",
    "href": "bigdata_lec4.html#distributed-computing-5",
    "title": "Big Data: Dask intro",
    "section": "Distributed computing",
    "text": "Distributed computing\n\n\n\nCloud architecture"
  },
  {
    "objectID": "applied.html",
    "href": "applied.html",
    "title": "Applied Data Analytics",
    "section": "",
    "text": "Course will cover the following:\n\ndata fetching/preprocessing\nspecialized data types:\n\ngeospatial\nfinancial\nsupply chain management\nmore\n\nvisualization/presentation (e.g. Gradio)\nLLM toolkits (LangChain/LangFlow)\nML Pipelines (MLFlow/KubeFlow/AirFlow/Prefect/Metaflow)\nMLOps\nAmazon Web Services\nDocker"
  },
  {
    "objectID": "applied.html#overview",
    "href": "applied.html#overview",
    "title": "Applied Data Analytics",
    "section": "",
    "text": "Course will cover the following:\n\ndata fetching/preprocessing\nspecialized data types:\n\ngeospatial\nfinancial\nsupply chain management\nmore\n\nvisualization/presentation (e.g. Gradio)\nLLM toolkits (LangChain/LangFlow)\nML Pipelines (MLFlow/KubeFlow/AirFlow/Prefect/Metaflow)\nMLOps\nAmazon Web Services\nDocker"
  },
  {
    "objectID": "applied.html#lectures",
    "href": "applied.html#lectures",
    "title": "Applied Data Analytics",
    "section": "Lectures",
    "text": "Lectures\nSlides"
  },
  {
    "objectID": "applied.html#labs",
    "href": "applied.html#labs",
    "title": "Applied Data Analytics",
    "section": "Labs",
    "text": "Labs\nLab 1"
  },
  {
    "objectID": "bigdata_lec2.html#moores-law",
    "href": "bigdata_lec2.html#moores-law",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\n\n\nDefinition (1965)\n\n\nNumber of transistors in an integrated circuit (IC) doubles about every two years."
  },
  {
    "objectID": "bigdata_lec2.html#moores-law-1",
    "href": "bigdata_lec2.html#moores-law-1",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law"
  },
  {
    "objectID": "bigdata_lec2.html#moores-law-2",
    "href": "bigdata_lec2.html#moores-law-2",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law"
  },
  {
    "objectID": "bigdata_lec2.html#moores-law-3",
    "href": "bigdata_lec2.html#moores-law-3",
    "title": "Big Data: Speeding up computation",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\n\n\nTrends\n\n\n\nGordon Moore: Moore’s law will end by around 2025.\nNvidia CEO Jensen Huang: declared Moore’s law dead in 2022.\n\n\n\n\n\n\n\nPat Gelsinger, Intel CEO, end of 2023\n\n\n\nWe’re no longer in the golden era of Moore’s Law, it’s much, much harder now, so we’re probably doubling effectively closer to every three years now, so we’ve definitely seen a slowing."
  },
  {
    "objectID": "bigdata_lec2.html#edholms-law",
    "href": "bigdata_lec2.html#edholms-law",
    "title": "Big Data: Speeding up computation",
    "section": "Edholm’s law",
    "text": "Edholm’s law\n\n\n\nDefinition (2004)\n\n\n\nthree categories of telecommunication, namely\n\nwireless (mobile),\nnomadic (wireless without mobility)\nand wired networks (fixed),\n\nare in lockstep and gradually converging\nthe bandwidth and data rates double every 18 months, which has proven to be true since the 1970s"
  },
  {
    "objectID": "bigdata_lec2.html#edholms-law-1",
    "href": "bigdata_lec2.html#edholms-law-1",
    "title": "Big Data: Speeding up computation",
    "section": "Edholm’s law",
    "text": "Edholm’s law"
  },
  {
    "objectID": "bigdata_lec2.html#edholms-law-2",
    "href": "bigdata_lec2.html#edholms-law-2",
    "title": "Big Data: Speeding up computation",
    "section": "Edholm’s law",
    "text": "Edholm’s law\n\n\n\n\nData deluge\n\n\n\n90% of data humankind has produced happened in the last two years.\n80% of data could be unstructured\n99% of data produced is never analyzed"
  },
  {
    "objectID": "bigdata_lec2.html#core-counts",
    "href": "bigdata_lec2.html#core-counts",
    "title": "Big Data: Speeding up computation",
    "section": "Core counts",
    "text": "Core counts"
  },
  {
    "objectID": "bigdata_lec2.html#concurrency-vs-parallelism",
    "href": "bigdata_lec2.html#concurrency-vs-parallelism",
    "title": "Big Data: Speeding up computation",
    "section": "Concurrency vs Parallelism",
    "text": "Concurrency vs Parallelism\n\n\n\n\n\nConcurrency Parallelism\n\n\nIndividual steps of both tasks are executed in an interleaved fashion\n\n\n\n\n\n\n\nParallelism\n\n\nTask statements are executed at the same time."
  },
  {
    "objectID": "bigdata_lec2.html#process",
    "href": "bigdata_lec2.html#process",
    "title": "Big Data: Speeding up computation",
    "section": "Process",
    "text": "Process\n\n\n\nDefinition\n\n\nA process can be defined as an instance of a running program with its own memory.\nAlternatively: a context maintained for an executing program.\nProcesses have:\n\nlifetimes\nparents\nchildren.\nmemory/resources allocated."
  },
  {
    "objectID": "bigdata_lec2.html#process-1",
    "href": "bigdata_lec2.html#process-1",
    "title": "Big Data: Speeding up computation",
    "section": "Process",
    "text": "Process"
  },
  {
    "objectID": "bigdata_lec2.html#threads",
    "href": "bigdata_lec2.html#threads",
    "title": "Big Data: Speeding up computation",
    "section": "Threads",
    "text": "Threads\n\n\n\nDefinition\n\n\nA lightweight unit of execution within a process that can operate independently.\nThread is a basic unit to which the operating system allocates processor time.\nManaged with help of thread context, which consists of processor context and information required for thread management."
  },
  {
    "objectID": "bigdata_lec2.html#threads-1",
    "href": "bigdata_lec2.html#threads-1",
    "title": "Big Data: Speeding up computation",
    "section": "Threads",
    "text": "Threads"
  },
  {
    "objectID": "bigdata_lec2.html#green-threadscoroutines",
    "href": "bigdata_lec2.html#green-threadscoroutines",
    "title": "Big Data: Speeding up computation",
    "section": "Green threads/Coroutines",
    "text": "Green threads/Coroutines\n\n\n\nDefinition\n\n\nThese are threads managed by the process runtime, multiplexed onto OS threads."
  },
  {
    "objectID": "bigdata_lec2.html#comparison-table",
    "href": "bigdata_lec2.html#comparison-table",
    "title": "Big Data: Speeding up computation",
    "section": "Comparison table",
    "text": "Comparison table"
  },
  {
    "objectID": "bigdata_lec2.html#imperative",
    "href": "bigdata_lec2.html#imperative",
    "title": "Big Data: Speeding up computation",
    "section": "Imperative",
    "text": "Imperative\n\n\n\nWikipedia definition\n\n\nA programming paradigm of software that uses statements that change a program’s state.\nImperative program is a step-by-step description of program’s algorithm.\n\n\n\n\n\n\nExamples\n\n\n\nFortran\nCOBOL\nC\nPython\nGo"
  },
  {
    "objectID": "bigdata_lec2.html#imperative-1",
    "href": "bigdata_lec2.html#imperative-1",
    "title": "Big Data: Speeding up computation",
    "section": "Imperative",
    "text": "Imperative\n\n\n\nCons\n\n\n\nDifficult to parallelize\nLots of state management\n\n\n\n\n\n\n\nPros\n\n\n\nIntuitive concept, maps well to how people think, program as a recipe.\nEasy to optimize by the compiler"
  },
  {
    "objectID": "bigdata_lec2.html#functional",
    "href": "bigdata_lec2.html#functional",
    "title": "Big Data: Speeding up computation",
    "section": "Functional",
    "text": "Functional\n\n\n\nWikipedia definition\n\n\nA programming paradigm where programs are constructed by applying and composing functions.\n\n\n\n\n\n\nExamples\n\n\n\nML (Ocaml)\nLisp\nHaskell\nJulia"
  },
  {
    "objectID": "bigdata_lec2.html#functional-1",
    "href": "bigdata_lec2.html#functional-1",
    "title": "Big Data: Speeding up computation",
    "section": "Functional",
    "text": "Functional\n\n\n\nCons\n\n\n\nOften non-intuitive to reason about\nDepending on a specific algorithm, might be slower\n\n\n\n\n\n\n\nPros\n\n\n\nEasier to parallelize\nLend themselves beautifully to certain types of problems"
  },
  {
    "objectID": "bigdata_lec2.html#object-oriented",
    "href": "bigdata_lec2.html#object-oriented",
    "title": "Big Data: Speeding up computation",
    "section": "Object-oriented",
    "text": "Object-oriented\n\n\n\nWikipedia definition\n\n\nA programming paradigm based on the concept of objects, which can contain data and code: data in the form of fields (often known as attributes or properties), and code in the form of procedures (often known as methods).\n\n\n\n\n\n\nExamples\n\n\n\nSmalltalk\nJava\nC++\nPython\nC#"
  },
  {
    "objectID": "bigdata_lec2.html#object-oriented-1",
    "href": "bigdata_lec2.html#object-oriented-1",
    "title": "Big Data: Speeding up computation",
    "section": "Object-oriented",
    "text": "Object-oriented"
  },
  {
    "objectID": "bigdata_lec2.html#object-oriented-2",
    "href": "bigdata_lec2.html#object-oriented-2",
    "title": "Big Data: Speeding up computation",
    "section": "Object-oriented",
    "text": "Object-oriented\n\n\n\nCons\n\n\n\nDoes not map well to many problems\nLots of state management\n\n\n\n\n\n\n\nPros\n\n\n\nIntuitively easy to grasp, as human thinking is largely noun-oriented\nUseful for UIs"
  },
  {
    "objectID": "bigdata_lec2.html#symbolic",
    "href": "bigdata_lec2.html#symbolic",
    "title": "Big Data: Speeding up computation",
    "section": "Symbolic",
    "text": "Symbolic\n\n\n\nWikipedia definition\n\n\nA programming paradigm in which the program can manipulate its own formulas and program components as if they were plain data.\n\n\n\n\n\n\nExamples\n\n\n\nLisp\nProlog\nJulia"
  },
  {
    "objectID": "bigdata_lec2.html#lisp",
    "href": "bigdata_lec2.html#lisp",
    "title": "Big Data: Speeding up computation",
    "section": "Lisp",
    "text": "Lisp"
  },
  {
    "objectID": "bigdata_lec2.html#prolog",
    "href": "bigdata_lec2.html#prolog",
    "title": "Big Data: Speeding up computation",
    "section": "Prolog",
    "text": "Prolog\n :::"
  },
  {
    "objectID": "bigdata_lec2.html#typing-1",
    "href": "bigdata_lec2.html#typing-1",
    "title": "Big Data: Speeding up computation",
    "section": "Typing",
    "text": "Typing\n\n\n\nStatic vs dynamic\n\n\n\nstatic: types are known and checked before running the program\ndynamic: types become known when the program is running\n\n\n\n\n\n\n\nStrong vs weak\n\n\n\nstrong: variable types are not changed easily\nweak: types can be changed by the compiler if necessary"
  },
  {
    "objectID": "bigdata_lec2.html#typing-2",
    "href": "bigdata_lec2.html#typing-2",
    "title": "Big Data: Speeding up computation",
    "section": "Typing",
    "text": "Typing"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analytics",
    "section": "",
    "text": "Big data analytics / Applied data analytics courses."
  },
  {
    "objectID": "bigdata_lab1.html",
    "href": "bigdata_lab1.html",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "Primary docs:\n\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/enhancingperf.html\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html\n\nPerformance deps for Pandas:\n\nhttps://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#install-recommended-dependencies\n\n\n\n\n\n\n\nchunksize parameter in Pandas functions.\n\n\n\nUse to_numeric (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html) for downcasting\n\n\n\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n\n\n\n\nuse DataFrame.memory_usage(deep=True) func.\nuse DataFrame.info() func\n\n\n\n\n\nWe’ll get the sample dataset from https://www.kaggle.com/datasets/anthonytherrien/depression-dataset.\n\nimport pandas as pd\n\ndd = pd.read_csv('files/depression_data.csv')\ndd.head()\n\n\n\n\n\n\n\n\nName\nAge\nMarital Status\nEducation Level\nNumber of Children\nSmoking Status\nPhysical Activity Level\nEmployment Status\nIncome\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\n0\nChristine Barker\n31\nMarried\nBachelor's Degree\n2\nNon-smoker\nActive\nUnemployed\n26265.67\nModerate\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n1\nJacqueline Lewis\n55\nMarried\nHigh School\n1\nNon-smoker\nSedentary\nEmployed\n42710.36\nHigh\nUnhealthy\nFair\nYes\nNo\nNo\nYes\n\n\n2\nShannon Church\n78\nWidowed\nMaster's Degree\n1\nNon-smoker\nSedentary\nEmployed\n125332.79\nLow\nUnhealthy\nGood\nNo\nNo\nYes\nNo\n\n\n3\nCharles Jordan\n58\nDivorced\nMaster's Degree\n3\nNon-smoker\nModerate\nUnemployed\n9992.78\nModerate\nModerate\nPoor\nNo\nNo\nNo\nNo\n\n\n4\nMichael Rich\n18\nSingle\nHigh School\n0\nNon-smoker\nSedentary\nUnemployed\n8595.08\nLow\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n\n\n\n\n\nLet’s see some memory usage stats:\n\ndd.info(memory_usage='deep')\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 413768 entries, 0 to 413767\nData columns (total 16 columns):\n #   Column                        Non-Null Count   Dtype  \n---  ------                        --------------   -----  \n 0   Name                          413768 non-null  object \n 1   Age                           413768 non-null  int64  \n 2   Marital Status                413768 non-null  object \n 3   Education Level               413768 non-null  object \n 4   Number of Children            413768 non-null  int64  \n 5   Smoking Status                413768 non-null  object \n 6   Physical Activity Level       413768 non-null  object \n 7   Employment Status             413768 non-null  object \n 8   Income                        413768 non-null  float64\n 9   Alcohol Consumption           413768 non-null  object \n 10  Dietary Habits                413768 non-null  object \n 11  Sleep Patterns                413768 non-null  object \n 12  History of Mental Illness     413768 non-null  object \n 13  History of Substance Abuse    413768 non-null  object \n 14  Family History of Depression  413768 non-null  object \n 15  Chronic Medical Conditions    413768 non-null  object \ndtypes: float64(1), int64(2), object(13)\nmemory usage: 294.9 MB\n\n\nNote: Pandas stores memory in blocks, managed by BlockManager class. There is a separate block class for each type, like ObjectBlock or FloatBlock.\nNice write-up here: https://uwekorn.com/2020/05/24/the-one-pandas-internal.html.\nLet’s examine memory usage for each type:\n\nfor dtype in ['float','int','object']:\n    selected_dtype = dd.select_dtypes(include=[dtype])\n    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n    mean_usage_mb = mean_usage_b / 1024 ** 2\n    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))\n\nAverage memory usage for float columns: 1.58 MB\nAverage memory usage for int columns: 2.10 MB\nAverage memory usage for object columns: 20.39 MB\n\n\n\n\nLet’s first use iinfo to check ranges for different subtypes:\n\nimport numpy as np\nint_types = [\"uint8\", \"int8\", \"int16\", \"int32\", \"int64\"]\nfor it in int_types:\n    print(np.iinfo(it))\n\nMachine parameters for uint8\n---------------------------------------------------------------\nmin = 0\nmax = 255\n---------------------------------------------------------------\n\nMachine parameters for int8\n---------------------------------------------------------------\nmin = -128\nmax = 127\n---------------------------------------------------------------\n\nMachine parameters for int16\n---------------------------------------------------------------\nmin = -32768\nmax = 32767\n---------------------------------------------------------------\n\nMachine parameters for int32\n---------------------------------------------------------------\nmin = -2147483648\nmax = 2147483647\n---------------------------------------------------------------\n\nMachine parameters for int64\n---------------------------------------------------------------\nmin = -9223372036854775808\nmax = 9223372036854775807\n---------------------------------------------------------------\n\n\n\nWe can use pd.to_numeric() to downcast numeric types.\nFirst, let’s write a helper function for memory usage display:\n\ndef mem_usage(pandas_obj):\n    if isinstance(pandas_obj,pd.DataFrame):\n        usage_b = pandas_obj.memory_usage(deep=True).sum()\n    else: # we assume if not a df it's a series\n        usage_b = pandas_obj.memory_usage(deep=True)\n    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n    return \"{:03.2f} MB\".format(usage_mb)\n\nNote that “Age” and “Number of Children” columns can be presented as unsigned ints. Let’s convert:\n\ndd_int = dd.select_dtypes(include=['int'])\nconverted_int = dd_int.apply(pd.to_numeric,downcast='unsigned')\nprint(\"Before: \", mem_usage(dd_int))\nprint(\"After: \", mem_usage(converted_int))\ncompare_ints = pd.concat([dd_int.dtypes,converted_int.dtypes],axis=1)\ncompare_ints.columns = ['before','after']\ncompare_ints.apply(pd.Series.value_counts)\n\nBefore:  6.31 MB\nAfter:  0.79 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nuint8\nNaN\n2.0\n\n\nint64\n2.0\nNaN\n\n\n\n\n\n\n\nNice. Now let’s process float columns (Income):\n\ndd_float = dd.select_dtypes(include=['float'])\nconverted_float = dd_float.apply(pd.to_numeric,downcast='float')\nprint(\"Before: \", mem_usage(dd_float))\nprint(\"After: \", mem_usage(converted_float))\ncompare_floats = pd.concat([dd_float.dtypes,converted_float.dtypes],axis=1)\ncompare_floats.columns = ['before','after']\ncompare_floats.apply(pd.Series.value_counts)\n\nBefore:  3.16 MB\nAfter:  3.16 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nfloat64\n1\n1\n\n\n\n\n\n\n\nNothing spectacular.\nNow let’s create a new optimized DataFrame:\n\noptimized_dd = dd.copy()\noptimized_dd[converted_int.columns] = converted_int\noptimized_dd[converted_float.columns] = converted_float\nprint(mem_usage(dd))\nprint(mem_usage(optimized_dd))\n\n294.92 MB\n289.40 MB\n\n\nJust a bit. Let’s proceed with object types.\nFirst, read https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/.\nNow, back to the dataset.\nWe can use categoricals (https://pandas.pydata.org/pandas-docs/stable/categorical.html) to optimize object columns.\n\nLet’s look at the number of unique values for each object type:\n\ndd_obj = dd.select_dtypes(include=['object']).copy()\ndd_obj.describe()\n\n\n\n\n\n\n\n\nName\nMarital Status\nEducation Level\nSmoking Status\nPhysical Activity Level\nEmployment Status\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\ncount\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n\n\nunique\n196851\n4\n5\n3\n3\n2\n3\n3\n3\n2\n2\n2\n2\n\n\ntop\nMichael Smith\nMarried\nBachelor's Degree\nNon-smoker\nSedentary\nEmployed\nModerate\nUnhealthy\nFair\nNo\nNo\nNo\nNo\n\n\nfreq\n198\n240444\n124329\n247416\n176850\n265659\n173440\n170817\n196789\n287943\n284880\n302515\n277561\n\n\n\n\n\n\n\nLet’s start with one column first: “Education Level”\n\nedu_level = dd_obj[\"Education Level\"]\nprint(edu_level.head())\nedu_level_cat = edu_level.astype('category')\nprint(edu_level_cat.head())\n\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: object\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: category\nCategories (5, object): ['Associate Degree', 'Bachelor's Degree', 'High School', 'Master's Degree', 'PhD']\n\n\nWe can look at the category codes:\n\nedu_level_cat.head().cat.codes\n\n0    1\n1    2\n2    3\n3    3\n4    2\ndtype: int8\n\n\nCompare memory usage:\n\nprint(mem_usage(edu_level))\nprint(mem_usage(edu_level_cat))\n\n24.92 MB\n0.40 MB\n\n\nWe should only convert objects to categoricals if most values are repeated. We have to pick a threshold, say, 25% of unique values.\nNote: when reading a csv, we can also provide a dtype dictionary param with preferred types.\nConverting all object columns and creating a new optimized DataFrame is left as part of Exercise 1.\n\n\n\n\n\nPick your own dataset from\n\n\nKaggle\nHuggingFace\nor https://archive.ics.uci.edu.\n\n\nUse Dora library installed in Applied lab1 for data cleaning.\nCreate a data type describing the data structures you’re working with in your Pandas DataFrame. Use mypy for type annotations.\n\n\n\n\n\n\n\nNote\n\n\n\nmypy does not improve performance. However, it makes the code easier to understand.\n\n\n\nPerform numerical and object types conversions aimed at minimizing storage space, as outlined above.\nMeasure performance impact and plot it via e.g. matplotlib."
  },
  {
    "objectID": "bigdata_lab1.html#notes",
    "href": "bigdata_lab1.html#notes",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "chunksize parameter in Pandas functions.\n\n\n\nUse to_numeric (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html) for downcasting\n\n\n\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n\n\n\n\nuse DataFrame.memory_usage(deep=True) func.\nuse DataFrame.info() func"
  },
  {
    "objectID": "bigdata_lab1.html#storage-optimization",
    "href": "bigdata_lab1.html#storage-optimization",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "We’ll get the sample dataset from https://www.kaggle.com/datasets/anthonytherrien/depression-dataset.\n\nimport pandas as pd\n\ndd = pd.read_csv('files/depression_data.csv')\ndd.head()\n\n\n\n\n\n\n\n\nName\nAge\nMarital Status\nEducation Level\nNumber of Children\nSmoking Status\nPhysical Activity Level\nEmployment Status\nIncome\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\n0\nChristine Barker\n31\nMarried\nBachelor's Degree\n2\nNon-smoker\nActive\nUnemployed\n26265.67\nModerate\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n1\nJacqueline Lewis\n55\nMarried\nHigh School\n1\nNon-smoker\nSedentary\nEmployed\n42710.36\nHigh\nUnhealthy\nFair\nYes\nNo\nNo\nYes\n\n\n2\nShannon Church\n78\nWidowed\nMaster's Degree\n1\nNon-smoker\nSedentary\nEmployed\n125332.79\nLow\nUnhealthy\nGood\nNo\nNo\nYes\nNo\n\n\n3\nCharles Jordan\n58\nDivorced\nMaster's Degree\n3\nNon-smoker\nModerate\nUnemployed\n9992.78\nModerate\nModerate\nPoor\nNo\nNo\nNo\nNo\n\n\n4\nMichael Rich\n18\nSingle\nHigh School\n0\nNon-smoker\nSedentary\nUnemployed\n8595.08\nLow\nModerate\nFair\nYes\nNo\nYes\nYes\n\n\n\n\n\n\n\nLet’s see some memory usage stats:\n\ndd.info(memory_usage='deep')\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 413768 entries, 0 to 413767\nData columns (total 16 columns):\n #   Column                        Non-Null Count   Dtype  \n---  ------                        --------------   -----  \n 0   Name                          413768 non-null  object \n 1   Age                           413768 non-null  int64  \n 2   Marital Status                413768 non-null  object \n 3   Education Level               413768 non-null  object \n 4   Number of Children            413768 non-null  int64  \n 5   Smoking Status                413768 non-null  object \n 6   Physical Activity Level       413768 non-null  object \n 7   Employment Status             413768 non-null  object \n 8   Income                        413768 non-null  float64\n 9   Alcohol Consumption           413768 non-null  object \n 10  Dietary Habits                413768 non-null  object \n 11  Sleep Patterns                413768 non-null  object \n 12  History of Mental Illness     413768 non-null  object \n 13  History of Substance Abuse    413768 non-null  object \n 14  Family History of Depression  413768 non-null  object \n 15  Chronic Medical Conditions    413768 non-null  object \ndtypes: float64(1), int64(2), object(13)\nmemory usage: 294.9 MB\n\n\nNote: Pandas stores memory in blocks, managed by BlockManager class. There is a separate block class for each type, like ObjectBlock or FloatBlock.\nNice write-up here: https://uwekorn.com/2020/05/24/the-one-pandas-internal.html.\nLet’s examine memory usage for each type:\n\nfor dtype in ['float','int','object']:\n    selected_dtype = dd.select_dtypes(include=[dtype])\n    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n    mean_usage_mb = mean_usage_b / 1024 ** 2\n    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))\n\nAverage memory usage for float columns: 1.58 MB\nAverage memory usage for int columns: 2.10 MB\nAverage memory usage for object columns: 20.39 MB\n\n\n\n\nLet’s first use iinfo to check ranges for different subtypes:\n\nimport numpy as np\nint_types = [\"uint8\", \"int8\", \"int16\", \"int32\", \"int64\"]\nfor it in int_types:\n    print(np.iinfo(it))\n\nMachine parameters for uint8\n---------------------------------------------------------------\nmin = 0\nmax = 255\n---------------------------------------------------------------\n\nMachine parameters for int8\n---------------------------------------------------------------\nmin = -128\nmax = 127\n---------------------------------------------------------------\n\nMachine parameters for int16\n---------------------------------------------------------------\nmin = -32768\nmax = 32767\n---------------------------------------------------------------\n\nMachine parameters for int32\n---------------------------------------------------------------\nmin = -2147483648\nmax = 2147483647\n---------------------------------------------------------------\n\nMachine parameters for int64\n---------------------------------------------------------------\nmin = -9223372036854775808\nmax = 9223372036854775807\n---------------------------------------------------------------\n\n\n\nWe can use pd.to_numeric() to downcast numeric types.\nFirst, let’s write a helper function for memory usage display:\n\ndef mem_usage(pandas_obj):\n    if isinstance(pandas_obj,pd.DataFrame):\n        usage_b = pandas_obj.memory_usage(deep=True).sum()\n    else: # we assume if not a df it's a series\n        usage_b = pandas_obj.memory_usage(deep=True)\n    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n    return \"{:03.2f} MB\".format(usage_mb)\n\nNote that “Age” and “Number of Children” columns can be presented as unsigned ints. Let’s convert:\n\ndd_int = dd.select_dtypes(include=['int'])\nconverted_int = dd_int.apply(pd.to_numeric,downcast='unsigned')\nprint(\"Before: \", mem_usage(dd_int))\nprint(\"After: \", mem_usage(converted_int))\ncompare_ints = pd.concat([dd_int.dtypes,converted_int.dtypes],axis=1)\ncompare_ints.columns = ['before','after']\ncompare_ints.apply(pd.Series.value_counts)\n\nBefore:  6.31 MB\nAfter:  0.79 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nuint8\nNaN\n2.0\n\n\nint64\n2.0\nNaN\n\n\n\n\n\n\n\nNice. Now let’s process float columns (Income):\n\ndd_float = dd.select_dtypes(include=['float'])\nconverted_float = dd_float.apply(pd.to_numeric,downcast='float')\nprint(\"Before: \", mem_usage(dd_float))\nprint(\"After: \", mem_usage(converted_float))\ncompare_floats = pd.concat([dd_float.dtypes,converted_float.dtypes],axis=1)\ncompare_floats.columns = ['before','after']\ncompare_floats.apply(pd.Series.value_counts)\n\nBefore:  3.16 MB\nAfter:  3.16 MB\n\n\n\n\n\n\n\n\n\nbefore\nafter\n\n\n\n\nfloat64\n1\n1\n\n\n\n\n\n\n\nNothing spectacular.\nNow let’s create a new optimized DataFrame:\n\noptimized_dd = dd.copy()\noptimized_dd[converted_int.columns] = converted_int\noptimized_dd[converted_float.columns] = converted_float\nprint(mem_usage(dd))\nprint(mem_usage(optimized_dd))\n\n294.92 MB\n289.40 MB\n\n\nJust a bit. Let’s proceed with object types.\nFirst, read https://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/.\nNow, back to the dataset.\nWe can use categoricals (https://pandas.pydata.org/pandas-docs/stable/categorical.html) to optimize object columns.\n\nLet’s look at the number of unique values for each object type:\n\ndd_obj = dd.select_dtypes(include=['object']).copy()\ndd_obj.describe()\n\n\n\n\n\n\n\n\nName\nMarital Status\nEducation Level\nSmoking Status\nPhysical Activity Level\nEmployment Status\nAlcohol Consumption\nDietary Habits\nSleep Patterns\nHistory of Mental Illness\nHistory of Substance Abuse\nFamily History of Depression\nChronic Medical Conditions\n\n\n\n\ncount\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n413768\n\n\nunique\n196851\n4\n5\n3\n3\n2\n3\n3\n3\n2\n2\n2\n2\n\n\ntop\nMichael Smith\nMarried\nBachelor's Degree\nNon-smoker\nSedentary\nEmployed\nModerate\nUnhealthy\nFair\nNo\nNo\nNo\nNo\n\n\nfreq\n198\n240444\n124329\n247416\n176850\n265659\n173440\n170817\n196789\n287943\n284880\n302515\n277561\n\n\n\n\n\n\n\nLet’s start with one column first: “Education Level”\n\nedu_level = dd_obj[\"Education Level\"]\nprint(edu_level.head())\nedu_level_cat = edu_level.astype('category')\nprint(edu_level_cat.head())\n\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: object\n0    Bachelor's Degree\n1          High School\n2      Master's Degree\n3      Master's Degree\n4          High School\nName: Education Level, dtype: category\nCategories (5, object): ['Associate Degree', 'Bachelor's Degree', 'High School', 'Master's Degree', 'PhD']\n\n\nWe can look at the category codes:\n\nedu_level_cat.head().cat.codes\n\n0    1\n1    2\n2    3\n3    3\n4    2\ndtype: int8\n\n\nCompare memory usage:\n\nprint(mem_usage(edu_level))\nprint(mem_usage(edu_level_cat))\n\n24.92 MB\n0.40 MB\n\n\nWe should only convert objects to categoricals if most values are repeated. We have to pick a threshold, say, 25% of unique values.\nNote: when reading a csv, we can also provide a dtype dictionary param with preferred types.\nConverting all object columns and creating a new optimized DataFrame is left as part of Exercise 1."
  },
  {
    "objectID": "bigdata_lab1.html#exercises",
    "href": "bigdata_lab1.html#exercises",
    "title": "Big Data Analytics: Lab 1",
    "section": "",
    "text": "Pick your own dataset from\n\n\nKaggle\nHuggingFace\nor https://archive.ics.uci.edu.\n\n\nUse Dora library installed in Applied lab1 for data cleaning.\nCreate a data type describing the data structures you’re working with in your Pandas DataFrame. Use mypy for type annotations.\n\n\n\n\n\n\n\nNote\n\n\n\nmypy does not improve performance. However, it makes the code easier to understand.\n\n\n\nPerform numerical and object types conversions aimed at minimizing storage space, as outlined above.\nMeasure performance impact and plot it via e.g. matplotlib."
  },
  {
    "objectID": "bigdata.html",
    "href": "bigdata.html",
    "title": "Big Data Analytics",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "bigdata.html#lectures",
    "href": "bigdata.html#lectures",
    "title": "Big Data Analytics",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "bigdata.html#labs",
    "href": "bigdata.html#labs",
    "title": "Big Data Analytics",
    "section": "Labs",
    "text": "Labs\nLab 1\nLab 2"
  },
  {
    "objectID": "bigdata.html#exam-questions",
    "href": "bigdata.html#exam-questions",
    "title": "Big Data Analytics",
    "section": "Exam questions",
    "text": "Exam questions"
  },
  {
    "objectID": "nb/lab6/05_futures.html#a-typical-workflow",
    "href": "nb/lab6/05_futures.html#a-typical-workflow",
    "title": "Futures - non-blocking distributed calculations",
    "section": "A Typical Workflow",
    "text": "A Typical Workflow\nThis is the same workflow that we saw in the delayed notebook. It is for-loopy and the data is not necessarily an array or a dataframe. The following example outlines a read-transform-write:\ndef process_file(filename):\n    data = read_a_file(filename)\n    data = do_a_transformation(data)\n    destination = f\"results/{filename}\"\n    write_out_data(data, destination)\n    return destination\n\nfutures = []\nfor filename in filenames:\n    future = client.submit(process_file, filename)\n    futures.append(future)\n    \nfutures"
  },
  {
    "objectID": "nb/lab6/05_futures.html#basics",
    "href": "nb/lab6/05_futures.html#basics",
    "title": "Futures - non-blocking distributed calculations",
    "section": "Basics",
    "text": "Basics\nJust like we did in the delayed notebook, let’s make some toy functions, inc and add, that sleep for a while to simulate work. We’ll then time running these functions normally.\n\nfrom time import sleep\nimport random\n\nrandom.seed()\n\n\ndef inc(x):\n    sleep(random.randrange(2,4))\n    return x + 1\n\n\ndef double(x):\n    sleep(random.randrange(2,4))\n    return 2 * x\n\n\ndef add(x, y):\n    sleep(random.randrange(2,4))\n    return x + y\n\nWe can run these locally\n\ninc(1)\n\nOr we can submit them to run remotely with Dask. This immediately returns a future that points to the ongoing computation, and eventually to the stored result.\n\nfuture = client.submit(inc, 1)  # returns immediately with pending future\nfuture\n\nIf you wait a second, and then check on the future again, you’ll see that it has finished.\n\nfuture\n\nYou can block on the computation and gather the result with the .result() method.\n\nfuture.result()\n\n\nOther ways to wait for a future\nfrom dask.distributed import wait, progress\nprogress(future)\nshows a progress bar in this notebook, rather than having to go to the dashboard. This progress bar is also asynchronous, and doesn’t block the execution of other code in the meanwhile.\nwait(future)\nblocks and forces the notebook to wait until the computation pointed to by future is done. However, note that if the result of inc() is sitting in the cluster, it would take no time to execute the computation now, because Dask notices that we are asking for the result of a computation it already knows about. More on this later.\n\n\nOther ways to gather results\nclient.gather(futures)\ngathers results from more than one future."
  },
  {
    "objectID": "nb/lab6/05_futures.html#client.compute",
    "href": "nb/lab6/05_futures.html#client.compute",
    "title": "Futures - non-blocking distributed calculations",
    "section": "client.compute",
    "text": "client.compute\nGenerally, any Dask operation that is executed using .compute() or dask.compute() can be submitted for asynchronous execution using client.compute() instead.\n\nimport dask\nimport random\n\n\n@dask.delayed\ndef inc(x):\n    sleep(random.randrange(2,4))\n    return x + 1\n\n\n@dask.delayed\ndef add(x, y):\n    sleep(random.randrange(2,4))\n    return x + y\n\n\nx = inc(1)\ny = inc(2)\nz = add(x, y)\n\nSo far we have a regular dask.delayed output. When we pass z to client.compute we get a future back and Dask starts evaluating the task graph.\n\n# notice the difference from z.compute()\n# notice that this cell completes immediately\nfuture = client.compute(z)\nfuture\n\n\nfuture.result()  # waits until result is ready\n\nWhen using futures, the computation moves to the data rather than the other way around, and the client, in the local Python session, need never see the intermediate values."
  },
  {
    "objectID": "nb/lab6/05_futures.html#client.submit",
    "href": "nb/lab6/05_futures.html#client.submit",
    "title": "Futures - non-blocking distributed calculations",
    "section": "client.submit",
    "text": "client.submit\nclient.submit takes a function and arguments, pushes these to the cluster, returning a Future representing the result to be computed. The function is passed to a worker process for evaluation. This looks a lot like doing client.compute(), above, except now we are passing the function and arguments directly to the cluster.\n\ndef inc(x):\n    sleep(random.randrange(2,4))\n    return x + 1\n\n\ndef double(x):\n    sleep(random.randrange(2,4))\n    return 2 * x\n\n\ndef add(x, y):\n    sleep(random.randrange(2,4))\n    return x + y\n\n\nfuture_x = client.submit(inc, 1)\nfuture_y = client.submit(inc, 2)\nfuture_z = client.submit(sum, [future_x, future_y])\nfuture_z\n\n\nfuture_z.result()  # waits until result is ready\n\nThe arguments toclient.submit can be regular Python functions and objects, Futures from other submit operations or dask.delayed objects. Thus we can create dependencies in a computation graph.\n\nHow does it work?\nEach future represents a result held, or being evaluated by the cluster. Thus we can control caching of intermediate values - when a future is no longer referenced, its value is forgotten. In the solution, above, futures are held for each of the function calls. These results would not need to be re-evaluated if we chose to submit more work that needed them.\nWe can explicitly pass data from our local session into the cluster using client.scatter(), but usually it is better to construct functions that do the loading of data within the workers themselves, so that there is no need to serialize and communicate the data. Most of the loading functions within Dask, such as dd.read_csv, work this way. Similarly, we normally don’t want to gather() results that are too big in memory."
  },
  {
    "objectID": "nb/lab6/05_futures.html#example-sporadically-failing-task",
    "href": "nb/lab6/05_futures.html#example-sporadically-failing-task",
    "title": "Futures - non-blocking distributed calculations",
    "section": "Example: Sporadically failing task",
    "text": "Example: Sporadically failing task\nLet’s imagine a task that sometimes fails. You might encounter this when dealing with input data where sometimes a file is malformed, or maybe a request times out.\n\nfrom random import randrange\n\ndef fetchRand():\n    random.seed()\n    randInt = randrange(0,2)\n    return randInt\n\ndef flaky_inc(i):\n    randInt = fetchRand()\n    print(\"randInt: %d\", randInt)\n    if randInt &lt; 1:\n        raise ValueError(\"You hit the error!\")\n    return i + 1\n\n\nrandrange(0,2)\n\nIf you run this function over and over again, it will sometimes fail.\n&gt;&gt;&gt; flaky_inc(2)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nInput In [65], in &lt;cell line: 1&gt;()\n----&gt; 1 flaky_inc(2)\n\nInput In [61], in flaky_inc(i)\n      3 def flaky_inc(i):\n      4     if random() &lt; 0.5:\n----&gt; 5         raise ValueError(\"You hit the error!\")\n      6     return i + 1\n\nValueError: You hit the error!\nWe can run this function on a range of inputs using client.map.\n\nfutures = client.map(flaky_inc, range(10))\n\nNotice how the cell returned even though some of the computations failed. We can inspect these futures one by one and find the ones that failed:\n\nfor i, future in enumerate(futures):\n    print(i, future.status)\n\nYou can rerun those specific futures to try to get the task to successfully complete:\n\nfutures[5].retry()\n\n\nfor i, future in enumerate(futures):\n    print(i, future.status)\n\nA more concise way of retrying in the case of sporadic failures is by setting the number of retries in the client.compute, client.submit or client.map method.\nNote: In this example we also need to set pure=False to let Dask know that the arguments to the function do not totally determine the output.\n\nfutures = client.map(flaky_inc, range(10), retries=5, pure=False)\nfuture_z = client.submit(sum, futures)\nfuture_z.result()\n\nYou will see a lot of warnings, but the computation should eventually succeed."
  },
  {
    "objectID": "nb/lab6/05_futures.html#example-many-tasks",
    "href": "nb/lab6/05_futures.html#example-many-tasks",
    "title": "Futures - non-blocking distributed calculations",
    "section": "Example: many tasks",
    "text": "Example: many tasks\n\nzs = []\n\nfor i in range(16):\n    x = client.submit(inc, i)     # x = inc(i)\n    y = client.submit(double, x)  # y = inc(x)\n    z = client.submit(add, x, y)  # z = inc(y)\n    zs.append(z)\n\ntotal = client.submit(sum, zs)\n\n\ntotal\n\n\ntotal.result()"
  },
  {
    "objectID": "nb/lab6/05_futures.html#example-tree-summation",
    "href": "nb/lab6/05_futures.html#example-tree-summation",
    "title": "Futures - non-blocking distributed calculations",
    "section": "Example: tree summation",
    "text": "Example: tree summation\nAs an example of a non-trivial algorithm, consider the classic tree reduction. We accomplish this with a nested for loop and a bit of normal Python logic.\nfinish           total             single output\n    ^          /        \\\n    |        c1          c2        neighbors merge\n    |       /  \\        /  \\\n    |     b1    b2    b3    b4     neighbors merge\n    ^    / \\   / \\   / \\   / \\\nstart   a1 a2 a3 a4 a5 a6 a7 a8    many inputs\n\nL = zs\nwhile len(L) &gt; 1:\n    new_L = []\n    for i in range(0, len(L), 2):\n        future = client.submit(add, L[i], L[i + 1])  # add neighbors\n        new_L.append(future)\n    L = new_L                                   # swap old list for new\n\nWe can explicitly wait until this work is done and gather the results to our local process by calling client.gather:\n\n# gather all futures\nallFutures = client.gather(L)\n\n\nallFutures[0]\n\nIf you’re watching the dashboard’s status page then you may want to note two things:\n\nThe red bars are for inter-worker communication. They happen as different workers need to combine their intermediate values\nThere is lots of parallelism at the beginning but less towards the end as we reach the top of the tree where there is less work to do.\n\nAlternatively you may want to navigate to the dashboard’s graph page and then run the cell above again. You will be able to see the task graph evolve during the computation."
  },
  {
    "objectID": "nb/lab6/05_futures.html#example-building-a-computation-dynamically",
    "href": "nb/lab6/05_futures.html#example-building-a-computation-dynamically",
    "title": "Futures - non-blocking distributed calculations",
    "section": "Example: Building a computation dynamically",
    "text": "Example: Building a computation dynamically\nIn the examples above we explicitly specify the task graph ahead of time. We know for example that the first two futures in the list L will be added together.\nSometimes this isn’t always best though, sometimes you want to dynamically define a computation as it is happening. For example we might want to sum up these values based on whichever futures show up first, rather than the order in which they were placed in the list to start with.\nFor this, we can use operations like as_completed. This returns an iterator that yields the input future objects in the order in which they complete.\nWe recommend watching the dashboard’s graph page when running this computation. You should see the graph construct itself during execution.\n\nfrom dask.distributed import as_completed\n\nzs = client.map(inc, zs)\nseq = as_completed(zs)\n\nwhile seq.count() &gt; 1:  # at least two futures left\n    a = next(seq)\n    b = next(seq)\n    new = client.submit(add, a, b, priority=1)  # add them together\n    seq.add(new)                                # add new future back into loop\n\n\nfor future in seq:\n    print(future.result())"
  },
  {
    "objectID": "nb/lab6/05_futures.html#why-use-futures",
    "href": "nb/lab6/05_futures.html#why-use-futures",
    "title": "Futures - non-blocking distributed calculations",
    "section": "Why use Futures?",
    "text": "Why use Futures?\nThe futures API offers a work submission style that can easily emulate the map/reduce paradigm. If that is familiar to you then futures might be the simplest entrypoint into Dask.\nThe other big benefit of futures is that the intermediate results, represented by futures, can be passed to new tasks without having to pull data locally from the cluster. New operations can be setup to work on the output of previous jobs that haven’t even begun yet.\nDocs: https://docs.dask.org/en/latest/futures.html"
  },
  {
    "objectID": "nb/lab6/05_futures.html#exercises",
    "href": "nb/lab6/05_futures.html#exercises",
    "title": "Futures - non-blocking distributed calculations",
    "section": "Exercises",
    "text": "Exercises\nUsing the depression dataset from lab1, calculate the below using dask.futures:\n\nMean income grouped by a) smoking status; b) education level; c) married status\nPerform tree reduction as given above, but using Income column of the depression dataset, filtered by age in [20,30] interval."
  },
  {
    "objectID": "nb/lab5/02_array.html#create-datasets",
    "href": "nb/lab5/02_array.html#create-datasets",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Create datasets",
    "text": "Create datasets\nCreate the datasets you will be using in this notebook:\n\n%run prep.py -d random"
  },
  {
    "objectID": "nb/lab5/02_array.html#start-the-client",
    "href": "nb/lab5/02_array.html#start-the-client",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Start the Client",
    "text": "Start the Client\n\nfrom dask.distributed import Client\n\nclient = Client(n_workers=4)\nclient"
  },
  {
    "objectID": "nb/lab5/02_array.html#blocked-algorithms-in-a-nutshell",
    "href": "nb/lab5/02_array.html#blocked-algorithms-in-a-nutshell",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Blocked Algorithms in a nutshell",
    "text": "Blocked Algorithms in a nutshell\nLet’s do side by side the sum of the elements of an array using a NumPy array and a Dask array.\n\nimport numpy as np\nimport dask.array as da\n\n\n# NumPy array\na_np = np.ones(10)\na_np\n\nWe know that we can use sum() to compute the sum of the elements of our array, but to show what a blocksized operation would look like, let’s do:\n\na_np_sum = a_np[:5].sum() + a_np[5:].sum()\na_np_sum\n\nNow notice that each sum in the computation above is completely independent so they could be done in parallel. To do this with Dask array, we need to define our “slices”, we do this by defining the amount of elements we want per block using the variable chunks.\n\na_da = da.ones(10, chunks=5)\na_da\n\nImportant!\nNote here that to get two blocks, we specify chunks=5, in other words, we have 5 elements per block.\n\na_da_sum = a_da.sum()\na_da_sum"
  },
  {
    "objectID": "nb/lab5/02_array.html#task-graphs",
    "href": "nb/lab5/02_array.html#task-graphs",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Task Graphs",
    "text": "Task Graphs\nIn general, the code that humans write rely on compilers or interpreters so the computers can understand what we wrote. When we move to parallel execution there is a desire to shift responsibility from the compilers to the human, as they often bring the analysis, optimization, and execution of code into the code itself. In these cases, we often represent the structure of our program explicitly as data within the program itself.\nIn Dask we use task scheduling, where we break our program into into many medium-sized tasks or units of computation.We represent these tasks as nodes in a graph with edges between nodes if one task depends on data produced by another. We call upon a task scheduler to execute this graph in a way that respects these data dependencies and leverages parallelism where possible, so multiple independent tasks can be run simultaneously.\n\n# visualize the low level Dask graph using cytoscape\na_da_sum.visualize(engine=\"cytoscape\")\n\n\na_da_sum.compute()"
  },
  {
    "objectID": "nb/lab5/02_array.html#performance-comparison",
    "href": "nb/lab5/02_array.html#performance-comparison",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Performance comparison",
    "text": "Performance comparison\nLet’s try a more interesting example. We will create a 20_000 x 20_000 array with normally distributed values, and take the mean along one of its axis.\nNote:\nIf you are running on Binder, the Numpy example might need to be a smaller one due to memory issues.\n\nNumpy version\n\n%%time\nxn = np.random.normal(10, 0.1, size=(30_000, 30_000))\nyn = xn.mean(axis=0)\nyn\n\n\n\nDask array version\n\nxd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\nxd\n\n\nxd.nbytes / 1e9  # Gigabytes of the input processed lazily\n\n\nyd = xd.mean(axis=0)\nyd\n\n\n%%time\nxd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\nyd = xd.mean(axis=0)\nyd.compute()\n\nQuestions to think about:\n\nWhat happens if the Dask chunks=(10000,10000)?\nWhat happens if the Dask chunks=(30,30)?\n\nExercise:\nFor Dask arrays, compute the mean along axis=1 of the sum of the x array and its transpose.\n\n# Your code here"
  },
  {
    "objectID": "nb/lab5/02_array.html#choosing-good-chunk-sizes",
    "href": "nb/lab5/02_array.html#choosing-good-chunk-sizes",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Choosing good chunk sizes",
    "text": "Choosing good chunk sizes\nThis section was inspired on a Dask blog by Genevieve Buckley you can read it here\nA common problem when getting started with Dask array is determine what is a good chunk size. But what is a good size, and how do we determine this?\n\nGet to know the chunks\nWe can think of Dask arrays as a big structure composed by chunks of a smaller size, where these chunks are typically an a single numpy array, and they are all arranged to form a larger Dask array.\nIf you have a Dask array and want to know more information about chunks and their size, you can use the chunksize and chunks attributes to access this information. If you are in a jupyter notebook you can also visualize the Dask array via its HTML representation.\n\ndarr = da.random.random((1000, 1000, 1000))\ndarr\n\nNotice that when we created the Dask array, we did not specify the chunks. Dask has set by default chunks='auto' which accommodates ideal chunk sizes. To learn more on how auto-chunking works you can go to this documentation https://docs.dask.org/en/stable/array-chunks.html#automatic-chunking\ndarr.chunksize shows the largest chunk size. If you expect your array to have uniform chunk sizes this is a a good summary of the chunk size information. But if your array have irregular chunks, darr.chunks will show you the explicit sizes of all the chunks along all the dimensions of your dask array.\n\ndarr.chunksize\n\n\ndarr.chunks\n\nLet’s modify our example to see explore chunking a bit more. We can rechunk our array:\n\ndarr = darr.rechunk({0: -1, 1: 100, 2: \"auto\"})\n\n\ndarr\n\n\ndarr.chunksize\n\n\ndarr.chunks\n\nExercise:\n\nWhat does -1 do when specified as the chunk on a certain axis?\n\n\n\nToo small is a problem\nIf your chunks are too small, the amount of actual work done by every task is very tiny, and the overhead of coordinating all these tasks results in a very inefficient process.\nIn general, the dask scheduler takes approximately one millisecond to coordinate a single task. That means we want the computation time to be comparatively large, i.e in the order of seconds.\nIntuitive analogy by Genevieve Buckley:\n\nLets imagine we are building a house. It is a pretty big job, and if there were only one worker it would take much too long to build. So we have a team of workers and a site foreman. The site foreman is equivalent to the Dask scheduler: their job is to tell the workers what tasks they need to do.\nSay we have a big pile of bricks to build a wall, sitting in the corner of the building site. If the foreman (the Dask scheduler) tells workers to go and fetch a single brick at a time, then bring each one to where the wall is being built, you can see how this is going to be very slow and inefficient! The workers are spending most of their time moving between the wall and the pile of bricks. Much less time is going towards doing the actual work of mortaring bricks onto the wall.\nInstead, we can do this in a smarter way. The foreman (Dask scheduler) can tell the workers to go and bring one full wheelbarrow load of bricks back each time. Now workers are spending much less time moving between the wall and the pile of bricks, and the wall will be finished much quicker.\n\n\n\nToo big is a problem\nIf your chunks are too big, this is also a problem because you will likely run out of memory. You will start seeing in the dashboard that data is being spill to disk and this will lead to performance decrements.\nIf we load to much data into memory, Dask workers will start to spill data to disk to avoid crashing. Spilling data to disk will slow things down significantly, because of all the extra read and write operations to disk. This is definitely a situation that we want to avoid, to watch out for this you can look at the worker memory plot on the dashboard. Orange bars are a warning you are close to the limit, and gray means data is being spilled to disk.\nTo watch out for this, look at the worker memory plot on the Dask dashboard. Orange bars are a warning you are close to the limit, and gray means data is being spilled to disk - not good! For more tips, see the section on using the Dask dashboard below. To learn more about the memory plot, check the dashboard documentation.\n\n\nRules of thumb\n\nUsers have reported that chunk sizes smaller than 1MB tend to be bad. In general, a chunk size between 100MB and 1GB is good, while going over 1 or 2GB means you have a really big dataset and/or a lot of memory available per worker.\nUpper bound: Avoid very large task graphs. More than 10,000 or 100,000 chunks may start to perform poorly.\nLower bound: To get the advantage of parallelization, you need the number of chunks to at least equal the number of worker cores available (or better, the number of worker cores times 2). Otherwise, some workers will stay idle.\nThe time taken to compute each task should be much larger than the time needed to schedule the task. The Dask scheduler takes roughly 1 millisecond to coordinate a single task, so a good task computation time would be in the order of seconds (not milliseconds).\nChunks should be aligned with array storage on disk. Modern NDArray storage formats (HDF5, NetCDF, TIFF, Zarr) allow arrays to be stored in chunks so that the blocks of data can be pulled efficiently. However, data stores often chunk more finely than is ideal for Dask array, so it is common to choose a chunking that is a multiple of your storage chunk size, otherwise you might incur high overhead. For example, if you are loading data that is chunked in blocks of (100, 100), the you might might choose a chunking strategy more like (1000, 2000) that is larger but still divisible by (100, 100).\n\nFor more more advice on chunking see https://docs.dask.org/en/stable/array-chunks.html"
  },
  {
    "objectID": "nb/lab5/02_array.html#example-of-chunked-data-with-zarr",
    "href": "nb/lab5/02_array.html#example-of-chunked-data-with-zarr",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Example of chunked data with Zarr",
    "text": "Example of chunked data with Zarr\nZarr is a format for the storage of chunked, compressed, N-dimensional arrays. Zarr provides classes and functions for working with N-dimensional arrays that behave like NumPy arrays (Dask array behave like Numpy arrays) but whose data is divided into chunks and each chunk is compressed. If you are already familiar with HDF5 then Zarr arrays provide similar functionality, but with some additional flexibility.\nFor extra material check the Zarr tutorial\nLet’s read an array from zarr:\n\nimport zarr\n\n\na = da.from_zarr(\"data/random.zarr\")\n\n\na\n\nNotice that the array is already chunked, and we didn’t specify anything when loading it. Now notice that the chunks have a nice chunk size, let’s compute the mean and see how long it takes to run\n\n%%time\na.mean().compute()\n\nLet’s load a separate example where the chunksize is much smaller, and see what happen\n\nb = da.from_zarr(\"data/random_sc.zarr\")\nb\n\n\n%%time\nb.mean().compute()\n\n\nExercise:\nProvide a chunksize when reading b that will improve the time of computation of the mean. Try multiple chunks values and see what happens.\n\n# Your code here\n\n\n%%time\nc.mean().compute()"
  },
  {
    "objectID": "nb/lab5/02_array.html#xarray",
    "href": "nb/lab5/02_array.html#xarray",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Xarray",
    "text": "Xarray\nIn some applications we have multidimensional data, and sometimes working with all this dimensions can be confusing. Xarray is an open source project and Python package that makes working with labeled multi-dimensional arrays easier.\nXarray is inspired by and borrows heavily from pandas, the popular data analysis package focused on labeled tabular data. It is particularly tailored to working with netCDF files, which were the source of xarray’s data model, and integrates tightly with Dask for parallel computing.\nXarray introduces labels in the form of dimensions, coordinates and attributes on top of raw NumPy-like arrays, which allows for a more intuitive, more concise, and less error-prone developer experience.\nLet’s learn how to use xarray and Dask together:\n\nimport xarray as xr\n\n\nds = xr.tutorial.open_dataset(\n    \"air_temperature\",\n    chunks={  # this tells xarray to open the dataset as a dask array\n        \"lat\": 25,\n        \"lon\": 25,\n        \"time\": -1,\n    },\n)\nds\n\n\nds.air\n\n\nds.air.chunks\n\n\nmean = ds.air.mean(\"time\")  # no activity on dashboard\nmean  # contains a dask array\n\n\n# we will see dashboard activity\nmean.load()\n\n\nStandard Xarray Operations\nLet’s grab the air variable and do some operations. Operations using xarray objects are identical, regardless if the underlying data is stored as a Dask array or a NumPy array.\n\ndair = ds.air\n\n\ndair2 = dair.groupby(\"time.month\").mean(\"time\")\ndair_new = dair - dair2\ndair_new\n\nCall .compute() or .load() when you want your result as a xarray.DataArray with data stored as NumPy arrays.\n\n# things happen in the dashboard\ndair_new.load()\n\n\n\nTime Series Operations with xarray\nBecause we have a datetime index time-series operations work efficiently, for example we can do a resample and then plot the result.\n\ndair_resample = dair.resample(time=\"1w\").mean(\"time\").std(\"time\")\n\nFor this one to work, we need to install matplotlib first (if not installed yet):\n\n!pip install matplotlib\n\n\ndair_resample.load().plot(figsize=(12, 8))\n\n\n\nLearn More\nBoth xarray and zarr have their own tutorials that go into greater depth:\n\nZarr tutorial\nXarray tutorial"
  },
  {
    "objectID": "nb/lab5/02_array.html#close-your-cluster",
    "href": "nb/lab5/02_array.html#close-your-cluster",
    "title": "Dask Arrays - parallelized numpy",
    "section": "Close your cluster",
    "text": "Close your cluster\nIt’s good practice to close any Dask cluster you create:\n\nclient.shutdown()"
  },
  {
    "objectID": "nb/lab3/lab3.html",
    "href": "nb/lab3/lab3.html",
    "title": "map-reduce cont’d",
    "section": "",
    "text": "Suppose we want to apply several transformations. What can we do?\n\nSeveral maps.\nChain functions with compose.\nCreate a function pipeline with pipe.\n\nSuppose we want to apply \\(f(x) = \\exp(\\sin(x))\\) to a list of numbers.\nOption 1:\n\nimport math\n\nfloat_list = [0.234, -1.96, 5.071]\n\n# several maps\nlst1 = map(math.sin, float_list)\nlst2 = map(math.exp, lst1)\n\nprint(list(lst2))\n\nOption 2: we’ll use the toolz package we installed in lab2.\n\nfrom toolz.functoolz import compose\n\ncomposed = compose(math.exp, math.sin) # note reverse order\n\nprint(list(map(composed, float_list)))\n\n** Option 3:**\nAnd there is also pipe:\n\nfrom toolz.functoolz import pipe\n\npiped = lambda x: pipe(x, math.sin, math.exp) # now the order is proper\n\nprint(list(map(piped, float_list)))"
  },
  {
    "objectID": "nb/lab3/lab3.html#function-pipelines",
    "href": "nb/lab3/lab3.html#function-pipelines",
    "title": "map-reduce cont’d",
    "section": "",
    "text": "Suppose we want to apply several transformations. What can we do?\n\nSeveral maps.\nChain functions with compose.\nCreate a function pipeline with pipe.\n\nSuppose we want to apply \\(f(x) = \\exp(\\sin(x))\\) to a list of numbers.\nOption 1:\n\nimport math\n\nfloat_list = [0.234, -1.96, 5.071]\n\n# several maps\nlst1 = map(math.sin, float_list)\nlst2 = map(math.exp, lst1)\n\nprint(list(lst2))\n\nOption 2: we’ll use the toolz package we installed in lab2.\n\nfrom toolz.functoolz import compose\n\ncomposed = compose(math.exp, math.sin) # note reverse order\n\nprint(list(map(composed, float_list)))\n\n** Option 3:**\nAnd there is also pipe:\n\nfrom toolz.functoolz import pipe\n\npiped = lambda x: pipe(x, math.sin, math.exp) # now the order is proper\n\nprint(list(map(piped, float_list)))"
  },
  {
    "objectID": "nb/lab3/lab3.html#laziness",
    "href": "nb/lab3/lab3.html#laziness",
    "title": "map-reduce cont’d",
    "section": "Laziness",
    "text": "Laziness\nmap is lazy! This is the opposite to being eager. That’s why we apply list first to materialize results of map application.\nLaziness provides memory efficiency."
  },
  {
    "objectID": "nb/lab3/lab3.html#filter",
    "href": "nb/lab3/lab3.html#filter",
    "title": "map-reduce cont’d",
    "section": "Filter",
    "text": "Filter\nIn addition to map, there are also filter and zip.\nVariations:\n\nfrom itertools import filterfalse\nfrom toolz.dicttoolz import keyfilter, valfilter, itemfilter\n\ndef is_even(x):\n    if x % 2 == 0: return True\n    else: return False\n        \ndef both_are_even(x):\n    k,v = x\n    if is_even(k) and is_even(v): return True\n    else: return False\n        \nprint(list(filterfalse(is_even, range(10))))\n# [1, 3, 5, 7, 9]\nprint(list(keyfilter(is_even, {1:2, 2:3, 3:4, 4:5, 5:6})))\n# [2, 4]\nprint(list(valfilter(is_even, {1:2, 2:3, 3:4, 4:5, 5:6})))\n# [1, 3, 5]\nprint(list(itemfilter(both_are_even, {1:5, 2:4, 3:3, 4:2, 5:1}))) # [2, 4]\n\n\nzip\n\nprices = [25, 20, 40]\nitems = [\"peas\", \"chicken\", \"beef\"]\n\nzipped = list(zip(prices, items))\n\nimport pandas as pd\n\ndf = pd.DataFrame(list(zipped), columns=['price', 'name'])\n\n\ndf"
  },
  {
    "objectID": "nb/lab3/lab3.html#iterators",
    "href": "nb/lab3/lab3.html#iterators",
    "title": "map-reduce cont’d",
    "section": "Iterators",
    "text": "Iterators\nPython documentation defines iterator as:\nAn iterator is an object representing a stream of data; this object returns the data one element at a time.\n\nAn object is called iterable if you can get an iterator for it.\nIterator classes implement __iter__() and __next__() methods. This means one can create their own iterator, like this:\n\nclass SequenceIterator:\n    def __init__(self, sequence):\n        self._sequence = sequence\n        self._index = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self._index &lt; len(self._sequence):\n            item = self._sequence[self._index]\n            self._index += 1\n            return item\n        else:\n            raise StopIteration\n\n\nfor el in SequenceIterator([1,2,3]):\n    print(el)"
  },
  {
    "objectID": "nb/lab3/lab3.html#generators",
    "href": "nb/lab3/lab3.html#generators",
    "title": "map-reduce cont’d",
    "section": "Generators",
    "text": "Generators\nGenerators are simply an easy way to create iterators.\nA generator example, using yield keyword:\n\ndef even_numbers(n):\n    i=1\n    while i &lt;= n:\n        yield i*2\n        i += 1\n\n\nfor i in even_numbers(20):\n    print(i)\n\nA generator example, using an ad-hoc syntax similar to list comprehensions:\n\nfirst_100_even = (i*2 for i in range(1,20))\nprint(list(first_100_even))\n\nAnd now, what is all the fuss about? We can actually use map, reduce, and friends, on iterables, not just on lists:\n\n# map example\ngen_mapped = map(lambda x: x*x, even_numbers(30))\nprint(list(gen_mapped))\n\n\n# reduce example\nfrom functools import reduce\ngen_reduced = reduce(lambda acc, el: acc+el, even_numbers(5), 0)\nprint(gen_reduced)"
  },
  {
    "objectID": "nb/lab3/lab3.html#exercises",
    "href": "nb/lab3/lab3.html#exercises",
    "title": "map-reduce cont’d",
    "section": "Exercises",
    "text": "Exercises\n\nImplement versions of map, filter, and compose using reduce.\nUsing a csv file you’ve downloaded in lab1, create a generator that will yield first 3 columns of the file. And use this generator to create a Pandas.DataFrame. Hint: use https://docs.python.org/3/library/csv.html\n\nAdditional reading: https://docs.python.org/3/howto/functional.html"
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#when-to-use-dask.dataframe",
    "href": "nb/lab4/01_dataframe.html#when-to-use-dask.dataframe",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "When to use dask.dataframe",
    "text": "When to use dask.dataframe\npandas is great for tabular datasets that fit in memory. A general rule of thumb for pandas is:\n\n\n“Have 5 to 10 times as much RAM as the size of your dataset”\n\nWes McKinney (2017) in 10 things I hate about pandas\n\n\n\nHere “size of dataset” means dataset size on the disk.\nDask becomes useful when the datasets exceed the above rule.\nIn this notebook, you will be working with the New York City Airline data. This dataset is only ~200MB, so that you can download it in a reasonable time, but dask.dataframe will scale to datasets much larger than memory."
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#create-datasets",
    "href": "nb/lab4/01_dataframe.html#create-datasets",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Create datasets",
    "text": "Create datasets\nCreate the datasets you will be using in this notebook:\n\n# This step is not necessary - dataset is already downloaded\n#%run prep.py -d flights"
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#suppress-futurewarnings",
    "href": "nb/lab4/01_dataframe.html#suppress-futurewarnings",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Suppress FutureWarnings",
    "text": "Suppress FutureWarnings\n\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)"
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#set-up-your-local-cluster",
    "href": "nb/lab4/01_dataframe.html#set-up-your-local-cluster",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Set up your local cluster",
    "text": "Set up your local cluster\nCreate a local Dask cluster and connect it to the client. Don’t worry about this bit of code for now, you will learn more in the Distributed notebook.\n\nfrom dask.distributed import Client\n\nclient = Client(n_workers=4)\nclient\n\n\nDask Diagnostic Dashboard\nDask Distributed provides a useful Dashboard to visualize the state of your cluster and computations.\nIf you’re on JupyterLab or Binder, you can use the Dask JupyterLab extension (which should be already installed in your environment) to open the dashboard plots: * Click on the Dask logo in the left sidebar * Click on the magnifying glass icon, which will automatically connect to the active dashboard (if that doesn’t work, you can type/paste the dashboard link http://127.0.0.1:8787 in the field) * Click on “Task Stream”, “Progress Bar”, and “Worker Memory”, which will open these plots in new tabs * Re-organize the tabs to suit your workflow!\nAlternatively, click on the dashboard link displayed in the Client details above: http://127.0.0.1:8787/status. It will open a new browser tab with the Dashboard."
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#reading-and-working-with-datasets",
    "href": "nb/lab4/01_dataframe.html#reading-and-working-with-datasets",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Reading and working with datasets",
    "text": "Reading and working with datasets\nLet’s read an extract of flights in the USA across several years. This data is specific to flights out of the three airports in the New York City area.\n\nimport os\nimport dask\n\nBy convention, we import the module dask.dataframe as dd, and call the corresponding DataFrame object ddf.\nNote: The term “Dask DataFrame” is slightly overloaded. Depending on the context, it can refer to the module or the DataFrame object. To avoid confusion, throughout this notebook: - dask.dataframe (note the all lowercase) refers to the API, and - DataFrame (note the CamelCase) refers to the object.\nThe following filename includes a glob pattern *, so all files in the path matching that pattern will be read into the same DataFrame.\n\nimport dask.dataframe as dd\n\nddf = dd.read_csv(\n    os.path.join(\"data\", \"nycflights\", \"*.csv\"), parse_dates={\"Date\": [0, 1, 2]}\n)\nddf\n\nDask has not loaded the data yet, it has: - investigated the input path and found that there are ten matching files - intelligently created a set of jobs for each chunk – one per original CSV file in this case\nNotice that the representation of the DataFrame object contains no data - Dask has just done enough to read the start of the first file, and infer the column names and dtypes.\n\nLazy Evaluation\nMost Dask Collections, including Dask DataFrame are evaluated lazily, which means Dask constructs the logic (called task graph) of your computation immediately but “evaluates” them only when necessary. You can view this task graph using .visualize().\nYou will learn more about this in the Delayed notebook, but for now, note that we need to call .compute() to trigger actual computations.\n\nddf.visualize()\n\nSome functions like len and head also trigger a computation. Specifically, calling len will: - load actual data, (that is, load each file into a pandas DataFrame) - then apply the corresponding functions to each pandas DataFrame (also known as a partition) - combine the subtotals to give you the final grand total\n\n# load and count number of rows\nlen(ddf)\n\nYou can view the start and end of the data as you would in pandas:\n\nddf.head()\n\nddf.tail()\n\n# ValueError: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n# +----------------+---------+----------+\n# | Column         | Found   | Expected |\n# +----------------+---------+----------+\n# | CRSElapsedTime | float64 | int64    |\n# | TailNum        | object  | float64  |\n# +----------------+---------+----------+\n\n# The following columns also raised exceptions on conversion:\n\n# - TailNum\n#   ValueError(\"could not convert string to float: 'N54711'\")\n\n# Usually this is due to dask's dtype inference failing, and\n# *may* be fixed by specifying dtypes manually by adding:\n\n# dtype={'CRSElapsedTime': 'float64',\n#        'TailNum': 'object'}\n\n# to the call to `read_csv`/`read_table`.\nUnlike pandas.read_csv which reads in the entire file before inferring datatypes, dask.dataframe.read_csv only reads in a sample from the beginning of the file (or first file if using a glob). These inferred datatypes are then enforced when reading all partitions.\nIn this case, the datatypes inferred in the sample are incorrect. The first n rows have no value for CRSElapsedTime (which pandas infers as a float), and later on turn out to be strings (object dtype). Note that Dask gives an informative error message about the mismatch. When this happens you have a few options:\n\nSpecify dtypes directly using the dtype keyword. This is the recommended solution, as it’s the least error prone (better to be explicit than implicit) and also the most performant.\nIncrease the size of the sample keyword (in bytes)\nUse assume_missing to make dask assume that columns inferred to be int (which don’t allow missing values) are actually floats (which do allow missing values). In our particular case this doesn’t apply.\n\nIn our case we’ll use the first option and directly specify the dtypes of the offending columns.\n\nddf = dd.read_csv(\n    os.path.join(\"data\", \"nycflights\", \"*.csv\"),\n    #parse_dates={\"Date\": [0, 1, 2]},\n    dtype={\"TailNum\": str, \"CRSElapsedTime\": float, \"Cancelled\": bool},\n)\n\n\nddf.tail()  # now works\n\n\n\nReading from remote storage\nIf you’re thinking about distributed computing, your data is probably stored remotely on services (like Amazon’s S3 or Google’s cloud storage) and is in a friendlier format (like Parquet). Dask can read data in various formats directly from these remote locations lazily and in parallel.\nHere’s how you can read the NYC taxi cab data from Amazon S3:\nddf = dd.read_parquet(\n    \"s3://nyc-tlc/trip data/yellow_tripdata_2012-*.parquet\",\n)\nYou can also leverage Parquet-specific optimizations like column selection and metadata handling, learn more in the Dask documentation on working with Parquet files."
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#computations-with-dask.dataframe",
    "href": "nb/lab4/01_dataframe.html#computations-with-dask.dataframe",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Computations with dask.dataframe",
    "text": "Computations with dask.dataframe\nLet’s compute the maximum of the flight delay.\nWith just pandas, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums.\nimport pandas as pd\n\nfiles = os.listdir(os.path.join('data', 'nycflights'))\n\nmaxes = []\n\nfor file in files:\n    df = pd.read_csv(os.path.join('data', 'nycflights', file))\n    maxes.append(df.DepDelay.max())\n    \nfinal_max = max(maxes)\ndask.dataframe lets us write pandas-like code, that operates on larger-than-memory datasets in parallel.\n\n%%time\nresult = ddf.DepDelay.max()\nresult.compute()\n\nThis creates the lazy computation for us and then runs it.\nNote: Dask will delete intermediate results (like the full pandas DataFrame for each file) as soon as possible. This means you can handle datasets that are larger than memory but, repeated computations will have to load all of the data in each time. (Run the code above again, is it faster or slower than you would expect?)\nYou can view the underlying task graph using .visualize():\n\n# notice the parallelism\nresult.visualize()"
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#exercises",
    "href": "nb/lab4/01_dataframe.html#exercises",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Exercises",
    "text": "Exercises\nIn this section you will do a few dask.dataframe computations. If you are comfortable with pandas then these should be familiar. You will have to think about when to call .compute().\n\n1. How many rows are in our dataset?\nHint: how would you check how many items are in a list?\n\n# Your code here\n\n\n\n2. In total, how many non-canceled flights were taken?\nHint: use boolean indexing.\n\n# Your code here\n\n\n\n3. In total, how many non-canceled flights were taken from each airport?\nHint: use groupby.\n\n# Your code here\n\n\n\n4. What was the average departure delay from each airport?\n\n# Your code here\n\n\n\n5. What day of the week has the worst average departure delay?\n\n# Your code here\n\n\n\n6. Let’s say the distance column is erroneous and you need to add 1 to all values, how would you do this?\n\n# Your code here"
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#sharing-intermediate-results",
    "href": "nb/lab4/01_dataframe.html#sharing-intermediate-results",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Sharing Intermediate Results",
    "text": "Sharing Intermediate Results\nWhen computing all of the above, we sometimes did the same operation more than once. For most operations, dask.dataframe stores the arguments, allowing duplicate computations to be shared and only computed once.\nFor example, let’s compute the mean and standard deviation for departure delay of all non-canceled flights. Since Dask operations are lazy, those values aren’t the final results yet. They’re just the steps required to get the result.\nIf you compute them with two calls to compute, there is no sharing of intermediate computations.\n\nnon_canceled = ddf[~ddf.Cancelled]\nmean_delay = non_canceled.DepDelay.mean()\nstd_delay = non_canceled.DepDelay.std()\n\n\n%%time\n\nmean_delay_res = mean_delay.compute()\nstd_delay_res = std_delay.compute()\n\n\ndask.compute\nBut let’s try by passing both to a single compute call.\n\n%%time\n\nmean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)\n\nUsing dask.compute takes roughly 1/2 the time. This is because the task graphs for both results are merged when calling dask.compute, allowing shared operations to only be done once instead of twice. In particular, using dask.compute only does the following once:\n\nthe calls to read_csv\nthe filter (df[~df.Cancelled])\nsome of the necessary reductions (sum, count)\n\nTo see what the merged task graphs between multiple results look like (and what’s shared), you can use the dask.visualize function (you might want to use filename='graph.pdf' to save the graph to disk so that you can zoom in more easily):\n\ndask.visualize(mean_delay, std_delay)#, engine=\"cytoscape\") # optionally we can use a different visualization engine\n\n\n\n.persist()\nWhile using a distributed scheduler (you will learn more about schedulers in the upcoming notebooks), you can keep some data that you want to use often in the distributed memory.\npersist generates “Futures” (more on this later as well) and stores them in the same structure as your output. You can use persist with any data or computation that fits in memory.\nIf you want to analyze data only for non-canceled flights departing from JFK airport, you can either have two compute calls like in the previous section:\n\nnon_cancelled = ddf[~ddf.Cancelled]\nddf_jfk = non_cancelled[non_cancelled.Origin == \"JFK\"]\n\n\n%%time\nddf_jfk.DepDelay.mean().compute()\nddf_jfk.DepDelay.sum().compute()\n\nOr, consider persisting that subset of data in memory.\nSee the “Graph” dashboard plot, the red squares indicate persisted data stored as Futures in memory. You will also notice an increase in Worker Memory (another dashboard plot) consumption.\n\nddf_jfk = ddf_jfk.persist()  # returns back control immediately\n\n\n%%time\nddf_jfk.DepDelay.mean().compute()\nddf_jfk.DepDelay.std().compute()\n\nAnalyses on this persisted data is faster because we are not repeating the loading and selecting (non-canceled, JFK departure) operations."
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#custom-code-with-dask-dataframe",
    "href": "nb/lab4/01_dataframe.html#custom-code-with-dask-dataframe",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Custom code with Dask DataFrame",
    "text": "Custom code with Dask DataFrame\ndask.dataframe only covers a small but well-used portion of the pandas API.\nThis limitation is for two reasons:\n\nThe Pandas API is huge\nSome operations are genuinely hard to do in parallel, e.g, sorting.\n\nAdditionally, some important operations like set_index work, but are slower than in pandas because they include substantial shuffling of data, and may write out to disk.\nWhat if you want to use some custom functions that aren’t (or can’t be) implemented for Dask DataFrame yet?\nYou can open an issue on the Dask issue tracker to check how feasible the function could be to implement, and you can consider contributing this function to Dask.\nIn case it’s a custom function or tricky to implement, dask.dataframe provides a few methods to make applying custom functions to Dask DataFrames easier:\n\nmap_partitions: to run a function on each partition (each pandas DataFrame) of the Dask DataFrame\nmap_overlap: to run a function on each partition (each pandas DataFrame) of the Dask DataFrame, with some rows shared between neighboring partitions\nreduction: for custom row-wise reduction operations.\n\nLet’s take a quick look at the map_partitions() function:\n\nhelp(ddf.map_partitions)\n\nThe “Distance” column in ddf is currently in miles. Let’s say we want to convert the units to kilometers and we have a general helper function as shown below. In this case, we can use map_partitions to apply this function across each of the internal pandas DataFrames in parallel.\n\nimport pandas as pd\n\ndef my_custom_converter(df, multiplier=1):\n    return df * multiplier\n\n\nmeta = pd.Series(name=\"Distance\", dtype=\"float64\")\n\ndistance_km = ddf.Distance.map_partitions(\n    my_custom_converter, multiplier=0.6, meta=meta\n)\n\n\ndistance_km.visualize()\n\n\ndistance_km.head()\n\n\nWhat is meta?\nSince Dask operates lazily, it doesn’t always have enough information to infer the output structure (which includes datatypes) of certain operations.\nmeta is a suggestion to Dask about the output of your computation. Importantly, meta never infers with the output structure. Dask uses this meta until it can determine the actual output structure.\nEven though there are many ways to define meta, we suggest using a small pandas Series or DataFrame that matches the structure of your final output."
  },
  {
    "objectID": "nb/lab4/01_dataframe.html#close-you-local-dask-cluster",
    "href": "nb/lab4/01_dataframe.html#close-you-local-dask-cluster",
    "title": "Dask DataFrame - parallelized pandas",
    "section": "Close you local Dask Cluster",
    "text": "Close you local Dask Cluster\nIt’s good practice to always close any Dask cluster you create:\n\nclient.shutdown()"
  },
  {
    "objectID": "applied_lec1.html#what-is-big-data",
    "href": "applied_lec1.html#what-is-big-data",
    "title": "Big Data: Intro",
    "section": "What is Big Data?",
    "text": "What is Big Data?\n\n\n\nDefinition\n\n\nBig Data is a set of technologies designed to store, manage and analyze data that is:\n\ntoo large to fit on a single machine\nwhile accommodating for the issue of growing discrepancy between capacity, throughput and latency."
  },
  {
    "objectID": "applied_lec1.html#prefixes",
    "href": "applied_lec1.html#prefixes",
    "title": "Big Data: Intro",
    "section": "Prefixes",
    "text": "Prefixes\n\n\n\nPrefixes\n\n\n\nkilo (k) 1,000 (3 zeros)\nMega (M) 1,000,000 (6 zeros)\nGiga (G) 1,000,000,000 (9 zeros)\nTera (T) 1,000,000,000,000 (12 zeros)\nPeta (P) 1,000,000,000,000,000 (15 zeros)\nExa (E) 1,000,000,000,000,000,000 (18 zeros)\nZetta (Z) 1,000,000,000,000,000,000,000 (21 zeros)\nYotta (Y) 1,000,000,000,000,000,000,000,000 (24 zeros)\nRonna (R) 1,000,000,000,000,000,000,000,000,000 (27 zeros)\nQuetta (Q) 1,000,000,000,000,000,000,000,000,000,000 (30 zeros)"
  },
  {
    "objectID": "applied_lec1.html#total-estimate",
    "href": "applied_lec1.html#total-estimate",
    "title": "Big Data: Intro",
    "section": "Total estimate",
    "text": "Total estimate\n\n\n\nEstimate\n\n\nThe total amount of data stored digitally worldwide is estimated to be getting close to 100 ZB as of 2021 (zettabytes)"
  },
  {
    "objectID": "applied_lec1.html#three-vs",
    "href": "applied_lec1.html#three-vs",
    "title": "Big Data: Intro",
    "section": "Three Vs",
    "text": "Three Vs\n\n\n\nVolume\nVariety\nVelocity"
  },
  {
    "objectID": "applied_lec1.html#volume",
    "href": "applied_lec1.html#volume",
    "title": "Big Data: Intro",
    "section": "Volume",
    "text": "Volume\n\n\n\nIssue\n\n\nData volume has exponentially increased in recent decades.\n\n\n\n\n\n\nWait but why?\n\n\n\nInternet-of-Things sensor data\nSocial networks\nStorage device progress"
  },
  {
    "objectID": "applied_lec1.html#variety",
    "href": "applied_lec1.html#variety",
    "title": "Big Data: Intro",
    "section": "Variety",
    "text": "Variety\n\n\n\nTypes\n\n\n\ntrees - XML, JSON, Parquet, Avro, etc\nunstructured - text, pictures, audio, video\ndata cubes\ngraphs"
  },
  {
    "objectID": "applied_lec1.html#velocity",
    "href": "applied_lec1.html#velocity",
    "title": "Big Data: Intro",
    "section": "Velocity",
    "text": "Velocity\n\n\n\nDefinion\n\n\nSpeed at which data is being generated, collected, and processed.\n\n\n\n\n\n\nAttributes\n\n\n\nCapacity: how much data can we store per unit of volume?\nThroughput: how many bytes can we read per unit of time?\nLatency: how much time do we need to wait until the bytes start arriving?"
  },
  {
    "objectID": "applied_lec1.html#velocity-1",
    "href": "applied_lec1.html#velocity-1",
    "title": "Big Data: Intro",
    "section": "Velocity",
    "text": "Velocity\nEvolution since 1950s"
  },
  {
    "objectID": "applied_lec1.html#features",
    "href": "applied_lec1.html#features",
    "title": "Big Data: Intro",
    "section": "Features",
    "text": "Features\n\n\n\nFeatures\n\n\n\nReliability\nScalability\nMaintainability"
  },
  {
    "objectID": "applied_lec1.html#reliability",
    "href": "applied_lec1.html#reliability",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nKleppmann’s definition\n\n\nThe system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity\n\nhardware faults\nsoftware faults\nand even human error"
  },
  {
    "objectID": "applied_lec1.html#reliability-1",
    "href": "applied_lec1.html#reliability-1",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nFaults\n\n\nBasically, theses are things that could go wrong.\nSystems that can anticipate faults are called fault-tolerant or resilient.\nFault can be defined as one component of the system deviating from the spec.\n\n\n\n\n\n\nFailures\n\n\nFailures occur when system stops providing services to the user.\n\n\n\nFaults might degenerate into failures."
  },
  {
    "objectID": "applied_lec1.html#reliability-2",
    "href": "applied_lec1.html#reliability-2",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability\n\n\n\nTypes of errors\n\n\n\nHardware\nSoftware\nHuman"
  },
  {
    "objectID": "applied_lec1.html#reliability-3",
    "href": "applied_lec1.html#reliability-3",
    "title": "Big Data: Intro",
    "section": "Reliability",
    "text": "Reliability"
  },
  {
    "objectID": "applied_lec1.html#scalability",
    "href": "applied_lec1.html#scalability",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nKleppmann\n\n\nAs the system grows (in data volume, traffic volume, or complexity), there should be reasonable ways of dealing with that growth.\nIn other words, scalability is a system’s ability to cope with increased load.\n\n\n\n\n\nNote that scalability is a multi-dimensional term. When saying “system scales well”, it’s important to state exactly along which axis."
  },
  {
    "objectID": "applied_lec1.html#scalability-1",
    "href": "applied_lec1.html#scalability-1",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nWhat is load?\n\n\nLoad is described by load parameters. These might include:\n\ndata set size\ndata write speed\ndata read speed\ncomputational complexity\netc…"
  },
  {
    "objectID": "applied_lec1.html#scalability-2",
    "href": "applied_lec1.html#scalability-2",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nPerformance\n\n\nIncreasing load affects performance. There are several meanings to this term:\n\nthroughput – time required to process a dataset of certain size\nresponse time – time between sending a request and receiving a response\nlatency – duration of waiting for a request to be processed. Included in response time.\n\n\n\n\n\n\nPerformance might be more strictly defined by service level objectives (SLOs) and service level agreements (SLAs)."
  },
  {
    "objectID": "applied_lec1.html#scalability-3",
    "href": "applied_lec1.html#scalability-3",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nHow to deal with load\n\n\n\nvertical scaling - scaling up\nhorizontal scaling - scaling out\narchitectural changes\n\n\n\n\n\n\n\nElasticity\n\n\nAn approach to load handling whereby a system automatically adds resources in case of load increase, and can decrease resources if load decreases."
  },
  {
    "objectID": "applied_lec1.html#scalability-4",
    "href": "applied_lec1.html#scalability-4",
    "title": "Big Data: Intro",
    "section": "Scalability",
    "text": "Scalability\n\n\n\nCommon wisdom\n\n\n\nKeep your database on a single node (scale up) until scaling cost or high-availability requirements forces you to make it distributed.\nOptimize code so that it can run on a single node."
  },
  {
    "objectID": "applied_lec1.html#maintainability",
    "href": "applied_lec1.html#maintainability",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nKleppmann\n\n\nOver time, many different people will work on the system\n\nengineering\noperations\nboth maintaining current behavior and adapting the system to new use cases),\n\nand they should all be able to work on it productively."
  },
  {
    "objectID": "applied_lec1.html#maintainability-1",
    "href": "applied_lec1.html#maintainability-1",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nPrinciples\n\n\n\nOperability – make it easy for operations teams to keep the system running smoothly.\nSimplicity – make it easy for new engineers to understand the system, by removing as much complexity as possible from the system.\nEvolvability – Make it easy for engineers to make changes to the system in the future, adapting it for unanticipated use cases as requirements change. Also known as extensibility, modifiability, or plasticity."
  },
  {
    "objectID": "applied_lec1.html#maintainability-2",
    "href": "applied_lec1.html#maintainability-2",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nOperability\n\n\n\nHealth monitoring\nGood deployment practices\nConfiguration management\nVisibility into the internals of the system\nKnowledge preservation – documentation (!).\netc…"
  },
  {
    "objectID": "applied_lec1.html#maintainability-3",
    "href": "applied_lec1.html#maintainability-3",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nComplexity symptoms\n\n\n\nLots of hidden state\nLoose cohesion, tight coupling\nBad naming (!)\nUnnecessary hacks\netc…"
  },
  {
    "objectID": "applied_lec1.html#maintainability-complexity",
    "href": "applied_lec1.html#maintainability-complexity",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity"
  },
  {
    "objectID": "applied_lec1.html#maintainability-complexity-1",
    "href": "applied_lec1.html#maintainability-complexity-1",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity\n\n\n\nTypes\n\n\n\nincidental\naccidental"
  },
  {
    "objectID": "applied_lec1.html#maintainability-complexity-2",
    "href": "applied_lec1.html#maintainability-complexity-2",
    "title": "Big Data: Intro",
    "section": "Maintainability: Complexity",
    "text": "Maintainability: Complexity\n\n\n\nIncidental\n\n\n\nEasy things can be complex. There can be complex constructs that are succinctly described, familiar, available and easy to use. That is incidental complexity.\n\nRich Hickey talk “Simple made easy”: https://www.youtube.com/watch?v=SxdOUGdseq4"
  },
  {
    "objectID": "applied_lec1.html#maintainability-complexity-3",
    "href": "applied_lec1.html#maintainability-complexity-3",
    "title": "Big Data: Intro",
    "section": "Maintainability: complexity",
    "text": "Maintainability: complexity\nHowever: Complexity is often caused by\n\n\n\nAccidental complexity\n\n\nMoseley and Marks define complexity as accidental if it is not inherent in the problem that the software solves (as seen by the users) but arises only from the implementation.\n\n\n\n\n\n\nHow to remove?\n\n\nBy providing proper abstractions."
  },
  {
    "objectID": "applied_lec1.html#maintainability-abstractions",
    "href": "applied_lec1.html#maintainability-abstractions",
    "title": "Big Data: Intro",
    "section": "Maintainability: abstractions",
    "text": "Maintainability: abstractions\n\n\n\n\n\n\nDefinition (Ousterhout)\n\n\nAn abstraction is a simplified view of an entity, which omits unimportant details.\nIn modular programming, each module provides an abstraction in the form of its interface."
  },
  {
    "objectID": "applied_lec1.html#maintainability-abstractions-1",
    "href": "applied_lec1.html#maintainability-abstractions-1",
    "title": "Big Data: Intro",
    "section": "Maintainability: abstractions",
    "text": "Maintainability: abstractions\n\n\n\n\n\n\nWhat can abstractions do?\n\n\n\nHide implementation details\nProvide reusable building blocks"
  },
  {
    "objectID": "applied_lec1.html#maintainability-4",
    "href": "applied_lec1.html#maintainability-4",
    "title": "Big Data: Intro",
    "section": "Maintainability",
    "text": "Maintainability\n\n\n\nEvolvability\n\n\nOne needs to adapt their big data system to possible future requirements changes.\nHowever, keep in mind the following:\n\nInability to foresee exact nature of changes\nNeed to strike the balance of flexibility and fitness for a particular task"
  },
  {
    "objectID": "applied_lec1.html#types-of-big-data-analytics",
    "href": "applied_lec1.html#types-of-big-data-analytics",
    "title": "Big Data: Intro",
    "section": "Types of big data analytics",
    "text": "Types of big data analytics\n\n\n\nTypes\n\n\n\nPrescriptive\nDiagnostic\nDescriptive\nPredictive"
  },
  {
    "objectID": "applied_lec1.html#types-prescriptive",
    "href": "applied_lec1.html#types-prescriptive",
    "title": "Big Data: Intro",
    "section": "Types: Prescriptive",
    "text": "Types: Prescriptive\n\n\n\nPrescriptive\n\n\n\nForward looking\nOptimal decisions for future situations"
  },
  {
    "objectID": "applied_lec1.html#types-diagnostic",
    "href": "applied_lec1.html#types-diagnostic",
    "title": "Big Data: Intro",
    "section": "Types: Diagnostic",
    "text": "Types: Diagnostic\n\n\n\nDiagnostic\n\n\n\nBackward looking\nFocused on causal relationships"
  },
  {
    "objectID": "applied_lec1.html#types-descriptive",
    "href": "applied_lec1.html#types-descriptive",
    "title": "Big Data: Intro",
    "section": "Types: Descriptive",
    "text": "Types: Descriptive\n\n\n\nDescriptive\n\n\n\nBackward looking\nFocused on descriptions and comparisons"
  },
  {
    "objectID": "applied_lec1.html#types-descriptive-1",
    "href": "applied_lec1.html#types-descriptive-1",
    "title": "Big Data: Intro",
    "section": "Types: Descriptive",
    "text": "Types: Descriptive\n\n\n\nPredictive\n\n\n\nForward looking\nFocused on the prediction of future states, relationship, and patterns"
  },
  {
    "objectID": "applied_lec1.html#challenges",
    "href": "applied_lec1.html#challenges",
    "title": "Big Data: Intro",
    "section": "Challenges",
    "text": "Challenges\nThere are 2 main challenges associated with Big Data.\n\n\n\nChallenges\n\n\n\nhow do we store and manage such a huge volume of data efficiently?\nhow do we process and extract valuable information from the data within the given time frame?"
  }
]